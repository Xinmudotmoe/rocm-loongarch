From 1f92cdd4884d9468621d94137c0db44e546ef6de Mon Sep 17 00:00:00 2001
From: yangxiaojuan <yangxiaojuan@loongson.cn>
Date: Tue, 26 Aug 2025 10:45:03 +0800
Subject: [PATCH] Add loongarch simd support for machines support 256-bit
 vector.

---
 CMakeLists.txt                                |   13 +-
 include/rppdefs.h                             |   19 +
 include/xxl-clang.h                           |  592 ++
 libs/third_party/ffts/libffts.a               |  Bin 106000 -> 202096 bytes
 src/include/cpu/rpp_loongarch_common.hpp      | 6736 +++++++++++++++++
 src/include/cpu/rpp_loongarch_filter.hpp      | 1242 +++
 src/include/cpu/rpp_loongarch_simd.hpp        | 4672 ++++++++++++
 src/modules/CMakeLists.txt                    |    6 +-
 .../cpu/host_arithmetic_operations.hpp        |    4 +
 src/modules/cpu/host_computer_vision.hpp      |    4 +
 src/modules/cpu/host_filter_operations.hpp    |    4 +
 src/modules/cpu/host_logical_operations.hpp   |    4 +
 .../cpu/host_morphological_transforms.hpp     |    4 +
 .../cpu/host_statistical_operations.hpp       |    4 +
 .../cpu/host_tensor_filter_augmentations.hpp  |    4 +
 .../host_tensor_geometric_augmentations.hpp   |    4 +
 src/modules/cpu/kernel/add_scalar.hpp         |    9 +
 src/modules/cpu/kernel/bitwise_and.hpp        |  211 +-
 src/modules/cpu/kernel/bitwise_or.hpp         |  210 +-
 src/modules/cpu/kernel/bitwise_xor.hpp        |   94 +-
 src/modules/cpu/kernel/blend.hpp              |  189 +
 .../cpu/kernel/box_filter_loongarch.hpp       | 2881 +++++++
 src/modules/cpu/kernel/brightness.hpp         |   85 +-
 src/modules/cpu/kernel/color_cast.hpp         |   37 +
 src/modules/cpu/kernel/color_jitter.hpp       |   21 +
 src/modules/cpu/kernel/color_temperature.hpp  |   21 +
 src/modules/cpu/kernel/color_to_greyscale.hpp |   85 +-
 src/modules/cpu/kernel/color_twist.hpp        |  109 +-
 src/modules/cpu/kernel/contrast.hpp           |   29 +
 src/modules/cpu/kernel/copy.hpp               |    5 +
 src/modules/cpu/kernel/crop.hpp               |    5 +
 src/modules/cpu/kernel/crop_and_patch.hpp     |   69 +-
 .../cpu/kernel/crop_mirror_normalize.hpp      |   67 +
 src/modules/cpu/kernel/down_mixing.hpp        |   29 +
 src/modules/cpu/kernel/erase.hpp              |    5 +
 src/modules/cpu/kernel/exposure.hpp           |   25 +
 src/modules/cpu/kernel/flip.hpp               |    5 +
 src/modules/cpu/kernel/flip_voxel.hpp         |   13 +-
 src/modules/cpu/kernel/fog.hpp                |  116 +-
 .../cpu/kernel/fused_multiply_add_scalar.hpp  |   19 +-
 src/modules/cpu/kernel/gamma_correction.hpp   |    5 +
 src/modules/cpu/kernel/gaussian_filter.hpp    |  134 +-
 src/modules/cpu/kernel/glitch.hpp             |  103 +-
 src/modules/cpu/kernel/gridmask.hpp           |  139 +
 src/modules/cpu/kernel/jitter.hpp             |  357 +-
 src/modules/cpu/kernel/lens_correction.hpp    |   86 +
 src/modules/cpu/kernel/log.hpp                |   28 +-
 src/modules/cpu/kernel/lut.hpp                |    4 +
 src/modules/cpu/kernel/magnitude.hpp          |  296 +-
 src/modules/cpu/kernel/mel_filter_bank.hpp    |   41 +
 src/modules/cpu/kernel/multiply_scalar.hpp    |   15 +-
 src/modules/cpu/kernel/noise_gaussian.hpp     |   65 +-
 .../cpu/kernel/noise_salt_and_pepper.hpp      |   71 +-
 src/modules/cpu/kernel/noise_shot.hpp         |   67 +-
 src/modules/cpu/kernel/non_linear_blend.hpp   |  165 +
 src/modules/cpu/kernel/normalize.hpp          |   42 +
 src/modules/cpu/kernel/phase.hpp              |  316 +-
 .../cpu/kernel/pre_emphasis_filter.hpp        |   17 +
 src/modules/cpu/kernel/rain.hpp               |   49 +-
 src/modules/cpu/kernel/remap_loongarch.hpp    | 1986 +++++
 src/modules/cpu/kernel/resample.hpp           |   22 +
 src/modules/cpu/kernel/resize.hpp             |  134 +
 src/modules/cpu/kernel/resize_crop_mirror.hpp |  129 +
 .../cpu/kernel/resize_mirror_normalize.hpp    |  222 +
 src/modules/cpu/kernel/ricap.hpp              |    5 +
 src/modules/cpu/kernel/slice.hpp              |    5 +
 src/modules/cpu/kernel/spatter.hpp            |  113 +-
 src/modules/cpu/kernel/spectrogram.hpp        |   26 +-
 src/modules/cpu/kernel/subtract_scalar.hpp    |    7 +-
 src/modules/cpu/kernel/swap_channels.hpp      |    5 +
 src/modules/cpu/kernel/tensor_max.hpp         |  135 +-
 src/modules/cpu/kernel/tensor_mean.hpp        |  165 +-
 src/modules/cpu/kernel/tensor_min.hpp         |  129 +-
 src/modules/cpu/kernel/tensor_stddev.hpp      |  277 +-
 src/modules/cpu/kernel/tensor_sum.hpp         |  117 +-
 src/modules/cpu/kernel/threshold.hpp          |   81 +-
 src/modules/cpu/kernel/transpose.hpp          |   23 +-
 src/modules/cpu/kernel/vignette.hpp           |  101 +-
 src/modules/cpu/kernel/warp_affine.hpp        |  290 +
 src/modules/cpu/kernel/warp_perspective.hpp   |  299 +-
 src/modules/cpu/kernel/water.hpp              |  153 +-
 .../cpu/loongarch_advanced_augmentations.hpp  | 2657 +++++++
 .../cpu/loongarch_color_model_conversions.hpp | 3585 +++++++++
 src/modules/cpu/loongarch_fused_functions.hpp | 6290 +++++++++++++++
 .../cpu/loongarch_geometry_transforms.hpp     | 4303 +++++++++++
 .../cpu/loongarch_image_augmentations.hpp     | 5335 +++++++++++++
 src/modules/hip/kernel/color_twist.hpp        |    4 +
 src/modules/hip/kernel/swap_channels.hpp      |    4 +
 src/modules/rppi_advanced_augmentations.cpp   |    4 +
 src/modules/rppi_color_model_conversions.cpp  |    4 +
 src/modules/rppi_fused_functions.cpp          |    4 +
 src/modules/rppi_geometry_transforms.cpp      |    4 +
 src/modules/rppi_image_augmentations.cpp      |    4 +
 93 files changed, 45561 insertions(+), 712 deletions(-)
 create mode 100644 include/xxl-clang.h
 create mode 100644 src/include/cpu/rpp_loongarch_common.hpp
 create mode 100644 src/include/cpu/rpp_loongarch_filter.hpp
 create mode 100644 src/include/cpu/rpp_loongarch_simd.hpp
 create mode 100644 src/modules/cpu/kernel/box_filter_loongarch.hpp
 create mode 100644 src/modules/cpu/kernel/remap_loongarch.hpp
 create mode 100644 src/modules/cpu/loongarch_advanced_augmentations.hpp
 create mode 100644 src/modules/cpu/loongarch_color_model_conversions.hpp
 create mode 100644 src/modules/cpu/loongarch_fused_functions.hpp
 create mode 100644 src/modules/cpu/loongarch_geometry_transforms.hpp
 create mode 100644 src/modules/cpu/loongarch_image_augmentations.hpp

diff --git a/CMakeLists.txt b/CMakeLists.txt
index 7ee884dd..b3c94140 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -169,15 +169,26 @@ endif()
 # -mavx2 -- Support MMX, SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2, AVX and AVX2 built-in functions and code generation.
 # -mfma -- Support MMX, SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2, AVX and FMA built-in functions and code generation.
 # -std=gnu++17 -- Conform to the ISO 2017 C++ standard with GNU extensions.
-set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -fPIC -mavx2 -mf16c -mfma -std=gnu++17")
+
+include(CheckCXXSourceCompiles)
+
+if(CMAKE_SYSTEM_PROCESSOR MATCHES "loongarch")
+    set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -fPIC -mlsx -mlasx -std=gnu++17")
+else()
+    set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -fPIC -mavx2 -mf16c -mfma -std=gnu++17")
+endif()
+#if(COMPILER_SUPPORTS_AVX OR COMPILER_SUPPORTS_LASX)
 
 # Check AVX/SSE support
 include(CheckCXXCompilerFlag)
 
 # Check AVX flag
 check_cxx_compiler_flag("-mavx" COMPILER_SUPPORTS_AVX)
+check_cxx_compiler_flag("-mlasx" COMPILER_SUPPORTS_LASX)
 if(COMPILER_SUPPORTS_AVX)
     add_definitions(-D__AVX2__)
+elseif(COMPILER_SUPPORTS_LASX)
+    add_definitions(-D__loongarch_asx)
 else()
     # Check SSE flag
     check_cxx_compiler_flag("-msse4" COMPILER_SUPPORTS_SSE)
diff --git a/include/rppdefs.h b/include/rppdefs.h
index 6bbee49e..e7aa27a5 100644
--- a/include/rppdefs.h
+++ b/include/rppdefs.h
@@ -47,10 +47,14 @@ typedef halfhpp Rpp16f;
 #if _WIN32
 #include <intrin.h>
 #else
+#ifdef __loongarch64
+#include "xxl-clang.h"
+#else
 #include <x86intrin.h>
 #include <smmintrin.h>
 #include <immintrin.h>
 #endif
+#endif
 #include<vector>
 
 /*! \brief 8 bit unsigned char minimum \ingroup group_rppdefs \page subpage_rpp */
@@ -745,6 +749,20 @@ typedef struct RpptResamplingWindow
         return current + weight * (next - current);
     }
 
+#ifdef __loongarch_sx
+    inline __m128 operator()(__m128 x)
+    {
+        __m128 pLocRaw = __lsx_vfadd_s(__lsx_vfmul_s(x, pScale), pCenter);
+        __m128i pxLocFloor = __lsx_vftintrz_w_s(pLocRaw);
+        __m128 pLocFloor = __lsx_vffint_s_w(pxLocFloor);
+        __m128 pWeight = __lsx_vfsub_s(pLocRaw, pLocFloor);
+        Rpp32s idx[4];
+        __lsx_vst(pxLocFloor, reinterpret_cast<__m128i*>(idx), 0);
+        __m128 pCurrent = lsx_setr_f32(lookup[idx[0]], lookup[idx[1]], lookup[idx[2]], lookup[idx[3]]);
+        __m128 pNext = lsx_setr_f32(lookup[idx[0] + 1], lookup[idx[1] + 1], lookup[idx[2] + 1], lookup[idx[3] + 1]);
+        return __lsx_vfadd_s(pCurrent, __lsx_vfmul_s(pWeight, __lsx_vfsub_s(pNext, pCurrent)));
+    }
+#else
     inline __m128 operator()(__m128 x)
     {
         __m128 pLocRaw = _mm_add_ps(_mm_mul_ps(x, pScale), pCenter);
@@ -757,6 +775,7 @@ typedef struct RpptResamplingWindow
         __m128 pNext = _mm_setr_ps(lookup[idx[0] + 1], lookup[idx[1] + 1], lookup[idx[2] + 1], lookup[idx[3] + 1]);
         return _mm_add_ps(pCurrent, _mm_mul_ps(pWeight, _mm_sub_ps(pNext, pCurrent)));
     }
+#endif
 
     Rpp32f scale = 1, center = 1;
     Rpp32s lobes = 0, coeffs = 0;
diff --git a/include/xxl-clang.h b/include/xxl-clang.h
new file mode 100644
index 00000000..15414a5e
--- /dev/null
+++ b/include/xxl-clang.h
@@ -0,0 +1,592 @@
+#ifndef _INCLUDE_XXL_
+#define _INCLUDE_XXL_
+
+#ifdef __loongarch_sx
+#include <lsxintrin.h>
+#endif // __loongarch_sx
+#ifdef __loongarch_asx
+#include <lasxintrin.h>
+#endif // __loongarch_asx
+
+#define XXL_INLINE extern __inline __attribute__((__gnu_inline__, __always_inline__, __artificial__))
+
+typedef long long int __m64;
+typedef v4i32 __v4si;
+typedef v2i64 __v2di;
+
+#define _CMP_EQ_OQ      0x00
+#define _CMP_LT_OS      0x01
+#define _CMP_LE_OS      0x02
+#define _CMP_UNORD_Q    0x03
+#define _CMP_NEQ_UQ     0x04
+#define _CMP_NLT_US     0x05
+#define _CMP_NLE_US     0x06
+#define _CMP_ORD_Q      0x07
+#define _CMP_EQ_UQ      0x08
+#define _CMP_NGE_US     0x09
+#define _CMP_NGT_US     0x0a
+#define _CMP_FALSE_OQ   0x0b
+#define _CMP_NEQ_OQ     0x0c
+#define _CMP_GE_OS      0x0d
+#define _CMP_GT_OS      0x0e
+#define _CMP_TRUE_UQ    0x0f
+#define _CMP_EQ_OS      0x10
+#define _CMP_LT_OQ      0x11
+#define _CMP_LE_OQ      0x12
+#define _CMP_UNORD_S    0x13
+#define _CMP_NEQ_US     0x14
+#define _CMP_NLT_UQ     0x15
+#define _CMP_NLE_UQ     0x16
+#define _CMP_ORD_S      0x17
+#define _CMP_EQ_US      0x18
+#define _CMP_NGE_UQ     0x19
+#define _CMP_NGT_UQ     0x1a
+#define _CMP_FALSE_OS   0x1b
+#define _CMP_NEQ_OS     0x1c
+#define _CMP_GE_OQ      0x1d
+#define _CMP_GT_OQ      0x1e
+#define _CMP_TRUE_US    0x1f
+
+
+#define _MM_FROUND_TO_NEAREST_INT       0x00
+#define _MM_FROUND_TO_NEG_INF           0x01
+#define _MM_FROUND_TO_POS_INF           0x02
+#define _MM_FROUND_TO_ZERO              0x03
+#define _MM_FROUND_CUR_DIRECTION        0x04
+
+#define _MM_FROUND_RAISE_EXC            0x00
+#define _MM_FROUND_NO_EXC               0x08
+
+#define _MM_FROUND_NINT                 (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_RAISE_EXC)
+#define _MM_FROUND_FLOOR                (_MM_FROUND_TO_NEG_INF     | _MM_FROUND_RAISE_EXC)
+#define _MM_FROUND_CEIL                 (_MM_FROUND_TO_POS_INF     | _MM_FROUND_RAISE_EXC)
+#define _MM_FROUND_TRUNC                (_MM_FROUND_TO_ZERO        | _MM_FROUND_RAISE_EXC)
+#define _MM_FROUND_RINT                 (_MM_FROUND_CUR_DIRECTION  | _MM_FROUND_RAISE_EXC)
+#define _MM_FROUND_NEARBYINT            (_MM_FROUND_CUR_DIRECTION  | _MM_FROUND_NO_EXC)
+
+#define FROUND_IS_VALID(c)              ((c) == (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC)) | \
+                                        ((c) == (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)) |     \
+                                        ((c) == (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)) |     \
+                                        ((c) == (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC))
+
+
+#define LSX_TRANSPOSE4_S(row0, row1, row2, row3)       \
+do {                                                   \
+  __m128i __r0 = (__m128i)(row0);                      \
+  __m128i __r1 = (__m128i)(row1);                      \
+  __m128i __r2 = (__m128i)(row2);                      \
+  __m128i __r3 = (__m128i)(row3);                      \
+  __m128i __t0 = __lsx_vilvl_w(__r1, __r0);            \
+  __m128i __t1 = __lsx_vilvh_w(__r1, __r0);            \
+  __m128i __t2 = __lsx_vilvl_w(__r3, __r2);            \
+  __m128i __t3 = __lsx_vilvh_w(__r3, __r2);            \
+  (row0) = (__m128)__lsx_vilvl_d(__t2, __t0);          \
+  (row1) = (__m128)__lsx_vilvh_d(__t2, __t0);          \
+  (row2) = (__m128)__lsx_vilvl_d(__t3, __t1);          \
+  (row3) = (__m128)__lsx_vilvh_d(__t3, __t1);          \
+} while (0)
+#define _MM_TRANSPOSE4_PS(a, b, c, d) LSX_TRANSPOSE4_S(a, b, c, d)
+
+#define LSX_SHUFFLE(a, b, c, d) ((a&3)*64+(b&3)*16+(c&3)*4+d)
+#define _MM_SHUFFLE(a, b, c, d) LSX_SHUFFLE((a), (b), (c), (d))
+
+#ifdef __loongarch_asx
+#define __lasx_cvt_128_256(a) ({ \
+  __m256i _0; \
+  __m128i _a = a; \
+  __builtin_memcpy(&_0, &_a, sizeof(__m128i)); \
+  _0; \
+})
+#define __lasx_cvt_256_128(a) ({ \
+  __m128i _0; \
+  __m256i _a = a; \
+  __builtin_memcpy(&_0, &_a, sizeof(__m128i)); \
+  _0; \
+})
+#define __lasx_xvperm_q(a, b, c) ({ \
+  __m256i _0; \
+  switch (((c>>2)&12)|(c&3)) { \
+    case 0: _0 = __lasx_xvpermi_q(a, b, 0); break; \
+    case 1: _0 = __lasx_xvpermi_q(a, b, 1); break; \
+    case 2: _0 = __lasx_xvpermi_q(a, b, 2); break; \
+    case 3: _0 = __lasx_xvpermi_q(a, b, 3); break; \
+    case 4: _0 = __lasx_xvpermi_q(a, b, 16); break; \
+    case 5: _0 = __lasx_xvpermi_q(a, b, 17); break; \
+    case 6: _0 = __lasx_xvpermi_q(a, b, 18); break; \
+    case 7: _0 = __lasx_xvpermi_q(a, b, 19); break; \
+    case 8: _0 = __lasx_xvpermi_q(a, b, 32); break; \
+    case 9: _0 = __lasx_xvpermi_q(a, b, 33); break; \
+    case 10: _0 = __lasx_xvpermi_q(a, b, 34); break; \
+    case 11: _0 = __lasx_xvpermi_q(a, b, 35); break; \
+    case 12: _0 = __lasx_xvpermi_q(a, b, 48); break; \
+    case 13: _0 = __lasx_xvpermi_q(a, b, 49); break; \
+    case 14: _0 = __lasx_xvpermi_q(a, b, 50); break; \
+    case 15: _0 = __lasx_xvpermi_q(a, b, 51); break; \
+  } \
+  _0; \
+})
+#endif // __loongarch_asx
+
+#define DEF_FCMP(pre, sub, typi, typo) \
+XXL_INLINE typo __##pre##fcmp_xxx_##sub(typi a, typi b, const int c) { \
+  switch (c&31) { \
+    case 0: return __##pre##fcmp_ceq_##sub(a, b); \
+    case 1: return __##pre##fcmp_slt_##sub(a, b); \
+    case 2: return __##pre##fcmp_sle_##sub(a, b); \
+    case 3: return __##pre##fcmp_cun_##sub(a, b); \
+    case 4: return __##pre##fcmp_cune_##sub(a, b); \
+    case 5: return __##pre##fcmp_sule_##sub(b, a); \
+    case 6: return __##pre##fcmp_sult_##sub(b, a); \
+    case 7: return __##pre##fcmp_cor_##sub(a, b); \
+    case 8: return __##pre##fcmp_cueq_##sub(a, b); \
+    case 9: return __##pre##fcmp_sult_##sub(a, b); \
+    case 10: return __##pre##fcmp_sule_##sub(a, b); \
+    case 11: return __##pre##fcmp_caf_##sub(a, b); \
+    case 12: return __##pre##fcmp_cne_##sub(a, b); \
+    case 13: return __##pre##fcmp_sle_##sub(b, a); \
+    case 14: return __##pre##fcmp_slt_##sub(b, a); \
+    case 15: return __##pre##xori_b(__##pre##fcmp_caf_##sub(a, b), 0xff); \
+    case 16: return __##pre##fcmp_seq_##sub(a, b); \
+    case 17: return __##pre##fcmp_clt_##sub(a, b); \
+    case 18: return __##pre##fcmp_cle_##sub(a, b); \
+    case 19: return __##pre##fcmp_sun_##sub(a, b); \
+    case 20: return __##pre##fcmp_sune_##sub(a, b); \
+    case 21: return __##pre##fcmp_cule_##sub(b, a); \
+    case 22: return __##pre##fcmp_cult_##sub(b, a); \
+    case 23: return __##pre##fcmp_sor_##sub(a, b); \
+    case 24: return __##pre##fcmp_sueq_##sub(a, b); \
+    case 25: return __##pre##fcmp_cult_##sub(a, b); \
+    case 26: return __##pre##fcmp_cule_##sub(a, b); \
+    case 27: return __##pre##fcmp_saf_##sub(a, b); \
+    case 28: return __##pre##fcmp_sne_##sub(a, b); \
+    case 29: return __##pre##fcmp_cle_##sub(b, a); \
+    case 30: return __##pre##fcmp_clt_##sub(b, a); \
+    case 31: return __##pre##xori_b(__##pre##fcmp_saf_##sub(a, b), 0xff); \
+  } \
+  __builtin_unreachable(); \
+}
+#ifdef __loongarch_asx
+DEF_FCMP(lasx_xv, d, __m256d, __m256i)
+DEF_FCMP(lasx_xv, s, __m256, __m256i)
+#endif // __loongarch_asx
+#ifdef __loongarch_sx
+DEF_FCMP(lsx_v, d, __m128d, __m128i)
+DEF_FCMP(lsx_v, s, __m128, __m128i)
+#endif // __loongarch_sx
+
+
+#define DEF_FROUND(pre, sub, typ) \
+XXL_INLINE typ __##pre##frintrxxx_##sub(typ a, const int b) { \
+  switch (b) { \
+  case 8: return __##pre##frintrne_##sub(a); \
+  case 9: return __##pre##frintrm_##sub(a); \
+  case 10: return __##pre##frintrp_##sub(a); \
+  case 11: return __##pre##frintrz_##sub(a); \
+  } \
+  __builtin_unreachable(); \
+}
+
+#ifdef __loongarch_asx
+DEF_FROUND(lasx_xv, d, __m256d)
+DEF_FROUND(lasx_xv, s, __m256)
+#endif // __loongarch_asx
+#ifdef __loongarch_sx
+DEF_FROUND(lsx_v, d, __m128d)
+DEF_FROUND(lsx_v, s, __m128)
+#endif // __loongarch_sx
+
+
+#define DEF_FROUND_F16(pre, typo, typi) \
+XXL_INLINE typo __##pre##fcvtrxxx_h_s(typi a, const int b) { \
+  typo res; \
+  int rm = 0; \
+  switch (b) { \
+  case 0: rm = 0; break; \
+  case 1: rm = 3; break; \
+  case 2: rm = 2; break; \
+  case 3: rm = 1; break; \
+  } \
+  unsigned int saved_fcsr = __builtin_loongarch_movfcsr2gr(3); \
+  __builtin_loongarch_movgr2fcsr(3, rm << 8); \
+  res = __##pre##fcvt_h_s((typi)__##pre##ldi(0), a); \
+  __builtin_loongarch_movgr2fcsr(3, saved_fcsr); \
+  return res; \
+}
+
+#ifdef __loongarch_asx
+DEF_FROUND_F16(lasx_xv, __m256i, __m256)
+#endif // __loongarch_asx
+#ifdef __loongarch_sx
+DEF_FROUND_F16(lsx_v, __m128i, __m128)
+#endif // __loongarch_asx
+
+#if defined(__loongarch_asx) && defined(__loongarch_sx)
+#define lasx_extractf128_f32(a, b) ({                       \
+  __m128 _0;                                                \
+  __m256i _1 = __lasx_xvpermi_q((__m256i)a, (__m256i)a, 1); \
+  __m256i _2 = b&1?_1:(__m256i)a;                           \
+  _0 = (__m128)__lasx_cvt_256_128(_2);                      \
+  _0;                                                       \
+})
+
+#define lasx_extractf128_m256i(a, b) ({   \
+  __m128i _0;                             \
+  __m256i _1 = __lasx_xvpermi_q(a, a, 1); \
+  __m256i _2 = b&1?_1:a;                  \
+  _0 = __lasx_cvt_256_128(_2);            \
+  _0;                                     \
+})
+
+#define lasx_insertf128_f32(a, b, c) ({                                             \
+  __m256 _0;                                                                        \
+  _0 = (__m256)__lasx_xvpermi_q(a, __lasx_cvt_128_256((__m128i)b), c & 1 ? 2 : 48); \
+  _0;                                                                               \
+})
+
+#define lasx_insertf128_m256i(a, b, c) ({                          \
+  __m256i _0;                                                      \
+  _0 = __lasx_xvpermi_q(a, __lasx_cvt_128_256(b), c & 1 ? 2 : 48); \
+  _0;                                                              \
+})
+
+#define lasx_cvti32_f64(a) ({            \
+  __m256d _0;                            \
+  __m256i _1 = __lasx_cvt_128_256(a);    \
+  __m256i _2 = __lasx_xvpermi_d(_1, 16); \
+  _0 = __lasx_xvffintl_d_w(_2);          \
+  _0;                                    \
+})
+
+#define lasx_cvtf32_f64(a) ({                  \
+  __m256d _0;                                  \
+  __m256i _1 = __lasx_cvt_128_256((__m128i)a); \
+  __m256i _2 = __lasx_xvpermi_d(_1, 16);       \
+  _0 = __lasx_xvfcvtl_d_s((__m256)_2);         \
+  _0;                                          \
+})
+
+static inline __m256 lasx_setr_m128(__m128 a, __m128 b) {
+  __m256 _0;
+  __builtin_memcpy(&_0, &a, sizeof(a));
+  __builtin_memcpy((char*)&_0 + sizeof(a), &b, sizeof(b));
+  return _0;
+}
+
+static inline __m256d lasx_setr_m128d(__m128d a, __m128d b) {
+  __m256d _0;
+  __builtin_memcpy(&_0, &a, sizeof(a));
+  __builtin_memcpy((char*)&_0 + sizeof(a), &b, sizeof(b));
+  return _0;
+}
+
+static inline __m256i lasx_setr_m128i(__m128i a, __m128i b)
+{
+    __m256i _0;
+    __builtin_memcpy(&_0, &a, sizeof(a));
+    __builtin_memcpy((char*)&_0 + sizeof(a), &b, sizeof(b));
+    return _0;
+}
+
+
+#define lasx_castm128_m256(a) ({               \
+  __m256 _0;                                   \
+  _0 = (__m256)__lasx_cvt_128_256((__m128i)a); \
+  _0;                                          \
+})
+
+#define lasx_castm128i_m256i(a) ({ \
+  __m256i _0;                      \
+  _0 = __lasx_cvt_128_256(a);      \
+  _0;                              \
+})
+
+#define lasx_extracti128_m256i(a, b) ({   \
+  __m128i _0;                             \
+  __m256i _1 = __lasx_xvpermi_q(a, a, 1); \
+  __m256i _2 = b&1?_1:a;                  \
+  _0 = __lasx_cvt_256_128(_2);            \
+  _0;                                     \
+})
+
+#define lasx_cvti8_i32(a) ({          \
+  __m256i _0;                         \
+  __m256i _1 = __lasx_cvt_128_256(a); \
+  _0 = __lasx_vext2xv_w_b(_1);        \
+  _0;                                 \
+})
+
+#define lasx_cvtph_f32(a) ({             \
+  __m256 _0;                             \
+  __m256i _1 = __lasx_cvt_128_256(a);    \
+  __m256i _2 = __lasx_xvpermi_d(_1, 16); \
+  _0 = __lasx_xvfcvtl_s_h(_2);           \
+  _0;                                    \
+})
+
+#define lasx_cvtf32_ph(a, b) ({                        \
+  __m128i _0;                                          \
+  __m256i _1 = __lasx_xvfcvtrxxx_h_s(a, (const int)b); \
+  __m256i _2 = __lasx_xvpermi_d(_1, 8);                \
+  _0 = __lasx_cvt_256_128(_2);                         \
+  _0;                                                  \
+})
+
+#endif // __loongarch_asx and __loongarch_sx
+
+#ifdef __loongarch_asx
+
+#define lasx_blend_f32(a, b, c) ({                            \
+  __m256 _0;                                                  \
+  __m256i _1 = __lasx_xvldi(c|0x00);                        \
+  __m256i _2 = __lasx_vext2xv_w_b(_1);                        \
+  _0 = (__m256)__lasx_xvbitsel_v((__m256i)a, (__m256i)b, _2); \
+  _0;                                                         \
+})
+
+#define lasx_blendv_f32(a, b, c) ({                           \
+  __m256 _0;                                                  \
+  __m256i _1 = __lasx_xvslti_w((__m256i)c, 0);                \
+  _0 = (__m256)__lasx_xvbitsel_v((__m256i)a, (__m256i)b, _1); \
+  _0;                                                         \
+})
+
+#define lasx_permute2f128_f32(a, b, c) ({                                        \
+  __m256 _0;                                                                     \
+  __m256i _1 = __lasx_xvldi(0);                                                  \
+  __m256i _2 = c&136 ? _1 : (__m256i)b;                                          \
+  __m256 _3 = c&128 ? (c&2?b:a) : c&0x8 ? (c&32?b:a) : a;                        \
+  int _4 = c&128 ? ((c&1)|32) : c&0x8 ? ((c&16)|2) : c&51;                       \
+  _0 = (c&136)==136 ? (__m256)_1 : (__m256)__lasx_xvperm_q(_2, (__m256i)_3, _4); \
+  _0;                                                                            \
+})
+
+#define lasx_permute2f128_m256i(a, b, c) ({                \
+  __m256i _0;                                              \
+  __m256i _1 = __lasx_xvldi(0);                            \
+  __m256i _2 = c&136 ? _1 : b;                             \
+  __m256i _3 = c&128 ? (c&2?b:a) : c&0x8 ? (c&32?b:a) : a; \
+  int _4 = c&128 ? ((c&1)|32) : c&0x8 ? ((c&16)|2) : c&51; \
+  _0 = (c&136)==136 ? _1 : __lasx_xvperm_q(_2, _3, _4);    \
+  _0;                                                      \
+})
+
+#define lasx_setr_f32(a, b, c, d, e, f, g, h) (__m256)(v8f32){a,b,c,d,e,f,g,h}
+
+#define lasx_setr_i8(a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z, A, B, C, D, E, F)  \
+    (__m256i)(v32i8){a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z,A,B,C,D,E,F}
+
+#define lasx_setr_i32(a, b, c, d, e, f, g, h) (__m256i)(v8i32){a,b,c,d,e,f,g,h}
+
+#define lasx_set1_f64(a) (__m256d)(v4f64){a,a,a,a}
+
+#define lasx_set1_f32(a) (__m256)(v8f32){a,a,a,a,a,a,a,a}
+
+#define lasx_permutevar8x32_i32(a, b) ({   \
+  __m256i _0;                              \
+  __m256i _1 = __lasx_xvpermi_q(a, a, 17); \
+  __m256i _2 = __lasx_xvpermi_q(a, a, 0);  \
+  _0 = __lasx_xvshuf_w(b, _1, _2);         \
+  _0;                                      \
+})
+
+#define lasx_permutevar8x32_f32(a, b) ({                     \
+  __m256 _0;                                                 \
+  __m256i _1 = __lasx_xvpermi_q((__m256i)a, (__m256i)a, 17); \
+  __m256i _2 = __lasx_xvpermi_q((__m256i)a, (__m256i)a, 0);  \
+  _0 = (__m256)__lasx_xvshuf_w(b, _1, _2);                   \
+  _0;                                                        \
+})
+
+#define lasx_shuffle_i8(a, b) ({         \
+  __m256i _0;                            \
+  __m256i _1 = __lasx_xvshuf_b(a, a, b); \
+  __m256i _2 = __lasx_xvslti_b(b, 0);    \
+  __m256i _3 = __lasx_xvxori_b(_2, 255); \
+  _0 = __lasx_xvand_v(_1, _3);           \
+  _0;                                    \
+})
+
+#define lasx_movemask_i8(a) ({                    \
+  int _0;                                         \
+  __m256i _1 = __lasx_xvmskltz_b(a);              \
+  unsigned int _2 = __lasx_xvpickve2gr_wu(_1, 0); \
+  unsigned int _3 = __lasx_xvpickve2gr_wu(_1, 4); \
+  _0 = (_2)|(_3<<16);                             \
+  _0;                                             \
+})
+
+#define lasx_packus_i16(a, b) ({       \
+  __m256i _0;                          \
+  __m256i _1 = __lasx_xvldi(0);        \
+  __m256i _3 = __lasx_xvmax_h(b, _1);  \
+  __m256i _5 = __lasx_xvsat_hu(_3, 7); \
+  __m256i _2 = __lasx_xvmax_h(a, _1);  \
+  __m256i _4 = __lasx_xvsat_hu(_2, 7); \
+  _0 = __lasx_xvpickev_b(_5, _4);      \
+  _0;                                  \
+})
+
+#define lasx_packus_i32(a, b) ({        \
+  __m256i _0;                           \
+  __m256i _1 = __lasx_xvldi(0);         \
+  __m256i _3 = __lasx_xvmax_w(b, _1);   \
+  __m256i _5 = __lasx_xvsat_wu(_3, 15); \
+  __m256i _2 = __lasx_xvmax_w(a, _1);   \
+  __m256i _4 = __lasx_xvsat_wu(_2, 15); \
+  _0 = __lasx_xvpickev_h(_5, _4);       \
+  _0;                                   \
+})
+
+#define lasx_slli_i32(a, b) ({                      \
+  __m256i _0;                                       \
+  __m256i _1 = __lasx_xvldi(0);                     \
+  __m256i _2 = __lasx_xvslli_w(a, (unsigned int)b); \
+  _0 = b>=32?_1:_2;                                 \
+  _0;                                               \
+})
+
+#define lasx_srli_i32(a, b) ({                      \
+  __m256i _0;                                       \
+  __m256i _1 = __lasx_xvldi(0);                     \
+  __m256i _2 = __lasx_xvsrli_w(a, (unsigned int)b); \
+  _0 = b>=32?_1:_2;                                 \
+  _0;                                               \
+})
+#endif // __loongarch_asx
+
+#ifdef __loongarch_sx
+
+#define lsx_set1_f32(a)  (__m128)(v4f32){a,a,a,a}
+
+#define lsx_set_f32(a, b, c, d) (__m128)(v4f32){d,c,b,a}
+
+#define lsx_setr_f32(a, b, c, d) (__m128)(v4f32){a,b,c,d}
+
+#define lsx_loadl_d(a) __lsx_vinsgr2vr_d(__lsx_vldi(0), *(long*)(a), 0)
+
+#define lsx_slli_m128i(a, b) ({                   \
+  __m128i _0;                                     \
+  __m128i _1 = __lsx_vldi(0);                     \
+  __m128i _2 = __lsx_vbsll_v(a, (unsigned int)b); \
+  _0 = b>=16?_1:_2;                               \
+  _0;                                             \
+})
+
+#define lsx_slli_i32(a, b) ({                     \
+  __m128i _0;                                     \
+  __m128i _1 = __lsx_vldi(0);                     \
+  __m128i _2 = __lsx_vslli_w(a, (unsigned int)b); \
+  _0 = b>=32?_1:_2;                               \
+  _0;                                             \
+})
+
+#define lsx_srli_m128i(a, b) ({                   \
+  __m128i _0;                                     \
+  __m128i _1 = __lsx_vldi(0);                     \
+  __m128i _2 = __lsx_vbsrl_v(a, (unsigned int)b); \
+  _0 = b>=16?_1:_2;                               \
+  _0;                                             \
+})
+
+#define lsx_srli_i16(a, b) ({                     \
+  __m128i _0;                                     \
+  __m128i _1 = __lsx_vldi(0);                     \
+  __m128i _2 = __lsx_vsrli_h(a, (unsigned int)b); \
+  _0 = b>=16?_1:_2;                               \
+  _0;                                             \
+})
+
+#define lsx_srli_i32(a, b) ({                     \
+  __m128i _0;                                     \
+  __m128i _1 = __lsx_vldi(0);                     \
+  __m128i _2 = __lsx_vsrli_w(a, (unsigned int)b); \
+  _0 = b>=32?_1:_2;                               \
+  _0;                                             \
+})
+
+#define lsx_set_i8(a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p)  (__m128i)(v16i8){p,o,n,m,l,k,j,i,h,g,f,e,d,c,b,a}
+
+#define lsx_setr_i32(a, b, c, d) ((__m128i)(v4i32){a, b, c, d})
+
+#define lsx_setr_i16(a, b, c, d, e, f, g, h) (__m128i)(v8i16){a,b,c,d,e,f,g,h}
+
+#define lsx_setr_i8(a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p) (__m128i)(v16i8){a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p}
+
+#define lsx_packs_i32(a, b) ({      \
+  __m128i _0;                       \
+  __m128i _2 = __lsx_vsat_w(b, 15); \
+  __m128i _1 = __lsx_vsat_w(a, 15); \
+  _0 = __lsx_vpickev_h(_2, _1);     \
+  _0;                               \
+})
+
+#define lsx_packus_i16(a, b) ({      \
+  __m128i _0;                        \
+  __m128i _1 = __lsx_vldi(0);        \
+  __m128i _3 = __lsx_vmax_h(b, _1);  \
+  __m128i _5 = __lsx_vsat_hu(_3, 7); \
+  __m128i _2 = __lsx_vmax_h(a, _1);  \
+  __m128i _4 = __lsx_vsat_hu(_2, 7); \
+  _0 = __lsx_vpickev_b(_5, _4);      \
+  _0;                                \
+})
+
+#define lsx_movemask_i8(a) ({      \
+  int _0;                          \
+  __m128i _1 = __lsx_vmskltz_b(a); \
+  _0 = __lsx_vpickve2gr_w(_1, 0);  \
+  _0;                              \
+})
+
+#define lsx_shufflelo_i16(a, b) ({                  \
+  __m128i _0;                                       \
+  __m128i _1 = __lsx_vshuf4i_h(a, (unsigned int)b); \
+  _0 = __lsx_vextrins_d(a, _1, 0);                  \
+  _0;                                               \
+})
+
+#define lsx_blend_f32(a, b, c) ({                                                                       \
+  __m128 _0;                                                                                            \
+  __m128i _1 = __lsx_vldi((c&1)|(c&1)<<1|(c&2)<<1|(c&2)<<2|(c&4)<<2|(c&4)<<3|(c&8)<<3|(c&8)<<4);        \
+  __m128i _2 = __lsx_vilvl_h(_1, _1);                                                                   \
+  _0 = (__m128)__lsx_vbitsel_v((__m128i)a, (__m128i)b, _2);                                             \
+  _0;                                                                                                   \
+})
+
+#define lsx_blendv_f32(a, b, c) ({                          \
+  __m128 _0;                                                \
+  __m128i _1 = __lsx_vslti_w((__m128i)c, 0);                \
+  _0 = (__m128)__lsx_vbitsel_v((__m128i)a, (__m128i)b, _1); \
+  _0;                                                       \
+})
+
+#define lsx_blend_i16(a, b, c) ({     \
+  __m128i _0;                         \
+  __m128i _1 = __lsx_vldi(c|0x00);  \
+  __m128i _2 = __lsx_vilvl_b(_1, _1); \
+  _0 = __lsx_vbitsel_v(a, b, _2);     \
+  _0;                                 \
+})
+
+#define lsx_packus_i32(a, b) ({       \
+  __m128i _0;                         \
+  __m128i _1 = __lsx_vldi(0);         \
+  __m128i _3 = __lsx_vmax_w(b, _1);   \
+  __m128i _5 = __lsx_vsat_wu(_3, 15); \
+  __m128i _2 = __lsx_vmax_w(a, _1);   \
+  __m128i _4 = __lsx_vsat_wu(_2, 15); \
+  _0 = __lsx_vpickev_h(_5, _4);       \
+  _0;                                 \
+})
+
+#define lsx_shuffle_i8(a, b) ({        \
+  __m128i _0;                          \
+  __m128i _1 = __lsx_vshuf_b(a, a, b); \
+  __m128i _2 = __lsx_vslti_b(b, 0);    \
+  __m128i _3 = __lsx_vxori_b(_2, 255); \
+  _0 = __lsx_vand_v(_1, _3);           \
+  _0;                                  \
+})
+#endif // __loongarch_sx
+
+#endif // _INCLUDE_XXL_
diff --git a/libs/third_party/ffts/libffts.a b/libs/third_party/ffts/libffts.a
index b30b1147133391c403074dbd380d91f356c094dd..3f35c3762c83d355fc5dfe52d3a1751bf4c629c7 100644
GIT binary patch
literal 202096
zcmeFa4}2BH*)}|9PXYlFI3Z9_NIen6vrWJp2yjr;4Frj8R1{jNrA=ZI#Tp<1st70>
z2uj-oqEZcr5)@kdmLOL9Xsgz9l4$8$MEaD+`k$a!^=VqLrPV5OzU!X7Cz*^{Yx{hC
zfA9DGI_I}%X0N&Co_qf6?9A-WY|ixZr4<V=7<f*&x5?K|J-7ZJ4#%d%m{B%R2$3U1
z%IKbBUb7IpR(r><77B4>xjX_Rn?=B0-S@Eve*c5Ry5z}gu35gUv}#3F#kZDM33+nO
z(kgacxA?l{rO`?cu2^{8(wfqhr38A2^j^%~@?@fZ60Iy<T2)@%i)$hib<Xmo<%^fq
zELp~}S5&R2DV;R&8c)ErRg0^ZmM^a=tyr?`y2Vwc@~>vejT8S4r4IozA${Pc;3)-W
z+{??auC79k$Yk_Hylhc<b#<wxi2}MMc)&GFmfl#tw6eeq*^L|gtII2{?=wJ;hY^A$
z#e8Cjl^T2{=o0~YJd6+=Lv)Hy6!0)YC{dpbqWScMSzf;Ux{6X+7ZBH?(rdJcOuy@T
z{JQJMHQ!oXfl5+YCB!-A1*%=2eK{vjQE1sfc`0z6pHMMj$vGZEHM(?hrG|9Z+8#{0
zSd?FzFr}bCGA8CvW>?w3HM2@CvH}4#4W0=I{Y;@k#7jK)OjwR|I%2TMptE3u5aU{L
z?2HZ-4U>n8`ut2076KuhV1;|ix-p!vZ|aQOE8}r{wHvo@E{ogcWeGby<XTUO;bI$(
zmvQ_J$B4jivHpjm@&+eiuMi1)O*n3^YKhyoIC1-N_<t_Nwa!R)tx?IRt#@v9#M!~&
z;vyVXIO=gE?@V6V^&Ln2?wcVo@}sA%Q`T_ti^U_v-F1$5yD}tF!FdOm<X#JI#R##b
zDkMmkt5#)-<{u_2b{~7%YFJ++phE;YL=b-j@kgLb<SEEZNBZvlR*D!O6jt64*J_xY
zCB{FNg|rVB4c{&n4H<>v(%T%dank5m!y_Rv1hTi}#O;Shi3J}*uW4ft7yP%B$L+0!
zGA|;?qsUF2@d)xLf;@`cTo#X1xCyH}ALB{uMj_6=8+lM4ZjY2BU)<5cd22>{q#yDF
z1u>7@N6&1Jko)M2wuqZ1i)!A{wecSJnHp}1af$<Zk)y~L#4)bIiIcoJRDK!A?~b(X
zOZQ}t5XaJ7YhNrRTF)OL{xg;-vL=VbFjSa_TYz005;c~vqE}>zVZY825AF{Msu!05
z+XK$Bwk#37Vz>wuW{F214T%T$A<R#+#1%i!5*yFW5<||<62~qcE)F0M-$tC95^?(t
z#JllG+zwqjT+}5*UQWvRWpxQBZ-@}59}C2-+w0<XXF%kMw0CRn2e)ZQ+!mohHCEc7
zniO%{(sgy@H#DO$kg!C3-m(gW4@lg}J5!9R4U6pBwE-v3Ep+l4k+&+Z8$^4=4S4m}
zjR>c~>F7SEJn8FDVJn=I){SNDc6m#?y{4?q9)Yw}p&lRo!l3HgPbRG!+;%&9$1rgh
z$}xPGC`uLUtrT!-5kHmNcHr*x&pEiVPH#mT?~1qC>8o9<(dn?k8F$zB#6qg8`@b=0
zetH=8MFZ!jqg?h|gBCz`Bpu-z0`UmaKsrRAOXMi(Jamav#M`^;Q(tKyjAW>ADvYWN
zeqpL{jjV9m?VG~w_G;+szBRCn^w<(`BV|IIH3DfGDa7#ogJ+3GT;HwZg{a2$zo;|r
zrEP#HYk7Ad@?yA1alO3w+q$@{$AnS)05gx$U)GthNe{YyRi0IMCL$v`pSJ$?W}Dsh
zK%3q1`mk>P7KC*>hdCYn3Pt^^!^F)kZIq8PuSwo(15b%&;v5?0{C0v6fBX}!Z<I&F
zv=Jf$`MMJMI28FDa$Ktw*pM>U+K9G<+JHOC;&xL@+-`{DT7*nD9Jd>txV_Ph+nYL(
zwqdUICtOn@$Qy)XAg<E^!0x_8R8kt|efYGMg|_O7Q%_s{z`OS15vUEr#X=k@)Rz6?
z!z=qaneu%0<({9!ANugh5Zb<r+mfYwl1b}8>MU^}HEyLN%z-20Mavkv?ww#`K#Y6n
zrI2VDASyQn#MupnackorlBFxl5|REWpSMNa>W6gALO7DE^5xRQkeT7h9D=k{KBd3e
zVT~vGXxl#gc1AVH-&FSzu8a5=+(6<B$5IQ$Lpexy;AG^E!jB?NiH{-@D+gA0GI9m-
zKZN7Q&|`05+`gmmBseEM4zAlxfscDQb#Z%dB5qUo=$Ucrp(Am-71wY>pv{`FF<Dvz
z{fCT+qm6Qc`!fqgS2R8L9Q2|*Xh4{}siR{D-%VamX@Z&8F}P6tE$Ef`cc`pIma_;l
z2B0j{Q6|3}H%9D!c#N2y>RQ!K!p=qcHVlXd8z0FMmEktKI+`VJaog;bo$<&sYaP)H
zxm(aq^@A+<Tch1{S)Q|jCx7$dmCvHAZ+$;mdRlX8<r0LUdNdgEbU}V=bc8s7V-f00
z26(&SUX1#(INTPgM%~$swzclskf;tP?D1upVo3j?;^!ZuPQq@bHZs*g8sov%^M{M)
zhrkV+I1P2ziY2(F+@EF^p>W2f4cfl+2WS)fi6hG%8gN(~%PAD!9k6{V;)opkX-IsB
z+-^MbZL-6ONCRxTX3=(7X#4W^<2p?X4630#&RCzf?Eb)QO9SHv)m&aMs&>NYK{b~Y
zWY?ZKvVTpgJ*XxpmYMMYu={boD?Kw~7uwoAXlGN-8dP&|_^8@vhG*AqgrA6J)a1Z#
zGq`8Mzkbe$jKRR?ri`k+A|<=F0`bkjk%OZiydmI;=_4{Wf=li9q0CXW-vZW=nO(aC
z;RXY*I5Z;TE5M!)jH;~wCmzVI{WAO(!*43^884om@j}L^+8OX`&wwBB3GmNZFd~C=
znTqp014q@~KQO!YFa1W<9_g1|d*idCYHOd(uKmr=N7X+6^X%F?>zuqtem1Ih&(E@J
zH`Nv9ZA=vAUG^WNY8ShsYVXGVV^d&8-ra4ZYBy~jRlBhcZro2MY#3F0d!R7y&V-ZK
z)P`_{GxC}W3-d1d59CE+MqXoJX5RYE*|lYEcJ2DYnR#yA%)B!<WY;zXI;^#cHF;^7
z-(A}E?Ng<@Gn}A1Iq0MxN1x;;!%)tsf91Hh1`dmc(+b6^&Nh4h!y&N*WnP1_twovk
zzc3_D2w|NZ77}kC%o3aD;`lu3>&%eYjCSKU@Y|ddw-3Sp+5K7K&P%hzveVH9%npfV
z@V^88i%=)-ER5TCf>XaQODz2&?$twwi;St6;tFtnc0TUmvqIvhg(308&4<Mu<#GGR
z(5-bi?(2)P#Ml0q1siSsdPiK2eo|vy+`eQ4{AOi|t9}8$>pZ`F^4k%&$CDrW2GiQ%
zSLOK)LBF9f5w|l)ChC6LQ}E02{2Gq{M;|KG-?h-ro&5vU*T8)#l}nKi%a9M1&_DBw
zAu%2Ok8z(vI%~4T^gm_69xhJ56zBDh7&0_dEJuD7L5Ev&681&V;i@~cL`Nt~)GtK4
zeFf4E-aD7WE=<_NAS(s3LjQ^UoRcM*W`Kjd_#65@A0ZDi2Dw&KdBXnSl8`u#yf_BE
zk0CEcAuo<0FODHECLyj(bqV_%#C7Zvr1yFFA-&``l>Bxi>>=cLah4eQEBKXre*KUa
zn-cINnWrHyw!ja0LE$zXN!Zjk+Z0IHJGKO69#HxzA1HkfWoC*8q2t@=H~bB0IUO=?
z3$)okfS#>5r#@U7`fT&4o}4*c){&#|+nm#8{~2}R1l1!|Pf(Ajj{FYt?ksGx(Vkn6
zQ2G4Qt0R|8@yh1|<j0?o2aV-z_8-W<JWI@f1LcSGP@VWS<Tlo|+0Q{X)rn_2;CGtm
z*95;D<VE9-HknV2;2cXunZ&_C9m%;3b|%uE>hUQLa2-S%6NroAJkn-U`@TQuT_XqR
z-hn>)MzniqXYGD@nS$DJYP$!G7M0g`QeVRfUW+=E%=(LLzYm{C&VK)|*#h^2$oSN_
zwO)u}8*xuaKe^pHXBztYFAWuY(0_gRfe!oKX~V^yz0e8wVH?N0>)P!99s^w}+Ux<i
zN4$SmNOVoi66?WBJNdNLzuvW|&z=Imw`R7ZKZE}In0DI@^>eNjiAX=(bDu#U;3(g}
z8o=ucw%g!D+>cWogo*S+KZ3%!3q+A{m&T~yhCWyXeXt1nED`j>BIxTxaNmj~Kl;mD
zN}u1c2yv!Euie+B&VOuHU_Qx8{^BolDL#^=<jC;`#SPslZqkD|QvaFSxB`7QHLf^1
zM#F<W;|-NIU2oF)45U@{KZ&FIpkW?Q`lW&Tpo4M$I)J_}$;`+fCis39Y4}N&cn;++
z$7krzC4Yb7v*pL4l=+YS<y5JXLvpEKo&3>>xrE_9nR^=a9f7{>NZ4h<$xWW|7wJcR
z@NWL96LYD4CYO9Xm->^Zr?^(h)DfcLZ)nq_DPnyp>VoT9yGDBLc`f<@GEdjVZMY-%
zK(@PjptBnFl=>mmhrMq|q3A&VpuM+O%RVvc&`k8bV*{NAj8U^uhn{+5q}T->$wpWy
zbJsejyFZQocjQkfw^aD=MjzwrlSYUxlpXPxLI>)DR6ml5@$AK-#Pa%2ZkEX1P1jXm
zylBMO)(ttiEz$hgF5H9mWV(?}{l(bUy}%tOw`?7bJDps2Qhx0F@M{gZs83?-H0ZD+
zKR>oH-?j6eM%uzo?g3o4Wn;jf1b$HDHqLSF>Y1+H3Ow(L;o{aB*FN^{aPh3`h)Z!^
zU4gKV4HuW-So9m$t}AnL%OAk;2*RTr%7QquoZKaIU3>8|$ZB`(tL_;t@^DnY>Dnv*
z?8^M0JV-}*(0%i!V8Ytnm^L42k34Z(u~^fRuuDD+iLaj-m;J&#<oDA!PQ!6J@~GBL
z*dOCMy&;~ko6&Yq-K4&h>`%i#o%61X`<{f~hq9#nrShe*yyDXNydksXxX`^L;8gx5
z8*>_m#ScS;Vk5$R2YuLG>4m7%!$l*`_hrScw}-~9XK~a6|86J_nNBbs5V`S5u~-Z2
z&WW%y^J8(?@l26>U{^?pC}Hr+hdn+&*807Wh{N5G6SuzeOtQ58RMM(XnI-B|AQy4d
zm?8sllI-WFrpFFE77_<?;t}*W&OSCJJ+?R;w-3}^jJhxKgu8W_i~i6O)Sc(yx5SOx
z)Gs*(|M%V+E^aS$?I_A_502x187?-~qD=n-Z8Gxv7-ZgF?n+rp%Hon&hN71;>zq82
z(R8_M-=1*oy$Bygy*Yre50Q<qo8EJ6#1~oIf-%{zUAxRx`V`8r`QY7t#FO7tSZ7nX
zeAZ>t<#l#bd7Ztnw$5(F7^$HyVeK5AEZuxXo@jn!yu8k+d}w}z><+f+-jKvN<f{I-
zZu?&>y853gMV*MC-b7F*WZOyCy=p(H-Xy>D-Y46KuO0s+Hv2fW72RQz4}94B{KgOG
z4o0~o-~ZFx`!HTWJaz)|PwPyrT-+ItgwP+Ldi^}wme2(l%c9NRit+0YFuvG?BYmwS
zs!@hrxV~uI(S_>@;{m$_*Ct)}m!dwq*QPqp^%vH2Q?tbG@fe2=aIK$$KVi&JaSyd!
zXfqmc4;+v42He{gV?444b}NlfFdjL8V-c>A+Yu%{KDDS3<B$y)m!!S*`=#H<*rYlG
z<Kltam;N`#9`E9O!@Iv<+KO>X2{`>RZmG_6tqp$w_QQ}U;p>aWAm@x6R5NBYk3F8u
z%y<ar58!+k+%)$1G5Yj0_PG3WqiU_w2Gz{^T(<20-vN#qgIo&EW`uigz^K~#Ij3bD
z7?52%6#g_Wp>fD%7?aGwn1se5GjN`RBMW>{a$3e6;Lvzv-cXE3fE5kRu3d((Ln-W`
z(=xsSY_8>vM;r@d4)`sF-&Ei<9{I{(Z#*({us0r=2>+Y~r*)4<er$|K-v7y{+CTgx
zyY~45qiP#5rpUlpWD~B5MvOZi`ro5!7pZXw#u|5H>_X#^MjnTZr*R04We#N5-j4Cd
zopptIKmOm|c;t>mVcuYjX&Nye+4vd8BaQ2EFT^+`@5jSLb8XKUWc*+!Sc3MyB{S$`
zT%RfaTow|4oQ&(bC1EcsOW2FiZ+-;*!DXEZ`_I5ruEsb67>zTQ#S``;q26PR6Bysn
z7~^*0<X~Je2Yu0Jv&7{Wp^sUXh|oM@l>7_h_QS;?G52R#Vglw64_t!#VP_(80R6$y
zQ$pf*1tGDqJZ|qn{cDB$z%%Hhp|9CWbrhW2sgGHQxrG|=XiUO>&^uC#>z6n*HhGTv
zuke%MIjrozCg6v@=>h7yYI138(uzEg;W0MJ=#T5>FM-O3ugesVV7&Aw#!fr4;ZJfY
z4)oi{-*Q+`KB3=gFZvvC^f4DBT}szG;n$dxup8mG@c?j)9lHIHkBx=!gDxA<PuKm%
z&A=Gw^C2+-;f7*-(pa9bV;6+PcwGOr$jbwy_cPQ#^76#%mnZs6SElTfCuG_NYxzZK
zka?Ma4j8-Wc`4I(Bw>H`ucGpjZ)S>1q0i-rpX$M7(CZ55MP=QAal{<~j3+R^e8o`Y
z_3c^W(sR+aL-shNePd3Wy{9N7qFXUmI1T<qp5F;7<H9!kZG_)59pi+1v&1j5v&0{#
zd;Xy*A@Mls-o|o-=XyY6kr$63-Q`}IPouo8YqMJ&<a<??2>lLzg`VF^^4kHw68K$*
zaq@pt{(64f$S(mu)X&yhjGZvPY`xs`qcKRen}{sNJ^TQT(<eb^%p<KBLv2~YI)Llt
z^B0Or*)JURN9rHiH0P2$@AzD5>yl5Ll<mEK|40t`z1NSN`Dba<SSC63&$6H1i9SZM
z;>29)b5b8V`SgjoV{t#A`xW)k=^i9G7oO<VukXY-C7J7CF6O%Fp3^JNtRBA9HHrAp
zNBjhD1of#sL!Fxdjv$|tn=n3i#q60g&kaY4s;<7Sd~rBhFk#|^{PEFgHale~^i-q?
zth_=5Rs;g0GJ=9I%p><QML5gTk)pw63+@hXN!{9STiU$?>H^MPR&&bsLHG2(Z=fY!
zD0;Ezm7>>+4qGBo^xLABie4^ywdl1X3>?CcfrsBcrf6`e=x(c|E&yurZ;2w~#{=_T
zhIHE93-Ney1FJ_S9MZ9(!N&t7cL(bN+Xq2KsOCVU;=_o@flpoFF6-{#_CZAhUnn9?
zB|d0QbIO*~ZE2RccYr0f_CtQ&HxPgK^vB;TI4F54O|L_$^Fk4&US-C81B+iqYG3#A
z;iaNii~EYw8Tf)T$a$gI3%M=LdGV!JyHB_FyQe?p>wN=}k1xOWdePQ?mG|^_?i+aT
z0L1??K*%qq5CdN*?vdgZ1_BL~3KR`O#)F8^_YNq&Z(xa+g5rDn&jUfq0A=7S_<N-o
zv6MhyQIAIV4w&;=u?z|+aNN_M6mnEZ0+7&GAl>tkCx}kZe$!2LG)26ZG;ul)%G&%%
zpP;JIhF1`hOp^w{f*j?e>Txy^tsY#3Q#fAM=w)(nMsmW87LnpGiXWC|J%0*!dLZdi
zIQoFSjKVv6p*`}C`{2c_hw@f(Dh>R65J>-pjH_!-@fB{$S#8Ks{QW-os|Nl>j<=I>
zl@29G;by!Cso~O$;ve$CU-iK|eeiT#Ho8&rLq2%e2aoySO+I*w55Ct2Kj?!W^}$nA
zMd!q+bPe;tANRovQuW;FsfQ{3`^Gm6=o@cH_rWvz#&`PQ9X|MrK6sZ8E^jDW)GB`#
z(v3zl3g6&^H~Zj+eDLEw_y<1tupyc<a4LPmKKOhee1#7l&gxr!fe-HZ;Ke@pJRiKt
z2QNCUZ+-Up;D>zh^wWFGp|hYcDl=1pc#_zc2@1Mv^fouh(@)A-CGk(nahXs36y0Dx
z8zvyO8Q1T37|%1V$4!hk8`shI7++!Vw=$k))T<W8&Ff+p<I9aC{4d5A7}xtljGNc{
zPZ$q<PCB~(e#ZEthW<}8KF`qe8ODzr*YB?wA8qh4Zh{GjlhW?_`!DGXh!s7MbiKs5
zx_>M0*BEa$?q6>(zSF?pW_+)KcQO8?fxpN2J_G+N<7q~H_=xd2MtYG{n1C?v3wTZ?
z@h0QCNN3!<FAQbeY_CQ#Znn3dXWVQL$1-lVV_#sr#HgR=FrII;OOqL|H}pS`@offP
z$hd1<k{2;P-N3)Z_@f3shw<G8{$<9u8TeI<uQuATGR99D{3^y><NEq0<BuBYs$sm|
zkbeW?>BjZAit%DYP95V%4gPJ6M~wW(vs_t!UNx@MI~m_^;9D49VCcD>@oEF#rQ{pe
z^^X`g?>i4OZr%qUQ@C+mJjHm}z}p$mGw@$B9y9Rg88`21|Hb$rL;qJ8KWgOv5ys8?
z*Y6l#VDP(?oS{?>J?1^en=*Saa{ZO@E(8CF@sNR|^A8gcHAcMs7<UXjo$+}FK2*sW
zM)G>h2*yVn{LzfBFz_=OKVje`F(`~m%h3dd(a8iJfgJN5OZ2D@{zV@=27D;;M~yF3
z4`3GKY8;Wry5Rjv$q#Y6eUk5c3w-2P_~6TZ@LPTG%|7^JKKRdl@D3mRmp=FrAN(U9
zd>HbjuW}jhgJ0x>f5QitD+MN0cnbz9mMp3%Us_eV<eF=iRV`m8O7a7qM^$zC)l1}3
z0`@9hQdt@E{Ale%_lk#wcvqiZB(JQlD!-=mx@CClzoNA2hN{J}2^C8gFI!%-bV<#G
zd@-S9ru>VNYa;#%<X;RfJd4HuWIB&dno3SOpE4PLQ$7Cza!#W^0w+!(A<@a+|0KAl
z;2;jUqw<f=qC}38UzDU4Q2YhtG12><<hdnl5=oCrO0<9kPb`o^37%L$!6z1wE)xp~
znIa=4c~eO1DMXzjNs|ejDt#nfDmhW6ZK~8n28ia%bWD<|lWIgKk;sXY<-b&8;$*4b
zWXYT?!%97(Qje(ABPyetIN8&WRGBFA&Wm*t1(gz|C@Dz_nmAb|%S*Ei5F?FaGTl?i
zIT30uLdh+!s#q=-eQWWe@*1udpS}jB0!uuWaMe=FJG$49^jaHJUVYuQi>oS2wE*3V
z6Z%}dLaRq0x^~%;rBJC`q3%FED;ngY1TS`N?L@a4y;hm{FSq!_`V%GV6RSY}&T<k@
z5I26Xu3@L@-l_RA6Q9RC2liLlO;!`TXTVl%EkRHE;0GCZU|+><vhw8pP6!9?uMxmT
z&3~EtsQIiy@D?aC><ui3^x2}}n>E}UTX~+_7$-Sj*Z3_O{~is0P~(3?!*~1eAJzD!
z8vk)0eq7_1Y5Wc!{);}in!hvijqX!qX5%Q9F7Hp_l^RCQ70f5D&R4j}SMz;lylP(0
z#MS+{5c10@LN;9-ui~qEspk6>uIg<B_ZKn_j;qPRW;){}ze0hqOBkO4_jV1Rr}6c)
z&1anQO}`&1T<4Ow1<a>B)A<TF`8CXUz^}rg%nA)xX(m5c!>?6j*c&u_p@ugx{v}{7
zec+}(o0(r?@D*<IA7#GMQI(gP-=uurp~d@-4`0o1I^bTX@kge3CzSseYxro!DW7$`
zh;aw_H#L4S<5VB6*YE|5lb-i$_(BbTK*N_auF}h=!qxbCyV0!SJ2g2w87F<JHC)Z-
zl0G_rpT=LL@zp%918lK|$9NoT>bZb%lAosWm;2zW8F#>4qVYE~PWsnqcr)W>ygM~H
zwHkk~hT|RNZu2<fRPOAQehRPbW_pe%nMCzL=PTUgA7m2c!!AwM3C0~@%QXB0#?5rq
z^z%r>SI=Y!S<Se~Z`SzVA|jg>#z`N&{_NIpy*@n3IOV@Lx9JH|^ZO1?u_mqCr1kfl
z*D%7>3Gab>0L43;{m2^0xC6I72RB;7zf43nO^lO1Hz^SIe#U3RP0x~)QTQjyh51yD
zI{zT!Ccl$$GhGXKsOdn?N);*WO2$dgOEkP;pm*ZHTBYGlgZjqfJfA}R)f#^v<EB0b
z88`F!MaD^=n>9H_JWoOV?`wDo<HWy3!^<@O4>Y_+<FC>1)f!*#FS#1u)%fbU4(UVp
z5@jA_zDloZ|MxTQfLN!<un%hZS`AmvflT>ozMIl}na2Ns<q%)jKZWOm2-o|2>5P+{
zb()-z#@DaAks7~V<E!~=lCSf}YW(#YKjI@lU*q4Z@e6$T(|z!2#?Ab1V4TY8K~28G
z&FiR%`6UKl;U<5ZCjW<;e1)6*7EQj+SGdW4l=)_Tc${%Fy{|IvK$qJzeL6LKgN7ev
zobu;?^?{rEyu*CTADypolYfHwrv7T4eKzD=ti^kh`PA;|c!=kDNe;!WjG9*^{ePrD
z*a|oG8Od@;hR#>G$seo5OK~Vei-^gPK9@+B_otp;5`MpiFJwMlci-0V4Vs*t8s4Pw
zzoX&1HNGzYaShk|X<Ztw^Pl8SIq9iizXut2fPYtuH=XBUReeigzX;<bN0*P~-O`wN
zu@63vamt?tFR1))VBCy%KjRKqjT-+QAO4FxA5Hv?I-hYfACCLrANb(u{G5Q~Z_?!F
zGfwg^)o}Hkfa=ulK5(-h##j#3Tb-|PlV7C8tJhogJb<uPEnbD2a!Ry#b-uz){(PT!
z7i#!rTD&VXe2#`=orpA)Z<lMhdhYH3YtrynnNRiSb`6j4{667#Xm~Z_X1(g-&YuJB
zof<!#=fz3RW(|*E@eUaWM=zXT&@%Dqr-2CL;P@Un*c35N>AgaMu;*y_Tn(SE;qx@S
zTEq4Fwp_#Y{;KPPH)yzCF3lSL6)oNt4cFU~y&A5|>F~k#Gfw$@mnOec<6o)a$2DA+
zpLV)OqWqk%@m0U90$4u{53_qBZ0<@+MB!6$rt|ZquQ#VMfZb#ju$$y?(<>2$lN_C|
z?s3-vA0(OHpTfV1(}5bU%I+Ls>3!f7k1nT-ImG8%t3(t|{0xbBe`<_F`NK`A^iX|!
zO4l%t+6$+2>HP09hvMZ+Qz8ndc!x5FtZnS3c==M3NDJc>?+A92)xvIySLg3#oaFE&
zD3M1QCplbYdf_BT=XWrl<ZxAz$UeqN&M0=1rErp?^Iv2R$x(HWpjR0uIYsOyOW`C(
z=XWuO>dEKXO_su`p2&Br*zY*ol>cY2n=FM>{_Ar7#2k_zW;a<1C;7UZkC;RJGucg+
z!ik@w;aLIig!p6oz==Ob!?T%B`j6`aC;iXT@EnbQb{{zLbvb7-pRVJG##i`cV7eUd
zJrE&?Z)<#o6JM9(y&vN7zo79IPJCU?Bu$@OjjwRx>vCo=pYn6O##cDyXP$=7()bhl
zz=^NR`8x9{|0&JND4g<>{FGU&$)`H4jKWENRDrNpYWzuk;KbME+^q2_Ze<isd|l2O
zjZb-}jKYbp%lVGRr)y6cg%f{@0%32|__024;_Gr+HU7C8U*W``rs0n;pYr~^K5$Bt
zE=N5#q55#X##cDWpRVE0X!=~(2TuG8G~9cC!>bF0ec+^zqu~=R?}YT3*#}PY>E7IJ
zCM>&Y5%#JR6P7Js>e&mGYbhRoPgq{HV!4=5S-!knOt^a4GBIK45;~bsv1~a#Rqi>%
zgIYXCUsSai^u>5)I$`#uUmm|~IrgOkizHPoEWKuF`J$@ssQ)L9FalFn>U{%}=g=RF
z`5tFjqCJ0f4@tq{^gz<}IA3%h@XeF%=@d^I^VGeH+BCw0j4P`|zX>T2HgR=BZ3M~u
zXT#GqO)`h#(9Il<KdggrhxlCCRQLJ{U&G-kZ*-&1m0bc)3eVZqO<nF2ze>Ndzs%vg
z1F~cDXUspU$0UoZd$*Wo4nN=UQ01qHR15nm|Gh;XqnSISRQ>~SsPL-%`-+=F9_P*j
zmX=AAj{hw<%<v@BEPD#z@Xg=`!&BuOamY}g&!9-Wm<k4Fxo36%o8iyN5BD<qspt0o
zd-fijQsC`K`q}p$TzH8W{Y+SnbUNtsQjTEnL9BP9JqBs7K<uAqH<xupwulbeM^Ek>
zxHaAuxyS9G{q*d6@noPLdtzZPKb!U>ybpe~AK{v?w;$nQ?3F<K4!#QB)MQ9F=M8X5
zl1Yp9Z<?|uZtea_T6H>XayPt{DW1Cvdn0`8Sgkl3Xw5SYZ;zg9h5PL^=b}NgM0$#A
zJ%@eY8n+CnPXFL(>o+Ho)-tT0#r1aj6;|9r{+}L&J=+p_6R=(nYaR2J4~$zGaPNj5
zL*&y8C%6vVTJ6N)VsEA$IIb3F)Ru{y+I1;s)Yhfs)E3ppY6Cb#`p^M|4RZ#(DGn_g
zBUaVAh3#KxgS#E;^3TM1TcMM;J|XfRz~R<G7LNNN>p0dNwm{xdaH}A*95UPCE>?1f
z4I5CndCq{bkU?@gu>QCm>x~<*mXY@6qCHlQ4TxJ=S0zhnuQW;zrG@mbFLYuL;Be1(
zVmolGMSL#es^oY`*I6o_Cg|TZX8^^~etuDmbl==BZh5*F#g5g*t<c)Ib?ctw^%+=;
zTr$K7Uh>#5;r^F!c4uI(gv_|rg1sdI=jF#N>^t!g_HGH(rvzGJcrR*0C@oBTBfzcv
zr62fW!<@A6d9m1sjiP!J_CnYgg`DZkyAb;%AZ@+mT&T%`zw`rN$@$qVQQb&;30w|2
z1@MP1)>P)n_@K+wSd8|;2*Qsz<e$@!5|A>W=Qmb~>gRrsHM9M(2O8?u{wIcF&l|@&
z0GsyxqWu$T-!EDRd>QJCJD`72R~23Ooy+#no^TJ7+X=cT`v&N;@o&k}-KjXw5f0VG
z)`y3Q)~B#`75SEjeI49P*A7n|8KeCVe}_E|&VtN3?1Na1`u@V}sOzZnFPuVMM}2>x
zAL=^Q_m^l-D%AIB{hSV}@3>YPHuRfM^6&f%b$t=y=t5jn=l@;T`>yNtT-W!bu0Lng
z^$OJUBNOvuSECMfp6k^gs`IL@9|1<3PUdviFVyu@T3wfNF369)0e%_kGx4a-OF5|P
z-EzRWn*CK>|G8GzE1<`bDd3<kQ=DFWUR@_W$-k5RyLCZb-@UG<&QTjpb&%?wv&QOf
zgLfya>aJVRHd6chUZ!Zp;oJUx&vJ^m{cWW7_xu#u{(f4Wq<vPX&7!vVAFY!qXuk?^
z%t*gtfM^KG_BWX3wZ93pVbuNxaGogq$J*aCC3o1o0b+A#Kni4#+<&C~twY<p6UREV
zy)8J-Oh2P`OnOdj9k;{O=BGB`-mxK8d*+5K287eScskJjlJ3<0db;<tzp765)&2&l
zPNE)B`>X1JS10GBxhQL@2gITJN*q;3TyB4<?Nu_UEw;G6b)xQhbrNz=C#em86PT3K
zT_>^g3FOH37j5sGsA~b#MQZ=vQ1uaUQr#t8$nQ<y0@xeicI$$+8G95*Xb)i8o0s+h
zM*l+Y&%3InXOCX=B_ilQpbwI;!t&9UgYHz{$5rTGWMB_A+QWx9N;lGr;#b#$pI)>_
z4_%KgbZdpXq&-=R-(0`=2*1>bGl=xkJ%{q0?l~&GbYI+obMmMArz#gOPTU(g&W#Dg
zc>Ui#HO}X7{ZLy@{iNiwPv`3SQGvL-+cK>zNH=Oz$X{>AOx?ODKc4t#?j?vndCuSF
zQh!R_Uxq;+v(J^B{h{R0^-g`QJ~>n0pG)lmai~q`lXK@unIHe|i+X+0p0;o(+Cr}{
z`u}cQI1Fv!oPVk<{49OZRMqC9FDlR%CEckn>em*2YG1U^z9#O6sMC5swWn{1YYpw!
z9^AuU#{F6LLs^Du3sqlJzdsAKg*(vylx?M}`n#Si)fUQrD6S!D3+eina#UZGu7B!_
z()CY$QLR0qYyaPU(SMJ-Xxydl`~U8X{@>dd{l9rGV68D1P=r3HgSmjKwYh+^nqsv%
zZmiaEKigb@s*}EbQS`59T%-C}YJ5cfEb6Dqz9=|)o2UAs)aG{gN72`$_En9uRQucA
z7lkb9qpJR@>|?6FDB4__3y^(2w5_twhqhMrzo<_~a|E(43cRN;>h)9q<$Y14pZdZS
zugWLc*F+x4J~HE~53TxM|Kq+W-)rbzsPaPgMf<oHsW|`L7ybKv(F*;0A(;Ox#~i1d
zCZ|v59lg4vXMbL+$KAEIJwjt|#Y31rzX?(i$2VOt-@39TZa;>8^Hv<GVedN@<(OAp
zg*jCWaOS!5Q=F3ig&3nJt+%jO8s1-^{duhn*sZV|@BsLxIKGpBZz-IHdD7PJWr=5o
zV-L0A*c((lV{K_!XAc4Q7`W50H!bahy9wXHz??3=vCwWeVlUlP>|3n%IjhDTLS^t7
zyMfYhZ_ruM8Mo(S{FCCkRx0}MH#?qQl@%RwF4*l#b*8faTj<;05@!E(9X9RL>UO3&
zXJzsJxn-UoeT#zjY~6`tb+ALGGyIWN7XA@-(H@x{5c7rw?#KSMJFrh~$w!zw#=e^~
z&>zdeu@-yM)?rWD-8+KSU2kNHjo3T1@i6vL%!ymi{RDG*<=Eq&=G5rGetdFX?MTp>
zi#h1j&i9vA4xA;r-bh+QkXKEZ(`ZF_`o7HZ;i=U_FyD3u=G(6L`7k`A87}6@`7|%@
zXdKv5_{<k}1~Bi7slNKnvDyk8Z{Yk0j<rpzYRel|)vi6VCa-SCnmnux&uhm#MF-|A
z?yp0?Cn54?fS-foq7AER--Q3I<!kcB)UT>tm$N1>6M5HG=j0_4PM-Ij27EsP^T8qL
zF!4Ld(uN&Li*)*NVchyPj{9*BD4gNMl+2@$M>2~bn`EMop0^Hh)4W4lVNtB1E@|b=
zD2mDc$Fs@nX%EUp*mIKR(W}8}d004&n8Umu?bkz@acc+KXlp}CAP@WRQXiMT$J2y-
zS`2;GqCHznZS7E$1?CW5{+6hI2>E|M!tBPp!w$5;6n4TBURe6xMbi`5uW~TLqAg!b
z^9~dia#o%zs&`X*5T+Gv`wqlUVJAJ0S$)L2Y%u1dpAU((mmwVZ@l5PZjJT&^PWRjj
zv2Z->^s%nh_~k$~?Z2IOSztcOA5WY*mo>i`61y>n+i=92!xcd%Z{*Cl)oje+?#3K$
z!@H<M(3AFKZowR`J85K$u7zh&N7v)f=hJXaL~e>>{-O-u7;><mbU2Q-0dpLf-&om#
zee=Oz<zha=`TO%3dHtQ>`2Nrlb=CbL*MC<8l1Xno>^Cn_xvr<p!L``k7F7m&+@!zW
z7Li`mrrZ>dM@FG8B`^E*c@N4z8Gn{*i=a5`D$Jpe#PxFSpOZ_HZ=d?q`3t(f#y_1Y
zx{#-PvCr~G9IdrpIZ#;#aIb7VlC-F-seT<pncbh`<Wd>$h&s6m?6FMO^oReIygoVN
z)8nG@-+Fs;$z`u3tz-D+O4t3TU>Ax#%Mw;24!XWW(C64$R4<%h`@<LHwk0meefHNE
z<UVucg51P=7vy#XF3b&LzLxaq*PIea-JBAL<GU;m;@E&=U14GFhRpaENY@7*z9o0<
z<%PNTCvM5THK#E5`@g;=w>|U1+_szxbJs@;b5oj90=tgfl3N!jlrnd`cMH;UVeYeq
z7v_@u`pm*ylCv|gCYRz`cf`qkAahOb+V`B?`*YUh;`>awJECiHZ~e8C`^@DR<|fK7
z%<ZVXFt@$#!rbo{uE{0cC_Kf}cDtu{L%_?+^*ai4cU=w{4;SX{C|{GC*l}TQeO+Ph
z&e}CH&ULpJ<~~q|xXPjb?Q3#FXQEut&eYX{zvGr%YIBCXajG;q`_x>Tr&0Hg<hqaM
zQk`4T>DBq|8$O<$y!+$1RQD8iOE?}$uK5`C!ZY+tm!5T|;y}g=dOrXYdx9S4n$JN0
z-hP(+2v_KPS<>TG^$ZwJEKVI<R!mzp&Y@ikOR&Y^eFN_f&e@i>eNbHhdod#Jfr+9Q
zuoa}Y#p5e-<4WvgS%eKE@o<^sEtG=z?|&#S3^9-E^1AoKbkug3q8|S0eK$!h8a$`?
zZfk4m{JOwB{hTc+7Cx0T=iW5uz5&iQ+M|+Gr~NixmwQ^iT>J_tKX^`E;BKoqxFuz4
z>OK9o^}ja_^cP=!4ck<{<PhvZ2>vQ|mMnS=7*WXi>03I=a{5>9=}6_O@9DT<pqF9P
zQuMW_qr&HVwC;a}SE`5m?4$4|AAGM5uGWC52<ROT(zA*6Gxb!s;;VR%GT+42yY(hM
zI!hs*t>jGi!AHs_y(bL4?@W4*W?bcylA~}HLgD#_9EDf<;LCmR6+ZYjAAG+LK30FB
z70N1o3ViT+K6s4}9@Afd)#R7?;I0q;xDWn;4?fy>r$(hKPk+%>(<kPGJ3jb41E+HK
z-!qS2tRWJcGHFwb-P=^j)70*Fmy|EP`$MgHnQ<L0X1tRce$6bE?tmCZHv-MvAl(76
z--vn@<HZKwWxSqlGMZU0-2q|l{q}9f)ml~+WFzBUc(9?HI~lLe&{2W!+>r^076ZSR
z@%idTCC}A*AnZkC@PEL3wWf>dVmISs(-q>`4>P_K-D2JRhdfCYVI73~33(n6n~nOP
zVBFke>=%ridp<qK_~Rp`qx<hSj4v?i^Gl3BIHH>>9fuh|Xj~U>F|O7;s8GLWyqR9k
z(#&z`4v4V^euD8LqrLqb<6W8za(>LX+RH<E2V}=2Ak>}?3h&4GNJIW0#v?}k3^Cqe
z=sBG6up#Gk#$Pn#oWb~Jqn?jt+`NC;j4w3gPhh;-cuCWHw>lsqMpF=De%!dO&u6^A
zNLL}_2Mzop#>Wm*IqBJI55K*}eNFA*chKOgJ^WUnpQanNhhKqly_9miM~&;QlJT8}
z{BJV;sKH;t_#A`(Eyi~nxZ1<75)(DLahV@Rhgvu588`3m-aFR;al(+_#QYD8cIJDG
zKWelOTNzht)l}Tu8IK$JzmxF=M!Y{{JZ9XlA7UJz66j@q!uT8me}eHc15Yq+-Zy{2
zxOty_j&bw8`5VT~`{PTDoA>d<jL$dPv9}oSF!0|qKH9j?zQ=f;(eAy^_~Qotv65rR
z56b&xKy(@HEjFu%2?(`jOl9^E#$Pqs)!|BxfsbN*tPyVx<LaGZCI4*3PZ<31jK>Z6
z6z(6{i<kE5Gpe$TdXf)*KJcLy_JZa5^D4Jzw1;0`a=zk&`|aUZ>%-^G>Et))S9b4J
z=bdU_J@>wB-o0m?+F-2D&!Q_&+Pn@sob~KRN4uWM{m@`X$r1H7mV-;~z((7u$sOI~
zW^_}@Ge#S?d3&Z2Wh(93HjO-Hk}K-%@g}!gi%P%fRJpm`6e36EZgJ6=M7%B6NSfUE
zjbf)w+vIL<auYeJ4^bzP_(`4^xa5v@6D1}?%0H<tg_Zlt(b+WG8BRt%jj*XwTZ%#M
zrswUpCv}MCQ*_aMnK~lGNX|45q*$lPKZ<6W3{Qe%bQ=A{)_GC6wO>^3N*9$|%|+#=
za#3&JKDl{b)DtZ?`HRM6D&+=x(J4}Qsc2O0_7|1;9QERg$sC+Ykw;}oL}kfDW0QM!
zChS#rO0Qdd-EvW(?J`FIZC?1P8_>y`MmxqCMggjOP{#g+urJcMHSBg^&(`p5>?XdB
zd;L%$9N@*wA?rzY6E~OLWGTE-!^j!ubCRL+6>jo7m{0Qa*iDwgP5wUSkbIr5aFhQI
z^UZuyd&4>KznD2>os{jgmk(dmaBojlAt>J_X!ty|A7mUHI=;;ZSK}k%e+h>&YA;ff
zU!p+RW4V85>YvZJ1I{HHKgKx8(d9R2_@x@ZS(9^_hCj%-S?&rq^LaP(sa*7OSGdXF
ztI5~PQQ;<E?KNhWyTVQWA(n6Eo15Y#(t$O{bISjBH2iW6|A2AI&wLGcc--n3_y!;R
zDB~n&ye8+k#=k<tNB8qih_B;$j61;RdKCFTopH0g6mI5^!+f*66mIg1H2HdYDcs~2
zYw~rz!cG1{=3m2UN@06gf6w8-n#UMft2O+~8s5OTna_tbzMlW1)4aeAEFIs>xY9>;
z|DH5(rKeipPIA72LzxsFAI$)u%THt6l%H?lO8#=j9mu&-35UHxldrcws|`6y|D78D
z98J!HKK%U#uJj4<c+-IlHRnj;hA~d*)vuS)8XhGgn|#J+<6Lhy6ke%_Y!@)!thWj`
z`7uqtUJn&+@~3O^b-uz)zQcS<??g@4d0M=Bx)g58nXkpG^A&FL7ijWzJr!>9%QX2q
zU*RUdlKGUbNm{yAXz}WLD%_N_T8mfbE8OI}ntWYPg`50(O}@@oxXD-Z_>`^!EnRB<
zUtNzW?AOKesXh6s#y`op1MDgdj|}ooh_B-@#vR~a^C<Gam~pfIE8Nt-g!yLuSGdWa
zqsiCvN8u)ao+e-CE8OHSXTF(l;dC#N4y*;9Q~qzz@ULsQ`bG-n+Y}A&)c86c$?!;y
z!B_LQgnt8vGTWF>@=Fy6doSar{!eQ1%QSw6CP$aA?-^IF@y8DKP8?WQYq;9$%`A6?
zoB14LIcB*l+~iMZKFQb1QQ;=v(d6rVg`4~`=9~GJ&huTA-U=<f%Qak=RnIt;OHAYM
z*Z4Y)&vi=U;9tk_IZ|mz&ScLi|5t1LDh+>7<Lmh28egT2g6(76EH8!YX(Il9=9}fE
zaFeh0AT!HL;U@o(CSNZvg`4~n%vWhjAthn(U2AC^{9o%i<$oGKA0d39hL2?2%;z$V
zujl`RK6setA(cL=f1>u}Qn=Eyf%&A*b(%hEZ!Buh^>{b?$bZtnm3+0q56S<gCO?Je
zMM=Ki{-iOk^i=xiYy7F2oB|*Id;?edY-8MkjO#TyEt-72{y(VU(=`5*jGOgV;guSf
zoN?xx^;Y2~zeAI+*F%My{C%2yov(0{zn}S(|L1DDc53nF_JNynj%xAhe1)6*cQpCB
zo(eblU7CEIuW*xpocWZl^ECZahI=Pwc`4kKlg50rycBNo(>3|Jo(eblAx*x{SGdU^
z#(YZGbS+&Y8CTa!3j3+|5U4$=*7yaQob?($opItX(s0LzU##&LYy1)){(OzUMB^{e
z__u0!jfU%bs`n+#^s4tFD1SC+a$2-_YczZ><5VBg`@qfof0X%DA9TLLP5$GW{8UZ8
z!cG2@ntYwFaFc&jlg~}Bj78xl{~b-f&R4j}S9=svetuiifAr_P6RIb*8m`_aA^vwX
z+|l?;HC(-CV)Dy0{xXfP-aj$<^~@*yyBdE3;|{Rp8veM3e@nxUYVqEn;c5K5$pLnw
zhKCua`lIiWq;NAo$1<PlkIq-P$ya+pn)Q6Arq2pZpA#B>lZK~_^iHU}=uoDJaR-2o
zyFU1n8ct!9*{9(v6$tyF5B?70X1Y#jeD7TqPuNMuNk_+n<bNeUFH>?<KcSg%st@{}
zQ`>y_PclyOS8MXs^Ei^Dw?FFJWyCMi`0Bk2lYdB)f3wDaQPWeGe^le&qVeC+__u1f
zzGtN_=cLA`eUZA&>7L8Xx3GbyaDW2F9h|RWjI0>rlwOAbHq#9`s$A3_Oe$U_zeSVp
zDj~3UYV!5^`Jj*deVQEox;}33RlLJkF$ZzgY4MI^oYF;yGGWF)alJEMsfcWkW&9J@
zyAMCYIK??rlcnBUqdZuv;nOvKk%rIF@L3wpupB$~)9^66C&C`o2R;>;&Q~(?fv0JF
zg--_7U&G7TO=An5+L4IDX-vRVp%Pbp6&mC6lz_z5y_@vpp-(TI^wjy^lfK?uDGzxi
zqHxlahuRX~%Y4$4hi<)a(lf*yvYuo&>B(JAiNqNvJ-JElg_E8-{{ZtTUcLoMMBx<g
zaORNJ$!?05n?{KoWt`%Tu$wG}Q@lF=ugsx%`4W+c!YN*ELL~m7{P*^09K~+36kY(g
zE(fnzNJH|oJ*W52ILTLal01SM|MMPG{wtjLx}5&Zr+RpX##cC%uPz6xp`;;way+N}
zS2)QJYxqQse`X&z@pU=gdtP4tjM4ZCr~J|7oU6$ntML_1@^v}h`(IvtK1<^(oYJey
zxm1&Xw#HXD$secT*emgWv}fY~scjO&(3z5^w@H)?hyGy9_jbb)?fIj+n}WmXfu!q$
z@gDFFSK@&<rFc{ur>-5!Gs1&7bhBBn%$f%mLpPEK7s>o*!_zY!lBu_WI2FwtKV1*H
zQEis8sV_j`P2N5ZAJzjimmrmnV$DssGVgrG^nV=zNS37nvaQ0GsfZX?`LF2n;pnUU
zI!Q5@FnPjI`B7LEUX@>8aTC9o8;daCMCe|)zK`$&fSF|nNG#;Zk8c|ZGm_)G$Dw-J
z;93%<88f{61mfb#2^ABToD+ueb~GfHa`)cjaGKiUF#Or~INW=tr@%~Dj&wR0^HPpD
z!G_NENF(iYD3TVfZC%k4w^w34>MAF0uMT7H!cOdG7?0aGxp8|d_C9<PoMzm?Ucu2H
zhkLPbehPa84w9GA-X3WPVtpOf)a?TIt-G?t`~Bk9uJl52(cK|&7LGdXy_NnZc<}G~
z&*b&D)j=PGt3sF}9Jk^%=Fusxbz0b^+@F{Jdoi|Qlc+?08%>`Rq_7$5+VN^G*0ENl
z$DYFB&JvEU!|q^vWGCqWKU0S)O^4gy|2A}Z+ck8kN{eml?^@HauCH5%?<Omt!#Lzg
z1bGrcUPX{E5#(0{`4vHaAs>6zHa1bdx=HIM=v`hGx3^FpwF&3;cf{Dty`mCpn8w}c
z#O)Q~xJ|l`#Nn=>VTcnPiMZd#8nn!H;C&-Kw*7c=W*+ir<JZ$;n_k1!KbBt0a8Pyo
zKDwe}ek1Z%uK)cD_AYwl)7Lao8l%W(iZ>s5E8|9bp<e|0MN+U1mpEgggUf5Z$G)k#
zwe^)qYh*0)9_b!e;mZ7zy3*QiTEk82yIWqg9ITI%>%3PZZ_-iTk0brG2Kr?j4H0X8
zs_@o*Q@nQrbDJ&ara0=R8@F*;%XQwEyE(lf5V!8ZTI+3C(@pEP8?cVM67f^Gqi?j!
ze0P^y4q-Pt-dgLUZSCGV?fI57`kg>!I_lcCY^!p^;is+i&Zn(3tbJY7{;`#o<60ZT
zZC1u(SStw~MnTUZ*w2#k{RcU1k?#lEBD;<xB0Cd_NLO>Vcyvc1@<?4G^5gPE<l(|Z
zqy=jvLub0yFOGa{KN~n@Ka+FH?kGHEA1Xg(|EBJg{hvEd*}qPlviE`4inWTg&amNo
z*`ncg9KXi#9FAY&cqUt{dL&z<ZA_Ln()v;8&{|G6IVZU1NHWrdb)5~VacdK<r9Cr8
z%Qd8{p2!v_-%3_g@j7&wA6AF$RtB6@q_-aVaZ8x<V{N;)F1;q;j6NzVcLv3{{Wvg2
zJbO{r7%@I5&UQ0J5v@-rIUP9P7CbCQB0tW6oeYi_JJ+PgX#cSc9E%ZV!kW>sJ(JR7
zL)JN1d+u5t+4Kqo+8nH3eQ1%WBtMD2mn_{6_kQr*UBVfO^|Z|oB}-Rydh3Z-w&415
zd#@{oUgIEb&Q93GBhKowp7qAhKnGf93|;5!hhE7aCFfF_sr-`fpPEZyZi;*1R=K@p
z)0*X@sPD9HS*=y3J;!9&+fL2x&I9P1yfP_et!(jR&~=<_PtLvB@%A@JCO^88uoa!3
zj4yeQ$Gatr^`XD@u1{)*(XPlgnA+K&;YfkbbS+T(r`qG!fKxfR)d2@>k4@JH#y`FG
zK%o6HQyka&qt$NzVMd4Db^kh>!q7EA`yIGr0}lD44fAkZ6Vyhz;edm-%pU#wY?0!+
z*10I_*qFGz9`#6ur#jRrWF1OD91=tQA${F_!f8O6G+sXj`=g=Gph@3_I#a~;CI#5o
zG4V(q>ee>Yk=2mfkS-Ra<2)U191-`cUKo-K-MZV>6!=ryi~NyoF15w7oZ`LPU8-Y!
z9TZlV_i1v7w>r#ac;1IsQu{8;c3pBV`6Yjk@|rz!=DFcWQPtJgl`jrQ3nomQkUu^;
z&1Pr!_)=e+8$+T-;^;OdgPpSGptn4g7OT?A)_c<|@tR!6+Or@Rg&RSjdEEiwC>|Ux
z-jY%maA=LIC9r-M3us@$!dD><;c#(u8(Ke0^0CBqYwEWCSV4<rwD+bVqp&1)`#?)B
zpnbWhXYH@Y!&2Cnv92~z^xNXW0x~|io_71d;@1zC+&92jJnXIH?IQ%J+Xv2h>6O=B
zKa3y{UaYUrEqZNEPnvZPL_lkDYmlJ%dMB!Aumb^GQy1KuCX*)@2V+SvR`^n4iW4%D
z*NT5D*8@AI{zX{%EQuPeXb`O>E_nrcdr!Zjm%6jibdVbV$~C~$L(|s)t8~+tr>`}@
z3eP8h%_#g)4I`(*)tFX!D|~^T&+w#iIq9SN^y%!c_zE}W)EIIUAN{{xM&S`^Q8c6Q
zJRf|44_@tq*ZAP8eefn9e7_I=fe)URq9qDWm9B^nzS9S9O6^;Y`rfSaRr1yMT}@nl
z$JNBsjlsB*6ZXL$^uZ7M;KzM%r@zvq$L?#*42`*I)Y)m&9~w&&zRkd=Fh0`AxAPb`
z*AP04ccF3A%`C<rH}EequD)Ze!d%99nGvtoh6jZDKCR+^mHA_hd?;lc_tjpeg7IGW
z1Yp-PZeG{dGw!Xylqd4fdwv@bFZMvv<vp(p^xmrhJ~v9=fG9BP`D(_^>$HyXbc25@
z<6{l{yNthTlw%X)=GxTnG42?0wlZE~;M*BDzrV1P@p(ph{gCkv<9gh~c(K8Mgz*Ii
z{y5{u4LMISo@eme7^mKhZhpb|J_Fy+c)cO#*NmSu;(dW}$G~4<yurXb8CPon<SG8%
zVtj?c|8K@)hCas_FE;om7|%EAzc(ix5HSPCr4EBV9Spo5<137I#=Bo(?YDssWxm-i
zk@SCLkARVo6QV$dIok)%^TDIQhaz3-J{9J2zQBh+%Lgy<!D(%9UwU2(oZ?mYk=;0#
zrhAggTd`HOqN?Is%d1KkSBmZxeY9AQfC(iNX+2s2El-;)7sk=jGr7V}E;Ezs$R^Xm
zv}tlJSw5{+qeHGxBPtFlfpC&CQ8LK8fL4<g(6X}v3OkWp6UjA6Mnt~Tq+~jk%i!c1
zwJAiIOk}ylZ5oN1M$6OWDz|BJ`5c9nivi{8J8$7%KKVqY4$+BHy?pOqE^V7w;AMtf
z!dSCpS(Ul2O)g(Uj_{JUMc&G`J{Gg7b#5}3<;ld#UNz_%K2w)V5uX>r*86&M7&rO#
zj61Nsd!z^KWPAqfE7?ufZgxAc>G_i~s{d%p_u3ury@N@L4EqqfO@12JD+m4s%pohn
zxT#OR#-FV5RsYnKznuAn;ClCdFVJdD&Qwi~TH{818lNfC%zRV6w-y{DOc19jGHi9d
zkestM{7IHW=^CfuaYMeEa#!~~2V7mBcbHH3xj2;R(&X#$9`}*2?!%<dd?f^S*z!&s
zSYF@AJI`mF_+B5y!)c8&nL-@XrOW%9?!zzA_~&c<Vjun-jemj0S8EAp0MqqZ=p(0^
zakCs%f1mWfP?Nun`3^8TlzC8-|1|}|-pe@2p+lJ`8J`V6zXdA1QW4pXGoK!P=zN8n
z{0>b%jTMzqxXItA$=CS`H~F2+p8>%OlyKNxj8j_qF`M*N;~tZbIUi|=PkzeK<O3N8
z$4qjt32Atd0%1>Q+$=}6w$TA+md3{%r8Fl0s3!j+jeneR((&sW{sH5k&_Bf^5nrzl
z!x$$%mAx_}8Fv6s87q^o@h?^&?3gD18ya4t$??V>p67DL&GfF;_+QfGY|!u$4OinK
z(*F_--=^^|)$lmuln*pVrOZ(c*Y)qxaGH2j<^v7?f&yU|@Wnv#b2WUrhU@Z+G(1n^
z<AH)Srk<4=u3s0fhU@8S&~Ux}t8uxR|8b44*OLy$DWAQuqNm?M#?5v^;gue(`~ML0
z&2~fKCjUiEzMfu%oBUTb`8r?WCjSKUDWCOrLp|iD^q$kLJ>I|V=bf1Rk&F{x^=*h*
zpz-zVRITA9{^dkuqt?=rd_Di?Y4UY`rG{Ujgut%R;??Wh2F4w5=4$+AP0l<GZ}E|H
zi1BNftNH_K4<gcE&;Nq{9+B_~9EPmv8m{NFl2-u^M<qjr<-gb08^~_5!tAELlFnCs
z5|W+99I_NneL7Y_`c9YsUYQSIH(3fNzAi`gNl4FhBC=V)HuXvM_azigdg}b^m`^s%
zi7TV<Zvx0rAnYaVrgRPI11CLoIo|w;x6US0<13ux>*K6?=1{t_H2wy5Q@Td=fm6D4
zzBk|D#VelzaTtYDyu&m(+t^L%{#+k8@pU;nH2w&UuW;h)a&|F?#v&s%zQQL1)8+h-
zIi$~N8eicgUzgL$94Z%nokt=Hr*hHdsBr=1^JvNR{>lQLP5C^w51jH@=c{o9>G^q$
zuW-^cTf=W;`NXHWa%B`wd|l2;jUQHI*a|1UF6S2JQ$FNqe1%g!gf;wjP5ziZaFTzf
zhX3R5jQyW_OUzR;42zPaw?*XR&>xIBe;k%*&mUb|DL9-SNV+=t;+iwi^B4e&;!$mq
zs*B?o4|=rje^+npx=~Mtb}HAGlFWZLJdNE+FS^cjqv8+iAlzL1-8PLG6{b^LKBNAZ
zad=A)47Uou&k(NiPtm^!M_>7$&lg3QJYgvRNhK9t<v)ox;|3%aQUEe+Q!!qN!wgT}
zX5Ir53;AZEu4}@`7|HQ{*K5w19$z1e_7z@E@GPw=ujaSC{@%M@lPAXt=smFTKk%;C
zK~Bp|DBwu7@Lm(Y>vhLoG4>&}^tYFFMAqY7t|olD;Zp0cI8c`-uISg<{r$K<{xl@6
z7+gpaf@kB~97iYT$Hrk_@;9UTvHEQ(fkZ&$rsBLU6Www6AHaU)GlmTdRE!*&ma{oC
zjlS*Ck{^q`1wMs;lf&;q*p3{L8vxc86}j~YBOphHZN4In-V4cT;_wCh9lEvH;~(!)
z)ziC70`D8)-7(Tjeh<w_S~ud|g%xG={zFHk8R@yn!TUj&e%Vsi5xEEHrEn|5cyFKu
z?-?LGy<0@zAFYF&PD|w0;N6wb+B~tOf2Zi25sP&W6}g#1@f`_#OYe_uAwm14(>pDl
z7hx+g$`9<rPv2P?hrQbAdnFYrKX4vLex$%(=1248X*t7?mZngegT347+ba~7zJWqv
zDO@Vf6UdJ^@*)KorC%UVW;AE^gvFlz^gX_=ZzoGh-y_fuX^9}+5m9dikakLQ1Zf`U
z%#6jH*&|~}Z=@m~x59FZBq#VU{ay#k06N&vLB9V%?*gWz;C&T*vxD-S$_Eu`p6lSF
z3&Br*r=#`ZEU`bDC7`oS-{sgH9Zr3w(?b@%*$O$Q;~N0<ZVSrJZo+$A&G<gYrj~W`
z8y#n9@0HN^MUGnheU9%5XBOTeO2NA;^xh%Gf0W)?S&es#2K9WiL*iEhe-!VsKo;>O
zwiZ~kaQY!$ii6%UA)NA+@*e4xc`n~QaeL=A)seoAqicJ<Kk!+<k)p>d-#xi4IhVpF
ze+3)zwm+JjjdVqlzxe0gMWMc_YFjCRWa!?akD2j~Nj~;n$6o97GQ_;CsdWKM(AyyN
zmc}b3FVo8=VT38e(QW$J&%Fqq_1y$GiXfNX@4$ZUcU#WZR7<>!w@e_F-_*$Oc~b*k
z#anqysf*Cv0i66R-^HMDg8nYXK8~FF1bw}Wq4538Q`Q%Vjyuai2GpFl;wxP76`n!~
z)QrLp`rscJIL&WTyeAo-&$5&pg{vqO9-=EkGYVI8xyno7YHrrV&1bEOzuHIsP9MA&
zSC?*-oH8HW^}*vlc&C9=Tz$QR;LR5VgsIP0q;hzMX2|&(<1GeW#yD;+y-X$J#YQ<U
zWPGHNKZ_V2Yv4;6&o}TJ8FvhPHRERf)G_WF`rpd<K?DCT<K{E!CdL~KzBhKo{^SO}
zmH9gjd^_W6?pL1TZztm$3_ijC$g|u%;D@BF-hV&z!5;xm&rVf6-@xVej1QmMg}&s!
z<b%KMgL_XMy-sECr?b(@Qi8Fs<_$fMY1N}!_Uw6PiH$ZT9gjRG%Ezqq3{`FzN{?UV
zlTbpu9ajm8(z8(c2v$CQ?b-Eo5<MQBM9*O-QLsrQZBnc!CeOtWJ-s;p(Oo~6mtS37
zr8F+>wfpBa$R{!>p4I()(ZnBTe+RZcCRgpX$uHvaH1Rm&Bu5{stM=FAtM;@I{4fq>
zhVgYc0|1Thl^M-A@pBXiJM6=cX#6uZex46Mrt!yU{OKA$U&H5UxUQ#a_s#UG_I(Cq
z(7cN>t`_fD1;XCI_-we<_>H_3?zM}Ac&35*#W>aZ3OD&pntZ)Wp>UJGS(C5x6>jo(
zYx1ejpp3#z{$5SK&R4j}f0g+&Ab64z4!etSswZb@_(_dV^Fzw$&*XIeFm5+VzRn-b
zILRNUgussY@bfkP*&5%`_%tu5OeN#0{-}AldQDD5kzuR$o%Gb}VUxzEfrm2cer)o$
zX?*INDbvEZ$=|K<a}^o(UX8C`FX~-EQ%;A*r+F!5_G`Gx1M)kh#jE$vU)A{fT%-zH
zfw29QaM)pX)3r!*EXsu0L35xwU!{fQ(EN!qs&0^+bOpjzILXoZCCs6+(w|`{oWjw(
zjWVj<Q@r||`$8sAy!xEG!YN*ze*^QcgRS3h75+`&LzqL>26j`tL)lGM1G_0+eXd#I
z6tB*|kNFhuFm{uraEe!-)4!iNls|H`$>$2E{L$q+#Qr3I1iQ&nILX)N=$~Q^$=Anx
z3Mcuxod2)S1$PEH!EA+lpUUSfDZm^94|C-Vabr()IDjLKc`EOy4J6!%rhC0A)4MI=
z>Lw2-B=esQ?>845rbNMn<$uqgieK4&bHNUW?+NL-R5*oS1_xo50@zmJ_Zhg#KLvjg
zj=u6gpD$8}1ylZ0JEg*_{O>DnlAXrorpj+1Fsk!rc#>=8Js>fX0>G$!2fVY-J{MfW
zjlkcV3qB_r!@O>f`FnH0(SrQRQzZSfuls$FI|wt82uCKw!Sl{^Itx~aQ`Rl<xLotM
zAL}@u$GXmyPQqS|wV7KnXSyvX9_hkyLp)(0#JbSG0=uyV-`@=<um&q(uW%FgO=SuD
z=1#1|OmVGi%Hnpl6Ssehb)Y-Zg1v${v%SNyuJXgDt&EYbHTcxi*6xo3)z4)d7CG*b
zWnpn>*%%y%!#Wdo>uH6e^<gS}CwLpa*LrKA$Xj2B@5v@aUIW%d&e*oTrlNU$jo7@t
zCZ}nAP2+3HQd*}e>etuIXjoqpgiPEIPLE>VP;6LVvmfg>b1acp?yjoMao5*8m+b`4
zJO1aTt6src%2d~Cxk)%11DN{`j2G`sOOG`ITlI@<vA0}Q{sHsf_-<=t;{ae!XN!ZE
zh{}fwah}2F=i|H_=b3yy9_Mv9AI|65SlhTG9%(oY=U9W;lp!j2Coo5g^_%<GI^v>r
zj<^l$H|e{;)#12}J`Z(~=IvkC9$8g7p1#pF&Y3NoH7#-b1<b=I!Mhprkf+Hz1J+mv
z+auq{`CBw+iDO!Sd><C`*~wQz;wc<oeKT2#u#s!gRl6qaeRq}OQgPgqgEe+Dg@f{v
zbD39RUEjR}3cKZ3Lw-M!k1(`mmgLt#{`>zO5+mOai7k+iIzV+Jf;tgFU5KDAQ2mLZ
zene1*BB&P;)Gw@uZnMIehfYTwN#1#CE|u@bPOOV|PFam{)H#u`Uq*UYI0?)jcSLAj
ze^qDP#`kc>ttpG!rI`P{5%uh*FzRMJZr_YNxy6m!A48||mV}M;*(;EL_kmAqhbfO9
z!*Lbnd7nbM%Fdyx?*#X2Y3k@BO*bJ;Rg_oAvuja4zr#9c%C830{cPwyD(qUjzbh(l
za1*vn!<$$)-;Oox>)Io)FG`R74986f4_(IH1vkyt9!C5Q$N!w=<tfc4cOYNi@8^}-
zv?THr$CX<Avl0KZI8^*kL6=onhrWALvI6mA4jTDhhJ1IB?_uP-K)xftkv|dSPXzfB
zLEhmRKtkp{>HGi7wdmTKbH8=zpSBjA)}dpauKdpUy(llW-u$LGo)@9Mt!#<gF9D}=
za32iH=NCiJ#?UvzF;<i7&sTI}pM<iwP0umV@09CryYM`NzFWQ#&nxc5vkH2q@i^)N
zU8^O*cGMp+ujtKoo9e6U1RcU9*3N5a^McMT;kf;I)Vu6CfyzBetVu^3x-lcAde4x<
z;w-dvJJDwT0LKBeo8O1s09k)Py*hBa*M>e25PA0(;``clBCjPO<TtO^;@j7Cbxxj}
zaAceMEZWVt(N@-@4W)10?m|1c@t4U`m5zF}wa=p64IsV@w6*nUf8&rF#y7oppWy@r
z+TqWmO>KQ_m~ek6oZV<o>08mqCZ@;spgkQjX>{y$w4bjvipq^>=Z~M89(xFFYUn)R
z?b+hvZvii49Bpei{&nExjH8Y1#%BSqV;pU5H;(pq<Bqs>4Dwsi<_;nGi1#TpUk!=4
z^#|yaLApGaEuQILD8gez8OF=wo|!a8>`Ix11N&3#%@(`x@vvPfg<@Bl_l!uE3BJSb
zK503rxc+ydoYrt#U9_&<-cIGW*>bX1Ta_43k6Y!&?fqEOjJeIT!`lO8)aKFmq^T{~
zeWA60u0y*1N$$~7taXoJ?KZA$+H2v7-v{QWl%~f*2y*~3Ho`yr15rk0|H^a8uhBC|
zyj(elWcVEoIe1R^jlld2tfg-}ku2?k-9_<SuEn#?i%04SzvK@-eP4(!=;U4}di0zb
zm_HcUf%7~am*Dy7G2BDbkv}!(hy}^te7Zc+aZ_vZYmLxt5B8-{I&R;QoGsf%xwiuL
z{6HL(M-B9y^3=GM4)^v$r)E>yk<Sra9}$$rxFY0d^6ZbUT+!*RzdxPwY44}!6U9@{
z@le@N868ELC6{^kC%^oqyx9KJQ&LYV!v;gg2FlNKK3xXsN&2rY>q!IcH$Zz0+!Xh|
zAx&vo*@E_)zmwjDy0yJAIdcT^cogbK@|BP0(jE)RJ5S2J2XtA<$C7hz343`%-y$b>
z^023q+FJnY+HJMhhHCR@4*|8e0PXEU>2rq#oa*2wpV?ylyKL`NTS(7u2V)<AyAf^^
z+Psb2=F#2)U6$A0b??LRsBj3snXjYneK?wh(+s_u-^|7yAmZ#)T&HDch_Y1l61%U_
z<G4mUKY5MHXS?%z_9IwX*0ZMtwZm=<aYD{mlyTW_MOg{*)BUz^c4DuAD&+M@TyM0$
zfh<$-i{N(87e$a0fu3@Yfn@s!SJJwFsxNpz`l;6;@l;r~U%@J;XFmbjJ4WpjL-E(+
zpt$p(R}pknbspzuZ(op{om}<l*A&U1@Dz4?NpdEos{}_1(lr)o*<PERtvIc~>0Uu;
zY{j+Jifap)y?uCcX7WoEPtRC~hhTV#{U6`wpagQ%SdC=jd9NENx^LjTt^Ky8&AZE5
zcz5ug{&fNRD*3$w(4uW0<Q%s8{tP+x9T+6vmtQCa@n7-an&Pef@LBVcd-@k|A2k2&
z;Fi>FY4;AeZ{R|DM17ZKi5HA56zJpN^iA^!L_FGCmhV0aP2hFvT<<&N_ogkNjTh*#
zb#riAe|&|U(lF4uryuNBD4voRi(bWE3-r}-+VFt(SD?KWNCP}R$HVj2is|8b9~&eP
z-FfA3@%Dj510h5P86^L)2g2*n1ve7W>s=*p4}@*~7u?g2o|;=?OA5Aju!Q$i{k4*p
zu-n7d)O*tq>&w_hfgZEd)(EsE0)qBDX!kymuBWz_JAku)<umpW<m=DaS8xTS3$w3h
z><Xv+(2c@(lfs%&_&Yv$ad(CH0x7<FhM`1_hlBKVnP0>56kp+ruW&U-Z{l=E*Nx)G
zeemN3J_CVCpA(Fy8~P|*#i8V<7;`-e5BuOx`rvUN++63Z<fEhA%P2g>2ha1tm;2xk
z`ruCgUeO8_?*bpZLrofSFvWk>2k$lEjWCMu4(L1Hy*_w{4}QW2|G)<i^IVpS-6n^H
zb;Ulo`sR<xKkCEJ(<kx~NXeh>gO~Z>qiIUYF#pK2`gt;}QRu+DIk<pGGp_fqOMXD8
z^-?P8s~K18u@rtS<DEu5sb+kyfiGn|&!{Ju8-NLjCZoL2$B_62L(W>pn+<#e<MZ_g
zHNYAf-*3cw2jkU7{x>tOzQrU@@#j6G2ne&hb~1mpk)IDTzMR`bC8w3~76q{VFym%B
z{1e78#_VN&%DDN?(Nm0vjd&A`k2KoJXBj_X@PEa)+1?&x+<YhHzZhR`$bXq}^uv0Y
z!;J4U+R3Ago9*YjjJFv4KQjKff&ZCtwJuPe;_m~-ryKl_89!v;Xc}O!uZn?tbK&^*
zvVnW|nSe0c!%UWQ(x{)GW87@tMl*iYxP)^U4;%G=9OGTab(F_=r-6HW{|1EmMxRVR
z{$k8G+ilusf%3s@pS^cL0wQi)UteUt`j()QKZo%WgZ~x9H`29$@tC3i)r`Mr@ULau
zG5FPtHyiOTWxU?Nz4a*pvD(1B>m2*M7&zvFW&Si6`e5E(;wKEek#Y0Ba3|yWM!U3y
zar6G7zRliY+>drK|DYk~M~pWZ_`{5+88~U**SmfmZe#+$#~_FLtfqO^2mh51PVaEi
zyJM>Vqxwy6`0(HL!H@gkr+n}b+QGi`&jwEVsD4HFJB8f~7_ifa`qsC4qipe~y>7;y
zJuiVlfO_3*rhL;3FNsa2-9F^j8*&qlXujNSL%u&Izb`Gn+wFb+T7GF-e&stlS-y<s
zg^*i{$a4}UxA=ffFQC!6+$n?{@`W|IO^NreTKAi51@yvN&n_eKbKLT?;PNZilZiK(
zq{{tErV`ovI(L+kAivIy&s!rhx%&x!T)RM~LJEz_y-T8U^OC5HE1EC&67hB#k=vzs
z+mVo}QJEidqZ03v-cq#OFU9-%cT_4AmC8)?z6#D?@|GK;$S;IP5mc7i-`l37*Nb?6
zcdrtU-Lqi{zS7-uGLeh6`?A3EEos#4V<DeAunXBume)TL!hw4R0c<cIB#kMjgK-D;
zOm>s?BD-h9p2%*p6ke%e<b0LSi(%`0g`0f!E;i||>;M0-_vP_X6zkgEodhNrU;;q_
zL1#jMgbtX50E3Q94+I5d2L%-m2}vM8SVE$J3QR(P0AbU(qb6(yO%M<jWq=7Hiw6)C
z6%{pM7lD8qD9C)z+g%lkmf_rUzkBcZ$9Mbp>#pjjpL*-9?XBwSrHp>e&|k#kWFrF2
z#Mc7gM!)fQImrR|69eEy45z%DY^b0gS1{a-vm^ocGTfZcl?>Ow?Jn^7JC0`g!HPj5
z`S%I@W(+sW*BGvWOJ&A4e*aJM*`sXz62Bie%i;Ie6fcz}-_8e!H-e4RG!T1oGF<VU
zHZk8E0>78Q=lAbseK2>bZ=}xy0)HaI&HN%kKFy!;t(4)E528LB1%4k+hRf#%%<?V4
z28s0PEAXuh*MQM`9ej%x`29HuH<saM`TTzs8aM+4z9z`&AmIHOPU)iekN7s&B)=KE
zz{j^5&P9EmW_-d2V&|JjkT1rYA3#1IZ<9XU7LlwCf_!~~ltr*zkUy1S<m_QM<+E{b
zYFykOfPYBfKWI?(<KY1OV*=m!-oOZSDgggV0G!Xmn9DJajl(G)@LgHWmdJ2Q*N_?%
z-|;h?+OBvj&+#;YOJ0rfsf~(!j+^;Of_&!HC34)%=l|m)`68d=W<LKf2<hKImuwt3
zZsrdY^cVRYH}gG=Px<+@Ait2|l%Mwscu@d;aR7c9!!^hlD#+O=;Nts8l?*qRd$U?b
zV3PBQAjigVbAQTlb9$o~-`t;a+{_pM!y)R=|F=Q<h`9K_4G}LE^cV4s0r33+@KXVB
zskV_Rq>u4Ek8v*kpTn5TF>wALLn_~42BaVPJfyjO^=9&k|Cqq<FU0!~0Z$X~;R2o`
z;K>3$h2bL^7R+1^!%3g%0-w)Ekv=H`UL?pLA>gF~o+{vceoO-<mi<N{-i`v_%W$qI
zxBrI)`RM}xn1J&zboo5PN&is-|BApD^$)WcB+7?Q0^iDTE}QEU$8eILA@H3H*MN!g
zhYI*;fj^Al6t6?TlNoM~*CWUg`2_+#Mvzm)aME+EfEP2I;>EM{nr)-NH|98vOJ0U^
zeR%!ZFUS$?VWog~u91#4tW0uvep=W(2I(W#!^Q$m@2&E!8N<0gT#l3Bl-_Zi3^z%T
zBc^wlfQ#~z1IXVX$QSFCPmptu5brJlcMAAn0T=V<lz@x<!ubF=e=flDftO1Zn<t_C
z6#MaLhEx8G7xal^`2E1dewpK9+ofQMjNik==eU{g5ahEih{Q-7H}jo>e38#_Grxx*
zU#y25H}iW7@<l$!&3yjcg!1!gU325`G?PR5IYGer^At0`NZ?Nt_@xZjfMp5z9)^<~
zv41%jKn{OiV=nje0^cRbzaqptNx*~I{1~MxTfiGLoaBq`G)mxW0zXmUi}|bxco%_>
z$Nl=o>oafHMFL-Jzhw+3J#z#(YX!cT-VFid9~Agvx(*BSMLFjMTodBGB*+o<j9`t5
z^c3YZW;o@iD2Jz;`qEn1`DSCD1h_0J9pO0jT_T_4(iVrEnO^;z<8*Jtx<nnnPd^&}
zmtrR8I(!VyL^(Yf|1P+!D(MKv>E2G1!}}CUS6!WH9C%xxbm3j@nvLU>E|LE%V^F;H
zYuJ*+af%o3GS+M_Fdy-)HK>IDOBfsRML8Q8PI@+Eesa9bPkJ_CesVZYdW!trj58W;
zBjzWE<CLEf0{$uUlb+%?JRB!IMLC}{KFM#){N!+)<liRX$C#he)l|SwF+ZiN)eUe;
zm&pH%aY#>IR|(=c>3KT=Y*S>zLwYvnAY6`<o}!#uj8FCU4uQ{cs<)z?I)eN=1wO|~
zz9^@jAm1kNIZpCLIZXulkpiFNB;PLJEg7Hkr^O9$${$hAAVGdhfzNT0A0^;Z7@x|q
zwSXJXZH&20ikoj7r*aheO9VY*I2kU-NzZ5j=ktnGj%{v$lb)g+e9xe7q>E~J^&`hg
zz9?s@z>gL994EdgXBp#@KJ5fP$4Q^I0{-uR+cs%JT1rkzyVUHQc+C3$PyO}G2AwHs
zqAjOnXmr44es992qt_19_JXl%*C5?j${Lx*jBFjC6c4xQ+=h@1AUp`WxD8|dm57m_
z6+;b7Wk)jqv*F{J%my_Q4No$QU$o(Lg`abm`dHG5?1#8bWZ`Wh2tU`KyFCO<q%Qss
zrvEV(UJ(Ps&%^KM5wYt$|2cgqJU1%8Q>-%c`6((t3d_Uu@+0x)xB*GYtW)IU4jMyD
z#%>ND3d~$~fTSs`6S&JH+>{)>55G}(tvyM({+^u9&EfBAZ+!7)V_O{ez%~5Fv16j~
zo3zA)j{o-m(n{HYq8sAZNC_PI|I+9ir6=%>(PVt{GbI_{AZhp}#D;I+Bz%ihiEoj7
z_!g<y7c;|yb<3K0lsN-})Ar_T(xC!PdcTQP`m7MLZnsK5&DNxcn}$oj9NVgVga1*j
z#y1id>$#=I_$Fi?{GH*Bg}V;D`eo?{=_#zGN9)bedV{qxgOX|ez<8xXsqSf&8vg80
zg^oez@I3~7|F#d`x$VOmg*!I~rPZob()Q!~hr>;pNeA%V#Gd92q|fl3M0H$4={tOn
zu?OF%{DAK-zQnf{6E$B9?lD?VviV}F@qJ2Nd^-{r>`^w|-c0%v``1mIVNLjE(h%%-
zVBgt9N?r^Z<;|_q>=b!u82CFc_(v_p_X$s7e*}AH?2WNs#J7Im!<~@qi$RCgpY#p1
z@4<Wc3)@@q9iH(m-h2nX3-}S~V&@ZaZ6NkU?4j6CC*a$>vs;yQ7q%+z{I*po{bQ^0
zmZO=&ivQSImA2AAxC7x9!7YLt135AHKA{)fUT~}7()SASxE_z|*>JPrT7X-CzcE=#
zTdd*x2E@g6SnF)&I5}KOThs&JuOu~d>>LPxfB1Vhb4<Wc>7*OyQ8>5Z+~$iu84nrJ
zIR9&_!s8<u6YviBNqm#Ch|-Ao=-b1$q5DGQ>)AxBwCC5W!;xpvCz{A<XPP&|`0WR2
zBI?P{hw%+1$~vQOtTbj{tQ5L0TsnxlHSoO%X>tR%bPMXvAK-RIy}5+^EJB#<TisGG
z;4#<>5mzDN8i;yi33p3P@g2d+wPYpnEn&m9QW@5_?2GcGI(PpZIgRjj_@1r+Yf@ID
zZY{yumFwJL4*Cvri3~s1AYEdI+ZL`HZXdWQaAV-+!nMOKf!h+_;~cO`ODg_~`FWjx
z_+HdYD(~_tt5hCNM$Cw35$TBc&==-v(mLc*d4*Nl`J+Fzacz9}F;kOP9z&gb7~j4X
z<2wltzI#i?w-Oq@d$ZxYHwoVxR^mN<AHJt3#&-{%>%V&oRPU(Vu4`B8uZ;TAYsI>n
zMqYi1dhsRdMO)-cA@b_^Lsq)C$FNBYdVO@5<29u1Rpi6clrTpD(p`kSSctV`7a`xa
z+I=xw+`gE%WvTV%6km)t*B4V-;)~f(;fq;Qh5r!Z8<_I5kof`DWyQPwdtj#btZgXK
z+MWK|R|nNYIrzW2I;ghQKZ^XdVy~q7YvbMgfAJnZSufuAiE9q_0M7da;^3w4i5bu1
zMe*y`*L%GVT4|jx@pAgL75Mlo^ZZs_4}PNq+3x?&cf<8D0B#hhKNL5PoBMc<o8R5z
zxDRO+H;z{Zz>CZgBk+yh6*BJEWXX63>sk!PeGuNQGs(B<6uf(7>PwCBzbp+l@!K#y
ze^$(6Fvh*I#Gh?)Jc03lHOVpN+vS@7U2wiJ{|Ws~={4pn<eD{FI6sNWk1%!KJsF;9
zN>^WoXPEed7|x&N>Zdr+7)T!(nDnQy72z2sd<4T!nc^MA@M7J)b{Na>W~TI_tz-N9
z+Gm9RJMSjvfZsr<w~Ki^*T0p6x2b3;CA{@RuM6q_-HNC8eT>zI$b((~rxslObtB`y
zTKdaQbkG+l!f&>4RX^x&5b3KL0iu_T664G-0#Q<Gwvo)o)uVeX+BEFqTucFh*fi{V
zo1qhIFB*=HH)MWtmNLIt{#u4>a3h$XoDIyc!R5A$AS<gcgo|?@%>=w25!n_c;Nl!g
z4~BOIPaMYgW;oZA&qG`i<cM=9^zIyO8g>!q@3@%p*Z_QI0DNcwoWEmD@<p4@?FZpv
zz8zz7c)ED`UJ~Ssb68gb@cXlNM)JiupfrY?<$DCaI0v*?z<C}+q_o{6hhO9GvQWI@
zTu)<`5Ocg~3@5%g*OS5UuE5!lA3og62)I~(JOW-{XBr1?Up1VH`CKT(E6y45_D=DN
ze4f5k;Nr6l8_sDXyG{3Xe6ukR&E1InE)0)@TbudG;W*7<)e`U_%ujPAbbrS;j*kJr
z?*sK~?Xsti!~ga$XOc6?a5K1XQhH`eyPWhXIZ`{C9+BFO$j<(sn;kJUx51$%DyEUH
zXmr44?ze3^dhHNsjwG68&J<QJ>6GH(dBfXYD~2;#jo^c<KGXor!^}wy919=G{LhBx
zb0k#$;>P2*i6Hzue(q9#&;26x*Z-jY{aAQrHfls%V&Ri(5W~askHfv-A*#j!<`y&K
zT*}TV|EYfS@I3!XcXQkngwKuiU>QqoDH6LmJjpfZJs_!=5ogkgu$l9U`o<fD@4<}B
z!$Mf{n!~eU!01u`!GFTHPmGH<W<~zv{{gD>7)o@5qv>7^wur2n7F4!4T+07T&Vag2
zOJ$Er-v)=i6d^ANaZ779giCwZ$(eMmFw`x*iR*MfQXlgK3!V#?%5Z%_T`A)n?&->~
zFG83~caXN$8!i!_zEQ9LnVjM20Bmizem(58oKe}*;G6-!zM^D!8VT2%DjAg_L0TDP
zoX7P=@k)mDyQ01A3&%P^Zs~kaC4*#zK*z9KrA(xv>2c^)j(cdDlkm({l0B<28jWr)
z7bhvnC8ZIv9eydewha5*R8~AHg+cXddJN$fVJH6rxEv?mIh@bOUJLxPxtg?q?h8YG
ziheJR`&Hu7e?X4nrS5W^lZ+!B5^<00QAb6D$lbByg??F<Lgc8$tu3t@x3)wrZUwg$
z+?E!5<0y+{4Ut#k`bq3seySxUKhlC^CPls&A<Nh`?DlBLi3*V`B%kWD`BWSJr9<Tw
zXs<(MhrF+cGNSu=|L~jtj{tpMQ!3xx(3{HNf6LAJg(&yB9{hvgU;O8w8oY8hc(?s)
zdF7a6@caM5EkNLO>GCkA<v)3Ip7nXei#aa;r#I(OI;hV0-@iGpELaPo{|^*4!kPi7
zS5%km!Lsas=jP!Sg=j(a9Y4*jRkHes)-U@f+&mn$8OjH}epC5%GWf*>l6C;+K98iO
zdi)ChYZEi8zmz$WIIb&0QY4R>Z1bq}znfrS_QrTBh25@bs;z;hrh0tJzp6X=MJ*)%
zJ2(H=K=75VHF7*FWUaVZ*5)9-6~I?4j*vWy<)pgOjEHjC78Hg$couUeWm7O;i2p-`
zV&182s62EH20sy~Lq&JPPR!G&?{2z7@+tdJ9@M^4rAx}bMwk<7g84bf9*4bXwkGXk
z{|EEnA2fxzJ_S+7mb)=;biqG-9p=AiJ}>@=Us;{sLn;mKDgA82{1L)Zo?4;Hh3#nB
z$lH~e1A9~2rSw{0mE87?YFBxqx@JJEME|C#j>p{8-nRPO-wL-BOZm7DdQ>CKN8t_P
zlDlXQYb}o=S>v;$u+CXsaXpIGY(PHO!GN<H{7J~~FfBrQ0P;wm!nTbid!(keNYK<m
z3}TaSO>zv3^eKIEHK`E0H4FbT8#6tkaPPB0nXeV!%I7Q#_udtBCaReu6}tYbvJ=De
za1`dc@``Gzl_D>Sn>E#_Y@UU`Y;FCatFQQ#w$DXK#jn=aQn8)`);NfH1ND9b>V4%Q
zMWcVa90Ql?S~$Wri1H}2MmNsX>nZHMf9oGL^>41#Em@a@ODV0Kj&ZB3l7jpR!+d11
z-KWwzA1!b`e!W#nO>kiS5KWEht7KL~Zqhk9bI~EcQeC$@ep<8*u3H+n>?0);+~c@j
z-O`6{UJF`Y;Z>KFcvYI4ERZYOq``Kk!p;`T+fX0;ZHj@H!@p1VD=Sg{Nx>s5-8^<n
ze#vEZX2oT7rMyX9R`sEp3O!PxPd?o3Rhs(I1x*cx&2EW3$}+;D6hvC`t1hc~@)flk
zI4j)<XWyjOgsDK712+g`zoO>3S(sdeNfE-7Ak4ZOgh@e|To$GRVM>HBxd^jFQ-hdp
zEecX}-O`}b%o2pFxT0=P(F`4B<V}?kmZ#;TEaY)bdcc?eV(9_b?!A^CH{ux(ARhZ)
zNYh{H%3QCyJjJV0ou{(gZr9Xp@sJPQLa+yiMOq>YM_6X!I^|Pd%3o@J?q528n+*Bk
zn)<eOb+~m)xO4_>tS#n2t$o|-Z8G2Pfo>kviuO{Cx!@M)8-uY271%A2(4W$q*V5^r
zHDhLWa5_i_1#K#r+7xWSu?qj#dBJWe`mCIpM0IlP&F$=)VOo&(Tb-o!mhMvBFpskS
zrCT$pZc`nv4L3BxqeNjmQB%ji{k^7+&q5s^@C@qst*GPATcudc!BhRFdYzo$#F{QX
z^;n{uSs!wuj!T(~P9aU9-KDZ()NAz3<CbmH>owV=>PAMrUQw|b<-S?3*EG*Q6ZJYD
z^_uE4)n)oWmlf;Mbjwe%bd@44yzIAC_|%W8eCi4r;j6rA5b`VvyIxO_b2F=qGT)X9
zUI}<^@a&ua&a)e3yv+?>3V6BTrQCprGR;GoZj-^Y`_%fq{KMl}d6uKS#qVn?@v@BH
zW|ZaMbZX>u<e>~ByZY1?BYZ~r?e&i>#IvylD8t_E>SwH=GMs<&u|w$UaA^tt)fED+
zUgn*C8<K^7a3cCa!bWO7y$t<#{x&2f8U3T?QI)1m<XG4i-4?-5c4!>RZ5irj$j9MQ
zB<fW;^q}iO@YhFO%zO{_s-weEjs83jylU`%Mm^~l6O@TRgUKzhmzAO~gWR8?(*($)
z@mFM$-4e7N<pnt;Pq#UU?+9e{gMNKaD?`Ij&z{MR$h0C{7}`Sr(|#rCW2<DXgMah5
z{fe~?#+tCF<Cj$^<Df?&VYm8}>OO6y6VQR`=dOw^>dula>Opy{8qz&PZs-n?>)S%)
zXzWn~L*yM*Thw}R$z~q+MOuE@KH75E($<#R&C)EvO(QJ@;l4J7xqjs{_f|C&?YIN>
zmC$)5bgpjFME+;)R&|ektNKaGRyAf%l*I=*rPv$o8Dp6h?^dfDMabF6%V+IwwI%rF
zkVRonLdF}w-Ug?3{}B23g&AtQrlTw&sGo^AZ_z76z6<`ka9h9F!ZH{97|3e5G15}2
z@fb_cdb{Oa$o;KqhDx%6H;%HLgxr0IgY=sl?p7DdZk6KNL2==H70zq*KwJlAsBPDe
zw1k9($O$-)Y=`zl@^RiOu7%}T#SGN~tmTGCOK{w1i_*q!8HMX4pJWAXNVDvKznsN2
z=Ypm#sM7SfNX{ypufX~7bu-kESmbpZ<fSD<R^e|2m&*ER$qe20wJeRaQ29sMA}v9s
z>6TiyREq_2^A2cgK?P)jOKBjvWZx+4a>8+)bUC(UhI%4*hPuOwF%s-RHS%|V*wx`-
zs27D5;nH2`U;Co&)kWJYq<RtVQ?_GY0h_v<Y-%k$Z-R}CY!D*X!cO^5HbSXqx3sF4
zYH3*ry~0N5`nQJeyuNb1NS~<2k(Syordmj!+4njec}*OSnVlVuS>X;xVU=IW15RnA
za++h;)Vz32ZII_5OZB{8J0<hjZ+@i@^f(vl3#zxVfmF7nfmHZGghch;GgsEaJ_(oB
z>_U2=e;=e{AKXGa>|Rxc-tVt;Z-L#~qOQu_qOM2^k>5a@<9%Xj#TJ$9WN1^Pjwfl6
zmixD-T3Vp*Pzpv^S_0pPJnN2jLw1zsGwGU-atv-9BF{ltQo6V0YP#KP1&q=iGB45+
zGcU~&1zSpe+8go~qd&H!T3SV3YbTWc7?f!+u(>F^StvWoj}X=lw`D<Y$!2}r@llGV
z)~eIO5>*Gf6(H{|X_jSGn^nsl7{@@*fw1$G&s6_(UD1YSqYXvjI_dP08~t=Mv>EKJ
z>ZDnCIR)2=w0t}O<4ir>tgO%;czP+UK7K(v(bH<z)Q{p(k5^nBOJzf4VBH=r4M1HQ
z$m)_6bt?+`bdU6?U2!kG5YJ|*yrXVKUBb@GU|?6DLh-k%b1mO_yp$G-dp6?BM|`A%
z9)EX-<0I6cxA7ZX`VX1^b^oA#tj|3iqh$6!?pMwN@7r9%y@4;Ntiq@>o|PE*Zw{Bv
z?ltOrdDS+hu;i*<PpIEo={EW<8|wO76`R%7s4v0ruSA_$h4Nc&->jCqx2SR*o5cnj
z&>HpQ7Pz(Gj`l_A<&wI6goVm$Wz}Z2EO(1)0k`(^Ei94q(k(4-O|`sPu|=hO13S_}
zX`}jAjPa<gO^7@*#jWNJaH}mkhRF5=*f+RgaO>b2wR;O}9OVt=0hL|8+pW$*Iju<9
zqOP=WQS<C>wE*S4tYnKC^t|07KV!2<h`;6YBP~%4Mp`24q+6)2QT-)*Nj8ggsJZ9E
zIjti{`8+qvt<FJt>*HnEd5V+9$9lZ*d)A?@R-s%f&}K_Cb>;<4%{`#0B^XOAlQ*k{
zvR^sZ(x=$!z+Pal#+Xg2WwY38rC8|xlH#Fwb93FQXNg<gpL=yIl^K-}wdHD*k@PR#
z0im)#0$WuDTSaxN8n%kZcf+_)k36VHZ|=sJ;d{SQ_uy9L9QwktuceHIxPRD(`WuG0
z$0eX%RQ;vwgZ|V{dSnH29CUwBp?Kg@+KMkct$M<P)RP#yP+E%RX{x76QH#UjUQkpW
zv%_BjEFSIwhPmM{0X6{cI)<gdp9?Gt?h=OO!k+?c3EV7(mB8-?whrz9hE>3C2X+8%
zJj1G{spop*-tYs=!y!GCpGG<)J)h7f=pGn+NiC;xA^p+EE<EC&fP1*;Fx-dZ`D<&M
z58&$o(pdKywCN3JZ@Kw+_ott(8}BY*<K4){5wdyAI|5@~AI5^mdayC?Z~r}G-ZaEr
zGw!YOsoN_sPDLHP(YROV^099TxVhlEZ#?$ZxqSSa0<Ig}+#8R7buJ$R+rgDF2EO~o
zW8j&n&wLE5us%03ECqdSq$Lky;I5P&^EjC5G#dvOwvvo-F#b!r(xaj5rT$c=RGw6>
zHr(GYfj=H&L>ezIIU!}xy$jV58vpuRpB|Em@o*Z(!;5jxlDQ4z*si#Tg5K?5?-SpX
zh9*OYLX>SG<~L{@O!qG<a6P7uPdWIUH1yaMOXgkmFb0Nw55b&8Jmk{Y7K?ZHr+5PG
z%PRb8ag|?<hn(^Xzp|*SG_)G?r`tPAL-WwqJ;)oXzi#A9SB%Nz!eq>c+AX<mr^AE1
zp?XI5b%`JPhhIRP`M57w(#}7j@K3aN{0n)}Rlid9oh9=ewfE-G9lB+r@7TB3nB$p=
ze;&_~<=9-e96LK##y^kc*jcg?n}<1_{9GyaXyJWX75VpN6?^W>$}6}pYq?vCU50L@
zBv-@#lIj1*+w`%*LcF6x?_Qn3|KqFQjgYFLv!{=&ok4!3BqTbrkY_gZjpt!=$PSJ}
z9;aX(QL>HasC|ZO>cQr6=0c<;<?cks_z$g;59tqwzQj-40Y0rQ3jbN;kH@O18_r6Z
z^q=GF_{jgi@#`}FX7DRa{1G?cV@#;aUm#^JfDKIPnCPH;iel0y@9+F`xHl#K!N(xq
z4Zf)VGJ}7%3F3wR6Oax{LnYF?p{<lzm<vAghvsiQkY9J3!5@$KQgAOtTpzfo&yB+H
znL}o;W{$8ZtP_B`RaAxezVR#jP_|^p$)1xfr~7Hqo)=%Zm)$SJj@Q`rZWs&RtLps}
z?4c(d=i&Flrgv3~?f2^T+;ai`cwiN9-3&Vb{{Ubma8nqz4*o1)xo~qCwgmnqz*69r
zFf0rHb->(kD;PEa{sX}5aH|*=-&H-==kNBs80+bh{igQdfjuw2;L`0o*>mdW4p#Zc
z2D0a3`ycSVx&5CQl7{x5DzyJJwEvl`{SR0p4ec$o{Zygt_k9L!KPUJ<)%J_g_Lm85
zzh4*D_B%*Jb5R#~+mA%Mx6^%y6=R7)n<bC6eQNguwp?rX13LOA^k(h8pWg0+|Hs<>
zS+sd-`!q)JG{JKz*6!1&UL#-EvUZ;(wEH#S(wzHeKPrD~_rZ}yJ4k;9?Vy3t4%VUF
z7o(jed<ed&-3NEQ0UzyN=g$M*)b4{v+<=dEuk#-PpW409j)U`Vz(>2+`E5x5>-qJU
z8T_m*(7%hJe>rRbrFZ?m)$SWf{V@(Htg0WB*|nLYZlq7C2ici$C%`4UM7DC_2MwgR
zH#d;bHiBS#gZAAE8=pH$^8F@j*6{Aeb1u}0PX`2L*1NP-3IE-%#D5qe;T?&lD<z-8
zpDnDCq*!`(`7zum^hwlj<;zm+O!Noa@c;Z|t39g(u5_hwcAoT9Rtk1c#f~<5pMbuB
zIMi=Yzw#;iBjQpRk-rRmM~U8dROo%jK9uW)SN+2mp+2ufyxXy_fq!O<<~V|#WN2ZA
z42rKLCdrYHf6?axcjLSQ=e<xL7Vr0urFo#RT0W)!=|3M`h&nd`c~gn?rhB6v(!3Md
z3YvFH#{5$9-Eoe(NF&+eJ|F%$_ES67j@WIL&O+}AC^rg6VTe!RtmxnMa9Ep~!t|g0
z=h%JFa~$F*2G{zDRiZh3;u>QFBaUKl{g3{s|BIZ;vK(be^I79io{QqIjwM^Z2ll%z
z?jI??J&;8<PPc1rhr<KA=jnj?1L)WHgnxoR=+8kkzVO$-!rV)PPNU4iEqG3M=b1sh
z*;v19wSR2QI3N82?;EJEp|O6k+=cZEm^*;Z#k_C8Siks!Q|}u*^0aIHK{)*3WUJx2
zxvhpjo@_eY6mHYu9{?-`ZZ5;J;Lie<3%7(}OW<DutORZa!`8vS4p;@;Dux|^{{XNm
zjPubi&^y0%k!SwbFAt)=&j0$wLBVh-{WLG#yVak=>3J5l8OmF$%|q=a#{c@w=Sf8w
zg-3pajB9<a|Jz>%org@CbM(J+b`XDFL}N^I8}i>{%&B`0;Ta^J6FrG<2hcCYd<br_
zSJG&$@0A#5(tqHCJ)W9%!7*+f{BUhI-Vwy}c|6B#$^KVP>swQsq%?KMx;Iou7NB0%
zf8Q#}SQ}lhr>HN5SdVAX@dna@<9JsA_i5#u8%l-uHk5KJXQ-a@GgNA4w8qAY?f&7x
zOg40ho`PqgQ#IpxDD|Jvw`N`R>#kqfs`!ukYtpXQ)m@5Kyj{_Xpj*we%?%2kZ5r<m
zl*mRuM&*#J_*74%PhGPZ^SLOiRO!4@OT)8HP0^oqHb7ns#Iw%cc-G0^8=z;M183v;
zAD(y8vrhVMq3~YZvm(BMXiNNAXP(=m=AzG}dkIep-gAIGD8#-z7kzArUoAmjOXKW4
zU$bYRd02}9I?{c|T!dMLXQ7KDv9<@Ehf+Q-!8nbchf@8*dzwwN6|_g>`x|&xxdwWa
zLa#G%^zFlbWgni8)~4sM*el#?)sxbGJfGdKoGgUR!L>?htvXv?tIoHtRa;=Zz777M
zu733+NtW-&xf1DDEsC$rI4KCvp?zv=<Wc$Vwiy2<I*!}cs>igo>O9EJOIfR)PhP9O
z6)VfFS0r1~6N6-H9a+wcmgS5UDVC6kAbAHcEh@#*dPS<GpEXF%K>tt=`H~TpX6bcr
zklfEn*OM)6n+0J`MV9k%exPoMJko}9$jyhWpD-6!2Xkii6GP-udx)$-ua)`HmQ3h6
z0K2-jrKL?$OUu5{5cw3|&!~+#@_bE}tC1&X@!Xd3D9jmwdy&?b3_H@;9Jeja8%y*|
zcU2R-&#|F}#o873NXYy0T;tj9$9R<kFR#j}c(<c#u+0*?)^4%mxxh#Ad3Brnyt*<j
z2=9pm$sgO#tG<-;>f2MU4&Q-i**iS2n<yWuXH=dZxVcT8j)gDN{ew?UMOo1OAC+C8
zov!otOdQYRTG|5Nw&;1u+oR`uwnvvi$6_rgnd)Q}Y;irfKDcg-L48-`r24qm)L`SQ
zbGJ(MtR7_ozGqn2vewXYq}N&i`_nO8T8F%;i+auLk`;BjTz`gtQNjDcO$VZW#ovl&
z!l>Wn=nE!S?uwb>+Z8jlcvsAmo?S6dCGU!PTH6&f9(8g8c55#@!$;YYzQyRDslHdr
zS}e^8wf)&Ye97D%Qc-A6srZbNO!D>jadKlN-$YsKhWSZ)=Y{Gwtpm~kb^KwwPZ^3i
zr6KV?rRN2|l7aJ4I8VoU8qO)6K@HK{E=1qi0?&>5COSq0;hq_F<Gx$)oy_@*llJ1<
zoJTgb#WyQ%>Ar5bf5g2@MF-7cZycXxkLZl|0AxAJ<<Or=2Df)Ox}(oIg0WG{cw>Ad
zVZMAI-WjNjb2uKsI|9p`4hPAu+ap4n1+2VDW2vG~l7sT~k(tOplndRz&|KKyk^a<p
z<Y_GOkLm-xSA~30;~`IrZ|u;&W6Fz_wC;@^$%ro*F7h}Uc^r*Ajz%6wBafqz$I<#c
zkhJ?>*PEgUnZ82_RIdo9yw>f=zmd<Mqi=Xx*`+L9gt2prjcQ)dM)jL~P5Kr#lk)qi
zPFAT8){~+4sdk|3LSQFozH$e`jYApjY@#`+JZkd4+4cE<AM%~@{{-?r@@9FDJf^&V
z1Z6Or@_+QT{J$>m2l`X}b8emw^`z+$`zr1V>WjAF^U7L%X?i2Hr(3Hl&`0w&bKr`;
zrV_O+&5myc<52cCkCKeBPcq67-9`VK<@Ge%t{7jboJ4Ib4f#&>p7Nj9{hD&1djB7+
z^OsSVvx9ai(@^h=Q5GdAi$`jElwo*zc?8OW?#rlNzluES3V*{4yrmB}`-#TVU$BET
z7g7BI+62ms$|&*yheJW$N1_bq-u_50hr>=~(9_|ddSBAR;gH8CI`ZH@f-(0Zlz|sI
z*ioLoEX|RPb|G2fv!vk8S-x?``!Q7i$K`3#;2}mEXp1(W_id>A<!A>U%Fhms9n>}o
zVFRc=_`>jY6Y_`hn(`NU7fpHWufB5A`iydmxX#WszkI_o^Y^=Xec)w8HuWfMHZP+F
zC>vf*|5ar~<wNDf%ZSQ{%88c|l?^YaKxIVZ<Nva98ie-pD8{fzJH0bhOyz^T4@Z3$
z?GxD{+jV^t%3eBed}1V>-%4kaG|@Hx$LYb=$9~)TXV2R#cRH=w_uaSbc5ZIyo73xf
zdGs^wwX0(<?0bA<u=dGd`w8z`5x&N4lLlY5hiZCoN%FvjeJ^(Jy=TVA*NRWLn9Jl>
z>hT4GBxA>YB9SFJM~AAPLj*V;;%nI1cHU*g^T)l0KEG`0_rl~&i<d8<SE;e_>!Dqk
zzTW7bC4=|BQewK^gk5j%ooDUw!M>;0nXbpM>w~-z?T$XMOR6wk@64`ed#CnzJ@zsC
zzPtVUR?YP5*`-^~%*@%d3Y<^;5+dE{-h21e>#jfC<wCok`<(c3f%i3Ar<Vq;yZGVn
z?E0;GK9274X2JNyd)H2M&U?PeRp0wTo4#h(>*?30c4>7s@`tq77J4sFxP8Hf`R6_y
z!LC=wQaQ}+Qh(8by6c;abY8ZpGul^4n+pCSLbWSfzy4B}^%0-r`fTU;A6`#+vF~l0
z?79Ngu9<Bp{OT^hm8EssHL}1tLtFn_qddo^*@ukq+al@u2VIg@pS1muw$R(+^;e!s
z34P?ATXhAhT@^ueeRr4BEzb?Cw|=y9OzH1^zJBT9O+$4Bs$Cz>>4xiHcPX#8<WT+f
zbDe$mhg!b<dh$IZbOowiB`>&f{l_jXJ1^)~+G(!$$r+!mROZjxG<y~)Q0=<=tIoLo
zN0-mOd#C7F<b3D7?=QbJ!#R6XQ(b{-*K1!i#r06v`q3+<Hyk$8JL1T`jma;rxo4EF
zK(#CW)7J)Fwl{I5wp-rfp)V$T<L>_KO8C0x?^&8c3RJtEes|4)%k~)8iA9??P3u3$
z+hO~yJ)4jJWYgoi0@bcneb4p0Z13!PYhljddauv%cIn<Lr_bE)?@81ZsCFg4bEeH@
zdr#N)5B^wpxMPa9_t=M~H+tgKJtxMK0@bc|_n%nvr~P5qqieD+VBu2drB{1Jw>y4y
zlhT}W0Qs|G&z4K}(XNKkyMDS-_Jnu+yPrIzeW*B}nYR!*Q0>Z+MqNE`pX$2ko!NNj
z!!w<)JURC2s1N?U=i6sqK@L>A@^^lJ<$!&ztM2AIcP;;Jmh;tdGq)_96XUGAH$S=H
zuWHxyk;^YWXn)D|&tngD@6#dI8z(mnO>KU8)AVsukJ^8)c10Yjdm;VK)viWwMz!+}
z$@DIM`JuX-8^m~9wLGwV!J%qb?1mWcs@pzr?F=vI^!u~|Z^4)2KWp5&i+Am?i=VzQ
zqS|%Ks?pAkmv*^6zq4bT+eSR@%v<39pvkxq-uo6Ld^&dVSy$@nHLH$HJLKBdw1xG=
zkQ}FV)fb<ijUDAR*$LM}Y3my%emCFCU88;TBez%loV%4Squn39$3Xabkn{TU7N+xV
zrt{RA^B;7(H`7UWN|GvFf9dUZ8vDI!rK{x}YNyFw<2$-a*N~ZX?(z0x{M|G6p`AVC
zH3U?;UOu@4=TCU6$=E@j*5K`9uk4u#(`ua0^F_4%A#J+zx4y=C`ry+0Z(0Aa_uxIG
zL8a@3T~pr*T0hm>PZv<>dj4g((OAb+=e@dsO4kEh$4z_x)~CF`{7M2UU6l!zUyj|8
z<J|i>38-{!I_UlUj$dbb6LkSNU-S5PwW{;H-|HGwy6(Mw!}_UT4|Hb!N&+ffLpRj#
z_fGR!-jkIipwe~zXxO~Bz8&g}oZKH8RJxwqwy@2Bqcfd<L=1oim9Dm5`?jxe&Ua4x
zzSHl0aUOHB&-;B}=<Zx%X>#fwxzhE`uV-7?dgprUrFASiA6)6`|G+y(-df^uW__Kq
z^gzQ(*TUN?GcQh?;T-hg(+9fruXMG4qC6+`iP7HY9N#|m<bW#I?V*3<9k6)3lso_a
z7O37}-7waEc)Q{4$lQK-yW#DKx1Yx@Uag+=XSM60hu81jyP?3l_rapa(+@3k`R-o*
z=<l;zIeGiy?TWWE-oDDS7N$RU9`@^jc~yl?J<dworK#i7-*#Qy`pnUvsv^C27s%y%
zf2?+On7(8G8GE|(_a^(Ev3;@L6+3ooX+d#MZ|I|&uY6Kf?dn_5#&)#D7-y3eFW1@s
z`@61}GhO>5wSG12m$zHqj(PiiYSp;E8ttogh4!vHbHaCzd%xKB+Rj;>ye|9Y_7`6L
zCCdBx>s1rewpF{%m7U3$eJ0mgFMIVveIMHD8h8BVNx`jqIo)49mo@d>YL{it3)y8`
zvz(KE`)1$y&K0gdowxVt)*{_|+e*uf2j8f6-8rP;C#A)6ymzN89B<vV%~cuxa;X$I
z!1+q(?x;UrsCM1+{K|8cZ5DbT=@PT<i3hj4)?IAkUN&o>cl85Vf2Pf;cD;9H(d38L
zKk3}znx7PL<YU*4Z(g13k56|_wv{Y@>G5jUuqNHUitkt8jNjV-`>)6Cblr7(|1;;l
zNO5xe#O)HdQ`|naz3Y~HlKWP>Ldu>yA)lM#9T2>>WpUmouJZi-{*b8|-nqxq%6@TF
zyG~5H^WC?fn&wUIH+@XrEqh!Ynhboc+l8jyZ|;8L;=ssi*IPvknr#fr^M-fpe!Sna
zdtK$Vvsyp%=qTq`anD?yUZ>hMx98ZyPsTp(ynVR+sXv;0>e{}n`Gs?B)4bcVk1d~a
z;hd}A<g26V#LsrF`KwdCjs5n!_RMVg{@w<`-qfBMC$+E6xwb9+ZOP8O>CVZ&uN(1M
z_yO0;Z@yn=ThtV1&MiOeI<)DWYx1Ojq!ea8?d<YxZ&%Xi2VAcWXxQwaZIt($wmTkj
zzkbej$EBl3)+Ntz)_VPmcD0H=bG1#`zHGr~k9+GL=>67Hx#wJU{xLS=YMp7`fw@)N
ztb;#yji`M7F6p1y-Ud}Y&J>I|=NddY|L2Fsrg-I%w-pU*c+l0j<)6Kt-aXMdy+VCp
zOxJU+ydF7o9dqV6$DEwf^~iyPuA$LW8&7hNa2}uid$8Sp&eb}-LFUc~kN3Mf-+KJz
zMPImVzZZVt`|A#`x~1)iHCNBNMm^ZI;B>?+?~c~4@22(s(q(s#?4{Nn;B;==^IpSm
z&$`~3X5G=rS>SxV`=4>MEMK`E>pi=&$pbmwSBf1Y2EKRJm0Wb<^vUm^^q$<)Jbv{@
zU%9?M^~)VUFK^=H{SWVVct6DZpZO<tXQzxl>w56s&O1^Hr+dG8)|=X<)7P$bUp(?!
zeEvx1-Mjl7NOYfd{rt#h&kd~3_s%<8+HGp}*RHp|y(K31n~~1L9lGS4yzQ*3PD8E3
zH}U}QC$4eou2tW-F77!#-a34mH+!e^*C)@OajhBp*1Kb$`iIvytSZ#{*tf2k?ic<!
z;+=Houq#)7IbU(c_15nh_oNJ&?mT&ie5hxG!>(Eb2W)z?;S}$0;feMKo;%|T8d-G5
z6PE{i|FJ#(rSUrsyB5v(=;3;^9&)~tU643`)EU>K{*QNlGJBkJ?$OzgPM-Rm>+ulp
zoHP9&cdpy)xv<`G##MX4(~qyKe8PLr@3kC{so%TY8P4W&>;A*p^TA22wp-4)?u)5+
zXhx$)onKC=9@ymY_pTG)clqez{%PJe&rK-vef5*8&cw2WH(C~WKYaPpw=3rV;M(}a
z`xSSdN$`HQxWlM7SN`OBZ&2+{eNWGEzTc(yoGZ;LU4<RDo$Syn+uLP)lkPJo{p3m+
z*Eiwy9rL~C{E;us$*y!|YWH`OADQQEFyyx`eY*VQdUi$l){XBi^e&qFSf7czD_!e<
z9RK}SpBH!~rP0tjp+C9yU8x`UO{=-y77yG$_?<hdTo;bLHTs^P7kb0Lu)pU0?6j-L
z$}P3G%*gll{Pz3QKG{`B|HpfNJ~zj^yrf>t$X8Cgc75~Sskq-eJ9lNQ>3q-5Dwi~J
zMyu~y&UYU9Q@K4f^|Z^nAZlRvlIh-ue`^2I)TT#V>lf{qyzY&e-n`cL6~7U4+V%K_
z$K*}XS>7KSeAhc<+!0sU<X^hvd^5{i^ma^jx9T5V+qwt8ccuAL&eOmA+49>hM_l)v
zdwyqb&jDWdoL}y*eCJ13PE^+HBlTu_J4~#)a<TqV*MwddTkW_q-rM!7oxe_>{G;n1
zBei8y2j_Z!dAQcPp~**GCx;AO{?3(L@8#5IdPd&;qwAI*Y^$3boay|azb)p^w~xBg
zdOmlwWtZvBx<%E?_gp^Zy72StrK^7!={@*R_d#D@J?cv7UzmLB=$TIUZH>lyyr*2_
z+8=0F^>Lo}%jTgif9ZY9_0EpJV#n0ZaBgk)VSVlCQ?9*ft}yR6dEU;Uue_4<@-bJ@
zD_P^JrssLbzu>>6-aV&Wk0+kKWc&3A?~liiZu;cJF<0w%_MLrpTdFhbwjoE)TsrBx
zZTGYL4(*!jeg47a&*UW>cMVa#nfp_NC%tdKJtp#pcTc*$s`Eoxr{5-ei_a7$j-Gbh
zwf?gDl5NNwZ_1>g;U}|Bx)#;SU$F1f$DM_8`j)459(T3*=$56YM|+&l`>Q)y+MaX`
zThhFK^+AvGhuG#1zHdF@>eR+R@SOpZyl<DD9-MUYgll|c-(5l81>Sq7oNw9W;S;VF
z-~V<w?cPEsA7AqEr1|_`{ojRVMQIcN?Lrv;7FNHAgAIEuKOtAEXG_YJwK^3;>q*)z
zT0N;k4}(dkYnvj~^91A07n3I%(9COBD102k<6@nP{<IpaPJ^(xzk@vpMA#(kYfUnG
zyc_~8UQqhee`gd)|IbX13M@%V%a9@%{~?X>2bvyG^BY~yOQIao(_-jZ2pqFb*EU@9
z1#2(JFDfquy&U{X$kJLM3cs^~NS$J0)RVqC^z}F29{$_4q*rT0z<0>700jLyzxv%Z
zMXvXeivOMe#=(3G(uKzeEZP-Nnzoj75dJ+vr?kaj$Et{uMw1@cY!<`i{8AR)K(EOu
z3xInA;JyI(p#b>#0JxPWj48zRObmcK1K?T!d}#ojud86rhckFEDsEh#^8xV2ET4H8
z&gbhbnDI*eK8&N+^8eQWco}^LB5Yj#_5k==@rNG>SHlm0N7w}60zWYT?g)Tu0q`CH
z@Sy?li~x9L0KBnXx^Cm?Z599@76A9PxK2z>zMTnxpAUd1N8MP?(g65jUEynCxc-*{
z;LcXpxa8yfq}DgaO9S9#0q|4NH<lAdhDF$Tyn_SaL)!>scsakogcB?9KbK66i-`=9
zYBS&UP7v&#d82=-;NgQAmfS$dPV&vh?8HEZ^K<?$AmRmG_tzF9<Ue#L#5fU-sSLjs
zgK;r~;k>`+LB=sWjswh{#BhfRpUiO0gg?!2n+eZlcn=dko8jhh#(aiHoA`_Mb6K1;
z*O4NIH#6~HWVn&;YZsR?e9ARQ_Z2g|z=W@0xFPS_#nlWiy$0#NH4NWq!rx)|853^s
zFau@6H#7br6TXe%+k>vZu#@3QrmDM#;r!n-Jk$Y($C@g{R}3%213_^!1|oP2XG{pz
zTso@zWNC`{Lo8rFGCZeg4ORD?(~l-4f&I#GlRDC$Ixb5~8|$8H2bsKB_t=EjV)zPE
zytgpC%%o>H!*|ioT!f9UE1O2o5QXh_#!nP~K!xizhC6Q)$ndse_+e8&t1{e211({T
zXLtsl+lyN#hM%#CDAst=@eq1({<^Ij!-rmnVof@RC!6qo3^%uz2X*}KcJEP!GxJp=
zErsEh8pQC7(s5a`nfjw~3@_CU^|e6R44-1M+f#KsM4EVws{3Xz{vnf`Sq%5IzCOTw
zhL=TOk1t_3UvH3KeV*Y5b(f=(^fJTE`TrWj&Hq$f&hSL#nhf%lGJL72U6(O@n2EoR
z;rS-~J%*Q=@J$RqXu^FA=l^KqT7As<9ANGz3?F9F^HYXLnQ$Y&WPD3<y`Zld-#p&_
zp5f;4`B8=+Gj(~V7+z_@&oX?E3ICbl8%-7WBE$JVrnt6O7;YY~DtbQ3(gxFb*urr0
z__-d#ub3J}Lx!&~*_kE`=j$MGeePuV6qB5m4Bucftt!I{P5gL<`%HGP6T{8-1x|*q
zxRYyQxNe3oHp#h<;e4GO9_j&x7lv}gaQicSs7cO)4Bu|Lk9d^f8JJrTw-kn3Y$A&P
zf-`*AE!X4Y7+z_z!`TckGIdB(8E!S<GZ<cG(q|UKmzwzV8NS`r&n{v3Fcba)!w;I|
zyvpzb6Ms3wOHKXGYKEUT@!w|nMic*ChMzK(`zD6JXsZ8sI~toT^)}%<8SXKSANDeQ
zn2G;6!_D`bUo(7zsUQD=;Y&^J=s1^e;{U|(!=`vIa5<**UgUg}oWB^JZyJXL>2?s~
zB2&E$Ww_78ug~xT6TcC|%S_{$rVKAKrPs#rp(cJSh95S`SGgQhdgB>>&?KiL!+V(G
zbu#>E(?Gl{!`GVlJs951c71#7%kb-d%n8b1hEFk#iymQkp();EhEFubJCfnaCfrz8
zSC$T$@JWn+(3EeJIc};y(->~GU!Sg-47Zr@xeUKz%7;Y^FEZiJG5lDI>*c?~@JbV2
z!ti1f{szNWnDi-Ucv$51^50>&$21=MfZ^tOfUOK~-0FHcA2Zy1pZ-sV7n{b12N-Ui
zPxy-A=6Q+l7;Z6*1CKI%qDh}08E&3usAjmsq|a{*H_ty@X80A;I8)Zg8MwbQjWcUA
zyg26i{GqcT$<#SAO3-yHa~WV=$f18h^Lf<8jC(u7`TV3Yj>JD)8P4YwlUVy6#&Czt
z5KzhF<N!ByIr^2n0QejxhtGrU*X^Suy~%JsfAu0CrwBi*G=3(Xnm%RX@b(EKJ0^~7
zH#&P#O1n7yH`b{WvWI7n9zPuai_J{Ozo*@OyY=WdFizt5#L?_hMoQ+$;o0e_DdX`E
z+1}ma=nzi_`lWR@EVa|S8~ww&8~xI{8~w$)8~xY18~r1@8~qKNAfgh9IQn0&NlH9P
ziP!&*P1p2)Y3qN-P9$0<;yEaAd?Ja7??4jc<8>r~kj{FH@p|OO548=6ae6Ge$OK|L
zD6-BJS!W|Kc{`Ef@%Z63j&T%KLI*uj@w)N}dMfoFdv{CF)2U19rpMbYfn?(wE;{NW
z+w0MHOCXu;<48_>J#0c}ok_w-c~V-(^=OHlNZdq<Adv`(q@R&mT~l30ibmI}1Ig(?
zLOSXP(zFv{9rc5rksWlU+Ec&;{SW1xbihcb@$2#S6gXa|#OpsOPtdh+kSY!m;?Q%R
zL^w!&Bb~aE@y1Wj^`eQ_L&fWOg06i$JL;+@>biH({d&&EcOcmbdJ^OHq{MeHIJ!PY
zJ<!t`PwO|Kr1cWib2h=q6aD|`@tq7gI!Dj(_)esHyk1W6dfCJ$P!i(XQ@+Hvr>mWH
zdPh<xA&~@k))^fs*E{P9#OY+c@bF1A;P{S)0tT)pBu=k_aXQ{nml#LK&N@Y}GYLAJ
zV5C(qrUX3?67>3(pjX8Nz0eYLtrI$t$o9I(_QwC=^_te+sFS+n1ikLI*Xv$_u53bQ
zJrt{NdZP3MCg@F~eVm>{NTF^Ec>6cbMx~EWpOli5J{;|Q^!Ri#0qMghPMDlPCgLWP
z8(hs8JxMn#QcC9NQRCCohNom^PDqtTPD)R|i6A{L*dh}}H$%hY(}qt<Ps!AEVFr^q
zBbf`cr*HiBOT*cd2BaUunE!sbZUN^uw}%OjVjK<bQ_N2e?_NkwR{@V>ev%{NNetKE
zKF$2(3}$}PWg7F7lg9iS-093uP6qQ6e+KiDlf(QPTo3b;Glluh{CtLMaC4cToC4-I
z^NSd+!OdfSa$aP9Gk*ocHMldGpPW+WH}f|zT!YJXBgjVwY#R1i<YC)(=9=?=7sGqN
z74!cf!!@|GnV+0W=GWlPVSaMXGe61i#{A^`%KRi>#4j<N_({x9&K2e-ygT#jj%=Ke
z9FZR^;3D6`aFTPM5mY~h349UfYb_9;cE0gv?SxU>d}}Po@5w>9d~F8eigJwkS3`y<
zCrZ#ql*3+p)n(Kc<a0btz{sl!azsAI&HUbi{=EcQg9SMvUc_)WxCH`#Bg02BjNj%Q
zV>s2R2L!$`A12|ys>a6od_I%%Ost2-e3g-x6gS_(6vIb(Df0PqDZ)j51mhF8w-Ho7
zHfFe4A7ehrkljb%YXSIag8aS$KO+FYP~aQyEE|Cq2jFiI`27X>8w2nU3;Y2BzcK(n
zSie6o(mPP#(`zTR{Vjh4{vd;*AGHAd9DzSr;PbTx%<}mfMjQ`jVO9uo9u(y8_czRZ
ze&0cLVXna6z~q>5AHy|p=L!5>LcAdY{wu?Iyu3a~FonB<`;fqI%y2WGzrVq8o?d=`
zM0y&}9t~khj9|tG2f#BJt|54#AwoaW{S$2(_66i&TLHtVJ>JVfxExOtF!B~MKD9fM
z&v7$<1LK?1yNlr(<SfL_w_gMB3s_-jATHu$xN8~C<@0jk@3C;4iN$~Sjlh(jiv>BC
z0^q^543Y+CiNLoqoa8(s;86nptbjWOIYk1_-%}dNFs^40#;1D4RMQ#xf}H0BIfVlL
zynwG|_(+Cv`8gH?(BQnlu8~u~a5I08z~^Hd;vW?7mk40nVTP0bLpTVx(j=dk7oTdN
z{Crv9hp{OL4cIFJ9>s98p0Nx!=i4wr&Qd{6nt;D5-~|EXC)F`X8l2YzexZQBF5oW;
zc(H&NGo17sD&Q*^&eO&7XTQJ~>%*x4IA7FGgX|JPzK1<`FyjRRzD(e6XE@h~*B>u?
zPC;^(3;bmE+=1{F0?wb05WZ5t3mBiueU*ThGTa=m#cGf=IBy920s$`-@Wl+Le12HK
zUu3vhK7T%<fxBAZZ)AMZ^GyNYA3)Av_FO{)_bq{+!En;&5dqI+IM;`lyHAi)Ch+$Q
zc)5U|G0EZOwSqlQ(ZF3J@Ha4=$IJPL1peCs|B8UG74V1%<HRhd8N(^PV!iFpa1Ho6
zBdC5HD&Xq{{OJJld$4(L4cvDGevyE05b$CFe^<au8BTgWD&T7c{H%cQ7v#Jr;D;D)
z&WB1tj+hS;dyb<)hA77t0FM&z_l0<!3@81E3An~^t`DzQnSz{Dfu9oqUm@Td1^!wA
z|3JWf0{)?ZA7VJyhu60#_P&Y+?k0inU^ulquYhL=d=Y;!0KO{#eklMR-PnjsGsQbJ
z0REzYZx;0QGTfZroF>=gZxQ(U3^(&j1^!lnzhA(80)8lfoJxk9+aq82PJ;|lPP3+l
zP%~~5@Ct*X9~}%Q`Hu;>li@sFydGu<az+UJi2?AX0=`Y)mkRht0=`kew+r}whI4&*
zJ#2QnLDJxSEb!wPPW5nyfF}!l5nmht_XWVu2f%GCqcl^z{RModppTc~q~|UH-y`sM
z3;1D%)A;`%H^9>bjJ%bM-vfS;&v7&Vm>_?+AdBN>{wYDe$mh73e}(bQ<(Ssoh)jd?
zi4bq8fd5m#HwbvLfFBEhH@?FlX>gJauYOEoIMsg<A13hk2>eWjoAoag_<IHZVnKe2
zfWIi<Vmm4paM8|}3AmW28yT)a_C7&=;hn|_;hzflVun+C(*^uR0q1F>%f$jdO5m>*
z<nI^oT>@W}zlY&mHme5GuYw%o-BLqXu+4Bu&kVz>AFVdygz(XZS3lZp#)-LIa9qqY
zVn#8(xm|GF%#UV#l0O_f-#BjO$Jz`A$rt$?H}m5dpVBqP2&x|w8P4-9m|b%)ob1MD
z3?rwv%{U?abHl42Gi=5Q$v<y+^<$3BI3fN)!>b>kwizeHKWBLL<6@g}V%BFr!!=y~
z!l3BKLpI}t<XkYk`Z3IY4IeAu{QC&AeEwXR^!!rb+n5|P9v1*lWH^;0h2fh+z%w}r
z*U4}yN0F}yxXAC%a1H#gu=6chzz=Z{ZVtmqpL=hBo9oFGAzqQsaWlV|@y+?UN6_bM
zA>KoRKH~(uQixZ?rAULs_2=IqaNMj<Fym8xlKz~}aWmh-_-6f!S{U(aaK16T`jNk{
zLi&ss@D+?t@rrmU!-+pZ;Fk$}5#P>mt`E0k{JR^D2Qyjxy#+VUzZK*MM;Rv?oWlZc
zVL0hCQNY6(PWp?uRp4g{{0M<B;>{RN`g|ww6B(`n`(D6%3-ZPK#_=?POWyvB&&!uz
z=D3;9zdth9AC8;(X(su+{&3vP=if1z%VjN-qd~zRxRP+a0$wTLK8BN?ly`jF!*If?
zI0$z?!zmv`esD{JWY%W|!!=w!Vo>zsT82}+lLUMt!%4n~Uopw&^|M(kBXCzZ*@7G!
z!?`}3&)<(S>z`@j2Qyi=)`mxebChA^L@}J=%@OcuhMVK<&v44KV*)=zkbhjjGZ}tA
zFtMIbWH{+B;sp#ReI6I&6f)eb5C86r^f@8$*E0V7z@8BB4GgDvMSP>c7wdyp;EVVr
zhI4&*K9@!tk!f&F8eaXlf#LV#OiZtj;iQj<A7nVic}kaH91by@<cN0WFvEM8@G}gj
z{G4ov(2wUC&h_W@s#%Od(%^{YXk)lJ-YABf<4t5Z#XH3isUIB-H^<B0yCeBxxeOKL
zi})~x6JIQsWPvZ@OHJ|e^6KBlP)>vMqv6$$gBecxPZe<fej~*zw(DfZr+g4`p0BCE
zYYB2}%%1?)as#{*Fp(d}I3y=j;3qOa$+6x5CpjX&GvkbgTSwq?e2j_j68K>PpX0=5
zQ`-8~?gGD_z~?ye>k2sKBW<KlecfjqI8J;~&S1u&d<bWLa)vTL<wFGXlf!Y!2a*2|
z#<>e_1Lh}(<CL!=pW>s9%7IPU>gOEqfHP4JZ<}<zk<K&@Im}Ob-gyI@^c4BL?os~S
zD)2c@`6Kc@j6-@hW`1%!%ujmW#{A^WV}9bZDI6W)IPse>4mr;;Kk<3p7IEUUsTZC3
zx_&hBjok`ghZDa!<B+p~`6=IQ%umin=BIpX#r))Oobt^^0NXY*m-5ZdLAV^Je6tDo
z$IMUs7B|3&A1UDbn4juT%NyWSf1(8Z3iDICS_`-&8z+>mHaEa2T_WH34pNdRUS`7e
zP#mXtMZWQUq>=wII@36Cobq3k!{5&#Jyn6v-_IdE9XG&9Pmv$V(nWg43Ve=}o+97)
zp3<-bZ3RBZDgQ+|iA+A#!*&9n<CHE@PIo~c3d1*!lRl&?-yRa=$8!)a$4R~@Cq<A?
zdBiu4lYCLm<AQv;#y5_W{PrA#JB9J7e$q9*ah&o;l#?gu(~*<ma-6uLoaY36s4V%$
zapH<{q;}a;$Khp3xH*#yH-q~orDvwJ%SoS-BehFQ$w`sgjmXZH+D)23C+$+RbMRK{
zwG(>D_SzZVAvI2<cBvD_jZ3F@y~j_;NpE+5-v`@f=cJ^L1%bq;XAB=XDP>$b*Yp3@
zwLt)7f!ODgOpOlM%>Nm%>FBis**qFkXxAX!rwuUx2Qwbj_fb5&&*wIpY#QM~*u`zv
zbw=UZ6;@m|0JRO0`JW9>b(3T^z%Fh){wX2|Ki8kTU4Y#vyv7>2%@p3s&biw=K=@=9
zKFk!JkKefaFbl7kcoSK8{v4fOws8-0Q{a;RVw<K*`#aN7l5G0%+JU&VahKX3_v;tf
z;XDg3V&pe-XE4mHKM!AQ3c&N7%Y71_8`ZxlQw_!;R<Wu6-GZHm=k<@oo8zXCi|?T$
zo>Q>F2aUzfU^j;k1!k^$fTW27>4;|ryr7KJzonnXS~m*c-(v)r%ffNTlpV%f+%OP1
zqf^_ZwwrKQobB3X_>E)7#6<l<Vq8aufh5<N(Y^P5ihNC?EQx14M)Z@f82)Z>G~G*U
zbWo0HLHmQ<Qa!A<@EO*a-itN)!p?712H?E>rEuxf5V!RCTC88YJ_2iXL`d;)%i&gI
z-(l@8y|c$Et@T-@HFzmv{VuDtZo5@_+l#Opuztfvi<T-~RVG(f#7rr!P^Wq-)F+cG
zR8LZH(o<T6x}yjSDM|EorWW)*hDNtU`l9hIcdHlhkNNkphRJTbFZvYl7w|9n$x=o1
zF<`%HzUavqCQP<fL=U7jD6mh)`Ye+bsnu9?v(;EXfz|*f8IvLJBo?&f;U{b0A>4eD
zm+XtV4Pl5k7P7};O_#B_Hn!LoGe(nQQ$12_+V+>T=i&PO>lRMxyJX>{ekBVh%|+PJ
z;9sn~8hzU4k4B%?dNjfnfR~SJX}C7p=8HL%?2k@{J|U2irU3^B@6)#Cx(q(zk4{U5
z9v-YMgZN0Vq;3A_A2omU<YHgUPm(`6AN&!}V`YH@Yxg;_iXh+SX%LzkJRnqFyl@hQ
zQHmE%8VUd7aB~p%xU~x>ML`bb$;AqPbPn`OL|l~4i&g&UV61u261xl@*Ci9@sh+s3
z9Yu;<lSkmsLwsW(lk%!?i6W;%&$UR;7$0!NJr|q;a5BJ2f{YPJZ%?{j3Cx2X97+S}
zO}bG&tzbHBfG!!u$V(gY$^$>bhJXBW_K(Fzx_KBq9)zd-rFh<bMv>EyC)Q^iSy4+I
zS;u?`%i_w>d@;XP8gj=Z`(nyxDRPQWiftLuIJ9f<z);eq1bOu^(w+pD;ux(-u@52d
z+9Ph#ao}?iaw5`F0uJR}4s;&)vZAk1+<A^7mry=K=h;ne&)T<6lFuKM<WbNkf4?Gs
zIUDgwQf&5YN0uG?k1Lvl%CGhhwUJE7djxX37R6<?M}2ajK5fEzH1uc<nVnF#VsUO?
z5to&<ATBF)K|<E7=Og67+vBn#=OtuSzpG%)WJO-NN0BX6ihT00B9~)L+()VvdC)RR
zUX`1ly=q&2w)Rp&*4}lBTq{2=OUh5k3VltIPd}%~rC1}r_G^lK{5eUsV4bK9k`|kW
zwnOW1cfKt`E|7d}Mk-otQp4M`B7jHD%+KBqd1DtRWEEg-n_F-#b#YwQZnUjvgzvFT
zkw3@!DSNSQ%FcLA{n)OldypSpx6O#PVW)LDk5<fxeHM3kZ(?2Z)mX2!6l;-}9hjjm
z#(GHYaD7+UmElimxhm{K^pxUU*oR!$hg>yP+NRj_SGKhvvJE%d5B{6B;{<HSv76YA
z@hG?d3fnPKlVX!SQfx4@9o$}sc49W_-AL56Q?Maa|3{+UQrqc{Fe6dlsGioeE41B_
zsLS`EO{YROuj{kH;r52w(Q1@$8tQ8b;$E5W$jZe|_F+T;?1iU)Xd3ER2Fg`~+Y@y$
z3-yl5F&loW58Pg;h^LKUFJ_@mLT>CF=;nr>+l-YSMV^H7)PlIIy{N1EP-nUA7>IV@
zhWs|TPPXHs<d2SjQ<1Y*DY9;dZ2sthw?^n|!IQiJ_uZbAm6xAQHh!HiKl_XA30VhF
z?`uKdR?jS)gmr3_$v(q&??OGLx;nWMay5VS`)FIb%w*U&$RZhJQ<ITb7U)dvU$^<l
zFK*A?f(;@Yi*b)`UoCSTSw~=B--Jz)f+Irh)&Zdw<P(LfjdaAqp9mNKXKAe>zetYW
z=TQD4Z~7x1vXejgAU7F$dHm61G+)f=N@jP9{n2%h7xhV3rejB(+hKDHH^b&aUyAEX
z<k2bMwGoGHV_a6ntb{DmvzXc{(lf>*#gYzl5g$)W5!zuXY{}SH75NR=5K0r-%!{9)
zoqw&!V{FI^=opme$co8zWOXea5UMSS2<?S<dLmsx*r`uwfb{i8y!El)y+DzB!rrfd
zo$3X=EA)J6ts-j&<Fb+!tQiZtV_UXx(nl!6Ub_`Jwg2r|sjxpYEAq3Kqb>x269n6p
zxE^*D+*Ge3hairjursm1^Rb^SljI#M6nQml+lV%|XN~aWXXl?%WaXp$Y#T5i)<T-M
zT9VzcCCdvXIR@@U*pE}NA17f?{(wDs6X&aNz8vQ(aNZI&r6KnHusI8{C;oVO_<zQ>
zaNBbMHUs0uR;k)mWtzvQPEYo!Gc=z%xftUaPeshs<cgRlHTZ29+ejGORQlAXd_MK*
zVjtE6cGs*GRR_~ky=0%VJ1NZZHsq~=+=Cb=%?fo(z3dI8>}3t5B8*kC+cc6si3oH2
zvj}S#!d9hY{G;ajrBN#*<Vfs2u+Pu)OXrhu4Srf1<^f#)EHTWHhVwlbb3Ifn$<gq4
z#l8gLX+1^%VoBbEG1^-mzqDTqb960M<ou<mH!C9KIjbV%6mTDa+(Ph2!haPqRGhzo
z>jk(z2G}h)PldZKUy{!tyd7hO+PKyo`+Vq-jBA^hO7cOJ)k?_TQK-msfVIH&YUsHS
zdnW4QMAXl0@CN2eaw>Se(5EfHcyTPstB_u-Z4#`lj4;-8KVRvK{<+u}eZgm>V<6H(
z`MMivIqtz&5&8L5k1)qUj5|L=ejY&D;*f@w7{~5L9!^AC`zP}A1p1rZNJ|Fv`4p~Y
zr6iArza_XaXn#K<ZIh9PbI`Rla4HM+-3WR2tCGAE?P@NrzmL3g;94a(i?IK>AwpgV
zm-6UA;O90<@=4V5`jxmns8nPJuJwe>4~|IkyGImxAY^ueoU_2=z<=|-2)WsNlH3UA
z*7p>-5Akt*;}?=#{)Hmn2LDgMI)VE>bUC|5k}s}N<Og0<<SgXjx4%pBpPx(ebE_5k
zyWbVr|G6SRk1{#<Df-|SCHebLQN~4*-2M|uE<`(u|3s1N9GBz-luN4%l59sgeOx8U
zN8g2=IIhSs7ZiCP#uYTq7>)R(Dn+xP?e4=qv8ylogZ+{`Wsf8`z#2qW=vMSR?D;~B
z387yQ^gMhD<yjOgKk|wq-v?~u2w(IAXC*lYW9*NmAEYVBFXCk(A5vcomOn>+jzT;q
zfKl3?dJA>ojR<)vc*~KeixE$%Cs-~*Tq7Aa_HeMA3U2DsU^yL_?uV>ez_N+6I#@2R
z2$t(W&PPQ9EFr*2mpI5OfALn!eZXe{i-*jWjUTkM!TD^Q@6Z}qtZ+X;ytVIm&{7Bf
z^VlD#Xk=*vcP(&D9%`8gf3L?HTh_jT{Q6Cj!%>Fkmt#Ekmn8qN6?ylEgtCl~yL_m~
zYf%RKK1N%h^4cKDC(2+aHz@MiGD*Ir6m_Ijk);P^Ej0r1V?T*F=3u=vs(-61G_~}A
zrnbh|joR|2TuuFOiKd2s3>%Me*8M2wl?9TVhq6pX+qQfo$@{nAT8gHwtkTrw7c_Ma
z+SPWn)s?7Qsk;&1T9i3-DM$T!0QJZluBq$Xo|v@*JTdD^5MR8eu3e|8%d#}}C5OZD
zBCfyg*3{PqXlhZerarqwQx{ce>cRt%ouXkaPfcB5*VNoBO`TPusreT)b!NP#&R?gI
zeT;^EjD~%ThJB2lV#C-@!q~3T1N-QKee|f)JRWs=vPYevVclk5#lIZyQ6BX|UG!I9
z9(1GeAjimdNZ9@FT)inT2C}M2UTb5m&kL|MXJBXkpV)fXvVgX}`2Q1Iuff*aVC$1_
zVC&Im{;#e7Ut3T8#{b&-|F!l1x3*rNll19xk};T*jKQ2_OiE=%Oqi6XyzKEszl5>q
zSJ*dVpZ%DWv8PAK&@d@i+0#rOTBvP{uI2G4w6@<Ij2{s$da{q^FVNq4Dxyzjw3Q$)
zn&vS4Z=JqrAM~`XBIdBAyY!S+5i?Dyi1{|SyYyXHcWE}(cE&2%L(-4NN~_?HqVs+@
z|HWE!zOph_^55eha{NfF&eu6A&d*tE&UMah{vmC<ZHqqB4*%xRDxwQJw3X)M`lAtd
zG~(3tpX#ZIrZI8|;^ArG=`hw(MOuFN@uq3P9D36j+~d6I@hNSvo^4;qDn}k=`g{tN
z*Bi*I`{je#_hB!^9Pi2!Uo`ShpVwQJ>x&-c^F?p4V}4Msh#rOU?Wjs$G|h3$0w*2U
z)|XU7zmr=Lovz^=*J|b!acvatE7FsFF{!xE&GBHa5!d%4z5|HoQ@1ZVr`VVeNb^Xs
zGVYz9h1>$T&p`Ghgx{BI%o$HYyps@i39c{3wg1E3w}3}cWNlZ^Brrh2OduhstWM-M
z?F38!H7IHi1Vs!QK$o!MIxzu)+%LK)s1pbRk^m8ui>QMVMNLS8_{pvy69@t17LaxE
zN>C6~kf12IBJ;neyDI4^GlAXjw%`A3KTl70zjf-=sZ&*_PF;F>0>UPNH;wUWUUPy5
z4B-n9z87-^6TAlB1)v=d+VO}#9^vB=2a9`&>a!^mBXRcj#WrIL2Q^E@IGWArHgP!S
z(JYvs%fj56)7Ncc5$4I7+NMpsjZK^Qnnz1~r9w-5)#FKgWA}vzqOFy>KAX?1w2)5!
zt;_$6E~~gMtNyLa|75*PgiUDzn@BceHrg23iQw74H)<o6nQZ3`?E|i}4|QS3V0VHu
zj}8&`L)7;syuJ(jQI57O>_-{ec2RD?Rl2g?W|-S7*^XIgqoQ5P_CnYaVJm1(f^5im
zv~RK@WJ?;dAHsIf{4UKc{#V!!n%5I@hP59H|G7H<s%}4wI^T3=eVs2e>9Uk`2|bbx
zUTD%;4s<B&udvG(v%@5R4)jQR)on3&gwy5!HCy}|bV+rU`u<3KPg<|<JHHrGkL@zB
zcfJ_XIV*s+F+*<~I^ChOdb{{9@jD`XmW%ow-M?tG9is35*Y`V+Pu+(adAQNM2)psW
z>35KSAZ&?jH)zd<>N@!w<acEK19pSf64n2N-;u!mjt=B^U@fpO{Ekfc8@=Iou>2&$
z?}(T-u@m<@Uj7&GJO2OF<^PV~@&BVPf8%$+PF%9z0ooA1qs3?cGJXe@RYQFx*$?4w
z{8!qKOYu7rxZm-Q+K>NDzeBI{Vf>E&i?;YTeh1a}!5@eE9faNbaj4%x823A{|M2pB
zjLrS1V~S(hWVZhK$=FMq*dF~t0{Vliqoz&Vj5TH_-*d5&`P_b|&;1JK=3n;u+#RD7
zCBmjC2~k#jCd#U8WdV1kGvKb!0`7n};1=;-)0q3!bmo5D%iOCcF!xGqllD89`(-C{
zzmmk<9kK2gfpw7ttot#nMclrMDI@$?ufjTT7S<GBNBA6s)B0f^!aHM)s>vLzHDa9u
ze?X}0V48coLvwG{G<Rh>_MJF2cU6++uJCH^zyz&c4xi8cs^8~+J>YY%uJJ(*BR?-|
z$decOX@~r@MSj{LKW(kb?Hv?lgi}#^cd#m1POI`d!sj5o2;q4M?~MF6LB5UrE081L
z-j4il^+6B*fP0%3a94Suj{x*h^WUit?CXIZXzh+cFaLY`utFbHAF%Nddbw16taAoj
zZ~3vt5(rw#y_lE5SfLDUbuH%iA3_}&jyh86!#X%{FP_KYd5souO~f<J&Ex<Z39KX#
za7_R<f-!fdgSkfmqp=6tzB>!g*vr#-4DQF`zHGfk8H;CX1KEgwE!J}<BWx1Fj-mcx
z51V_>vrM6I$Ey*_R_K-1Pe)+wjOtw<tfS!%S(WC6OgU8)q09q)9_s3LtR2N*&Cb@v
zVf!{Q&PHoa{kOKZjimLb)OcG~hqku)yRjyWYZ=zx3RexW<*kah-Mt?59Ww0+VE;qF
zqBu&JvS>HfplKbi#G)KoXi;c=u<3hDS&O|Dv`(0~3;Q@Ku`awFGE`vgZ#&uq)?%|N
zEy^~gxkvt)Dd||p-GlfgSPyQ2I?{X@((zf8y*|zDMP3#Zffj8?#U8EMSQoBB9*U3_
z<#EqK&0X!pUM|1p-dm=*cO+@<otoy}9MIhFSEG%kqpf+-?rPA^YSG>%Xzna6U=i(J
zT*H+wm2D2nh|1Yqwp7+sw!;xV0^wBFRIXIkqMS#etf)+>e6w*+<!vtC)36hwd>;g#
zD%cQFzV<(34+53%O01(|KT96UTSeQ+%ESI7UarY6MkxLHdg`ecB9s;=)6FPbst*_s
zyL%@(^s?Pq+{(tZcw3Ry+Ln%RD%<`qw6!^2Y-=k=`F2_{#Fme8Ojr?bTYxft4P}>y
za_)*XU={1O1B;mwjk?#n82cu$uV5$2I0tL8S=d(-h4NsKVJph|8kBn{)QwY!lYsj1
zKGWP!qkO3x_h4V&c$9P0GL$pQy2{U#c_`!K$PYvPdEcSAv%%+@xg-bKRoGJGm&$jq
z(<t9vWroe9@(l!x^4**cyXr-`*1)dTqMRo*T)tVp3zn%3>~#s*8{@5@y=bm?FxUL1
z7kfTYZm)Y$t|+_FD7#T8-`8s@TyF$0Z;r6(D6>qIeHrRTv6E~;g{#zwxn5tuRpJl0
zR{1JiYy8-k%iH^KDu2YG`lRy;1Y8e+4^H=^2%DoEqWAGS=VR~1*SJ50>lau<pNX@m
zCg8dc`j~<HFLBaZdmq$)3$A1vyx=()eb3c+Zi;g_(tHg(w&E-SpFKF!px12(dl}~>
z#39==5LgN7O>e}db^IGqE+^Na4A92rylYWbyvvl9C<}&uZXLp@ZXK(J?Lj-90=jt6
z<)LjqgZf5cPi(U&)3(88;CVdqMA)-c5z1+tH)Eec&rM7j|1?t`eA=SGC)C5Uu*blb
z%akri$H%dQUt5$vP}<9}F9Ca4F`wi9G4~I)^4zwzQ@gQO0_P)GQ&VT5Us)ERJc)gx
zqhSjd&lqAmJEM*5_i6XpdSefUGZ*_UpkI<BxHCfWfTllaF~{Z}pEl6een4wm>j4g%
z7ww|UI$D2^w?!kL8$p|kx*mi57x&=INBxha@HW`<^8Qt}tF~Wd^FxocpTrsOunod>
z4Cp4~+7@wX@5$%5wm=-BD`1BhTi4FEY!UXm;yKS3WO=m~W#`**wiS0=WwV1$+wn(R
z{~cG_ZjMD8?bpt>@txn<ngRD>@7eR<#}+$m3E0Om?ar%g?^R(h)0@~Ii@hf=eGWZQ
z{3Y#dw5O@(4ZpMfr3(8MKSvoshs_b@1Ku=`DNbB-aaz#s6kIi&3^?tHngzLv(0`PJ
zp6*w}K0O6p3<nM2#lY$Qb>Or54YRF6*dWlA5svFqumyR<2hT_FJRJSbXxOd}xQ~7Y
z<%s>y9T5LVwD)7M-+91eaL;g`in1qPN=1E2KpotUuoUdmvB5qSRiZwjuB`&Un73o~
zu(2o`8|>6t+TVNt{9v0}B5d_Bru_LB_7!1&&uG+NJJHW)%Jr~6GmDsV7wl9t(Lwg!
zuwzk3kD>0_AXoFJuulC7>NfVpko`Iin^+55c`d@a<2;3ascIMcB?ddhJnmDF@hhBF
z$am2Oq=ontugU>C<nXw+qO9}IS@iugqap8BCv23{<36$gw(~sd6Xh3aS0-uhElD1C
z`<=K4zd>JFlr27BxZm&dxUCB;%F-I9titnN*tabUHTRb&|74W!!G#|5X&(0$Kk#oX
z%I`m6%09oxT@?UUhW#|yr$u|RJ}mRNcLpF=HRv?#cUjAn{C(KJ={5GaY)J<#`0v&{
z?p^5~cNY5L-YA#Kl_&$)@GV}=-RDP(@&@YMX0OM+quS%Htns+F*MMIwY;mop@qIFQ
zmqGNk;`*Pl$*{lfDX=fHO(s8!Y;rMda)~qGDrZ=ib5^*@d;!>E*l1scYn|V)$>^)y
z4}r%B#2Jq`W}A$D+C2?nt6@LMCX)@Fi#AN{mh?0O^@nWo{jkYoPiNw}Bd)z+ldZUp
z1W&TbWQQl>KAY~*Hku)<J<eIM72Cj@_K1;f9*z1x7j<k4!g_5$J)-)AzS4#|*Mzr`
zBbD$MKu3GJvQT%v#&ry2OoCsvED!Y>Hm(C`D_~cid<%B?EwowKxqAcXzY(_LdGIZZ
zP+kRgjM^0T?EYmr_Nl#LgkQ5ZLcz9Wr5t&vMZIK@p%k<fw+GtvO5ERtwnP2ZjZN;i
zW#mOD$?rud*KUeX-XeWK7euR~&3y1We6!b?@&@9pZ#&Fpe+vG`>Im5K2<2&9gKtGB
zZ$Fh`d+!~Kvhf}CIoRvhvuRsfOxrk{0={Rl=g$jUJ9#Z^ZP`_}0Q52e?YE{|x@~lk
zk=L*N76spbbPrlO$VPq%*=M3_16j^?bJ%7fKH2Lx;vn0>Hn#rJm)oNAue43MFWyG>
za8JSSZ12Lh-i+`CD2whWlf0!4+qC=I+BSA~*q-~OwXHVlO50&7lVtP}h(r7D$d1+k
z%fvJF)nrTC<31M{+1F_BxeI3%o^{&_eNvtI9C^-y9i={l`inb2J03DqSPpFJZFr`A
zhD1YqMRuT!M<hRBdm{NN+X>K|D1%->{|w52o?oqo-7A9+2E8++(cX{pg-&u{1J+er
zl)h-UcfpS&o0;<o+Aj1`fcra9K2f-?u3*Yw*tl;lV1MQX`18OXfc;zzKW->&Xz6<v
z<#O1txJ9tfkS7iD(Ac&N_3ihtKLgRn%!fP_Ms~Ieb&qUR4xS58*NgC+2Vd`s(@Z&d
z*6`^HK%Wgi{@eBN=}~4~K|2OEc|X}!U@h_7r+_K@nPFo|4^PiP7};2ae+}QE_owwX
z_9)p{*wS{VVe`+z4|N(g_9)~yg44ANGDAOEkcUAhH1-homf9NG+bq<12l}Tx+`sw~
zY@HAG5%;rZBHlXKPxN8O&LBS871>@YuWFyiosIT!3bM9GJIg-`Tj|%_9bQHJGWejS
z*tZOuJNl$W*@b?AY_1iyGvOfiiXMjD-wIx^xqap#KgTV~?u8!rUO#NM7yMsFe*~Mo
zQ}ejrPxrXV-tL+}V;94&ZwnZ9JWy@e;2kxF4c=bsaSMCi=$^#}ZSgOb{J>_sdsV3}
zac*{v!8%KR7yKuPR;f<&Z&9cFwy2MLx2R8OThu3=ThvT%r8<>Ws*l!eQ6CFzp?4S5
z{7B9EymzzfCEsS(3fSKlu&(+Z&JS_=Mm5u>)@*h^8rbZ9%)i+^&9~X@y^m>+VGZ`r
z2w#e8C9bqbpcC$CZPtcoTBG_yFlY&O-#4UZpMa$;&Ii5>-aZZ3wALP0faftdp9AfN
zlMeRzTMpRk3fCCy%e)b5g*~xGnBobzx?~1ie`w)hJ#n_h`DQZ@t9c+oo9^B0eq7t^
ze!{ug{UqD$UJPD%C$#e(r2nKZpkhC~`gkCqK2Z};Nw$ISI@s<}Q7PTA7x-G-R}YU$
zIfXQQc+bdpb>Nx;tdlIlo{EtH?DwdtaK(T|tW8b;wm&l}B^hxs&g(Sd&kk0FK59JZ
zCSM3z;6r!DzIXL|^fTmlW&<aC;=u{}&XW<J-oL5p5|v_y4N3s6;iR=r1{+j>cVBL*
zz&?1`7TRAw4f*^OI+=s_Vjcp&v0ltOI59tgy4d}2!1c-4sFWLW1FlXhu~!7VCnCM)
z!RvXfZ%@Wv^<(Jko=19QuRhuBU{3%ipYaQ_uLwUhAS$KGhIcGKbFk^iQ&;#5A1rgQ
z!w9#4?z8)&Qa-sXDuv!<90C55!T&|@`y*s|3F#!x!B`e;X*2dRKZ)>L5x*BOvW=PO
zGe?5HH~72&+7*a59nX)0?sMpe<fQjeez6{6w;<llh)4T>D!}VOoK=uLI5UVr6~?d#
ztAI>j0DlkkUm<<Gh@-d&2euhu8$o*(>D~l91^7zfUm)!=r1cB>h8uzR1Wx(NMA*&X
z(HR)&gVK8jbR7}?GVZCoijY2y-);a+58%{?yo_|nf9M2kCCLMf`jZX_&&BiAh(~4j
zIC%d6e%FKUI)pt4oaCYMXb<cm=ph04NyH)9YjO65o!X7-i^$(X<YgFSE`gmcM7(O8
z{UJ+h*lZ1HI$*cSZ;t_A8|q0n^u5V5EK1F<7`y)pTlBJnErufyxH=`J)PwQ&&lrdQ
zjQ-pcu$0DNPDaCg$3V?Zb-3jj^gXbRYp|c%pNYNIu*3PV_kQfB_MU|Q5Bpa<5n;KS
zTl*SgFO2KntiW?2<~)z0Y+yIv*oWtpcw-%Q(RP?AuOG&9x#q6<6n)WY&5e5NqIU)Z
z*QcZe4kDlbfp!Bsiv31ze+<q8=o=1Llvf98?%TE_?F7gHTc3yivG;cLLz$pS*4!N{
z;Zwl=C&2f~Lcg$TqUK%&9hIOTB>Av+$^B{}_L1*1_M)#`iSTmGU2<A;KiT~J?XU+f
z*aH`A0qP;{y)$4tX25pLfbE#!J_&oY;vLu^oV!OyrF{G??8C^Yl(B&dbpYDVN5>rO
zgRH2O0@T|scs}^0gLwmrdu+AhZiQ#q2X%Oj;vQG4xT~)<=ysPnSSDz4KvR-{b_tp>
zpcw%g+QTs(VGkmV;*JKM4V?IA0Urgtvk!irm31QfFdu1Tf@V0Ya6JSc^Hc%)a^#}~
z`C02T_AXcUiAt&7;$X38vt&bx{Me6nLsW_tb2qz@uacD(#Wj~H*+`G%p)p(SYP=Hz
zdoT)VSKb+wLhq*T-|1lYfCs(LLbiqE`)0L8nSi=G($5r<k=}>d9Uqmlb%}%Rt#Ys;
zC(^ntC1oP$9jND&2F(?d9n{~c1x~tb4IgtXuwyaUzYjj0(LdAt*bey1!$C6wVKpdo
zuZ6j*)?=;;K9P!ffMb{sn2GU!Q5k$5kLEr;P;)02qpwA~NhrpARV>;l&i*T7Z1iqJ
zQ?$cHXos~4=+B+4ZAretwobm*wjy6F<`!aYnP@*Xh+B-dMdO6sppA!Jz5=>tXkRO-
zt)V?rn{NiZ72$|OZJq4Xa(`PJ`I5<3w}Q_L{{grUac)Dpi8yWWpXTA2#>W&!c9HDG
zGRV9X@{-@>#+e6NvQzVc=L4Sy+6dq@ws;O$9-dp^nQYQxgwwcv9-arIJwA)@KcG#1
zgEBb=J-N_c63~VwRicelVk{7VzTh9F!zb;%6m4uH+AregA%4k5yc?2?_b0%!>aXzW
zUWFe&5p#X;UA`*R+)dz{MZkCL0)O;+_@CeH)7(c6gMJ@u#6HmGYVMyZfR$tJ5AO(l
zwNi7RIF0zIca=A$q?~;fdBoUc6#TGVe?eQBiE@rHe5}#C;YaN-{HVMH%^g8L6Wa1T
z@O-)uX(WRu?8ldw2bc@KbIO5v;MY}X?o{}Aeb6qB9Y=Yfe!Yw`p!tv414%aH{g4@_
zL7Qp3BeDWM0lk~~Ij)uPM{2q=jeLPB_z1I1eoB!1lt1kmLUa@-2tS4VlBqQ_+>Zuk
zxF7S+a8L8iz<3jTSG|?wYq{V<x!^;&;6u5l`M0>>XSv{Gx!_~Dwj!-n;13_n^|*J7
zE6Z1D30e;frnJMI|Bz30Xxk<GRN2Ts`BYECmMrlGT#I4*m!ZAZq7855Hlr2V`%(@2
zrqD*oHqG`0Tm=ZPhTUj}u!S`N*K&Wr^?V@UIs&_p4F6;#{F-g8qf#mtIar=E;A#zw
z_NG>EgN?sEDkTQ-s`omW4OjS7YC6K{-C?p*kHMFjrhyjr>Ot7M5wLe3+#QwT0PQ^3
zI<g-ly#d!_i1P{TaV6|ArEv^te|%L`iWjzUJZ#psm*As;=P`s$0G~<VGYxSkB77vm
zZ$jIqG&`W3kuA<bcn<p1xo8KEfi4?#WV0^^9qkLg9ds=)R{aLy%`jH&2U=>kD(pnm
z>r9D#-J;JY(fd*KjuzScv4~IY+XkBEpt%>g4RyRVXhx$wJn|t^9{v!|XqOD_@-)Uy
zr!Xf;>mfAfwiWhy&)b+Ez}RYZIp(O=pznd*-rY4N<y+MIJ3m6-`w_;1u$}v1E541<
z+~1(SUxRk5qP?yHMmFi21kL?5>U}e`Q5)Lm`>+R9um@i!!(L9*+&M3z-Rwf23R<!Q
z$8$Be>p9p6$kZQl?s>z(ju(Qq5bYLv%EK7xScT?3x)0b&%+=?j&A}#*T!nShRZKZv
zZrFsH)9?piuXCW^RM?#U++NdsL`Of`Cv4LW*krQRcj0P>j@5)#wmnLmEg5#XH;c0+
zz&1bRZ)Kygd67THM*k1c2YQ?c+C13r=zONnXX;~R;AES}z^;viU8@3L8hdZYRgAxB
zE_XZ99RWNEWwIBxvI@5DP3X7;I!;76Qa?)X2rm%jFbnf9voMZ@JsS;s)C^^0Ls{*)
zCMBgQ(%K1q(faYX;CCC!vOnx;Z|H7k492^#8SfWjE^P&LOtu#KQJ<yxRE(<&(LW_%
zEekrVO2!zs0`?94*i4K+ca&p1=m9nm<G_6wXXc@f<UuCz+mea3lEX-IF4BP>0~4_}
zc3N`>pZui{+L;UO$Axx8`_?=4{2+$K?2ln>as2_;Nx0Tl$FRF_?SVE#|4lN*7f@+z
z5Q}H=PAk2qOE#PQgQ+#4W4!6!(DBvWnc#u;J-G&JWq!<!1+XvO8BoV*0rg3(LLK3)
zP{;c!)C;hCkNeU8!`4moVZSWKa%<pklx=k|YTr)0_wP&yT(bb<fca>1Xs<4MUmAQ}
z)RxQoMx{*j!vDY+g~nfN@l0{cF^-w+hpz)39dPCZum<DAJR$JcK-&Wtjk9PhMB_2y
zbu-?%?}a_{XzTEwD)jxi!!_*VN7$gZ@gFiP9gM~|<fjl{tlM?I9oMx9hA-6bx~P<O
zc)xiuutz~R8gzoU?i=CxN$Vjt2l2>$j6{8%3Oe%hYhXJLqfV0la}Q|fe?4gI7mf1W
zjyCJXT%3aXK5b@DUwgO;VeJs+MLhCPitv8*9}q@uyCuS+5T+rl0z7X8KeAO<0{<Ow
z+IK9{BR{Gq;*m{if$(Msw}IY?xHP`Of1Y%14%`NOwMmY)h(osF3WT*q7z6!Er1f{C
zlY;Yd;Fkd>85Ep1A?`Aq<Of9p_X116N%oS)W8_c80#87iH8>N%oBXLJz$1V=!8;e{
zBn7;|zYXv>;tyI6_)r@kof~A-CZ<52<Y&?QvR>$n#&QoJo&~-pgPnOE@h2iqEl%>C
z#F!~&oiS$efqo$3J%j5;)PH&0wD%_mTLC=qNJ`2nw8y$})AtVMM4VGGn)|zfntRQU
zupd8Kl+WQWsAyB)C1~!GXr~3Ijj<E8vy;j2g(hn5F?<YlB3E;#eQk`NP84eHL_T&p
zu~KtK9X7^I-|U0!K8$$fn)}-d*xS=M;RDgwY5Sy<lx-f^kAtvb2Ql{+13NMh<DCP>
z*r_T3HY5}NG#@u@O@=+02phA@7%Nrg!nRakz6yOohe~5yl~9Q}tU}lu*lW_?=6#5>
z67y7tL3<i@r(AO%x*g*tt-^&ijsF6vaGgV*(XO$6THy+A>cT$S5QFXv^FAF|dN`kk
z@V1Ee6~<3T&Hh%4twVyaaaSD+-Y)xC)4ic%GxEcN^M2BOr!3#U;9M8>Jk`y0-TvMs
z`(^*0>-zUx*T3hw{-@@;(4U=QrnxSRw_&qsZt3E4T^OsvX1@U&>4%MEnCGJZ!urSO
zy5M`mX1_YnJlBOW0PJ??To>bWUGTHvudkX&b6x0n;FJD$&2{a&gt@LU7!%N37v^|W
znv0@w&py*!S2p~A^IX?x&<qC+jVH$;jOMy1?nA({fYUtE2;k#^&-O88A<cDRzJ%tw
zXk0cC{mc%GH}+s|sLSSntH&1sSGO6)+}2haAHipR3x4Hd_^`<@2VA=cN2N4@e}CWu
z2g_fHJpgl=LgU%q@bPC|jq&a(W3G$FjR!E-MRWZ$*F`?#AcWKW6wPbV*p=p}zFviR
zpxFlBoBZT`7}s_L9rEC+?wyh{1@sQgLC_vi@=qwAdpA26`GxQ)@y=$2i}t%kVBB`R
z0Ar~bW6tX}j0-Rx*Vh<muFD5MuxK6r(=9=B(>#|A<EIBP7FoC&Yi1tJ{T1p<7RD-h
ztC`XS{u0f1y^irw4Aw{0%VKQ3aZme&o`Mf_Eyhz0ORVkDwykWvm$tGc<Nm3oG5Yvw
z3*yq87mXKfplzLRQQBf$MeE-*=SAz}<U84bwL;iT_&t8$<cE=87zv!_pJ)t5<E`P~
z(FNy7%yqp1x@?pk{f7#Ty~qcg26-Pt9<o47V`bWlvk-EQ1wI-$@pu|1t;7Ba7^T-7
zC;6=pBb?Up`s29=+TJ4we;Uty@Z1yg&RGbjeY?$Hfe*9-`M|!9rrR-Jh<z$-JLVm+
zCip$t%wUY$XzuGPw4H@mQ_cUgMHzR9DStYIxzA+7Pb<dwx&-6v?{YQw*$QBV@Yk?!
zqYw6aoZM&3X_2q?<zbWq#@wSZ=B7EVYvD_MS&q3T%yWHvT62?6)qfxSD$IXnVLna8
z9Kh4yN%KiFW8i<$JQwD@<`!yhnrE7s0G=3M(*O6&0e|wPW+uZYn}@l9a?Rbl2K)Jr
zKo1_x-E#?3tjNc#JdAhIRuZ7|85Q`?l!5qPlzpgInVS2liJ0>`40%rDzfc&)AM#um
z_fzz_u6ghN_s(@K*zup6>oWUPKdifCpXy(IuFDI1mkoROkIZ#VfGs3DYM$#F4?gC(
zt{jA$=en{GPWE;L#zSJRYaIAwg3dhGwG%dx=DKLV0{u^snCsew_I3bmjP_1-Lc62(
zI}-8Db6xL)hUU6xU)68sx~gC^u~t>?NBe{g+)nc-m`~{pJD0HTpPB1IAB?sq&vos<
zI#Ct+PqI(N$Op}JeT8-~g4!C|pP1MBDpzxNqWLeh(XR?Ieu52t2>K~P|8gAd=-58Q
zTZwt^ryw83L$@REqoBjD%F(W1PmZEp9X^dVj(INHGiif8uEJiIV>rh`caccDH}paK
zZfI_7E^P9eIN0a7SR4JH70r3wxi|**t!yQ;THE&E%!-Y%MMhx$3p!*Gt@XVsG~bmp
z2euykuAXCzoyq=ohHXtoo2R)h1@x7;lAWbJL$o)9{$q*ey6l)!qH?J!!`>2<K@7?u
zsRXtI{m*LXz6iRH#{Mz21mjiM#Cf3IjrpxS<aZZzbPa6k3DDEK5~DG{bvyXdzVs&0
z(@yBE1=%Z<CtDBt80^<hFy^(kflq(*)15G9p9Q<PEg5rR<<PUo7<;zIoMImIMDt|&
z+}1$YL!Z%iR_+5$rZLy$T#U3aCrWc&6{r6_*Y)qYu7A&U{qN0nVSOF*UB8{{GR97q
zHrHj0os79IW8C!5&2<@Lr~0`rW8Abgxv{w}V_Zdh28_8b*qZ?Mz{1}AkIZ!mf9tBX
z|L|Ow!i@i>$6_ICf|Cxy=)Nh=A0n>16&GLJsV(o}yycHG$J~l@6X6Y<0^fuvi07-L
z0T2D)aRgmmIB<dK*GA$S;kWncbEDIhI{d*=nd6+vJ-T-5n$#t^r^;`cgQurFHVA+0
zs3=!NTUkF~;y?BEU<+&MQ_`0x>#cu|_)DbrZWCzXm(;(do=g2X_3x=InTB8yilF)c
z{Y65S&C}2?X)PyA*x3BN7Mr4$C`&ENtji<TNB+6VUz)ya+t3VgoBu8KVCqMyA9LQn
zq@GXxHT6PjkWy&@25C6g{TK4f0IvutNO7bhFBwZLORdWymPh6)TJs8I6e9KeSQjC6
zMbotBo4jDNWPg(OY3gUG6{#PkA-vzm{R}naDk84&-6rpU6K^WF4QAeWCak&v>_`LH
zxdyN(i%^1bC4No`qk7T9jHR`7px9bc45CjrhvAv%v(1=9?=y#iwinLDW-PU(H`}Op
z?>5=cbffLPW}BKXQH)w<=|}a?Tt{>n4w&O}p2y8tV?6aL9l}!Q=|YO;18erIml;cK
z>9_P-Vx9h+<@u&7Y%er>vH43aa+ReK%OaOI(bh-*x#eH%?>a1OL(Ilj@3r0(i>SP2
z<IZSDzw{Gz4b}gX)K61CPd$|OA?g@aeNYJ|CKf!RXe5w{<T~&#D3@7x=6kA!5$tr(
zjEQ@zZehh!{ng6M88qXV*^v%Ao-N=bLhuhd`k~u5bUy+Y(F7i1vk)loDT0t+1wK6t
zo+l+B=$D4U*N4H6*g_K!eA@E-h|qSpq5QVv`0+?1M9>Rd&<i|6;!}s~En866>tRk9
ze03N+f(pWPsJJINZ5)^76u5{Y<VltI)Zwv$(zptIN*H``7`#@(Z{TvC;rJ9@4uZeH
zMd|`~(DrN7A@I~N_^>ee^f0)~{2@K0Bl!0VgXfq(PzU+}I7$CwI4<W);35lx{~U>r
zz&C`!4}`(bgu%tP>E(0-<`3^7P^4SaQBWFJfgcHjJ32K?9~%bm7Y0uYgI9;a4}`%p
zIyWrOurPQ{82m^WJjc~A|1n|ksbTP5*EGyW3xlVI!TW{5)5755Tcsj(QNHaG1!7zU
z?hJ!>2!p%A;E7@Id&1!T=;LFiL&&o(48A@Lz99@A2!kIFgSREaWjcgBS{OVv4Bjsc
zo)!jQ90m`B!6&4cgv70oGmjiW(;@JMVen$jM2uTOe<lnb>uH#NUEhXrNB@TL9%1m5
zFt{%aUK|D=c4x!sPPn^ae0mssDQ5r8htN+=7@XZ>rmokgKciv%{IG^`=l!8LJy{ud
zN~9ob-N-sLz^ae1L$NIVv1urXr?EH{6TQYM@;EM@#g&4sAzv3Jgs~<1ufkR|*Kg{6
zR&abybRDGMtmJrys8IY>j?baNk?D9-zgO6l)+P|QWgK_33dP^nAAhU2zi@g@@)Izz
zfCJOQQa@X%<B|H9M!)4hJM?Q4Go;_|;ke|MvJW`U-J!a82RSaOfqlktNe%1^j!SMA
zJEr4!P4Eu=rtaq?$Jc9hkbd(6$3K<o;m;gjCDn6dz@acP9uVA&xd7-*%CJozFVoU;
z=n=*7N{QaVaoHb^<M=trPrjVv)e?Rc$7O%D1ILd@ic>khLBhLoT=plsb9{{X!^y~E
z3dd!C@@9_bNPcE-j<1Uk)z9r55BUHyNYXgIT1s~y$IsBmNKJ?F&01_3k?Qk(oc_qw
zCSu%rIWGH+!#Q3f`I8TEJkc4-XFSKJCWPWsIPPp8ia*NnF&#qjCpa$mJ2N@nHYJpP
z4#y8l{U+Xt!~we`^@G?0tK;}EZODOdV(Iv5Ed*wG4^YQ_5{`Gkb$pD6uJy<398Z*R
z>?Ovbu;Ws{`8LN-NbUDsj?4W^1;-<#{%#w`E2aK%H^=8l{oMx~_ey*|;`n084jkh6
z7zsbh@eHXS{FdWwCHyqU4@mvvS&ko+>=J3sTK5G~da6IHAz!sO@L$2F73M3%xJJ04
zEitb}xER0m;B2nqxEQD8ar_33i}8rS@8-A|cL+R(<6?Xv@LZ0Ial`>VpNuWzxbXiY
zc)H~r7k<8=f1l&RKNs|0a$NY~b9mQxp5wxQ7W8qL5+XeaKiS9WujRP#Zw37w92b7<
zao*n!=eY3S1kOf`m@@eR@o`r6`yPXDdm!V16qc5x7!PTEJ-0p3w{OTpH*>59CJoPe
zV8jF6yN&n_I!Z^org`q_)9>y<Nk(|q_;E;X98w)M?veUv^)PYjnwH!XKi#_1Pcr?`
zV|RM&PLF+(@N-=^{M<l4eRNE}Ba%M&RCQXP>*xnxuuki91AdZ|=s7uwD3g2Wzuk45
zVo@ZDqm$qh*lFE}e>V#1MvvX;u{%9pN2J&3v_#y4upWAJ!n%``J&5=Q!fqh!2BN#3
z=&mR1dOa@DT~Bm9DYPd^*^{K{NvZXu&}2Q~<Rl_WPNL|^NfbXhi5Mj7`bgGwk*o`r
ztScwEyB^wI&xoE>a(7~$+?^OF>+&V*0w!O79WSr3nPbL0(5=2CCUfBW9->5bV8r-I
zf6APc-9sj<!zKC$GqXlF7@<BuiorRhgh?Ug0f}BP`b~uT04WAfA^G|+NgzOqAz1x3
ziYcT%j{cG32c}HQ95)$hjcuS^)Q1_ZgWC~t8NqKvdlu%^1N<pLgzFT3Pxx#Tp2zQH
zKGQiv4dG^AZw1E%eLEgMAmRLxeaanU;xm`i(&Ze-Wj<maSKxxrux3W&+i@9(Q!^f!
zz_a<YxOh2zKLBQWfy?yS5`V!*;4*!V#9z=0T&7>h$4{jHrv)*tZTSF!@OdVDIv-aN
zo^QhQxnC^wBlKU!@!p`te-PCj>%!0<G|^A4qh{>WF!YSiHxqwRmnfQ;&ld7Ta9;g5
zy<A=bm-Umz>E-egxJ;ie@#o1hfy?ywNc;u8z-9XBe1Jy!FA&7IM)3ZT@cAY@r?v4w
z_yQ9?HP(0#`Vr-|g5#vmhfVbUF!Yrs`bSLk+r!YGFwy_XL@(y`g**{F%*n^i8lD%L
z=sTG3LKB{7!k;nWV*Z};Wgew_IlV|%=wT_xiQYV<Sse!75C-2B2CoT&i**B8&O|<*
zljRiSKdR4DP5BkLTz}FyAF9u0dV$OI*;2ZqTvqULqy|2VOzD<!T*xWtkGD05h<>q&
z{yfKJ`W}4TN%YT}=+ih(`gzoZr*mBJ5#^X?qBocO9FEI+6S%C0d`>UxP2e*9DoGxp
zhfn#qS%d7)ndGnKxR77ayLjaz`Xwg%UL2R}NqQJO+k~6-Cf1$gbeEdaU1~~qg$Z9~
z!dIE_<tALLFUfKq<n$t4QC<!{?kD<ACO*zEc#kl6Y8X5t4DL7K=6bS<<FcG$-a@WF
z0+;JUCFdj8AA!sCpGxV9`Vi|hl%nDCym71lx}3&?kW<im6AWC#WrcC8|4!k!OkaGp
zL8RgGf^n<=R&t!m<uMb!o#TR!D97_Adb2)Rdn2-}H-XD~h~V_H-UKewx8w9e9-)VO
zIvD9{xV&iG>c2T0m+Ac-4I&MfmyBEecOAz`59WMr4}*Vd!p(XU>!NbHV*Y^iGtHE)
zz~%fN=ln@OW_p3k^kRKgPFJj>%D7lBm2t5yD&u1PQ^v(Qr;HzW8JVQ~PB-P3sm254
z%Z%GNPV|qP=#xzJW?Za)%IS)AP8r{Rjloowr-tLQJZ;_e^yYNiah&MQxLE&`(-rHS
zGCrkCJ%6(v=>O{IkmXruqJP35(SKK%=*{>l6aAAW`hbbvj8~fIeJ1(?CVDe|&_thW
zqCa7xH{*$24I;UGdvRRWr&zz0^(NM7W!%xdp1)b29XKxQGtoqE)<cSk-i%`kMn6b?
zv!1g}^kzKAMDH`A>c10A^k#gqiQcTYRVI2fe%?fHu7?pljKpMpiuGq%Z(^NU#*1$-
z_;_&f8@KweSWhN=r3pX4>194AI8Jtz4~g_x=S=)xHu2F?j0fWXiU}9%)iVDvz3S;#
zndr~mY&^*A-Fb_F%k^QZX5f@=k#Vd4PS=bFIo-t^r+jf&S|{{#ocO$|BgW4ejuZXw
zO!zsD3;ykRm{_kMKChYRT?F6|^by7*(>PB2&3aDbI9*nATDtV;V?4<Esphz>pI&_p
zdg3$FxYd7;a9oyW)gATpubb$1ah&9wVZy69F8ByNu>J-S(VO*b<2cFlhKWy-iQX)y
z*e@XX3%xBh@p;q4N9+qA{>3Id!0BZ^2Tkdk^?$^~zr@5R@lNA`_^&bHV&8$xKjZFt
z`cf19iFD&ZF2|^W1}>Ll_TYLxWhOp39GBDeaa`7azKKt{iI3RVK=hqV_z{VJUHRT)
z5Ro3%n&@LWF3Xe1apG^*bBc-2TP8lv-y09Ie%5nb){iU0peK1|nfO$5T$X3yuzLD+
zCi?XpCpq&>_y&#(K0*&CO!Ve@eum>D&)X(G?d~@oh`(7*!7mGV6BE6Y-*>~cWdk^w
zV>5jxPIE1;O-=Ly?;+85=QN{mZDyhu_(Q;KCY+mpmISPM1Ne2o%zU_+q5m;wd<s*K
zC2->3LPw0BLHwThM>l{Ie>0yPPD6TgnCQpwd(vB6132ZwOh2E~P`a&5^a7`JV@$ZP
zIh3#125`z(YZJbn-;<p2CVT_GCpkqOAV}aOr<wk93dBKjUZ&p}KLRH?+nVrS_&v#a
zMFTj=dASKU_H8gma$ae|jeQ%8k(?bHz)4Osy;CuWC|##<tN#j|(!I)r8~ZJoUQe!W
z04Mqc6K?E>Fz7opfD?Ut6Q0fGp?q~V;W-?qe06OAr+k^|r*nGBmujLHIHl_{;ZK|B
z-3{PGe~k%WZKCHcxgIKTqE9r@2RJ?Di`u+6DmhO1qPijufm6QB^kRLP_#_KrTo0P~
z5HE2EocNgOkDKHXHiY7wF!3Q9Bo2WSA2a=V6CbM6b%({cBU!Gm198J~;$x=AD<t|s
z`Jym!#G3e|2oP6+6RnxPgNaX16TQpC=f(zb;$fywG4Z*{MBmHA=avR=;$x=o%jv0X
zxy!7_5;*CgmyQ@e_n6YX)r4o5($yNkDP1!??Qfui^y$&>jGr+kKB*1h#K%nkFsCQ^
z`<UnjPV)CQ;d!QXMc+Zeb4=;pW}+83rE8{NYT`q_g*aB2`1BVbt^y}MX8QG<p5(v7
zL@#iXpL`T?95kiNT}VCXQ&YNWf*4nUQ@UpQ6DB?bO!T!TKIskM#K%k@Wi^OM{<}@|
z0w?+JGU17wp2~fo2~XlUwZ}mX;FPYJUNi9-Y@$y!@%du|IPo#l)4mfr$PVAD-x)sw
zCp&qM2_Mht$>wnvQV$h4*(5WcM>##|XNXQ|{0N-%W9Bo{L_gF-FL0vguBaaRs!9I)
zO!NXL`Tt<TUpMg|)&NfY&3rbP=()?O#}YWvoB15%^jW|k&?$|dPdQHIOEQT=;8ZSV
z`d>Lcl}n}|##P|N!)wBu@&O%{%YzNzR4!&dF`S;tg=7_nz$ss5K3ACNhYMm{1y1zY
zCj1soPx6m!04MoJm~fAYzwkvVSm4Cp%;z2xJ*6QIffK!%59>Pl(Xmr9AH;RaB;%SR
z?j{W%li793@TpT+*X+zGnXK!BlP9yTlg86S*R07?lCeXl{s|j+j0e^=Yy8-;!^a`o
zxbahlcfI|td%IxgRMuz^5Xa#;4~&?UIo7N#A>}`Qbs~y_vxyW{<InPbcc(;m1)fP|
znuH&pY5-#Wm&OE?j*Zg^ADG5Hg!5|2Ls19KjiL@UXibm0uSGon+4$reQTnaSOmQ#L
zcS^W}KZ`5%y$!|pars3)1sh(y>fY0&K851*?m>{Tr98f`4l!;-x&kH}O7W?^nUA*I
zNIT6S?unKTaiwI$y&k}SQp`r)6d==1;_+qqMf@C*Va`L8zhF5QHx1RFg?wNq_SI1R
zX^vCG7kVXm<g@`X|AX|)Sb!I7H15~ql;bx8Cf6N6%=<9?GL|a^%K480jYOiyTfJPr
zgsjC7BFE1fpFMo!@Nr$Ux{klri6e=A_+i`|zt{KZp`+I&CtYvcd7D4kH|;ijsn1AA
z=WTwfgW38m#(f`LH2qe?98XJnvf9&U#O7m4=~U!Kt9{k+_?8hHtkHg-<&@IgIzK^E
z=|7YURHl~TUs&gN3|RbKnx@)=!MIbWms$(1!+-5`(bOo=+Z`LNrHZB&H)CqCjdk#C
zY+}$AF0~fN;oBh5ni_C0E!vrD$&U<J3Vyv+z5BblvG93fOK$dCPvJyb9SdSKwIqsl
z@L$K{4@5fn!=we5X{rNh*pU|Mg8tp-GLNSE@V#fhq9vBzfpjB~?%R>6QIIb`d8xJZ
z-hicefFWNr;+}+jCrCaA==w&cMuV;d{7*o>laS9*_C=hd$wq6^@!WXQf#dhLSSeo%
zp@Rj`g|3HK$W{?y#4TTHT>u>vKnKz3i{sFiWAl-Zk~m6>bv%K*6t_iMcSIO6O<Za{
ziM*UZUZNs=mZxG^b`<nc0(}&dK0cirD|nDTuot~!G4W_e`m`GQ+y{MvM=9we{iirb
ztBuyO;}vZj-FjN>mG4=iZ9a=3FH>LrO!3;%*T=n9IWM;8#ksNTu38^EdES+=Hpq9Z
z*D7mKFTZv1gR`uY`_Hzb9$%hp30RIRMQxlGmgwuKB_7NDHvVYB==dHRkH`Pe{O9;d
z&=>5_<qpu<9*(i1;U)TpgKmPuT6D&0T|6V%IvMAnakH&<(4A7Cn*kM;8kCW*NK2e^
zG@#CotWalltWX#Ctxy*Y$F~4xc~ttJm)4-448C6QM3cRI5bmj5iwc{>X~6s*Em6o=
zlu>3S8U49hqL3$YX-z!I<FC*X4?s?levvQPO6mI*PvYW1?^>grK1-3OLS2{XQFryS
zWE798u*4$0#YoSG_(BdJ(!Tq6YMd9C4`~aTPTjo9su6~?3o<J#LMD5YQSnZ~k^ZUZ
zZr0t`#nAkfuB!z|k5L$8DE&Ft((?RF%id8+PRY-kEcOGrma^QDY(C^zI9yQ|WGX65
zUuUQO*B}~9OT(u<eNSBU`ZwZEC2Z8|Tj{9$_%5io&B{KEv#sAA*R*VTTx9y@*u__-
z$1b{ZZR|5wmc<t0TzF-E+^I;XwdC0wtSZ{P4d=1gjn*QZi#si`PX7H8>!4d^Ta(AV
zVs*T_#Hxa)>$npC8qUb8kH;^*;_LXq8;-|+zwCH?^5tL0r{DM=Z5DJ4s0Dok>Vm8C
z<Hk~(8avyn{<W9Y7I&-l*y3BQMXb4Xv8B0nvL)U+sCOG{GOmvOR_m$zS6U06^{8`)
z2h=&UP&bMK>a48+b@9=Fx+oHLqhp0y*tbGmIJ`n#Fe_J$ZtJt`&WOygqs<hotgtM2
zK~aM{f5ot)VI2`(^7ST*BX)+x-pglsD`78N>-ifi&&2mo9<8txj8fD^V-@w8iHchI
zu!3~{0sTXU;;|K$Qt$|FX<P<7d(X09Y=qZ-KG$+$IIhsqseQ|=)P_sZwu@m`OB0wH
z?0&w{yhMXf!C$T3Xz(fCZT0GQZsIa4*%PueC&A|g@p<i+M){CDr8ir3c}`vjepF`V
z%dBL_N?_j>^hCS84t%a{oR2O~k;Rbb1zd^Gz-3mlr6sVjCoY3sjRT)I|9(+<7DQVN
z{X|-fd?hcllI<p&UOE)^@m}z`>Ee7!A6ATfjm4Gp40)*UC`F%A0R1c={k;40#pU^o
z8S?xUSK?EM@<P5!(YF;NU!|0<?ic4%+SlN7@@CHGG~|JvOVOVeK+g+E&s)wlE)S)D
zdU<`kz5&bksLQ8Omupd%iN`ZNnOb-=Qx|%`Z}KJZD_mZmALzx*kMg$mGNzXQj;Up&
zuRAY+-$2MG^qd^Zk97V_N5}_10{bmcj*as(`tRlS<v%=Nc?S6{M1B_{zf>324uyR8
zL%vM#Yu`9Oqh2nrub0U0TJS3ezcTQnI{H2Gdm8zzrTqSKw$bu1+RO6#_JZ;-^P{#C
zQXa<|=V!Fz<@N0t`ZMR3+VuC(-)ZQtmh`u+aejt<LHP>%B9#Z(nzi6p4t{0C@7=S5
z3+}J51g9B#+}eS8FV_EL1uj^o`S9I6Z$N!F%4flPh~)_<zO^yAHG8kNHJdgK-{{cr
zeZ+bF82&3(eFv*NMfiXIOlO5UwI-lG8VIP5`2*@lz?b7Zhch@YION2YKFf-Ut=S61
ze>cm)UR=<R(KjDcagM~f6=yIjIAj|>AWk?vQ&{ls;E-b%QrWR#4t6Zhh(qa7*fFnz
z1+NVbIr+y3R;)0~Yti^W{iN2cxV$wxmW1#*4bTTK3`xWPMMGX!+u0h!dxxEu;{TWP
z;i~}eO#gq2eBhfM_(qB!-%9b(H{bB>Hx1u_a|T>lT7^aEFdzPP%XCYfue%3*?tJ0v
z!k+jyT2fo0jqibPN51ZSlpFcfE#Z5vQD6hfXNCRpeZ=Wv44--p?9tx2SH_;YY>~Ak
z=rP8*yaYbi0*_Dilg|YoX8u6KclQs7Fw!VQ8uufOdo@+~n#Gg``Ob%}2F+>kj|o`T
zG}F{@zLD^Uyxc!(wsvWp#{DAU6FKp$zP-sxK9TT+7_hG^mbM{Z$hLNSou5WNn)+y>
z)#m{Y{WL}1n~)BWr#;A%$XkH&mFr2It86y%Sm8;WlQ2V_o!nb}s;5Qocl`Ufo|2K*
zzMC!cZ|<$GS$b70>2weL>NUDw4V{v2em|$BJpT@Pj?>hZ>1*S}xFn>k&{zA9Du#{*
z)|b^?7{>@**7Ju>(U<zha(d#AvR#9+#azqfRrn}I082%YxG%yzm6g!7A2|7(tr12(
zv5)IDMCZ_{(6g?0=u_yJ^iFydJ}~K=bk=WsY8>g=2b~GM+ThRXI#qfnHVybJ1@~^Y
z%<({<11)O7CHSE&;Y+L;72$0OpJaC?u6jLQW~Dm326dbI-1)FK4b|tDuKN{3Qq4*{
z=KrYZJa(PLm3YJ~vr_$DgL+T>?HbtVV0`0#md>ZZYVetbEAc^pOFCbJvLHWe4cYgZ
z7aFzidY*o?7<oF0EAbh~^MpDe@)T@waX$0!H~8$jm-8WcP{-Dw?&<XbeNJ%E#q+gh
ztD&FW8@YaxArIAS_*n&appN&osKKi)&S!oXuIG-N56Odez6R~T7~|kIF_6dJxF4$P
z=UHaxrvO*dPa))iO<6;JO(x15eRJ@Y#{Ea5oR`;^Gs<Tl`sHf$%X`r;Q<;~eUoL}=
zsLu+<Ujje)HFa`~4dq8N6=uQ*8P3!NqrlI43H;!9)#(TAH$*<@svI(wLB>)l$G<i1
zpX&KdhJ2#j(ALcSs0<75u&4_MSkwh*f5E#iQ9e<hx=8XtrgG@541Ikm`ud>z68OQ7
z)7vi9bNCG*@_|<&cr6641>`rhxCDNrKhgdqe$*Dr!LJPbN{QdMm!Q9WTt2cb5<jvt
zA@Xf9{2A(hX+DSg)A{J@3YzggajU7HK;54Ydn4?)-X|gr-(ge#5QFxCwi41mU@oPu
zFQ<8{UpalbxqoP&@1g$oGTz@7*7rS&t{fCA>^j-;$xko0Qr{qKJK6F@cgD7%{$UX2
zn}m%gyFR%|jFsjk2HoO^O>LmxM4zz;a}j>fQvdr*CgxsXKm6tO{eHp33d{Uap5NMQ
z>dQPPdC68lUoF$WtM4n#x`2K2@8G&X{)As+FFPK03UsBISD-%O6y^x1TuR|ztw}(i
ziMpWAclE8;V=?B~)^yQS%-vpYgD=1!zwif0H%);bLm#T=3wja#YYp|I(1++_slO#(
zB&45(9zyzAkEd>~$IJUoQ63sSqwiGnu8eikJ^D;hF6yWmRz{eocU`}$&bl+6%Eg!4
zJ286D_ShVn>%44ZUD>D|ZARZ)(L2$G_yq|w)w#+RwV>w~%bess|9t;TeFOEKWQ);v
z&WDYX>-9@rOnvXqyiYhS^$Do|qEBe7&TopT=e6cngV%2A&#7N1=lv!he^VR1>$`^A
z!%IJ&ulENq4?sS}F5<`g7ro!4ev14k-QHfjpPK(G=R^HDmxt=o8k%>(_<PNDusd&F
zyx&|i+~BjD`g76WkzGSQuk!~wU!2eU-G+X4QGYJ-MP)_)iSTnuFP^V8*BN|vQ-4l;
z(2rBQ)%}5o(S|Xn6}<H#eDrv{f1|R#SpRk@_P(6kXDaWD+2>2K_l1y;`Zp@~i}i2Y
zE}{QGe^jTZ+=|ftP0Ry6ih01tp}*iI@FV#|`CZI^xfK5Z{Pg~p>ea>c_s%8AR}T3^
zeeWCEpHtm0?}`5RX7s-vw1ZbJfnPG@qx_O>k@8D+rVzf+LbS~VXy?IqjQ%%^1ufH^
z0esUbpti#}e^oxqe$<EYzc--%DG*Sn`i*b(E<@N~aHhbLCfa?Lo4#z#@-uOcjbk@p
z9KURCKh_u54mg9Q#$3TtgfBsyi1BfHSYCcV7F-k@k~b`Zoo(e{XPfve=O;PX(9&QQ
z(sCg!7t(eiZT<VEkja$?e$$;8v$Og!`+V{lYByP0A9>HR3w_;MC+3SZ%olm_J+b5%
zW_S85_-`I{Ya~<m!=Km}%~X62RV}$$vz}Dh7{@KPILGphR`d^T0?@O)gU?bO7f^S>
z=3q`RqvW$qmX_@>pWUTdPD|cbmqdHmZhQ}P&)W_bP*_^@(p*ac(#Vg*_*Z+cZrzMw
z%w$g=R5zbMYj+Fye8O&&%Y67L{&K#)g}!$I+I0bJRdo9C`ZX%78=OR(6Ifr`MZQQ+
zzHWBfv_^&g*}Sf|i~OM-5ym>yK=>@s3(bi-%D#(pT#dYb&DYI7#99>kbo08|Zt{)#
z@^!PAi?5sQf?whrYN=bxJBd8dJhvl#Lmc@Hlpdu~hCGzg8X4-{F7i=6Qw;egLO#^b
z(%&K94xi<ugJp!QJ-*!V+T(8WTkf?Q>wEj^b>~PwXQYeuHd;%pMY<=E?g>g4eF@FY
z=<|4m&>zx0fpnwti{gaukPn}Sd<H-K72z`^K6gBxd<H*!mPJ^Xq4l+Vth)-IZt^ut
zjI}HHFoQmxZ7r?{sBazisZp^$%i7bLI{PEV8<U<_w<ebi*>rwCYjr+I@myp5&EM0L
zSpI0PDtMD$BK#oop+bG2`t>d^`7}ts0DLsMuh8n|9cDpZCt>iR%<El6C;Yk`GzT#&
z*Q3&U7p;5k-N!=Lyb7`ARRCK%_Y&8<Fi)^xV5BiGe+RBe|I|dx%R|?DssB__-WX2=
zTQ@!~*Virn9AT_mlt<L9H&FeedZ(8k*{>yy_vc~PmasNeP>8jqXR)@l3~NhRr}{_o
z8Jm7SZc&rmIAhIjee7b?^~qQJt%IIkVok>S+`3WuZ5BC?$FJ>qJbvIy$Kw}bjc);b
z%f**FvBuWMItc4)$yjf5yt>GGs@W^nV(59@&!{_>Rj6xwR;cAeE7UU7h0?hmHTXm0
zdDL}<wIY4Z32Q}09mkpzbWZjDBy@g)bbgV!8GX$OYelpMc2cYr?ZcWAbWUv!YlxNt
z()sg^`z$)2XwC;dqCOWNW2{|~jj3C^db9DdDe+mr*NRSztzRo5d$yN+A^2jn4ifC#
zI3Ha<SS!-?gS8@EKk&u$x&*sgSC=l*SL^vQtrh9{D&+csO|R3>t&R85Mm(%dkX|Vs
ze29>>3FtUvE#_3?Ygk5pjI{~sCr*N2qiYj;8|P=N$r)=;x}H()-<#I5NarDIPp>u3
z&nSOmEk-Z@zEXbC_lB&+EV=}K`?&t7US3Rp-!{(AXfM#8XfGJchUgD%=X;EKPh<Y0
zHW~HpBKp($!3V3eRTnFd4;o(^Gi)fzL)cKr7g8ROZ!P32hkRuuAAGv#Tjs?2(D#)r
z+i1;4J1O{j6V@Eh4(@`_aTD%CpICD925Ua%(+iFwYzT$1SRec(9ghb7<C7Sl0Qb@U
zAJE1CI}KVlusNXBeJSAZF$zusZw=h07_{Wi91g-i=ihAB&ReExThwVao7MF`jlG+v
zu}7U-)ailE>f`>+>hyRA+c-yKPkJ}2mo3oPA<z_KZK(k3I*<6asE@HNYR{|~_8;&g
zX)Wp~{J{vEnf@*6pS)YtG0uQG7O&Neosg?e_2sH#7Urst`g7H>wYahw>X@V%s#nWZ
zy@6cy@$_7Enm1RSUWWT>+}GgVF+-i|oPl-4Geh#Q?<WuP<wbee-;frLd$QsG`lj2p
zd3bJ$^DCqsi4(Zx3EyVu_Q;R{?YFqb`SERf_?=NbV#q)29Kd?iXvEEj&lz=B%MhQ_
zk9OV?J}3O6&TpWN&4<sK51%s{K4<aI(K*rZIZKp)B_BR#adK2jNx6gh;d4g8=S)+W
z+CMU2iGt5b*EH-+V){J2F{gyI6QdA6AMrK#4uY2txaQI7!oV+4gYO{xTRj~9gNRFf
zH24mo@jZ3nM*RA<3L}1fxDg+7D?BfHIObmje<OZx_$Mqet<oQ>VV)%)>m4T&`o`y9
z*3Oz<w54u+DJ^%4YVhlm7!?IQah_=_t@?bK<^it7uxRL?^o@C{F)iWa??D?t{@Q3;
zt!gy%eOJ*IHGRhxOMzM61ELz$H@;J&I+_^z)~^=~eMg_zWGV5)Fxs<Jf;0;zMvaL<
zn#mYfz^`n>t~Saz#=<&Bb;4YRd0cU_#XmK!5cwwEiF@)7L!ME_A<rn|kY|)}$TP|~
z<QaZp$aAjrTp>Lxy+i3y#-Z`0XOweDxY8%|8D$v~o+~|9NY6@YD7{B|&Xt}kq-UkC
z6kmGIm7Xj3^Z89!7rZU;#Q{~80?%z~H0ou+9Zl5esWFV|Lk#Lgmupx^o$!rqqP{q>
z%2GI>N{vE&Ear703hln&WV0O6HfXF_fU#yi#+p9f?sr+4`gv2M9@2GJ(5QzcGgieG
z&nS+~k9^iza&LikS9_n@CFhzp#WbH86KBnje8IXKYvQ}QYw818n_aCiX5N1j`iglP
z`@a2Nw!aVhxITYl`)|iJ>k!+2C;H@NZQ=&)@mat3jfx-iMSJV+Cz0-_n=JX*H@E-p
zfZEDC)3SfOPd#)i#=ppGE9CVwY$E<|bxbtc0QEhnpKTCN4ZOtER><q_SJ1cecGn8|
zP3zPwN35BWolEN$l`?28HlbP!JBPghY6-V<;-2i>uCDmsbsy|%q-OQqY|(C;x!Dzk
zy=Ra30+y#PYt4pZZJ)lSJJIV?CujjRxaq)<G|=GQH4#1ozRhg$d2Fd$Vath5rwJY!
zLgB&IkPEcFAFZiT>D^){`U0+ejNkJ|oo<sKW3wirEsuenPrf=}L70m1e<#1HXf(%n
z8O~-nZ8(oK@!);YIK0Cehxby^Uto;2Ct;Sgt!<N~XZr$cQ<Q<)ahA0S+hnnIEU?D<
zH(4le-Y=BY&agLQkw#*hfTaZdOPcj%8{va(z`EO7th;q-!ql6aGqqPsrgm#(&=TFC
zKAT<T_qS)s&`)yzF09)ZLD!&`9yZC%GVU7}!^T=?SW0?Wa^@(S`V{*5=P@Tj=}_NJ
z@e8RhuT?UlyP^J`++^8+G~Y#<8?pbf1nI6ooC5Uwi%~8wL>lo)7X|nR_w$HPdoMQ3
z)fnmIm*&bCgsGEfYV5ul8iR*$d)bT)vHJeq)$TT>GuFkHpnf`9#amBBmRL*1zHKGD
zo~^{OB<T3st5Ju$X=>t)41F8BrxJAweXR?9z6(Bq3;n(eK7|W@gbV(G3%-R5{)7wu
zgbRLzD=V<2Zhr^qwV^W#WpZmPpXIqY>zGGScc|Vp!TwymPIv?Aa1C{5=KdkLx`GcJ
zHR_Nz;CkunU;DiDYXobGc?;&SrZ>b}V59>*|B=ByPhNAO555uZN<^I-^t8r?O^RW@
z&P;m-<+An!%c1h@Y0=aal&xO=t&DOxdCi6KV>?~AucQlO6R%{fyl)JPiBhtoBVb>y
zwq)!^eJhEaZ7rnoLD}kM3LlQjG<d4<@#6r@l?CG)ACHfPO_&ZlF%dRmuBq;<KwT)r
zc%T6Oq@E`HNj=SX8&9(UyeO|-P<IQ!a|QT!<@xT?2KD-Kp8qR3KWnhj{Q#|Chr8Vv
zkrAC5!)Wc$(8p`m-{|7Y#>cJ&C{y^7E^71Bt9{r5<x|Jh;>0*@YK>3zGWeEG_>4*L
z9UZx96SUzXw2@V4Bi+ze$v$>M+okr|5oOo`d0dBflYd7Ho8$|qtzjd{R`x)?$8=?5
zTEk9GhOdCRoi@(vneLx-Zxc|v-H3MB#j5**6i@dFKZ*$JqlmiJ&{fw(-`VxLXvTH=
zx5mfKb?pH%hqQ;udyV#R_ZRi;;iJDB?EyBOY%BTab$0c>Zw$Ly`uB#%7~RhfPQ(}^
zA7c#I_>Lv8MZxxeADoCW24Q~GF&&3(3bp{wX+Lj`{_h7^m8L)Iw1L*t4{fl19uNE!
z@KfgoM`0WkXsxOF=!-tT0{FMU|7PHX`7svKd1H(eEd9CRanquo1}DN!1zK?acJMj-
zq#9gs4!EN84_v{+u0q%`q=EgNzZo~(`BTH=rtZXF;d#|mKiZNm6WUVn3L|fL`AN^4
zgX-9K;6Lw2J#Va)kvGTXz`vn%f7E5w^@es(jJ8?=I}pSelk^GqSm)Cpx=f=n+Xp}B
zd^G5R^wA%BNTadhyFchM5r6!-fsr=aV$@d%vm0sPWgi1?3H+xg)!^APb!Fp#-HQ4i
zv|pYHKjyrN_$ZJ`Q#It7WbThVr_p%yCQ2XjA`d!_JfzWhv^{WM{{2x;(r7$t|6wq-
zktXP4hX(>iUB$kr;Lcq`CSZ*qcx&V1YomRbY=vkad~KwmHbvM=TVelE->I$#|8&f#
z<8PrZV+>3(JK!HgeGycHhkp3&xcFA+iqn1G9(CqR(A|o9z_I=qKc~_7x%>BZ80tjl
z65zq6X9vH5J~H^p>A_LxM}qI53~5XVyECXsz%l?QjSI&@CxivRy`Yx?jo~$vXBx_J
zfQ5A&gK!OH8{Bz8ulMAyX{cvus7rO>u*JbQ5g&Dj;-{hh)rEmJxZuLzWRz3#EqEu1
zK?aojK-lpCs2e(e*uUUtN}tC^T^WFIJq$F#yAj6c3`jPTCk=Y5OXEt=xGxNjK-mUc
zP&xkl_`j@yzINBAeeJ`(=xfjTxUYRkjoo!@Kvp{+puc+T!#?-eGrq<32b@Q6e;#DJ
zab7^QOozwr+F^I?Nwp8HPPMyE*oPh@PO0{cPZ07cptF7LRXfT7eu=mMhwZ@iv_0d?
zRQs@Rjnt56)sb@0?irAk?X%hKuEU_Cbk5k@9J5#L{m$<C)njk-YhQbtJsx}2$?x#<
z(|5h?YP;q5$zJZj@AT!)+w5xE@+92HzKox=mkA#8va>fhe~2Wik3i!0Q|(o_oG~((
zu^oaz@QkyN<`^VCj4<N<#doRpA=QYqAELWHMaXv$=W9?y*kPwZae{y^bb%q(w@5p2
zIY>t(;pq(Mp(0nN2ECkwz~vtM&=Vk;vfP1$5PH-;?C;RSAu!u(&-fnT*Y=FV_95r`
z%?w1?T_4!1f|PItq&RNR_z>aSP!bSyFEX<qCp1}AO_F{DNnPJV(v#5Ourv0I19lfw
zJ?t~EKMGlpi8dc0>3ekj46E1Au$LVq5%gmp_A5A}u!iljXZ#51E<@5E*oXW=5|AVa
zB@vMRux%uRo@>zUCoST-le7zcKp3!$MLhs`0Ro(aAV*PhR5!Nb8bp46vAdAQFvy<q
z6QCUkI+JQ|Q;93IZ{&Mu66pcTLG5wvgc1*d`WWsX)eMTE+o91Q$ODvXsB!2vk{LyU
z0Mrdc{Rj{HklrVD7gexNPzJ+32AloBNkKcQ4D}j9RYuo?8cX#l4V=E?4FK9Bjv3%Q
z45c~@In6i$rf3)$AL02l?tesHb&ZhBz^NVWM%Z5w=kLhX*Y-B3%X)_Q*sIP^d7&*)
zX4{;F0*)f{6%hImDRB39_NslP#2zHcbiHP0>m`?}ms~rl*;DkAOQVug?|C_?pWW44
zv%C6x?XDZa*9rc6p;O$ZJM2Si?L!Zfny3}phi<kH-De+G3n7t5y}BSbuHH`jkRPBn
zs&Z6R4^DtmciS_lT2R4P+0`3U?P_nZR{Nt)tAo6toIol4Asy*Zi^@qtiEKeZoY9qy
zpCe#5^fR2O4Z{$U@gwNZ;U44}7wki}KxT*CH7E(X&?R(z1MTfag9Obnd&Y4j=0I9W
z-2?sReSF8f+O&eGrw0_o&K*$D4%ZIz1{5Ub4=9*Yzy=ibnh)MBAxe)KkOd{<>NOL{
zZD?*oQEi9rrQAWlTBABsRj#iwAbi3Jnf6hwvAYsyB05#P4taJL3elDL6jdWoJ%T(A
z+YdHSZpJsr85HFDJ#@d7XBUkS=vO$YibE3_Xa%n6vlzI5-W4$$5x&BCS9-shRbB{j
z3Uo+|D)&&zcO^KVAR$pAU`o~7^#xM=8R6Se6sJf|@a&MQtJ&__W#lYlCnCe9W_)EI
z24m;?4Drs`U8j*HB%JXvasVTq@g<&#@5yr5E96t{kc*$#8HkK3OO0QRA~+(iFX|b|
zt#)_{&pl>S<?eUWcKfVF7uY}XEqMnzNb&Bu=qokcz@Mx?w3g}po^P_EY5I%J(q6Fj
zTVlz;m$UT`uv=DDWQZ@q(<iY%!$+|(eQf;K8T`qRgVM?iK^W_S=cQ&WwWZc?MblhG
zqwl-Zx7wd?V!;>djgQWMl3LO4Q#}FVRc(%kdyG5T5i^$BvR3K0G~)TD_biK~Z+@r0
z)IxjCvb>4Ki_gGYwHKRXn&WePBK||;6Y~7C-k+r&LJ}I{<|@xwmROB%gx7yHoFbW1
z`OUoo<iGQq*)71IHjPH`lpXX(0yx^y55BP^aC)Re;IW*}KpAu5hWNDQcp3?gL(mJ{
zK<jZ$;v?v@NifqvWlQ{XI6fU4Tj>z=0yofl{0JsF&Bs7I5FhcaCz+4HMI6CD+x$m!
z2pq@x<Z!%<M-cP^m-$SQ_z3!iVem6y@V4~lBBo;&?kQjGIKH}>5h(Zz+!Td1^leT-
zU)jM-S+CFSVeoTda7V|6`Ai6duMUI9VjyEagq&I!+#3eBU1#P}pD%}m6IDasA~kTX
zTobF8^?P0^P55gZCmUlviuH%4db=^><s45kgSh_>j%yPBF2~a)Jiu|Ugm2~e1POni
z<30)B%khO0et_eC3ICYmWfES)@qmPX#qnwhKcPP>EFUxT=HoQSZLpl?<0p;>xZM$9
zzi?dq$DzQ{RB&K%N1~79xI@C5bNqN61LMZdaXG)S9G@W3x8-<032(=7@!xMEI6=po
zvKj&KYiEwvN_ZEJ8~LjbG`{`Wl*#$($?2W?wVwV~jweZYUyf@MekaG%C43;qy%Iiz
z;}azOevbPjJd5KCC43ae{SrQ&<7E>5FvkNDK8@qm5}wQP8VR4x@mdMT`U?()RZ9Jr
z_-5zS=z1N~-4agUPr_Glysd<<<oJoGP(H76T>d8Un;d5ny`dL;TU3(sZJi!=Olo(=
zw>(jQB)o#tJ0;xsmS<C1PYOA9b9zm}jc<82W$6-be9N;b^Gdk!EzhQGf`lLC{CyH`
ze9N;bTPWeDIlW)P&vLv>!hhv>K*Fv1b5m9=;mtT+BjI+A*GhOC$I<zO99QVL!WLt}
z!hAS6UWN^;=A$FWr%3oU98Z<-YdPLd!h3K$MM}3P$Hh0nMB2A-JXWIbqvMf+tnPY;
zj-y^l`K51^(!W4ECH!8Fi~nvEjPB#OCededJYB*^a@;H7V>v!S!Y6awC*hBBe4&It
z$#K7g&*FHQgwNA)_@%mIBXY%9A;;HA_!5rG-$;Ie<KExZ^Psy|IG!lsuX8+4;$O<~
zeiHsR$DI=XF2}{Ur$p)%9N!@MtJ`!ul4*jFUw7+xQx=f)^8v@JCHy0f*GTvwj@L@~
zQI5+4n{PSpkOn-bIqsD3vm8&7@LxHuNw`&)1NKeAn{nJL;dYKskZ}6;Ewu}ugkQn&
zg%X~i<B`nA^(!u&IUW!Izjon#{1Tt;950jbo*WNI_^lkTmhiqDuaWROIbJK_134b@
zfh5Q}gyRm$q}<POr-Wy5JW0Ywaa@z|@f=T=@P|3>mGEgCpCIA69QR51Y>qFK@B)te
zC43Rb%Ordm#{&}n63440yolp968<K~YbCs#;|wp3nvZul?vQZ2Wq|{BTf(>KIGF4G
zXFaDK9M>d#uZ}BhT~a+Q-Tjs0Nr|EOCmf$H;fFasLBfx5+=l%+=HsM}qrFJ-{J`m_
zy3N##{mk)LS12CjxcH`%2#wV1x58Z42*kLy;JCkYC?3P{q)wrDJjZ>KJip_(SHjzK
z{7lDCJ}So#O6gw9@x<<-^w)7*9*^I|aW7m)^KmQ3+e&yJj{D5SxWAp_)l&VrljCzF
zJe}j3gx|~Yb5ec&1IOid^Z>^fO68u-@f?ZID2|K&Bo^5i$MIeg{bY_G(XWCU-_7KB
zlr#_aIL8l4Qw=jXK1af5b38@D^ErOpR-Y!_6>@y3gg?jeeiHsX$InUS_!7rgOZY1s
z?<M7HHOGya-FlfyI6g<8@vX<#ay-RRkI~&b9IwSoTjpZ}$5%^u1;@P-Ud8dY626n;
z)sj3$f2Oc9X<qTKoPM1&p8A;MK1u#V9QR6mjQXK4hlGF4=>sw5+%onZ$0tbm4;*(&
z_&JVOOY&g*6%LGB^=<d{X;}1njCtTT^%&hX<#@V;M{#_qgvW4veS9dNHXNTI;a70n
zCE-qv*GT+3ay&_zFTF<ak;Ws}a$H`p=)v&}iT*~8r%MYQw{kpBqVLObzeL}k<29Fs
z>fvsVFO=|mIF5ZfA;(aT*Gl~V$nglt-_GXvQb|7#aoi`-PvCf2hmd61BOK392*sy!
zJXOMHaQwV8lztA!*Gc$%j-SI`OY^aq<7E=QT=191BZmK^U{9f$3%-xW={rbxF~^r)
z6-vLB<C=u8=XivqpZ7Svu6-z<N{(ks`1>4>m2ksvLwmeBl+TBpeu{*D%JB{o{sqSm
zNab>z<MMp?cN}lqBQ)JJ9Iuu9fxmM+yK^YL;V;5}ljxi1{j|cI68|WUZ<pv>ar}fD
z%D*kgKb7dO6u9JHb>O)8R-xc_4aep6f+UWwkoaHE@d?)o4#u??$Ezhjvp2`ZH!Vf1
zejJ}8`JZ=jJX^x=;kfu_rr`4jj-QeIJuk<nOaAi+j-Qk0$8h|hl<p*syQKV1<#?*Z
z{|SzZZ&V67^Ehsk=${t+B|R_X_z|g|Jjd~3$zHACxL?9w=J*(?ov!A1o`kRA_)-a9
z$MF;i{|m>{B|N}!XD6Wn<66b>SP9?7@$=n6>GunINk0cUo+i<M&hcysKf>`E3IB%U
z3#Im3%kex3|B2&EB|hglUMArd-7kdyAmKKSx0CRe9G@=Xv4W2@&bplA=Q@Y#&B^g<
zslIjMxcH{4h?OYllLca2yK#Jigr{)4Qj+r)j<=QUPb$ZwB)mVz=Sck1IUXzFzvsA1
z!iRBOlkf*Q?w9Z!j@u+VFplHrB{`>XyoZ$E$2fjm(z8$Sk?^NDo*~JTFZf9KGaT<B
z*@2}TcS-n*9N#YCt2n+&!r$Qd90@Puc#ees2gmzKIMo(w-4_%>>yTCdIzuS70eo6v
ze}!02@8|e`ot=Mp9LHJ5&r~Ezq9jV=G*06rN}{B0>!{k*@74+DoLZ}`nyRhRx~-|D
zSaOv_wuP;L3I<dV)NKt25FkK+0RskH5TJqp14=7EfB^#n%v13oK!5=Q1`HT5KjwXB
zUhCu6+JCywqussFedar}GqZcA)9EhWzZ`hly{^FL3V7+-mn4+JS0xM{?jIf8J3QFG
zfA8R6sk?DgY5)GwBYOvj`=2|cca0tn{Ce@wVE9gzgGWY34j&q6XmocD9=h&MO_Pkh
z`wqQyaJ1q1BZvEMD0ArgG7a68#!^^ivdU%EB&+b1F5Q(DS%q(U>8`ZNs$EtcvdUr+
zEt6@XOiDEgXp+5LO677-CI#f{V!Bn)l5$I}7^Zb)TA7NKOsmVZJ}qC#wZc3$QY*~0
znYotDwXD`z$+c{*WpmAM(z4pN7Ae#wDXk*3O1V~vYZ0?W+SL+mtwRr`(xi1XX&p_X
zRGPGoCat4M>uA<e&Dy?j4hE-L+ZWCY+1IkoTDDosHcQ!d!5wnDL$nSl-7a^uNoKpm
zv`LvPiz!-DY+sAk)S@-%K~-ARY0;YWKq@U-Q;W7zk1x}9XL>xDwj<N_W!mmcdnnVM
z%d|(cIDnbdmBq|fJ<e9GT|2JQs>j)?wQGM?TJ<<vwRY|1N~^ZKP0MQUSK74YZBjN<
zGi%d!wrNhA9)%uzrA>3vy_(amIqlj8?W9V(wxM0iYNuA(wGHiBR=c#)u4Oy4toC%J
zL)*}yWpw~59a?XPmep~obV#c*Et@w=*<8mp*IvrC<+=7}u5&Hd*_!Jl&-5rV&B?Sq
znf6kq<CW_y$s46rnT}(o$DYSiMMqiMCoPgTs3$<r6FoKbY|s-xM@yU3B#JgjXQZAC
zdZy?Jr011RCLL4h9qBWj3e8f3Hbm!;P8%I(ZL}V#4xr9F9XOqFI*>ZPI_x?dbdu=o
z(=O1l)8Wz)(m~O2(5~0s)<N!&7U<w++SNL$S)+`zcAt)g4v~(W4odh+nQ(X3s1qr6
zyIzkn?VL<|C(~ZY^fHj?DV^zQoaw2XX-8!`teKvonVy`Pp5B?B_L-i*nGRW|U7YD~
zXL`xWVh3w4=CQ1HO{SfZ=}D1m_h#{Q)+w54KV*9NnI2l+C<BsfXXe`1dAf`$*OugZ
z`sCWSTu<9vM<CY`$@Lg>J(2S`opc0p9rIjk&*LPB^C4cGbv9*@$LSX*S-fEDX`bm+
z$aG}0crno#lIbvKI%TqWI%|6}ZD*!Ok?9d-dMU_smSj2+vN*tc3THY$vN-yB`O9=p
zWIEoNo}QV`mrMsg(^ELpX_V;+p6g7=8|8$_8)aJOIxF%<IVEzPMww2rJf1G`%!y;C
z7rH#2P}(zj=o5W3z4l<d|D}Wb!o%voVCg{r@bIC1rTy|)ZOi_&Q<b*%^kZkZy<}3@
z?n(Q?e*=63{5Qd8z<&#TKJD)cTR!DBJ^)vC&!pR3VN2h*_m`6|?L3?IWpg#{3;*r3
zFPrvF@rLl<iMzVC@4yxQyWsZsn{r_1z{CG?=_3B`fv+ZB4(#{AYxM8A@%uCR5`=Mo
zT(!xUexPsM`|<r1F`rMljeEZx`Sgu@e-GNvr~1ad--mqq#=URv&y#t=mp+Vpe-ioh
zjeCC@?SBH}V%+;P$fs}I`?JXBOCQF)KZkt!#=XCYe7=NY-1|$&r*GW*E68s|{}}ha
zy$?y|Cw=4I55LstQV#1sh`YMB_g~7m^H(<GKBp$-NWJupdw+BCC7-`?8u$Jd<kL6q
z{q4x-ODo2`--vwr#=YN(e7jal#~Js27xL*F_kJJp`POCQ-XB0dedFFgn*4HTvt6sD
z{U;K)$D^$R!!lPDx8<<;!?>&K#l)Wuw_3rM68Aar?>p<?Q)>BL`Z{y`US;{-R$ICg
z_c_CfmxK66CWq~40iQ&EJN)=}ha1~7hnzo#f2M$+M}7zV_`3a#`B#y19)8tL@kTjp
z$m<f9etu(AbRA#!zA?WOIe!AbyMPZN|4s1Y>&7?cA4g7nPk1aeRlsMFzXSR4b=Mp7
zmyz>1_$vkc3i6+XA78h;F@NjLQ7VVc3jF#4UQS&4|0($Kb-x?)2axlp@P`ZdG30*{
z{P?=rjrr#b{FTJ_rwlv4wpB;q+rK?=?~lP}e;zIHm*8`J!{_Ge;`6uMvfdufx2=hL
ze+WM3|8Rjn51;4bnF7E1*7f%A`e6SrCH+>8imvVdqpY3ja(C_jqr~NZe$}OX^KD!v
zQE%!h&7UveXAAhn0>1S&jnnU|XTJUam9@v(e=Pa({Q1vfQC-i1uY#XTd{<yn#unoP
z2GVvp`CZ{QedFH0h<g7VS+zCshGhH&xV=AK+9P9Ti*cX7E#*ji=o|NbWAY`xV&P$H
z-22VQr*GW*JCNUreB<6PBcHx;@9#$bn~`ta`+Ja2-?;aOk^dIt8~6SQ^64A*{si)6
z3~Vv({bR_dZ`}JQkuP(?7USMOg?#$Py?+|{Z?*8SHSYac<kL6q{qxC}^Wra&Uv+!D
zA$$$I8T_xn2f+Ut-2T5s`tofXaNnMzDM$K-zH#qQBwzC1j(p?ZKZbnz#=Sp@{BGnM
z_x=>}=^OX{3FN;6`Nq9}68ZFvdw&-B&miBp_vesL-?;Y|kpE8P8~6Sq^64A*{yF6L
zAm6z6myu83xc66)|1RVk_x>94=^OX{Rph@L`NqAkUo{>)ng8^Sdw<&<G1MNH?Ri31
z;^lDn-$X^%Jq3I`aXH^Fz@KpWRq4Lt$oX6NQw4lBae2&q5BxcoZ|z@3&fmdbDd1O-
z-wXe0LH^b|qf`!?zlUF6z{`nC`=5p1nYi!&0pwhSKU~0%A^*Mbj~C>hM$SLLpDW<!
zkpDjT%LVzDk@Ju6uNLqvwNaA(*$sbd;=X@&Bwh~gKf&)T;C;w{Km37${8{+d&9?%-
zbXN?O_A|dKao_$P_&na;0{;Yj_W#KO{}O!8hsy<i_`EP(ZkV5m`}R-5=lL>K;IG2}
zK&(pF_IV1HZ`X&?-BFV3$tC#p;QtKX3;r+Q<KX`aKApHc?tgFt?yr+G$(QRYedFFg
zjr={xH}3se<kL6q{YB(|G4hRje+l{YjeCCu`5!{Qaqq7ppT2SLuOWXg@{N1{0`ln_
z_kQ)}7%KDUGV&Y2{|&qk{NKT6!2bh$HgTCheH(D!p1I`9{Go5$`)82fk9_0aUqC*6
z<KACF{&UDT?)~t6Ke|Z!=^OX{dF1ayzH#rbAfLW*?_WSZ?=wy&)n7zD?=wy&{uSg8
z3P_7_?_WheedFG*z9$CC{IP4F5aZsjNxt+yedFHWg8UCla9WIee=G9o8~6S;<PTYR
z*c$i#cI4AH?)^^WKaYIl-tR&_edFHWjr<poZ`}KPkWb&Z_s5X`BJz!U|0we58~6UH
z<l9`Z=e<jbmqVRbQ17_{el>BqPW~tS@Qnw$SbkNyuR8H^xcy&=$)>h|cO@?Q{|&#p
zAiozmSK;p|;FHMzANW%R`7_A*U-%~r_(J0Hyl5EyqRY4bUqjCS!M|9*tM84H<R5@v
zleqQ2<!?{CEbgZD0?h@y2l)rr^-^y^{wQ)bA^&IrpGN*6<j)l3pFvJ|yXv)Nv4F24
zf8?4SUQ;T_Z`=|k>CZ2L-<-Jb&vE#N;ZGF!=iqZbEEo87_pP^w?XOSVw`T-C`*XCw
zUx3f?T`cfxU%lQQ&bQ5p`}Xv~KZ5?T&o49It`B3$F9-J(7~eC<Nxuqc+W7*05jj=J
zvCk><_3pkuHcsk|pScuokAZ(YB3(~_--P^QiOb{iD177oI($6&^0-XjxcBFi@7uqU
z_|A}XGul&=PJH3j;LYH-fRBL3&zOn_aRU4{_{-om;1?4whvK(`Z+Rf*?F^ecz`McY
zXIMpVAaUu>mm<=&ao;~f$(R15Z`}K1$p2F08~6TE<kL6q{b}TX8S;&Le+K#VjeCC~
z`Q?yZi~d|fy<@1?xX(F@dg&YY{tEIxf_&rNUqwEB<KDN=k&$u#a^xHL{uSiYH}3t?
zgE3U{zXJKjy<e4lnOF3Ud%p(xN0D#b`?biYZ`}J^k^hy*H}3s9<kL6q{dVMk74nUH
ze+TmE8~1(>^1mAS#=YN*eEP<{Ka%`%$i55b#Z=;U{Y;ms(j5F(!aq~M&nGU=6UM`j
zEh`22SCMl!@~a+-CCgz$UYEG!e>5t(u20;z|7ZcPs*8ms=WF7wuB+?f4WF|u@p8C3
zer90w8Vh)L;*$ThQPp)%;y!;Q@p7>4K|hQY@TtTle<J+YGF=z<eExjm<#6|_kiSsC
zR}z=}uM0o6tk%UnpI>@7@^aYRi~Q;WUZ1$+e|;>f>urhq{4V6g&t{B;dJ6b3@;`?B
zk%Ih5<lMJuy})z<pG#cYe{5YZ%_r{LU-d|oykDKT_j}>9{kseNlknM}rwaVb@HxI$
z3jEGT*W1te)|I$#{}g=A|LFpM4gN>bpBD=J=KA&ajKl8+=kwB`0zO{ArwaJ#0=`hd
z7ZbPssY-3N&((c8+~)K4iN~TK{KZX44x8hN?+p8|2Di`Q6~B62FP%+(Ik5ZThwnMo
zMfl5cSJzj-<7b@4JFg}#&j&sM-}nHS?mixYJRhK6O5FR~6ZidNpQ9x8elkYt+PKeY
zM!ocndw)mrCI2|`jeEb0eEP<{--Y~dK)!MBcO##^aqsU&{x>4uxcB!UpT2SL4<Y}X
zkZ;`k!^o#^-1}q5pG3ZK?;k}zedFFghWu|vzH#p#M?QVy-k(PPw;<oR_h*n#-?;Zr
zA^%&EZ`}K*kx$>a_vev6g?!`QKZAVw#=XCU{BJ|Paqpi+K7HffKac!xN4|0IuOOek
zaqm~XCWgxO;X9CT-22tZm+KyV<KEwj{AuJH_kJDn=^OX{j^vj^_5-;7?*U&xy+hy+
z!XHk2S8%@*zHwjgNb;p0=o|O`O!D`q?5eb#O}rd#KZN$21OKJu%4P+;4*qK5yMpsw
z@QwTStR-LCPv5xrYq!Nv-+ud?L8<q<QLk~IvnAz7z4VQHzb^TbKZAVZ-mgbKedFG5
zME>_6-?;aikx$>a_shutUgR71ekbzj8~1(>^1l!H#=YN*eEP<{--rAY$T#l&0p!y+
z?)?$ue?Rh#dw&%9^o@Ie9Qi+heB<7qKt6rr-k(JN4<g^V_ot9g-?;ZrApa!tjeGwj
z^64A*{w(r;2>HgnKZkt!#=XCQ{2xZXaqll8pT2SLpF{qSAm6z6myu83xc66)e+v1=
zy}yQh`o_I~3Hd*YeB<7~jC}gWy>I{YE7#8-L%wnESG_h$UCuY|{o3S9{*NQyxc4_B
zpT2SL*CGEj@{N1H9{Kc*d%qF+pF+NI?>8f#zH#rDk^d9OH}3sT<kL6q{T}51B=U`W
zzZd!RjeEZj`LoD3?)?Ge(>Lz@5#;|A@{N0c6#4Xxdw(4HKaG6j-k(4|edFGrME=hp
z-?;aukWb&Z_ZO324%rXm`Q94%ub|#b;E%w+oVc8)bMTG(daop3t^@Rqd%ym5G1Nal
zoJ+hM?tT>QSqA^*SX9@m;Pvp=5|{RW8oqJgo(svB_R}}+{mrkBp}zg~iSG<|KZf>n
zB`&W=ePJxB>pk!vhd&Iz2L4g_uYrFGd>i<s#C<#M^A>l8?AOBY+8!$_ht1E&U0wHr
zzYhK=`0K&XfNuvsoA}O<`UH6O6R|SspJDLM#I61I_m`o>%fWAeKY<+f^KtMyV^zAI
zPF&ja8Ss<ve-`{y;=Vm+k;DE!2mS`Ma}E3#kbg1pa$w=(&#o=@Iq$-M?iwwXY8v7V
z&$ohS$f*a<!8;R|{%3wy;{JGt;LoEy;k)T|@%}t~&Z{#8e)St-j<kpUQ<J#Q-=26m
z`0=}<V)-8A@VI&la#oPjjQ+U_-U43J80(RGTfu7+m+Q|jMn%`gZ@8W$U#>s&Hz)4>
zZHfDS*ps*)$G!r83O>)58Sp#N{!@wX42<W?BK$4z&w`%;Ujygs6<5G{ov+PeVL2}H
zdT>6j?I_^A1$-Es`4ix5=L|UOoiE^L!I`sG!0kgr{PAwd*ZW~1?&>;xh?6ejFT`D4
zcY*V``oLd?KUTn}z?px#fG-sA6>#>$6>zqvwkbAFj*Gm$fbRh3eC`DQ434)eaep2i
zh0p88c!7TkKIi`|_-9e?BKQLMd2sg6MR3kH{nFS_-uKVu<`^oR<GvMq5%t!C|0;MV
z_^*Kv6!3`xexiUc7Vy;qUeyvCA;-n`)PVmw+G)GeeIK_Q*tUoLQQWRh`{K84TED}%
z_iN$XXM71V?tPmZlHY-R<KC~6u&{{#M)-9C!{YsI5v%_u_{P1TF1PCMfN$LUa$6Q@
z&y(<td%rAlSj2x>_u{IY_Qn51+LujN+86&R<Qw;X5B&I@j}aL6{%-hX<Qw;XAN-y0
zjeCCx{?qV{dw&Fe1-^0bkHPPRZ`}Li@E6j)Y{t{R`22gsvBahS-;8|Y-k*g37Wl@!
zKMj8ueB<6f0ly2raqpjk|5o_My*~^8ZSakIe;)qZ;T!k<0(@T2jf>B}r!2wmM!s?H
zpM(Dn_{P0|9{w}%jeCC;{yX6t_x=U=J@Act{}TLn!8h*xEAZb9-?;bfe->muU-!6~
z%8EaiGG$}$c^3aY$T#kNdk?Mnz3`2D-`=Ao{<H9nd*A+MC;ofk8~48b?kWEJ;2Zb8
z{f;UAZurK%Z;x@}zaPGF@7pz2d_E5`E<Rsx=}MX6e*pQ$z25`>gYb=e-;Po8_rN#q
z{XXQ!@AeG^!(!a~L-0R@eB<69fxj2Naqo}8w>C(7jC+3^KA$fc7oWdx9)sVH_89m6
zB>d;#8~6S+{P>;7()v=#xc5)MA3(lw@1KG{2;aE(XW{RMZ`}Ly@IMUSxcBY#s(s-Y
zZ_*=;tGcu=zxLdm_GMF-_S?dieq-X2b4%KnO?sNFzb@Xk0hb*59m#n<Y;T2c`~~OR
zb7QIZcKBmyU)q1i23+c;e=IqYQw#ri+LxTWHsF#&-~L7~Id{W9k@h9$-VM0q(En6&
zq#y2qZ(RCeGq}BGB`*WLY6C9gz?|jeNIUO?e?IL?JMZ6sOFQY?<EG@;T#{lJWfd05
zc}(}>%DCjvuS#=Oj`u<M#^rb)0Js0C5x;H&F8)K{_Fi<Erw?zy<=3-1aC_}n@*mxR
zOa3F^U8((&U%vsDeCCuIj(p_6X#aCzJ9;>7hiva~_&$k-(ZR9NQo}(1Xn(2Uxg$qP
z4N4CmiX?6u_8l3O&->nY=)i%&gF!!dXmqe)*E7#Pab&cA-;1ft;LzUvhx-o<qE(;&
zZe1uUhstwb(#o<5ORXFzmOAXquFR>b@KX*yT$YC7t?}--tqB{cr#jxe_BC>kC;p1W
zZPVlGHJ*mWZt7<!OXlZWzBQDU%rCf>7<a7x1y`Z9-*kC7ZKLDw`gAnLZ;e|yetCV!
z%5RBSKl@UucxAe6n_4-c!V<q%EAGqMu1&U;?v?T~Pq~zzjJN6t!@eDkZJ!0B+Ly|c
z$-b2{F6WIf-+xxV#}%;SxAHQJH|oEo^yGOg4XpH^bfuNI{@bW-$)0^m*5UPvN8(l<
s8(s*qFMmtme(p$#3w^RKl~(y+8E(77(Z`UD%9lT@#jks<M~E-~{|ES+0{{R3

literal 106000
zcmeFa3wRVo);B(r3=kluCLtgy%CL^^%!-&Gx{PF<8JJ)<bO13(f>ACB5m#<PBDf-g
zlLUIlR(93Zb-lfNQ&(MeS6$SonE<(PO^DnD0t#dZkN^UaOY;An>h4Kr$f)f5?)Uzm
z@A>+9X1e-R)v4R5bE;0Asv3|sbzbg_+k4z(>-eM?K2^8z-)0+n>rgGklpZFNX@JRO
z>DT_VZ#S8IZ>zrv%T1>IXVu@LDwAnhUgy8vZ<tK~ji1?5l%HQPfAaK&({l?8rknUj
z{=Dg$>w(!16iiOdQ^C129+)?G@~<WnP=#>s*_t>17;OAV&6_-L`qY^n!bU;{8^aXL
zn>u^`+&S~L)N`jVoICmE!TBnJ`=`&IK5uHl^vSt%=07ld`egn$cg}*rKM<r71iT=f
zz#|dgIwUgff~nJHPDhRKVzehbf7aBQGbbBzq65MhR6zcmc?+h_%Nr6IGYpRiKW%F6
zgPlfb_lSt0VHz4Gg`txp!t01GiqP&65kpHM_0}j!z#}4tj_Tql2A}pg1yc(i$eqk-
z0cp*eoNq`d((i$Gzc78|7tYQFk>pJ`nQn^gpoae0ne&!gb?B5HyenulKPY$5oSRe#
zQS{{5c?L+BYTIGlrJ;((6K)+cgohY<^I$E=lpc?#OCw{<X0QhJF`I5TwSVbQysvI|
zd$`kMN-$k!I?1j%D+Kxo0hU@OxN6x;<?Pim_R(f%%_`R4XEJ#wrpuMyyS)=c`GPmo
zD}Osd-g1|`PVgV;D;L<~{jHY83)7WMd)pm~#VzlU5Bub;lHBBzGwpHda%)3rTU(pg
ze<a2mJQPFl0N^VrgXwaMPd?~Xk_Y*mr+jj4UA-JM?`U`u;XjFmzfay*x8Gdn^`Eyq
z{z|}Sk5|T>^C@#%{rg+J&aFO5ym*&#SF9r4XkucWkQ)?o*X0Xrb{YLu2hYajvzbBv
zB73|=$lWX`L$*+<y)4nU#Z>qcpZS7+Rh;>#kh@FBT|+VI)VOSBIfc2!D)?9Tz2{zU
zj`v>gecs8aj3SE9>uZ7_Ul0OcfXAAI;<f_25cseLZl3K13Zwyqz-JUj1AVrFd=S2i
z26)V7{Q7Kpq_;7!+;ndU{-za**PPGh^d@&cn|&gmHSH2!0AwA$TD`nxsUQvcY;FbW
zxLgQy%V#IHc<c82gE1qO_#|&#z0bKhpS9K?NTo^4txh)w^I85{fXXv+tG#A;%&E^8
z+3b*%yE7xVjGWE+Y}dYgwilu1AavvEd^WEcImzD+03!TyRX&?m#}YkON*y6)u3?Gf
zUK|9_Mko+sW`!WH#dnHOT>VH7A$Ox-UMtAsY(ik1Lnz*C0673cC?hBZprIP*v|!#U
z$a9AZwNq^PIq*xtZ>Zoe^9XXcRr!OA?bY^r8Luv)XHM^Egl=uGTS0~^R^_*cW&J(e
z-#d{A-kUk$E|B>`P?{%C2w1Lj0Er;`>@(0){B34davu-~${lT@+$I#a6fXMQ1Hc#f
zX-zI$>_TxFE4r13yE=HGtxb|wzQ@tLK&gxVw*G9HQ*~Kb(Z9NKSR5;Q3@$-lg#a<E
zC<`ATWKSC_+D-647odQ*zYMR&1l~_onxYiZS7zi1H>>$&#Z!Ay48Z-X7aHhUkef6z
zgjXfuGjTjTXkcX(zKJ@~4*l$$u!X=siO97`+${t?rEu+d0xKhFobt1<1Dz9O<#G+Z
z-W3Rlwr%elX?7KBrNxe*rqK0Z3p{@)od<QgycBi79KRq>=3HdAN^*-3Y>1`y8G2G+
zrDa0ycKR5HpHGl;>?!`1{*QMTig&T%_YjxLnR@yQ8{$iW88)6;>S;l)q(}}eai2Yf
zCjhz-0*iDoVhRBul+f({LoJg0#!7?|<QLw+ON}Y7;SK+4Yg^lBCCx1$a|o$tFpdWj
zwLtR_D0SB-F^G)bT*kZbNs*QA6)V&G#sTX6lt@Y$eV&Su<(0px+h11?Plpv?@HRMy
z_JvcFCs>8tnlsyl7vQvsa(ap&Z|9N0($Y=-mM)7Yq|5ujj?MCRpS%++cBC@oWkfZ3
z{UKt{;0^M|bh%kZRi15YTS<%<pPZpr^B5cJ>d)+i_c!0QwWXVPfjxIAd`kdz#+eFk
zqiTth<OV?*-6qtA1b@&h1ov4ab0}Z$k3zsoeYmS+J}byzvSn7mRaF?Gm_r5SvROd;
z36%>;3i2McqLhp{fshvLHmwi>S0GXxP+J8h`6_Ua7QxkA*nseA)uVMrDQOp|P=c&A
zB$^a7h@O~@`lJprR0tGMdW-B+giqf3oB9Z6_6xO#1;sL6`K8$_ZxJBF9D=igBG}|c
z!QUDqfElFtnC~c2$&~ce)5S;GQ;(oe$}{a&Um(x!^T{V6_S$(0uVtTn+P})&;6atT
zb{8HQuJ~>=p}<xkvAP=AQ}+^s0D+BDo7(#^5rdc4zE9q+j5XKQd+YXrbIac$ia;5f
z;;sAI2YMJv1wm9n-U;g2?87Je0y>2A#7Jde!bqiGSNYU%CH^+^1y!Ca1(%u9m9(Ca
z7pv0cwO;d1pOSZ($;X7P@*Z=mmkHm3#PYpt_F1pIIo%9SKdmyI35O72WASErBNKr+
z&75vtOM$-iDRvivY$Rz5SpWf{3Wh3tKsfvoz-c?UU|_Q=yvmU7Jf*EHv9F%kmxSkp
zno@<H5^-|WnZMi&BBB6CfM-LLM5gVagjix$JrZi>ZR2yDWKS}14GEQFr5bZ+?Q@Pd
zyS(Pi?&7k=i8Nr@2SWREOIw?Njk&Oi7#?r&&rKe*N;`GP{^e*D;xnL=;9(09eM*d=
z3^xn%GY)D&=Bp8E8^D)eAmRfl$<Mq7m$<a8t((d8aFUQ)V9yfRy9M?*VQB?NkHpeb
ztGz3c(YDmKj|U*Q%57x>x0U@>gpdgMIgn65vU@%OAtNuXg1ol@E?pS#FfN4AxV6Ti
zKOup^!x4_1=wqQ|1R{CYQJ922haaaCP2M~P4nR&bEB+I_Ad(^?C_I8kmru8}ho9eR
z_<ObR%yA>-n>o>OLT;X!B?O2-KZpm1+-4t1_Hmq-ah@%2ud7!IJm-Ym(?alIS6((L
zFvJd~D#|0(&YnscK*Og9O4=I2%|nTDi7KQ}6Zct&`=d*L76)&R*sF|b73vOPym&Mm
zMLyO+E+7cxIJzAAf8E#UgK?h{#{!E(K1r@fh;YMPn-EyRQ<9s7U`y;M*>WPI_%th;
zN2DU}0C(dF3c(PhmSnf_+=Zd?nhxKhyjuN6<AbwTVgSX8_99gw*9U6CsM;aQJ4OHg
zW)Y(XxlB^xd65Nq8wf5a2EXni2J0;-R=FV8q+<LR9?vfBHk|dbbd$_`nbX%B1E9qw
z0>nw?GB0ylk^$&SfCR~0L4eBzf4Nn1Z7y65PM^A&^<P32b&~bLz?wPo?ADVkN!nsU
z2cE(vu4Yc*wUb_!6#pf2dftPpj|DEFjKj9^0G=8S-Uhre@TymV&6tfWcoW=M#85>*
z-bJ%oC6auIR|UkcS|#RPwGcm|zrrlJ(3Wi3LP%8V_CjbRdbQRr@@}oVkD|{S8kd^3
z_-Mg{)Hxw$f)JU!I!RcI)O1LhYA$#%Rxr1$KyAd3@Ig5SOXhOs^dh4{l0PA;3)DkW
z{5DjE1!V(wr!pb9jlwafSTDG?Eci+Y9zbc1^4y1h!ACZrh{&FG^J(OL#l`Te2p%5a
za#_OV$Qq{fKBrQAr*dQu<hLT2>kew7BnVl?MTxC$KP25yh%)(IbQ$88Uiq+mJY7CV
zBBi*^$HtuTIyVnj7F}jC??_kdgYQrV#(14|Bb0$XhATs2QSRYN@~~pm_3U!E+)Uh7
zT#b*N_;4|?w)ix-HzokH*8*@m6St-pg9K-v!1V7b5*}VAZZ19=g}ZkUA?1iNjEP$y
zKhlf8p(Ft4iTGcp69P36fNln8i!MYe&0g?!uYa`}FT^?PCAm_;MGKp@DN4H0q)H-M
zY#5|BdvXcLEA$%bOzWE%7*;M>PTc`d=>2c0HE6J?52so2J7L&DTj(%8C?rXHFa;Bh
zwzOx7;caxAOm~bJahuICVOrtrf<l{X(2zkXH>MWyuhhkZQwF6B8I<Z&;T#1xoQb#-
zNq{@XOrW&N2&Si%3^zUIUyV1J7h26%#dnKafy_`GC>r^5mG%aEGo@(mILh$w1nbi=
z=?Q(GiXEO{Q!METjwieL5>iUKjz}2l?=~`FT+IH2q27cPZ-N5>h9~qzgy9KsG+14z
z;dz|i=?T`7n3%$ZI5k};G5K>nrGba?@&<RAOr!**hhU}LWilmdZvpQb%{@0Bj**l<
zB&+3bIOPx7De3B^{P_~zGsmp!(OC|?m^5|%P?jvF<GkG>L&SEjc|l$4wcRmBEHFhr
zr~<T+R&coWy1!b3+wi7`*FWAa{3v{i7KPU*U?Vm7dm3D?OMsCZqK1o~SQCaRojkE7
z%GJeBtf~7&z<C^UG5!S_+!hgIkp`#sjC^8E7+FU=OYz-V*yp3bUyB0&I12oAz>`p)
z_0%cx==Amh+yrmd;5yBHuEDKZ6LY!)Y#n}`<#Tfs_>m~^)+q3Jv}<ShZ;ApZ9Ykm0
z)1tt=QQ+B8;J=Il_eX)h6b1fa6!?ZH@B>lcd<9@ouDTGAJ7?D1sq>~!o|B(He|o`u
z6*zt7)M<10@_+`OJSQ)2sOm@S8R1oo8PLCzayW11^r`uiAD9nKe(vPy4^N*xbWrY`
z+4BqL&YLrLP>N}gG=l#PCReIy@*UFH;a+L-*pVaeO3$2}=^ZYmPd3d$;DYJ71*Tbr
zvu916tL5bTmr`=EQZPp=iMl9YP=bfoBs#3MOr80_{j;a%O*X^~E4w>g458Hsr18-F
zIrEUWaL&S!+BM`{$*nI|gj4UBuc%7{(UvhT*7JwgHzJbguv#&B^85m<cTCQm@!;hA
zsSnIVU%3<XKo4=&B>c|P%nDDQ0p801FEqeO7V6=NZs{RBM)<Q);D3w)|Ca_Qws9GL
zI{t|ozaac14|VuQ8vJK~U15M%8^T{{fS)(OjrmN~L`!5n_zZAkJ!Bi;Mm!HjfiH;y
z|4kJ5i&5ZTXmF~Z1Vg#vG!a7HBm?#Ox!eG!@re!}WPls<f2RRXGE@)$a1{8fn#iGa
zuQ7yQZ-5)|*BjtQ{I6=Fk?{04;8|sW+YInV1Kdcrv6=uPdKh2`Kg0n4i2<HxfM08X
zKdQm0o|ysuj3K<;0I$^Gk@bJn05_J`rj3{)!{20p8`~>jfE(NQ4^iN6MS=gF#(8)s
zU1Pg^6b1fy6!--L+?XFsdH55F|0xarGx%PIpH5eQHNdYo!0(_+$3u92ik}|-ttfoM
zhA{L{d(lsShG-5QZqvTiaa4PFeLP62D0=kp`grhd6&!}wYl)Oa!1eLYml1G%yki<P
z|B+c(<HoySp8B4lyXHZ}4l0<wu)s7ZZ)(9*)1Ybd=bHx2n?oOia_1MM-fDsboi%Iv
zY=oE%xi|<jlp8rq*S_zcT{wuu>-2dAk02)Dn?7T5{=BKPrfd26@4jq6Kuw@^<1jjp
z=31h)o9+=gh9X*_HudNMw72zg)lo&Bc!+!{9epHCJf9w-5rS)MoIw03i%3_8YviJT
zng+r}IRD-FsfbHBiRTy}J^d6T5N^Hv`r8c{#g7cvtmU8bXMFVh>u&*`y4x78pPqmb
z*h$zdz-ZQ@k#9J(O^a^}LsW-e|2lXs9F%@!e2*5tZ$#!I<3EVFowfgVA`Cols4w3B
z#Nze%di!^lHbu$OCKzQ}!xDQUP-J|<8QJy#nfmf2Vw34~ME)Z_InK`FFCk`)=U3`0
z{2v+rCZn+etP_l%s$2aJ+b*OG)eM?`yzN3?EiaS_56?-v?SglrAb;n*Ta>qkS8V$V
zfo1gwCIw!n){_G7+VBzrZ&E{Joly#W{2sg!iocYcHKvSWtgYS)Iuc>Z&|h#K7W`*#
z7nBM8z3lli!Fhr{+4JSzq8e5-8>~W9CSvXHYz!-zg0Bp@C3UkHY>0uPunTly%o^W^
zSqu7Dz0A=snK^>2V&iV+sAUxu%bBBrIllI?ikxKYZoKl6t;<<<Bg_7hWv^q|t6A0-
zmJ?)-wJf=c^{!z~sW+75nY~GqXp>y!g@2Ufvx0yBc~P$Mp012#MX6Lqr5`tM5S6<u
zlHAWeR!QzBnomLt+($C+hkDi8lm5&nS#}RNvSUuNtl-JvFmzaBB9B-J3lZe#&V4hW
zCzNE1od-vDc*v_Q8LmyNq#RW@MzL(b3)L{rG6pM5l9JpZ9iY$dA(06PGGghN;08Ux
zcuFukMk0$45ylCWe2)$(cA*p>&NQYrR_SL^Q>)MlhP12+geg{ytCb7MFl$#8XS=6X
zlr9#@)$;I6v?A7SD!d<~22GN)RdCfvY{W+P#Qhi?V!6)@nt%(-EE#f@pk$un3$$Ww
z11W?>a}7&`ZkKf7<%QoOgy^qt9i=39&2R+^td+hkob%&NH3j{yMC#?80xc$3>}}7N
z0VsmPR1G0{s#M2H-a&sP+7p~RIo*}KL;$%Wb+ZsW76U~zXlw~$L)#wT3nZ2U3LzSk
zdRcd~3STb}6lg6>XT4QA>wO7yCb+g1{!xJO2<Yq$kl0n!ag;P(5<yvD$xx(UfaWYW
zqbY_-=3`XNyjeMM5tSuy0^`Ou<QYeQF%s6w;K<ejpfM2O7ifzzR<aW1!Mg1lDi>Fp
z^JFq9nRq1APfsS?+#FNSNIIr;R2o(@p{IeSeK<=u0uq#wD1z@6DpM@s$I=3*rCT^M
zl_fg^6a-=lmjSu9GV%!fs>pA#2!W;blx?+j18`KfttbP8vz4{H@<#NRoXvv2<#ti=
z+53Z+XrJIOMdhsM5y&4ArX&afrt%=Z(9L0I5<H9s-`_=D!$5n-qP>$X%&`y69gF5h
zyF1o`jbvJQD@zt?{KL$#1Dqw6TD*Z}f5n+g70W^JaukU<)^X;t9aaJHUGbOGl@{V{
z0_gZ9THo~7pI2E6<5|~Ml9W4X`KTwFy_cw@#Z&jwdRQFlx|`bjP$fWUsel)CN-L^(
zE;YC2jrxSQF9B`d%KR$(jX#B&qU9p2%iM|19p51d;GmWU1%;7lX`Ci-*pFBa;|-=J
zAi63azDv+o!Fb-&dWy=Bc&vbk<}dZEg9)j4K}q4f126GDlpyRE&iF-UVLp?IFQ?&+
zD84!i{^RSAw`}x$dP!RV3KJ4g*CB+2a1lXNYTANz<#IE~webDWpJ6aWYqUoETRHx%
z6xs46NB?C6`mKZ>!8(R8Vi#T83jZ1U4vVfTU8R<(Z$z<LR@1+<8c4G(tMDhSg40+v
zKKLO>{OS%Nv<j*~_RGwu{K9`+R8zEqt?^oxUX^+}^jG8nWFD%~)}qNa>wi$5Pycc9
z{NWDrd`748+!B`O$&lyUA;{5}8d$|1^tN@(v7dUNQJx=W+3Q($9hc`DSWY=H<m4LG
zyNX0P<hk@c^89A%biw_x5?1x%NP=f5$;pD83=6JubdP?zq~)YfVx6Qp#Ck`G-ko41
z$EmGic_<-}Z0Ajh9@twI=9TDqx_BK8Q7y`wNv3P5P@*Kz2nG6wHHKrjC_zo4(?CGY
zjT#&y04!FkqFo<DbRyBmAOVu<u;}}5OZ0hBCHkzeM9=*`iLUY4|0mb~pIrC-kX*0)
zAC>DQ(j7Y>-Qz8q=&s071w42Q#5&C%;PR1x$6Nu-hb9f3%org4xKPh*7wWHmuTUpp
z4#D0Fg1u+wLS5zUQH8oH9KKJeXDAjVk%lB7-1)e{IM&jc7Y!UVp&lM-aldc^7YTJe
z?~v$%c^z034bZsorV%Jj2@HAXX>x&2Qw8{k@m?gVifBmELGJ#C<+`PFscjfG_&S&T
zC<Bd9`KUpa>nA&v>)$7}(Xv!)-y`{bmn7F;Gsty=c{OZjt~smO&hr_$!QS^XnyDKN
zfIs7tbwh~adbCJKD`DLzzTK9I#t)k9wtVVy0XXjx<a694n|3TTP^*e^1vish&Zk*j
z?#JR#@zIZM@bP|z2=Fhl;{7?T8i`7Yhcc%0UeSavxuhNe1f|RPGcfI7V!0^4LW5Xu
zWeK6yVwdqSzN--8vogWo>~;pdbq6wXkLot6z(ZKYfQrsP)WnKj<GHcyMqJep4HG_^
zC~8?Qq56Rl7|;1SgdL-#9Sw)Y43&q2FL2uslx!ejM*c2B!WvDpd^%`RL&<w!Lxq5p
zVL>xA%P2ldW>0Z!`Cl>lAVXx>ApeY}Mb}X>u!<7RO@dMwC!E<nN{K%T%a^v3?7n)n
zzX^&D_P8#K-NRm~6kTW7@5(%JmFQ&{B*OBT0{QG1N2=ejbbkPe2+D|7b-r^C&3BZ$
z+JNRV%w@mIkZWPXCc->7j#cb3iNO%&2GV6P#M;*-!&#Lf2f3{svtG`uV=;GWunGQ3
z{3`GZ*=Uwi5d@oD1vZKFJvLU+FaeGkeTi@O#<;Bvc~A7jsUdeGB>7tThz-LzisaQJ
zJ<u_d7ZZ`h2dpgdZrGZRX_j17thfMmA;~o~ZxS%#5?z?ky^c!bb+ewV0i7GQvinY9
zmXPREhfKcpdX@N2!38j8SVD~8+A;sZ_PGAC*h`4-TJ`WvJT9#BLV^?A0d<ZcTON(*
zdBXUI(uXDHZ6HYA8jI-MEx8&OOqXC|?c9zQiDe~Ak$usBp{?LiL0M>)0(Lu15}L{c
zn4H!g6U|4s?I^6}Fxn>*Z{pqYf~#UdBe!+0Zxzg|1Xl~`L3V3;5Kh`*qDQ3|Cd$Ns
zCArzKKY{Q^#uZ=?EV_2jKh*d!Z+~PMh2+_)ZSe0i3*D<=`v0>saQ(8-a6Wljf^wKv
zg3htKL2ZxKqQ8kIlT<5-n3n1)Fk+Az^bTHCYxq<~aIGq=4DEnLVg3p+@Hy{WVXg!~
zN4^CB?^K2mCG|+W1K*P5Gq4=xT`?3(OeT!B@Xf~0G@Om^BARK5`%>(^sG_a-VT5~0
zwqAJ%ud7(sW74GBO>w)fbh7KZ$Zl#}?3AvZ+D(n@8GEI<Qy(7|Kq#Fe{OI}e$E7p9
z^K``TO^0;rM|djGqtjcpFB?PlWdc5R+o_FIVNq?w1hNs662`^89E%dctS!QdEnQoY
zqy#!XL~j2{8!>8Qqm3AEGr(@t;5AgYcz7M-JqkYx|DLuc!|Q|s;Q3G5g5~fOh#D_`
zI<xJXieOZ>$Y&7%ox#bbtTXshz>{LkCadPj+ZnKDqJ)1j3Y>U&XZSyi0@sX<)J@M}
z+aqlcbo=g9!wyv4Xs6xNuys}U<eRI%$EGM^2euxKZ@SJkSOD?u*4Uc;y~aV|jm{|J
z8jW74n~+mqv)&p7ekcn3oB>YtspA=}RW9K-+B|$<fE#U84jACZwf@VrwKBqEgx?$m
zE*Rjw4f(lWgJU#eqV+aCpN|nzJdySAR22BDQQ-eHz>W3rxdCphXN#e{M!1dVMqRU{
zyz0+#%|S5w>3pC$0<QA`vLU2L53l!^|IV8JkpKN9eUlAkqM|{S*WvMKUwgFr);+aq
z)<3zu5w*1^3{f4gkwF%o7qoi2)HVHUh}b%LG<0DfS~G{Jx@cYZ$Hq@ZT*4W-rf<{I
zr?MFz{ab(E4o`}oVhjYgU5iioGd_C$_4gg{)ZNB#<9=lMCn6GwFrrK2qsO-yfpAk@
z8J{`$p!6f-&(+c&7m>Nh_`gKl&f0&dwx&M?{`64$U&@+(SsxzFWYsc8JcexE^z>=m
zLQk|c{TeL*Uwc)9^8bk9L|xN=LCgOmhPdR6jL!v%c3=UP^nY+of5>3{6oenYra$Gf
z_Dp!V(*yg+cuY*xIlL&h@&$bX#|_ZbU0c%^0<ThbCF~XsENekLfzO_=1Wc5_O;0_F
zNj0%M+M63l5!t(^L~Qxsn|H7$L^2=rYAdgS5*|AcC%Hlk)@rf%;_P1U7t|owsM<jJ
z^7=ogx49BW5Ku+2r+$lnD2j{lqDlPk2$43IALUQG%`_%pAIggSfb#tOPR-AuQGp@1
zMP^7aAM>_7YcFd@G%65(8)d1oLzYGbl81`^N_FF~pK=nwpV!6J4(#a#m_<a>VO2MN
zm|7jW3L*HMN!P(prN&8tW%WopqqwZ#@xZ^Rn8l|T&r03Zm>4b}&AD8A7H5fpwC&h%
z%O`Q9r#Xb3)U-Rim`X?6eFMqZkSkTiX=~HMFl)go%nsvdchm`9lYtNEMF^!paXp-_
zJ&(U3$(7s_XC)A(4k{qtLgA?tLQvBiLikr>MZM`B^<;$bJ~V?4h*J*C5G(?1ysJWN
z`J5=9qRsn7+wm2#VPDs;yh_O^T^_;=(ks8jRWPDl#9ImAPcRpxPf`*IxvM07(V8#U
zXnWGgpOb~AYf490eZLy{@uF(`ziInFcBMZx1#eX3XeIL(EvhC8JD%I?Gth(ArvEU?
zEVM}M-4NgRJWAWk##KDLCJDb<QP|<Qp6V1ky5c^MgDdP`j}N|!5JBFAo6>(9Z_KD+
zWpJhto?XR2H=i;eThm3_`UVo2PIGbz`)Z4pQuM%+YB7{Y<9IYN@BuG#h7#`<lz}cn
z12xn&aahI<6q*_n)%}smIFQ7u0+{X82*ZY$3y+BbOE*!5A;vi*Qur#iV`4%5F+H!A
z{uzN%dp#1wVtV0H+WhB2yQHUnqv{OmBXkC0z})zOi1_3!Z*BJt0fmXzf?BzuwR}#m
z^BX$g<P@g|e(r-oX$bm>*D6j`zjZ}Jkq!`>?Y^)AskgF-(+F{Os{As%;s09<AL*0m
zBwX<-QDDa*D*hD0aRgysA`4L?nQd_6{EXS_z%zE!`59|?6VqNbmI(YqcvlQZ4Y&BK
zh{uch1?4B~Uyl#y`gp=LpF%T3rFZ6(;D3zL>{hTi+@SJySTkvjDJb_s<M$9~ZX|Y@
zJ`yJka~1Rx0_kx?5YV=f+Q!%f*q@({bhv+EJ=#*MI=|Y6@QN+0MkAvA8XiIgHkNAr
zmV%0b+j)KwALR8mZ*}a=#2FUgUX9h4q6*>in;Pd*bs+-c=(>=5&~gU-K!h&Dr|uye
z7kgdj8V|9j9nojtN6!xW<9Y~RLSKKZFToa5p3v8qVD%=%O@Y@6^2alXS}m$hLff}C
zF7_)*vooDU2H+z)(N#!5`z<Aahua)Sq%o@F-Z;{G^zS^4;ZgM-gqy1DKdJkm#ErTS
z-bR4Y(1zesC<AzSeXCB4qa$Yms>h}8ce_}Z;Q%V4v&e_W(4E0EqQJ*Tfzw!+_MLf%
zF7R;rz&j7W&hS4N1wJndd`T3z0yw2Rmj=RkbUSrb$3cU`4ttkyzLomTcTVA>^X*T3
zWXY8s?Ps<{oE?X_xvJ7a+mqCZx<c1vaCH0wr^u-f<(nSLHjS_KIedSOE+XOE3~(Bf
z>fygNz>WJ-x@he{c#QBqQQ$v~0$*r=Q#<PEmK)%tj?m$IqQGy*3J*P$ADZv!;qNoR
zjre;|Ks@v^>Q<gNz>VRz8Q{kJoQeWZ(YgqE8}l>H05{^_Y=9f{qjy=tV}##gs0U;H
zPcr1k7~Y2X^iVV{D*v9z|J&g@-*`3xuJ=0<-1O*p^nP*ax|09h9S=5?fk-=&BYFU%
z=c)(~YAfSIe3q)(V+6vzMB5jn&u!_G(uvc;_0`_jYjBM`HK@sSPei)9ht>`C&#3zy
zer$YQH<A*@s5<oc10&#i`uh7e`0H+C&}Ba||D=oWOt-OIi*GX`g<G#b9Xt&VO5aG!
z<oAXa-xHC!$oTgnY-jDaf|w<qDg0mE_e^p1_<H+wmNrFcphDqk(b6Y5O~1(a(R3U2
z<bj8D@x~{jI8k*Q&07BTey!t(jL$^|oye#ixcEc+6^7iLq8_?v`|-Ms6(mTs$HU+4
zx{bS_!FUfaOueZcrNGmC1rhVe!1Fc$aNY>ky5qb0&&9E)9t8jgj&%3RyRiJa0@gwO
zo?wot&CG$bwL%`|u#_-I9jiF<8mm};idEFT&$78;V)k0BZYCu!#lkm0<tJFpIsp3g
zTgv^9;CmIUU+6TKB|w__gq1n6v2N}wfnzPJXav5mfp0A$Z{fB^>nVEPQmcya0CO|~
z2TvpuK%T;C6{BwnZF|Uk0_dg!o38{8prkB*)d3~XQO;6YXIHaL+0szjRFqmKeM@XW
zne#*nb95(+c%#rEVBFWC&}0vbOr~g=1Uj<AP09fzdlkNrLmckt^esujRNCrwtzu8V
ziAkWQT9V|7duW|Du&fO2(^k*x8)lV@6One;X6Cq_TUH0v>IuG&>ORG?Hz22!ECL-t
zDXGGcQjj^SC}}fRl<`@vR*k9fS(IZuYNe4m;O+3DKB;29J%v3XyqdlSTD8`*HzML%
zoFr3&BW23C2{)BNL*=oC(lIGASl7_{wx&BXD$X!#TUL+qgcWCBsMN=|#$X9v-yK7?
z>zdWNn0p2k_#q-e`PBw8q&?5faTU%rg9fN)YMHM4VvcF37L+B~FZd68P#|BiDcsbK
zDppa2`dCBNmaWoX5vw@d-q^f~EJZB41_i5QS!-b!TWkWF%wp8hBqB&Z9N$BbMV%2O
zfEq!<*4^nVCVKQ01KTtp%k;y6oayu!k;9xK&|_p4eE@dIBavffaSC%>$sCd&Zy22D
zeWsj903B4dCFqk^1ab@kc7Pg>Xd)Ak?6vq>Pqz5TpGJ)p?CG~ajj*_Zd0Ng0ZiDb4
zY#=V<BO6S$<Jy%O>e;N!(F+;LK-=W?Z01`5c2Zqed<Bvs`aA;qgoWT5q}r#SoqO@3
zA?=_>pccE{(<m?Lrott_crWVd2y=W3Cyp>eeE{DHkSGl7`qZ$Bb<~0?Q4)>9VCE~N
z31B@8XaLyHvi33ObZaQJBZdB-vHzOj)hxdv^f>yVi>eOi7>FAO2Enka5+i0o-tB`y
ziqd5gEZ0P+{HTxUY8?<ij&>8ug<72Y*JM!e+=$}o3La4@!Xas@{-+)h8t~)FAPx)^
z*$JrXs!PZPtsP0mPPKLg_d|X5GJJJIuw!9tTvm|o=r4psGwcfca`{2NufX>c$~Q_y
z8Z)>^9|k?0rmIouM}%fgjiVpDNf0#+Bg6z$?P^z1FOBiUPEDnAaYBYP3SXiRLe}_^
zo>1e9m!#Dp{Ob6{iZ@LiF6B%k((CBDo$)k)f<DF{6Z<ms9C2eXSP=k9!;hY4$v;9z
z(ryQ%>3%%1>pIu{jO<C{rR9VOPZs?dY)~Y8{+VES=)4n3M~NAg(D%t$Z-TAF0z(sj
z7bzhnHWt_&P!)OglaogFjF}VxihfiP)ilO>|GBe@XqaQv#qc@>?0OA8MH{5@x&&;P
z9$uUC@p=J_p8uqdCIk41ULv0v0CWb=jRKzqIO#;DX!+-D2N<2X)EOS?>7Bu!i30zx
zC~#G?q7GU+YF+eW<sX>+aHk#yos538{`V+mA~Z323{4%=u59@M8<LApyhpj{>8;fp
z2{fV|T~~2^6!_1hz(*V4MjgQ81~}Ebj{k@OZq#A))#gl;AEU0~wkU8qhnb$p@N@vI
z4mau^UNOLp<$W&-ygUkgn*nYt?|uW^h(FU%UL)M5W|Pkyjq^uO1YGa8Pe#Bw!v*Y(
z2)N!K{%_NT^w#FIT5&Jx%h7Zp9-8>#SwajL&!y@@CPbvGV!2rFFc2Pb_(kecer)_`
zx)41+;Ws{7akyL|cr;ze(jS@ssJaj>zRj38+*DTMGYub<CnGJB{~KETX8JHZk@2JH
zLdxRRNK>@9)P59Kk596lp3c%H_zPOVdac5U90+A({Ajw6g_Izkp!y2`Yv{t?#LFVv
zo;+>T>G8}|U*Z4A_&25Mcgv8K#}C_i+;Z#SOV)*~B*v;e9{z6Eg=l*{!v{CuG}5Il
z2tx;NV;=q+YCj<WBLffC9~a>WZ~2T67|gde2-uJx7zHqnyQZ!6xEx?PwFkNT29`G<
zrvE!Ld#VdyINBViG{?a@VJ*Vq$^?E+FMAgo2UPQflTu)&y)Qo~T2ykjVATAbe3;Dk
zakj66P$3s|f_Pv_J={_N7ZV&UxN(ePzd(NJsj#eov3w<#uy-OB%)7m9zbk`JhWUVG
z{#tTfDBMTqoA<^33M-8Au^jKv&o);P^W)RQ=;y{245u8WNH|b@C=QE_MG|8001jwx
z$dFHAff(sY@_yjJjuJ}P+x8r#Czu;E%->2bEcX}g#du#ZOc+t{P;X3F^tpQnT>;ri
zgbixw@Ngl}7kUF;|7Z)eKpZ})enok=Nb?|{odVQTqH;V=IK}l1)djv$iiJ;WU=zXT
zFTB*S%i&E1n|I`!*U>?mNa6fdHQO2H^FnM6_PV-;*bDat2wj2Q;Z-J>sjAdXJ5*0r
z0X8@`>O{B|fN0sjk{SoC{61c~-3j)bBKt%w8ZiYdLsgOqEL90=Ad0%Q9#NzKoM_77
zgdN5(d3FafNp#RxQ2MPw2!ovu9WN}OmE?n{bFIEb^H-vIujD#axR18Z@NBE?t7jSw
z?nhn)nC#f%0v5aL3Cb~h`fjuY8fd7YfjX#iJY1JUd!_z=Xs5&bnb9nVSE~`gJ8?VU
zd+l`i!7X&R3~t+yPjU+kP@@A>7~g)ChHk5p3s#?Ad>YgCG#G(k!u}y_2-4+usa9!@
zF3QWw$f<NG<yJb&$oN9cU4scVZ%#qRalG110SHU^)Qx^FAf&-drG3?2Vn4i~EMQO7
zB6+Wp(Uw}K+K>=E2a;ETU#&60A_oV)<H!tbh;h{x9zr(R^4}qX5a_9y)f}hO;c*@U
zPUJq0Q!@lJon0Pd)B*55#%(laQ+5JrcB9?SvU)0;W*?G)03h3V%=e#?;Py9|Q+J_X
z##0&dSKwrh-3YB(^o&w)ii~h36`Q&Hh_dA?<Za<%SB^cN6;Veb>mNEFQ5gYUs0*9F
z*H9-*Eo;2I{hVpyl_E5zMZ*wKa4qU$Dm)?tEL{aThUYg0V7@0v8G-Uc+f?)~;1C1v
zQblG2URU}0s!$&1K_2K6cCn(LBa;6djIrn@6=gKki)kCI2>&sqPX<E>=MTbOXp@?*
zILs1Xh<$4D34+`M9H;O%xLE;uOA;6`L_u#OLO{LICvjUb@UMCT$T4EzdCG}o-Yf)S
zs2$Kwu-5BCeyn&Tas~S=@Yx1G>4W^M9oRE#=$)`@aq+aGB*atTX>p{i_*>AQhA|Zw
z8{ZD|CthF0UvkCs5955EDy~DWkyPU|D}JEh2o!n{g=)9s3-sU?Pdfh}trXm73N0cN
zH@@pJB*AImK?eCCZ$ak-E8^YJ*hy6`r+1)nXhT({MkivU3toXeg1QoohG^75xxJG?
zhejus>pC>vcJ!fkOLdoK6C!qIB&C{A)xwt>Nuf*aCdV4hK8m{Q^6iu;Su+W+a|(C8
zQ-XO<CtWXWG*|F=RP(7ISv^hBQNl)!CljP({4P_$qZv?*@ytWdMbyTPtVpwnQ3=eb
z7Q~4%hBAT2Rb8Y&$0G=`H!iGE^Dm4u6@puE^Qf~_ix(MK>Foy%;#HxcyfZC9O6X~B
zJt3_R0J)wYK{ks4Zn0O7pc?g4mPOfAup*QJG(0jo5@NN-4;x{@Jdf<eSkV*&)eXgX
z>Bu-t3_Qz=jWBVj@ZoW+ctjYb+Nuaa6~ZmU0vL;_9gxCyC_F1_L%#@hLF`M^3(}n2
ziXTmVFGVLPA-t{=TnGwQb$;z<nz!rxG~25s=GbwaEB&}8P5=sh-2Iy{zVA7Md~R%k
zxw9k4fya*DO8n^gKKn6Bvz`Lsd5$Io?RraYv&PS5_lD0smlCGLE{*Bber$M8s*iz`
zZ#+w>*S6zoKjWR_67~IY)2TWxwnY=QSujU+(H|x8pVUnfEf{r^ylnw<YVajm8F?E4
zM)q11-lk2c{!iJDjYGPW*T^RyfX?8BQQ*Id0w>)h=`6X@3T{pxc>g9!_!pwU$*i=q
z{Jawd{z(-0dcY|^uW561PGf+z>p1yUi5<0>v$fkZFH&=A9tJh14*4E6=MUMRsTYps
zUb+#Qt}t!al<IetnoNJAGU6dVWz@;VYIGP0zk&K5o=A9o6!`NR-BNg?j_?BmoWW1m
ziLQwP-x>veC<^>%lpr2@kv>Alzt{jb>Q;Xr1#UIyER8x;<GId8{6<|YZRgU{4H|T!
zwB1LC>$*Hj-WdK)jSdNJ3}0b@8`DjWLqI$f-dL`|QQ!ds+*q#H4R9m=HUr$4pB&9T
zCNlpo8Q{k7hYfIJc~3=YM;jFi50RHfV*Gn9|8Ixue6utHuJg@rBH+BD0HgT<Jvx59
z|Nq~nGajJn?$pv=%rm3uj8mv{;(1N0d_qH2rjY|bBcA-P*Wg++5{N%jBhuC3TD9w+
zQSIk`Z2VMUBAh8<jH*MAKQIEWr?0=G>5P>hng3D1PhHbUqm<+dExwID43A!aI%pak
zl$Xf(?`iS%ae^K`GXA~rpg!o)18DCW>cn^!sxPkdr?`6jz8bu<w8_g6uLf{v8M{=S
z@i=1oc=o8T@V{Q~I)JhnNju~xXnNY2dO+<R8ULo6Z{@mT{qKkD=LX;UZ|n`!crRta
z!}H;R{=BDd-z3-g%e#edXkB`w20_wsv>+2}1i6&kZUoQ97FWy9+G+Hx+}RB`PyvE(
z;#&Uiy0NEbp$$hU$+)&kJ#rMMx8^qBn#5vCPRiXcpYxf^ae1QIrz9u&oU5eV!&2_?
zjNDxrxJwv!216+;<W`BnZ(^aC#Sc5Al;jN*8CR|0dw;Bex7Xi-w9H6Lgz8%U!2TKn
z2=Ydpn9>ERA6!h_=F^X;#3rH@6d#Xdh9<-dU3IDb4SeEKUW!X2GaP^uD<I@@3Wi6}
zwi{_WZEVRR90I1H@8Jz~hJcx983(BdaGzS0fMBlyCgP}1^BK`xrOzqznFIIt4u@@%
z&FUq6Htt}|9|<F%l$v}Wb1W>Qj{+Ni%}l|0FnxTE8ienYi5I-`tbjQr{?W(Rmm>99
z<Nz#5*iLA3RG$KXMzTIL`<4Nilw-p!kq!!m(0P5!U=d|sjwlPOC<?;DD9ru>g)Fe)
zPDuw3l4*Z~AbD0E2$+~N$G(z4GgGR3grbTvBq0wYaT{IAcKXV+@?zwuz#Qv#N-+~(
zD0V$1mT9X;T*=Nu`%>s5)83#)tw$)y!Gq;E$S23%!YcqCNkRi9onxaHDx)4zP=0zL
z3-wfYIeqIXS8-~R<bzxh>ZBZN1EtxQ(sB^mp*;Tr&q)@~!VI3=Lc){POh_|r{3WIE
zm(SWvXmcF6q#1QgFk}KBNm(ttSSm>B+f2F4w6#!ZAOAx3D3bbt98vfrpS1-Ey6}6*
zioXYJ_?w!-oCUbh87*RegE{jOQ{c?AFK5oQMCP2CXp_v<%y~TrRx#(pYRCrW%(vl-
z0?$MvqMgx76fq9}Q1Ub9z&_k>J994TLy;EQTY#^RjYCl>i~3OHMfRbR8Mnkar!?a$
z&o018@B;e`0>Z-2ITuEG&Y7qRN-7KPG%Js|kb^12BC9RKOw=*C8Go1KH<rqG2XdHm
zId11mz8np4xfShnIf}E%=PKp=gFXEX29bha?R8j_P<thHGU|qcxm4|R2td8cLm{cJ
zdR#@E->|3iDE`vb#HS1@#Y<FhATr4vD}MhS9Y_0_H$t>~(e6lC9uxb9C5lo4=|_7=
zR$Tg=UK4l`NTc7GrcD$s3a<_PFM=C{iNUY4yVpnr(z!AEqn{h2PA|jLMt$p37@&u@
zX$~q;hf;md9T^v0kMX~_J%q-lRQkx8^Z-UrxIWZRPK%n8zXsr+;zy1z5;|`(9@ZHD
zr}&`p(!U#@#-5aZ3Vy~%PydI;rnevx#gBY$`;qxiN5sy?rXDT6Ednu>)%eiZlG5)$
zJNTZW#h+me40mMwEWkP&o7QU$+E<HDQ$xz99$$xdmNrGH($a6H4?HwfCLEFR2^QIQ
z0GU>5=?|a}!xLH8(Z;4NTKxMBamg7OpX*%;<~?x#kKEfacyNkl(D38;c2wQjo(T_k
zdZ72fIp??~UEy{Yf)h8Aw=n;3G~)?oy^#W8jp~GtSyg|<f(zk~3&BG%IG+#4BQTst
zz=K}LgF10K+-80y0dT6w=5_VTYw2J;GmLu)@5eR-O~;;)@IA0(8{w=JT$Rl40yc#7
znyYHR6P(Qj7A%z2Q;J$@bmbr2AG~}e95_9%`UDQms~)aciivoT<*Ru<UJ#Y|AjXSp
zNaWA~3d`vZK{{=Z!dIMN_f;XR3qMXCc$5CYbOL8R4l$o#*}SkcW(lI`*+Iaw%jgfr
z{#ja#EIr1L_91+$2p`hV-isfu7k6v|+N@fFmH`6OdfM%gy%o?LWHGA-m(3O=V0CCQ
zjUU(2GEq%FOFn?#S~}GbM|C@Uz#@7HWgit+`yDIVK+AJjS}p!FIl^`K#A*$`{3V3d
z>#oX+O*^=w+_57gw`??O4R#RWT9fIV0kn?)Aa0(;0gAOvLiaONgO$iyS?ELf(mJP?
z+iRy*6PkUY7(mrAeVA;>BNX}qt8}XUid=;g4=D$<meRe7Fj8TwLpM|06@nb}*K7VE
zp?i?GG4~&f#TEFg#JZ9im$2~chkA$8rS(5rhtI<z?{yBU7#TJnILe{;bV05t*9oZ8
zdjwbO!y^TGe?u!qI8?e+G{7XBosXjxW9b`T81&NT2psBwQEr}ab38{^cpWXW;!0rc
z92HPEt<zyOcU$8-SXW<(T(&i?Ye33;U77L&3xTCa`hrLenJTi!0YLe}4KA;MK<T89
z-{Gx{Y#Y$biUV*8pPJ|-Mf?eJe}1h8UodZtVMWv7E#+=at^ULYe;j#Qx>oDP&b2~r
zwNJ4;2e=8V#<+$att(*qhvP0sEB%6!!l_u4_hl&kaHY1SU<na@Q*Ad;TEDwMfk_tZ
z(QsO5{Mj#&ZVMsffc>?tIQIN`gy9_^b+d9`tE7yL%P?<-{fTl<?8hVm8vCOjaG+&r
zwNI{)*PhvfOH%4uB^<h$dz!$frQC9*U~mlX3y|b3bSDYjhMv1wa&DkuzlH9s#&5p`
zx2w>28`NJ>29mhD9KW^sY!YaHQni=$5?ffWbmZG%8IsSkEXjzSU1XuM3`f>+8obw>
zfu;ZAxo8qP1BNq%PfuEIapU)eAoshR2F274Xx=ond9TKRmzwIHd)2)L{KERiYPDHw
zc(WQ-oAdeJknq{P)Xs0920FC!3jk>Cd=i{e#L7Rl^&##)r(V}`1gcKl?t%X1gXL6l
zGY*=?poUMro5k8D^a%3tu%ZhT5ats@b;}S*8F&+L#Icf3uO%Fr_PKn$J44BDCG!GD
z!<GD8WlTlphADflsEimVD)*pu6Cbr==gu#XM>=K{XWoVbEo2k1u8M2uIUE;{%)FuH
zGiqN<QS73>)yhf+qN4bX@)>edhP*pNKA9mO6lEA#%(OeSxjk8=(Yd`1zs0&RtiM$J
zR%C~o7>$=>p9>ybU|%>!QLoexVf8UiQ07_D3&Cs>$*jq>PNa)ZkVD)WgZoxaVU51c
z%bqD0hG{wMMu^!nK}4!x6D@W`EU?>r-D@*YC#|CYEM2dXcn3~=eZJgRRLhD+;Z!RL
zod5=DXLdmI(FKNfo2;S>*OI(aF1n7hSAyQSwXEnT2#1Sf>^M{zx5p48M>Flgg-f^r
zM&3l<Fo$W1fg`z<{?0%`b_M%OYNEe`g1M6TB)P}fPOzlKDfqkFfxj_B!Mw7!=HoUJ
z#IugHQkc7~1l$Nf(#SXjy00(%XWR1ASvKzb?lTgTLo3ZolOA?ZKya@<vr{PIH3*y^
zN3cDl|G=Tb_=R{x+)#g*JN5!?%~}d`5ETz$Qu7^sY{4IT8C>frKpp({_*kM)PoS?@
z;=*QftR}y&C?jj&IS#RNY6*S{!8ed&1Hf5j<T`>5x|uo7;*{DuZ2$nua@L`b9@Bd-
z>+>CRY{S=4)(0EyHnEAEDY1%ebl~xwIJ<ZotJu`WdgD&|L@|zbeF2O;T~*{i3jVld
zieSUB?~R{|JPLH2h!uJaVTzxIIFMjz$qzQ3i?erwtcdK6AP_-npeIgnHsXlPVYj0#
zFC#XB3U=&)R}6mR<3uHS2$3j6a+PF0jMq;5NPcfSfzm)GVSj33571jR-ovp;=>*(m
zY~pIVcuScZ1GKnF1wkld7SZ)BE2%>Fq14cdYHDbz*#DUnBU4-b@O_k$O;1U5BJ1r%
zgDAk6&X-a?t#Bv|9VP9tWSGCnkoROb4`S4CfcVE)Wzi{|v$UN((G6l%k`H9axL#^c
zhTNDTUm)v50q11js0w5<b%Lo9#AZ{&Bd5Tg$<9<zp0zIlA`ai%Fcom-NV8MN#v#if
za?Uky)U%A1u+FqE4BwLjHU-mEK34sJqksgPRB2>r)(`B<ff|Ka1HLBW&H$aQ&F149
z7{F2-TmY2FSs+e}Mp$qX;y_>|f^!cqHD%NzxQ?-s4>1)%<!%IhJHRMgU<C-;*dRD@
z8g(OZWmg~-4Cr!#2(^I#*mXFmOijR1K>!*`5yJw};Lvf@sX$g`CC>gjlb5$<U@UDu
zCITtU#hi457f#jQhTYb&0`618)mwDqiZZG}QpSTF!Js%pJ}x?slTlfgq>O8j<c;t?
zLMO?Qw}7h*E}$MDDHdAD3K@cA?h*2tA^Jn$v&YF|c^n;wp5dy(tD<|WG)yAM%Qzr?
zEqmE#zu(hik^TMw(DgowbI{vjJnZ?(0j-h?DN;O%I5K<1hcAT78gND$2D50&A8=HV
zFSKB<l=-?>3^<eFT1#mNE@a%xUM8TY=nN~m_G(Z8fJ1d(fxR5`#BF3n-QhzcvKa!-
z_>#=q!SxPMIbd|`vyVfiCbgbO2k*scUOh2TJ}oMbS|u~=n}G~myeBFNXSAOX>4GY`
zBE!60GIQq796${E01qp$XMxo@zLlI?=@WE|2xKHX6r?cIK8}xL!G@W3fV)TJ2e}`!
zp$2WlYB}EpwcbH`+oU@%e(5t7?9u^3u-;#fzd5A%ba3vuhR6q=Npl*Ab3z*Qxxbm1
z>{bf0f&Mtp-AJ%KV51lWIliQzTi`dTKAkzvpF}RF5T6A@#&;D3-GINWS`~->xsGD&
zOlN)0A=DxK_NKFnLlb!PzAC6<Z(BO+eTYC=aX>L?C!x5&gIMW%ALboRK=Bpe*{wb{
zaTg+feKH;4ZNT>R1aeLxY+nNreigggxG$-Y9Xq(MmE!INHfWsTf-K?!FS`<;9762)
zhTw+)KZsIgQGq7yqCDIOb~dS+f>HL*L82T$00+)e&pt+hzCq5;BM9D(y{ylAybiI6
z>yV4RyqNb<#`a>Sg1qs<w-MI-IF1|AW#0ti3R59o_Oagk$Sp#?RJU;`G3|N|!>AOw
zL&6x1BJ1etlwXLm98?bnLX@Bdk#{uLq;Jr-r%}$!5Ml%NC{rO=pAg!YIS#Oj4Rosi
zD3aZdYF2Sz0^)9>D9FYE*82cCkv3YBO<K$P1ldGf5J0lei@b4pC~ERh6NUV&f6*$r
zYFKe5*&BoR(ij2kQ{aqq9DRVyiEC+x|I-Y-XfK9$f=mMhM<oif1uZeq&WCA$970rX
zo(}nm9)&N+KI<fsdkBkJJWXKh1i4mlo`owYI74`SjoCP4W%fzpcMz_eesR=5b}e|{
zmtgJRfdAE_FatC<?xdPXMV@M*Y#c=NI<PrJ*av}(ZQ9wJfaD+|V?5!Yne<M=ycJle
zng`&GslmhzaElznM!K2+!7#}reya=aN!18J!W-*v6LDPxBs2z@tmG>`OyE_3_H-N|
z0nKZGIbfsdIHQ*QN|byQm0T_LzHq6LS1R=?t<)H9<WOg&%FEtI8HEhUTCEqAGOSlY
zv|^(Id9hK{4#nP1*dWrQ6&ndsgBZptofdf&D+!S}kjulZE|HRl56z)!poR^i(Ac;V
zZgG$Z>34Xm)M?p+7()T6ovDdYVkC@R^=hMcXy~8lrPoS4O>bxzq&pi3jVd?4zoCC(
zXy{RDLnCTujT~oaWJp+8wc;p9|Hl~B^BpZ1S!Bo%lel!EfyC?$nC%PBy|__A#7M)r
z1EYBmKs|))SXKJcQYP*i!)=+8;yD7TRW7-3B*`mf>D_C%#C6qT>LBrXM9|m0+FMk`
zihjc<cw^cKq5nHvmAC`|5-ai!P4YmNiMTiuL(&YzGK^%{He4qH>4d(Tv;i$Zw}}uK
z!-_@2Fe*(#XT-W{R}m{%{Y{W6SoaKK%ukn!V7Sk$50O}T2-kZOqzRBdUO=pO*5hv~
zmq6RmzF=QTQ@QM^Mu<DP?BPuR8!*)DePC=kI}nV|8rE_~3>Ml*3^R+19GWtKUn4B0
zc9W{HL>EGU-JpOCV7t3H+sz`j3lKOR7%$FXNEE<A!NUj(pYT)@mOX@JCx{1q8=pJr
z6KQjXpJkK_psISF6&w(;4*VT~cL3=qtoT*z;3-b(OE4enjSD5gzBzkESt-Fu;H8*L
z!GAj?fI5KS0Y<8k_M$>)6GP9c;OX!pa~X&1&=e<UJ^aAF)B>x8)k;Vbf{Lf}YPA+9
zGOq;`o7VvW^!d!#_W2A}_0;*ySenn=fz3tLthhg)&p?;69~ZUY8cWw^R@9aAYTngo
z-h$(EhHzGmUdp*N#?vaVQfD)qS7E}Ejh+k+4fdT~1zttHSLIchpP?8kLq)NWcz2-P
zpmwT@nQb+jh`V77Jk}hg`*9x2Cepx~SnP4;CnXlK5V|=Nd`8V>67tRGP~fPX+|WGX
zobJ|gO6+7)IJ*cz0Ax9wUE+AiayYwTCfS}{kTL>oqnN~tQ6(f2Xb4Wsl*&Dvjxgzg
zQ)-$2zuH+NuVn0zqbx&X%~@g#GOboGAB=-t9M*W(PK_;sAzfUtJ$^*R{s^9?Rw`H;
zW+VOrA_pZMn-VbnXcnE#>J3Fy*L=%eGY%})I1Notp>#tHBta}CZ6WOiN)-zZM5+ta
zFD^>W?1h^REAv-?SC~|Nf;zTEC$V~J@So&C7wH2hvqDk8tWXl5OOY0VFX1n2KnKFW
zppQUsu2{f)AFj=Smy-ihF#9|lIp9E!<M{X%KVmb`7GNk>jXoEm{wi`d15`A8%0e?t
zIt?a6ngj~6f%O6Nb)3SW2pw$$@BAWlGz_h%qiq1!IYkWMAwI|x^)B~ADg+DkOOjm1
zhezfL5m)^dC33nYT?n7MqsOJ8$3c6k+->GF5i6#b)^QjX)T0TgJLtp!_ZlL&NMX!H
zQ+V#@I?Vj<aDuA|rvWt%+NK_z1T!5Y{+fRS|FQXZVreFfTXcRKiF&kR`I-<z=48_#
zZVi4kzRJ=@PJE`SbrTPD6YL7fgkI7SZ-+kNWDDs-iC!AG?wsEk>xpL*gTxbY#VlPB
zE4j`-yi=^idfO5=)S`ixxX}R=u>H6KmK7g?nifh}(xV!z4s?Axc4d0~-^8-wZonYO
zr-a~9?2@${MQD6t<|&vl6Q^CT{tLJd%16k;O3_bAeW_|iEeA;Rk8^@Z7YdUl+8B!N
zLymD&E6DH&2nr~k;j+XcPzv9K=fBX6m2861qa^3qJzV*zu4{x98habw_=Z+aFk|0C
zS1V%_LHg`7V7bs)%oXL;7&o+W&Zo?_!W0YFgW+0kWzo$k*q>(aOIpng1x6pZ*z^Wr
zSTd^jnM$nAT#NY7l44?ev#<L$Tszi^hDmfnvuP&BLvf_p#0)jnH(;Ac8j{2Z;z+HT
z;o3%nlnmDayjG`o-|s8h#)`(0>ekgtx=sP}`@SCbyGn1|Sypr_dEid7u}VBNQ%Sp1
zPmfj>#l~gGptz)sACnGUy-B=_h{MQMfSRoKp7ihQiTZMo@{p8@Nxq&mq3M%H&}>M`
z>=yiJ_K3BU9FnZ){|*cpl5rndUXNz=qc=Y6I50d&eZH3^75|s}QsLPv`RDoU^30!u
ztsS#ryj8di^LW8uZT9;2n+xuvNegpS;I~C^R&f^1JIzkD-&~2R!=`H``3mv^P1A@L
zFPctpr46`r4IzL)z(6(;!UzgI{Gmf9;@odVz26SH_NxR}16%eeR6mm380rSOtgXKx
z1xQ>jgyrp{ap@b$hCGbWlmc9w+n+dt2`SK|<p}tx72o49`3fNwK59LT_i8muIV8cT
zB_%7k2-hycv`Ldu=yM|J-@LfBthw;b(8EWtvB}>CQm0kgPD%ZW{Ro0Py1`5r^K~1f
z4Z7zvkL++wJQ3i7P?T&(jZY-@tg1smSC1mAR1b)<5H1h&qH0$_s^|*8hW#hs?`Wj|
zeMJ9I`lBaeL<{0aAK>(#N8JsCF3RP4Cjg1^4j?J{SYx^X>JMV6fm8pr2I}vH96;lz
z=GWJKlZ0)T_`t1b-<f%yk=3-D(FLge%)SAeM0p2X)CF+&o(p2&<zR;EOX>;f-D`bC
z*qQlX;OC<26!{3Q4b;oLaacnAHHl0dXg^j-rVj9JxC20zC+&{x07%<v;q;%$>7Uer
zNtr6`^B$26G3hu9x`(!c)H|I1A;m~D02L5jubi04>7VpL6G0%o0Z9&bg80DzUoH)%
zPGF%<;A`&$Pe^jL+6nfV3v$&?fUf3P(^)6LKn$~WItfqf1ejb<Cs>8w*Z6_zcqafA
zs-1wcvl(^Bdje4b_5g-^g8eE9n)YSu!44$(>(B+RW&jgKB39&J1Aep)L3HjRI){VL
zN4MjR$vR0EeJY*^dq80KrGUm0F^mL(ORlQI4?{&{e<n9zD0Opqy;Z}H;kRRVH2kDq
zKq-v)sVo@xX^C`)%ZsIgn!@)&P6+x2l)@C6OB)x1U&lzUb8Ojo7>k5EjA1<sD_H_9
zX`OB;-@_#cx({H;7S^#ad;)htw})nW1PP9`VT7#c8O|?21ddfuTgFJv{gj__E(53y
z5;2HOoQ6`ju%eyBVEyNz2HuZY;Oy4Bs6ti7p^T0t$cVmJ4I8FcX#<M;C4OkgFzX=Z
z<pIK0R`dy<D4ayMwS5lKHZ@LE_b20A0N25SX~?gEF$Iwm3}G;Xq*Ep+1yFr<r_5<6
zZ?vH;yo3{wh+fCT(cY`5M5CG|*LQ`_VWEJ+lmx*o!8Br4k`xx%h)QT?+E~fjIFiY*
z?!`cYYy!ZzX+Dc$E~1i8qRAPYHaIu&!Cs=T7cxjI(&dGiBVZ!2)(o2h-0C6MK(~PV
zH<Rna1Di>-0uw`EFi;P1&f%7o7<+s%TDF`MmC?B8yA2z;AH>C|;k;2+hT?*XQ^=>9
zQL>AcD<zmm5J)()S5Ow6#adwtF7gG_d8JHneWx-Iv|v7#vY=ik-FO#PLN+7Nv4aEq
zdRfAAeaymOSiMRedLQZ{IL`_4UTPOi<C#@N<*U<p5{1wYQ?_ly7%GChG1A2+U+x+Y
zfm6$R*V0s)C633Xqg)**v`;7L4pF{5CeU;e9T+#<zl1@cs`eX)E$>)so_(4GM0}kj
zpTzKu+rQv#7M!Hq*@bwxsNL$d&^V5&584m(k+aNyYkNEp3r>HG9*0QV-<5|?UWML+
zu&^Sc6%r(wh&Hkg2YxDrU<vJ^g`co}#X6mSmVzCGJ|#v&ofM~Dc+^EDS0h{Y4os~i
z*>2B}kKm{eTBLEP>(9KuLj8ZbA!*n5^jA?y*Zb>HR&;`oEf1sHt<F%=d4FZlPN|bs
zREOYN!-_T&aqy0siXl_Sjw&jJm)KE(FdO|MLtaB2)k`K$rp5`lyA%8UGhFor_jK&1
z75`2@MafP9FJw*-w+qFfEt>@#0`d|nSacm_B{igGz?G>OI>&HJ4uOr>0NxG7(gmoj
zZXimQVR|Ig(y|kJsgHMh>mnK{YyEVDv7g4GpO&GY*8F?@lnfm(fEo?4^O%{8Vdz=n
z7G#{^l4DQ7B=sr>y46*n>RM5`%Yhz<rKB2+pFMF~)%p~a3Fig>*|TifXkvrt>zoJb
zGhueeDI72(&|(lq%GCcxV4!nt9R3$w%X?8pC3QG-K`civXT5_gxpp|jh|P-sWUTnC
zwu!DU3$f9mW0!i5&IeG2)Pb;nLAe&phv6KFQC4!DI^m-})ivCo1Wf*xJ*q`QV68tz
zi9ln}F+4D+(%is%56v}I%+#BJ9m|O!CjzPc0Jp^G%^aeBl#I4UKY~8=3@9rH^Cf6N
zX|#3-Ov-M@ND!j~$-I~AL)Gysk=*z=bP8O)Qf>uTh5;Mq_^OP+EFJ*hA^=yTP)J>U
zL{oekXrDkg;ktV0W7hM|PO+ee;bMjan5L8A7~&y&_<}k@`l>K{Y(Pt&Rqc5~UFl+L
zCG+MM-c?X!&bL-zrfId{0;j{A)zdW8>RW}VWoC_i#e)^i+Y$QJanA6~Y}o@4`=JmS
zr>LY4&^a1-w9^}rArT$UW+<vgKu(g75w;FVtM#lUs3vBigL9D`%6~b}1USe1uR>%B
zgPtDZXLiUZLpx#3f_Wj9$1#S0?TiP-xrLJrP7dlz;*tjI*mW?(9xAU#-sC-@uE+33
z&W3UoG8r-4WhL#g)|yzZq(MS#3pB_?*8r2+X$yWAsQK_cuZ1eVk_P)U2E@RbkIcTK
z$v<X7?P`3oNX$NsVIih%O>JnC78EG0S^GH4CZ5OlqGqmzZv~d`@LPvCIT+AoUDWiX
zj7h_o1xsL@2BZdUod(T|h&0}aBm@B-Iw5#4@mb4?2V&S3x~YNcfEMKCW1;Kt;Vq@g
z`V{=w6uKSKhS#XKl$fbD_eV8yiC^3w-k}o^`)a#+xoH!R86ihPAEUKYMEt~&c0^!{
zAeLfyhc@c=jochEG!J-HmlZvb8;^w&L9pDGFq8maa*YeU1KTV<_(Ud(kV%X=$$pV+
z0-wPLoiGN42kZxN=^0rt{)%p!QU>12&)%V#6^|_P`;iLn<=4Pfa!_hn>SoTLVYIXc
z;}73BGiVCGM6Q(aqTka39mr8&BT1}l+8Hpj1-SXUNkal#NA2|HRjLKtF@IItDuV$W
z^gXz@jLtw>dc+zRVfhvZfM)sjA=<?&pCH4xzrm>)Ilc-f2oiDyqEv3dz1G~y@jih4
zWaYR34k7m#E?wegjxbp~DY4Q6g2Footgn2|{TaF6DDg?b{V^Fzj@=A(N*CBalDVVO
z4@-ZXDd5bJTHO8qG9|NgANqFxlfqvF<paA3go5rfN-4G5xbwsc%<&-Z%CWa*cs&MR
z5MJ8~8zp%Mj6Bh~^2r~=Pwch18)G!pD)NZ8_AEBmSyE4@ZcbMw(X{kBULqw8gAjEh
zouS+n%L%6u3w|Uw$q;gvd`uY~b7l|NuH0DHs+&g&xv-hW<fe)GGxpmZuuvC$GM;h^
ze3p3pRp~a7tEmq|dwTbJSZYd+W@YB!7-b$!iLsXKgi&1fIw5xp8Ca5qTks&P+#WO&
zmLd2{PA1IuWXwh}a>g<``fP|mo9U0Q0+B8N(?YNW=xt~on?sfbIkYyDv!6m`CR_2!
z${d0*K+bVWmnLl82^DF24#NRe50KFEDTY+p&&FG+AYdt!9;U&>DCjdt=x8Abt26jS
z5&0aMmeI43JtWMs&)|a=!n2_7A=Z)=1Xg%tRlyco4U-c@+gT;o7FPT`pK+{;!QMs-
z!f`qqfi8gY1MEIIx$1VG*Ffb+1WYPNSbmzkpOU6aH~iexKn+TK)JX53&JLEIaR6xa
z{s@YQs}lNx^GJcGZFp(-Cy+h*ID|uQ<k^9MNHi5=J!c(-p_vH`ewVlj?6Kf_IsZ8e
zE9ytsa&ec!$3H<f$l4dz_w}YMIfFiBN}H0V(1*Di;}J79XQBsiy`nPktf27m2aeU+
zl!{RbY$_ZDIC<SU1lB7-%wX0R9#!H^!VAH3B;Gkr>OYu-A%|BJR-*gj)0htN{^R|W
zm?^OME;D^h-mtXohHS$jzk~l!9sE&$CsEJRb_=v!iWC>43^dXCVi;TqF91E2Zf44P
zA@?XHL<vxE%MOD3Qd{UFZwD<<SnY>J+`C9{DfK)u7!N6W4ALMgJ|1#nJtmp)X#o2y
z1oOq?pJx7vzO*=>3#WsXAV;Y%L*7euw(iU}7@xxy8TGS?+b0XTP)d@$M$<PqCyII+
z;TlNN#y$`dunI`aNm5ac;-R{L`4(j*rG+`xLsvuX)(e7&XP62b3MJrEr4zM%Lk`d&
z>chN?w%;nryb_^Cc^OTLquSN0lJUkD|E9ep9(+?R6EOvzwG&xEKc1}V$7!cE5i4Gt
zSXF*Pq+=pm@h@;T66Nck8a<i5AP<d_ia&y!5-U9*blkQF)t!1|nNEz<ZYZCnI}VDg
zw4_p?(64xbX#BvLnobouAN%3jji_1cKts)DA5mM88g0_i_L>gYG6s+&_}5VlgPaHs
z!hsEcEJo#>Q5L5oBVi$iMp83Xk{bBEB!vVoCMhVZbdq|yC+fqDNwM0zyiuX*skFR6
zwFQGjwU<;{mZ;xDis_98DgGk}L64To>B!POT+1{Z=W-$3j^QSYg}{p5WTDZBxsqCv
zTdIfNL<6hijUZf{ZBYiIK7DWWWu6<*-Lp1xz`@eDlFM2jJif6o-{M&|>cBgW-`?!m
zlm2;i+GoAW&We}E^-r^U0-?ng>%FIls0>dtQC_FduHXKl&$PBC_hp~;ulm#O;<Td5
zJ?Co&$nKYC`9FVZnKSLO4MkIyUubf#`{wTSj9ZqLF3$hcnF#}a?@lpa***8_bD!RO
zrnPC_*(Ueh+5gzE;e+L+8zwwkIDPvc-DS7EdC%#R>(ds`dgSW?r<>fp|7IUlI&o%c
z^7hM)Pw)Sxd&P@)U-ns&v-F*Zo|^t#bCdg((ZAmJ<dsX(>TNBL%%1+PyY-9Te%-k5
zC#AP7H@~wX)a1TpaYfCM0n^h?U$ODGwk`i~4}9>8H<rI9mUh4Av-4~BHMz$u|C#OU
zYk!$`#h+j7QFHo3_lq;#H9zr;ExrD_+CM4Vo7|O&%YT0QvE`)$KX8`*_41G1%eT(n
z-1n!qmp=N}Sr>Y5Y;t#(F7waXU0AxM{FTb$pO?A^oV)q>%O~xnb+7K5n^)fCKKAyJ
z86`)SrX|dO^X@Tsf8m~W;Kg~CpN>lNZhP*bM?P$F$E|;U{@Y(Xl(yj1&W+8kp!;mv
zRT(3$onCtRU*aC0@OqQG|HNKv-*|0V>1|V=nr*Erch@Jr_=YLv&a^*tueN{pe3N_F
z@BVVE{%22>PP*Os{=*YiyWc-^h4+ue<4fNh{m|LGWlipn&Og22?tlC$t->AfCGTJ3
zuGsnVg0|G@X$x%s^{3w#Ho0@I7_lvN?DDkKFUIZJF{{#j(^cb+9NRKA?Y*2co1UN6
z<gV*~>(4HqR+!fFuMecHY^icTp|pK`#jI(iH{R59*px9%?k;aXchG!nVd<Tg_pW>G
ziM8%`ifY=rJTjwHK9KkJ$)Qc|gY)`-`0k^RmFA9J{L3eLu6N&Z#rRi79KW)3=WP$4
z8UK?e_gl{@{Z__5QJOd+{lM5~Hn`vE_Rvp%d(Zu8+fsgeZgGz$w=6#R)vpE?rd>68
zz@y(?vB|yqk5?Z*c4J;?`TTGGwD9;b_t*uk_xDIGN&EYSpC_ywTjO5uzwTcfk}RdU
z;*3L{ZO7c@FP-{L<r9n37My;6+UCSs_ltk~SC4Z0!nA^(d#kp8dd$6G-hES_oVh6N
z_Fa<OS6AzP<<4IHzO>z6y7R`0yS=X-b6?Z)_5Sy#EKBS9>Xt!WpWW=faq8+nDw_*S
zFRPW_dUWYAcaQrXoYC6jvC{EN_mx{S>)g}oe|M8<-Tcy|eZrCD(~h|_7Zjbi`@yND
z=KRZ_&FS@}yYF>pM=h$Jo3=Q3!{}cQKjwZyD3FIP`*qqc4=o(NzxGRamgAAW^Ssm2
z4wRg>3>a|C{nP15Gb@vql<w~TR^f|JZ*kjBKe?ss!ZoEge17A!zqcNB-#=ma^25o+
zr4>JQ?|w|$>K@?DA9cfJcc!J4um7mmuA}aMK4z`>dD`-{SJTg?6vu6I-z$~WUopC%
z^bfBMoi_fXqwXos9z1+#&#y`kt-m_;%~jjnI~q=2bK*}|q>WStZ+dXWQFr;3*IExw
zEJ(|J^Y4G%|JZi7^{-p%jt;!PG;pwb{?rGKx+na?RWbF+#iiSxDb4-a&v&@r-!kcy
z)S~>f+p05ahj@>=PfXhU-1w%V(qA8aW5gp(JKS&W>gim%Ge7OCTW&8nboo(tk6xZz
zcAD=jUF)88L)Bk*y3edXFxxu$vC{dKY2W_p=n?ndv)=mf!AI{aEz8;0-FojXx8M8x
zx@rHMo|bd|{K@9v5%*iCXAGM<adFzAYhY)U^p(5o_&Y!STd#$srxJ$@82#K4cWnN%
z*F1b~eCd6wQ-43Z;w$&lkFUy3D84)G5A&A~3EY3geNWq(%C#l4(&Vp8?pg52Zg*jq
z(q%`+6{fxa*^=Y`7<$CrO<7d<e*MFx!%lY{dhZQ;+};^!SId{(mnKe_cm3+PBkqyT
zgzb;_z9((#!lv<8e6`1YaL?_l&eS|s`m^Weyj`}f(cNS2+k;=fZh7e^FShLZb6~G~
z<-`99_CGSXbn}W^?*H3g8r>h=)$QkF4lhgl*X`1>^Zn}GPu^O7=$29QOK+cjMY?}p
zqkHPCF@s;N2$UXc`^oQ@&98UQ^xQGRJn7e^NfS@qo^gAl`<Xu{ezEeSr%Ioe@6DK7
zUGM%!X!f3Mb<0alF}<^TbZ>NTJkL^gUN4tkJNl~3e_pfCef*oZ9vF7wsnWzP170cJ
zeAq4g<@0WzKVDQS?%I=^F@GP*zh?c3W6Mha^j`_i{68FaSMB_$A?36yt!l>KUBfE(
zxlQ?xU%&ghK-&JZF;{iZJ?yqB_VJ0oSzMakc=PWcxpKezA5T{-c>i^O=@UO4`P%Ew
z!|uZ4_nJR-JXE?jX}8p6)_!;Vf|LJWdtU+`RgtdUSqO>5oS-p+q7yaFbR1;D4jM^!
z4A5Z$MnvdfD}g`+MTig)6)_r?v>_NBhjG-)FzR@ntE0G#0?9B5XoI)~+*m{cD5Oy!
z3d#=u`_|H_K1tM>JNMpso}cqP-RIO>RbPFzef3qHI$sSa**v%A(U;QpU%fw=xoTka
z>&Gv+Cu7fnqnB=3xg&GXzC~;9&$+h7J^#SqhF61`C8@X1+mTRK)4O>4@k7?m%&9{T
z{XB4dN=??MYYxu7Gnjc(k?X0uM%-U>AV211Z^6#Y-J`ruzk2-snq!5J<Xn73Ftfw9
zls|X=s4Qc7UP{{0mv&~3&-v}nO9#x(IP=l{PrrMtG4q>m=RNu7ZACR7jU0I0rjt7}
z$L1|4NS;)d;Xe1ATT5#iGjHp;zT5V<9;o@`g1Ac$<bIL)YT!iITiQ>|_<Oe%mTOjH
z=6mBa<7+lQP}4u|i6^oj|046zCvLxO`|Jm5ru^<yhlDE|Gw<lN=Wxow88yK#cUG_4
z^+o0{UVVS>Z&np%+<xw;onIf`oq6utzj=S-T7S)=>z{t)fgWFGj*8su|LUx}YhHTk
zmW#Ljb$8~cXKZ_+?{~#DOTJ#vYtp@6X1;Rlm%mRLHNR%;w5Tz=Z{MByP|Tdl_dmEJ
zV?o4(mG`XqGV`+49iH4XsWfBJsr`LpFW;T%d-wut|3{@6+qzzG{Tqq9GW%Y3YWS<y
zPOEw8xjiGYckjxaa`CXWQ8kq{SIum?v~&Kh%u7E1?%4RN7G%74<$Xnqm+Z<MIsTQi
zdu^?#vAd;G(3C0lj3ZY`ed0j%HNU&Fy6WkNX;ulB_KzE&^o<&4&L2nAKk-MKe`n?2
zv*y7>&+_+Yy=?POQ~uZ0Bz4>Q>$Q$Ln}2`hKfUHI&mX$pc+vY;K%2w0U-6eS;OdgH
z$@AuxXUsUz&haz%dsjfaGW_!geADgQp}T^WHH%aF{{Fg`53Rr|lJHL!deo!=|5ZMv
z>b<4K84oV%e6se9sOnA1KSB84HQ?uaFWxqO@xq#8Q_rt_tzzGbams&xSL$K@0oFt7
z&wQnGQO2>9U*6YqyQ8`sS{(VCF8qH#;FY8g;Xf~9%C<j@eQemd)fWi{_Ggw|#`u38
z@ZAgJ`>ri2&$!R^%6I2H(64&lMj3zA#q?i3Ap6g|Q?`v?SaZoAp15ah+~_NzoC*4M
zQS^U%z@AI@4NrJwQpPRMeLr;5@AIp@f`R>+E9PGf|4jq_lJM|G>lJ^-(E7O8Et~GV
za-3jbf94;5=Z1f9z@_~wuYRttzvk}yHvB2FVs7=kxny8}<`tjzhyRfQ8@9gs=oc4P
zWL)*e(}(ZNm{)zCU;z5r$LGO6F7uU1&&)pCS5`A_$NS3)9((c1n*{^=GrNDV__|{k
zb<Qm8_VgtqKfbdj{fZ68JHEW=$|uK?f&H1Y{`%sz$1Y0C-1X4ks_)I4U(@@|<eUqp
ztgF65Ft9)K*<t%~j$PD0^ZA7(BNF~Fzh=O|Atm@Y>B?S$f&G~UuYP^mv5Rst-&}s=
z<<I(zt;xN0<m_{1G+w!D3K`g+*=_Ky7mr?)pLxTJ)4v)2()}5S|6_==+m|P+BQIbH
zpnRTrcje)WCS{)OT>I7W7iQGF^4E3uxK>1FJo4Z|l)(PX+Z{KbY`W;K%tJM0oz~=+
zWjt~BttW3@e)LMMA8Wl>>&bTgSnI`FPuBXe)_=9$tMy>5|7!hI>!n&x)%vN{-?ZMQ
z^*F7+>G55U>w28m<Gb$f`mW`GzRUit|J#};G-S$tzw=5NQoExv!rhnH+*jM&g(3Gg
zF>fiO%>Y~tnI}ZQy*EQFy1;$ueD>3V8f?QB{-et1F0Hvnx!*2(AN_1(jU>>J`S|Vt
z+%szSi<x>urt7SizIfu@GE9wR{0D23F5fnOcE)$Zq<g}M=LUCpCBNn)+8sxFzgv6P
z^HHzdRdbCX(2%+4@rZM7?RQtkRf0f6=CA*L+r4ij-&1qoAPF>NHuQ)+@Wq8C8Si~a
z0u7nfAJu$#;lZ++UV;GJFW#{=X8!{<p9>BeGOs%SwO8)iG(2PSK@w=l^uA_Y^Xdh2
zYj!t~KtpEJ&iDtP-{Q@<_|80V(2)7tRSPe>c4t|}k)&(EK||){n`+;DCZi(b-p~7f
zKMd})-9z6P_Pc=@565<HyfUI8bMwKyKTpZMzb0XPpGTXb8#42L{pybAA1=+febd+{
z*Pq>xx$yjk$%pQ}FXOruv(^vDYsl<5<1Zy~GbYviw%?X}?!I<==J|0)9#|h+TEo)$
z@k_UpCN<A2R`bgS`#iH0v|9VwlVBM0Dpd}3*XapbSXf!&hQGM$cCGH<M=a3&{U5c0
z#Ad@4MYg|x#LhG0bit3;I&j3R6MpI9$>CVQyC)$%@+KSxH>88GlbEagxD+SPZ^J=s
zWl@6@mc~Xs52MGO+zE;9gxEo})yJ^2RQSO^$Aisovdxb4;Dm;VD9<@Coa|0W$9ZqC
ztqdE{SqX_*39-U(vI`{MRrH?W{TiMThcFAGa3GvIBQ6{K&cu0eI5jSGM4Y?B;Dp%K
zfp_0?IC>!(4?y7cHS-Lf7DOWCJf1UG7LAM~M%BR*G-D$#NIRQ(>4%?3#e0<n-ZEZv
z)a<Z$gMZN2Gf0r!KW-Ze`v+RNTC?567Zj$wXI;k^0*^pzaVa_}V#y9Rt`)9eM~Zsr
zqj2qJ8{y@`k8}!D{1lc8u50llUK%f)4t@SLT~VTyO5FVa*PV5zj8>Y&MY^rmMwB>(
zC_M0u!WrXG*$R1%Q+Nna4>v2E{mHy03kP;u*{<6ao?^piDBMox9);_EsiT!D+@%3^
z7t4Z<_KrLo{-AKTcNEz0Duox@@I}IHF9nC=NrkKG4#j#(JjDnZ5(3pjl(3Ezjy?w`
zZHv`MN_>IxejWEj^=#hWZ3Ev5oOB9Q*ATLa5z)bjyX3&^5Hbkxg$l3wB?C2|J4stU
zt)-jY2A-qoca?O6Ji+5+g{S{Y;zC~V(J;m&p9?3<EFROdM^T?%MUKL$)5lDoG-Zr9
zD$v2F!l@IAB;Nc)ym*odG;!?YqA}Aa6po!z;^>xpb$9-y^RFlWdhxFh|9bPUFaP@S
zuRs5~i-6rlobEjYB4Ty#70#NYb*tfH#!N36TX^f3!il$zDH=OzvW#@|geeoIjV+k~
z*1_lm=4Zl~;;DD`U?zVcNGrY*CruMY;}|=6(#=yQj2|Oyk2jyk*mhbhO;OaLAgv$2
zs7z>aFc#;sXUOVuoTu)@O?%cH!gvv<YKFygaSbGh%N`!XjkxUi2?M88l6oS>Z?qd;
z3{PA&E63Re&agV&Ul};V$K%I~N67HfbW)|$!S>2a<GTM|Ys0lpP-w$-x;K)5qGOiB
zs|HSfba{GJzhrus;HT-lr*O@WPl+0Bk6YujRL1%#hKciSqNejJ)xYiZ+p7v@_+%se
zgDM^3=Nfni4w$&?_*|ifx5W1|ax3ARE@;L%R%$-a<G_oH^gH9H@tBK<N;`F_xNH0(
zg)_WK=XMq%E<2xRGqS?X{2oya*B-v`7p;eX$JshQD6Mt;@iy?XU&`?IdZ{&VvY_+(
zp@Cm&;GI-QARV%%!_QSX^Tj@^@iz>d?NGO$1{<#1+o;PC2p8#??W)kg&xfC;zqcDh
z%kb<g8h0vrZ_n>6g_Ay;h7MnC;N(;Dv(tua{;hNb!bLh}zq`=D&3gBB7r{k%Ha(qg
zT#wfAt_J=yBYZCdH|by5vvoSt4BX6bO&j<&h1<(5MUof%Gtc^>4$q4ie_DUYBZy1m
zS|2DCXfv+$fqEOR^?@Wq?va*DwHto$$V=1L<NH5uuYcOq@ncKII=abwx5DWq7?_(K
z-7#u6yD*Ob9~wYKepB#c*7bOaM{&8-eW$8F<!qrtvG1^MT{?ieHw=*vOO*UhfRpJc
z@f^}4DZYrS;Q~+cx<oew4X8WAnis2&H2>N7>_()?dN(hZO23MX8&`_)%*X!tF+Oz(
z^QzC1t`rl*v(CT14+7R&{3aE@)D}Ncx%GWS8}W;;mH~A-nhfiTm%fk1Gt*BeXk0T@
zeAPWdJ`BrC-)}LVg@^ikM#VQVKI?cIzZ(ynMcK>$ITb%e2UK@^ejWi%n)di>Rs2$0
zg7)~8h}&BGEzOe&J|}PDjG1^m>-c(}+*;a<;!%nttpxJ2@Vf{<d;B<HQTW-fyc~%v
zu<>tmiZf_hi|^z_9oLic7W%ix7rHdm{gO$A!iwyVo^Q~*M=yDb?oYQN+i+7Td523p
zx=<bo4#WOo#p6rhkz4U#Ic}bc2Oi_i?@Mj)RGgPnacv4sHPsW?m{XD8iMyH`v7hy~
z2!(I%QvZW@cqHOWOZ^<1t9G8{RCLzovmyu#O3A%ccuvRBk2<)MABTvO9fzVR!CLH`
z3sF_rSSaKd4&Y=GbKj;ts3x5CN0-OSUd2y-8fijmBZ6K<6!+fh?n%tS$ENVC4?0oq
zRn{m$#R_bAnTCR#FyzyOPoSNacc<n13I;TtFk_j~$?|o<4_nCUY1Oh*mgS31#Ktu|
zcr#$Y-MIz7dRVO3@-j9!BHcj!%RshUz7J^k0(&ZV!?qK=YT;auO_F%_eu4WI?wXYG
zeH$(BYG(2-^gTcy<Fm5&VetZleCsVQzIt?i1!$iY*h|{G@Y`(pKIT^$2+@EvwgUQ^
zU-H7#h2`9j2;ahRhLw!XcV}(D_q#A~k_<-A+D+s5`0l<D&zr2X-n7!Dq`(M$H!JNX
znAnfB(uTt~Z5*-D=`e&9QIelifsZ7d@%@;4{&jI;Bku2VD)M6CNJ1HT<I`Cv`15v2
z!BbKSHpu-B*_rz|w?j3Ta`3lsH`@%(#op;A)`$35%oBl%;UAimyAxW`Tvi*N*^2Zt
zTakK>Kr7NPTakWdE7H&4H1rInBpEi3dfEZos2a}Pw^e((8u>}bUvxjTrFuNXJ3SRK
zuup+a^l)`e@l?!!4a4o0cQdyB&4<-0>}Ew_yut5E=1gb@u`)``Yliv*tUZ_tL&c$H
z*1;Z@<^39UwH9@b%1P>mPXS?;2!1fI*o4a2jtY5?iCkzUZ?w+(hHNAwWc-B)151tG
zTKv{qzDBeNz$9|mN?v6ppJ2OL53!~T8mR@=v6;Y93#fa8sLKSjHMCJ5tkev_p6D5g
z*!GLIch+XhIUE+-&O&oYy9pjs(y{N?3N*fqZM^ti5L<Y$;TP@63hW2;G4|^+tK_4O
z$!3XUk#D3Ps6~F?#cpEJ{n!y35L`6Lv;vSu!M!8$kMG7%$yS;NM;*A*MSm2Z7Xht@
zSzl|;3+S8_D>)Un_hScdR2#ANS9j`kXDML1{8cuS+rjxkly<Y2$wy;J628r12%;w>
zozOT!(kX?K0TL&;fmu;|`1!#Qa)pvyGYD0Nj^Rvoq894VwXug9+nrIF>;s(!q0v(w
zB%xfLIO4&Z3cEGQAE00lBT^lnYpt{D;6(v~uHt0jku}(?ViJh;8Ec8<z!C$6&5#fu
z0p9_95CC5T#RUMirwy==hSro^4?99Tt+P;*vKx;?AHKp$8-noY!`MG9yYLj%g(=UH
z&_kUWFc%o=!lL8Gi;m0AoFO_h0?00WNOa*h$z||cKK~3%qjkf{Q){_LHq5+3HiwOD
z4ip$6*u!aU33e|6xlyQf=6eDJI`SY|4Z1g~L5N{MVFH^U1vWHycXx^*2GHL_;vrr4
zYl;e^ze9wC`aVt~jSGp0y6vzuAyP-j$C!bHJ&D0zpwTHC0ICzS2eaN22BBh;DFvK|
zQFiE{-p?S#Q8ktq4&Qcw(HEkxVTf4A5>7x3Ur0HaY<a&!&3%kYv%LE(-{%Yy_?F{D
zJ$~O<zE3#h1e%DzfEf@<Z_sJNW_?gx#qt^m&LI2);By=9pU5`JA!7}PF2EnbR)gi;
z1tTMDD4$r~qX74cuIF2aCbJV?ZHX}|`D>V2gs}P6O8ykR|3fsB%g~Irqn)CyzGJ0L
zqu?KhjyW7H2@S}@mXwEwnP^G-@quk3ngGsnMUElClDZ+gsol_2=I2znI2^<S@kalg
zna7_uwGW=+`%qXSCv%?`ZAnN-7H)8{ZciZz5?qe&Y?}L@Xc!cq5NN^6gcsUZ3n3d^
zMBB(!$h067+=>r#&`v|{Mc8}+HwteJFcu=L(n4H`(h~(4d>yR7j|1-qEeBr6zr8O9
zA2^Iunuo8dpRO<AlV($2!pQYoDYd9cN!>{+J{awigr^~?Ajq_&f&h~n_@(X?(wEX(
z=u64}N`2|Fw)7>|Idl#xOnB^)NQDV?-ld<^m?-by<e!%sgf|0%@n5Pj-TCjQF^y8<
z7J}4C;p*%H-6JG!DS*B4khB;EpfEYnRUiWYyZVwT1P$THmNQp2HRwxPb#glm-3f9v
zq&q>bhGZt>s(cnhxfqz&p%tjn9b>f^nlWyMkIr%w4!$D>x6t^BO6Mr)<mpmvjh~RR
zH2Aa@;x-jxr?uge1)9i`>qo)(+W*=BdZRgjLea=TjfE;ST2v@UO`!~Dq_AdnjUE;v
zPY$04gt&8s#?QfO{1l@k^`Hjv8BLfT{6AQ)vL-_#p&W&&_(HxTdqSi$3>D!nDfml5
zlZc)V-4@TOP7F_Iz|iReYrn7ptG~eKYhqTi7J3UuNQ`07l%1=rvsS}=x;1AQrp3m{
zN(BnTqVMy;pfkYoVE|jlG%#lFLu-Ncc3-`fya}nqUkDSpYpvw{uoO#HE@Vo+cPuZ)
zv`uUWNrJLDR)9bN1is<K#(7IR;^Sko*$_?`Zx0an!j>!*wdD6<--IJ>@;BC5&~1mO
zU>t;QIs=s~24)yJ!w5Jm1@_Tlox}2Bcs^(a4ni5;2P?9c7b@`%sM2KPUg$L;`{z`K
zKNLC;RDV&@urtU3nxnF(462R8O7mi11yj(bp;LpyJfRWeTO>8*E&R9Clr9pQQu8=`
zT2E@BCzXu&r*)+N6au4JN8;y3N-wqR*+y>){sk`_jK&w7O_fK@yp{g6n?d-^Dtd?1
zlfJa+Nss+BJxS|lp!0+J(wsK*r5yk9>plLDuY;v^d=T9!w_<9F=l-Lml;-o-qRHa~
z4VR~Kp#bLG83&;3D9+3{6HI$5=2tPCbCAn&jx4a8MVOPjFtx<YY6i_HquZ^lMF{Y=
zumN{`JO37fceC;_-OP7j-m26|D<5uPoY5-LvB~mcre6n(K3x9sUMO@#Z-eE7B%4<X
zVoZif<ii|Gz9(G}gSPkqkeL$ket?8A-}BZA&xK1wtRq(72*P+=B%MDqPZ0KDdgwd?
zvxY~Y!qX2PnBpA)0TA)NjY%c~V2XJRl58W)9$9Gv7=zu-^1|X9#oJ=RR@&?om;(^=
z(YLI$*F~afvt3wr5^@#i3(c!_fc>=D1y-6U7Rxy-4~1Ba0!&BA;fIAPXZccu!u_!&
z3?0}+o(nAJ6qJ;=m?9T*puk4vzd#_~)m&rZng-@Z^XeH1Y_jZbN|bDFCzsBJvd&a7
zl$SDK{xSh_CVkin90t)+17|66e&bx(gut>qQN{C?3YS=JtLLN%rZO<W$zRXZP$Hbm
zVPX^blzy;|o{wQflqd!zsQzA@Pr-!~NZSut2clrpMkWHnD%&TjNCjVMm`Xc8maA_l
zQl@LAjm+a*i^BR%lw-Zbicz{vc*HmMox*x3K861jri^m@pXAUo=Xw|?KI-vDr{>h{
zi)w5_XY^D=r^22qP1{uVDr?*t2Ia#3A<;ENE2IB{6kr+WC;~%|gB2s3YeZ`}&xIm^
z$cT0vxPOCSja9($t$St3<zQn^4vt)9w)nN|$J8`pb6^}AAJ<3h%YiZ4oU-l5bI=(h
z8lmg0Z}3EXPrE`nWjkSWW!LeX2&@tbfs_Lq`}hDh;`3Z>sj)I6s(Wn?EG}2(evA_M
zvMGlVBQ^>-gJO+_H6CY|D14)gc699$IC@2L+;n!0!T=nNW>9Yh8qjH1Tlr&`LXAK>
z$X{aRyO%<XK<Ly+(YDYbKg0?tNm-*XTs8_cIz}wrasb9Mr!y&rIz`|pNL*WM<=<5g
z`-Gv!<(3GW)KSZOn3d>^Zja^yB|vm(cLOM+Bjz-bD4IeYH3ZlqUCZw2ZV+BnBp8PZ
z34+w(naygo<=X~JAy5>enXNjuEx<wr83-^E;jo0~TLpcA?g+Twgu50EiS8(T^WP=L
zO6%1gr+~)55sk8>!JW6Pxo<%Jw`f{(*cu#^g2?xcY`@pTRfIA|C+UM8D@9>Gqz%^G
z>bNc!co*fegKqD;VhIy+4IXl!fuI<L{&rFdrh||)hCG3ANt^Bh`C`in9V%@Qk{N`a
zK&c_t3endFLT}2y4HEG!EJIs?ui5`5C=9El2<QFU^1Vqj{%|5KPQTNN60w$zl`XOA
z@p=mNiZse2ESRq>pv(6qk}F2PchTj1(IJE6u?ex+FoQc`k5!Ql>;?qQ^Fu14l;x|S
zgo&JeEOYjim9|LGfFj6l9DqAT(0pE|@GSSMqC*#m^cM5fc%+^uZi|Y=t+g!gu>yOT
z<?D5pvE+j+?-3k88~&I~AT!Pvmym|Y{34MQ_#-2$t+Z!lD$h2ER2G-=mNrkOIj>f@
z7NrY6bQvN120EBzqDmz2ktlS9d9fz6!{XqDGWbZ8fe?9Ih;}|(CCQ02DCNMxTNq44
zD$d1AWzr2Iv3d0}>=NNx)Ff{IoidP_7aTNaevwS`S((^8rsApi9YvO>Vt69#1AXi$
zs=_(RXohEru2HrMtq_Bfv$#pflZi{wr%H>-ogq@jDUmW7A|;C=#c~!jg-aCgQbVGY
zrjr2Q2MM8KIjfe6ex}6Aa0rpR!^BFKM8d>MDMtfvqa{n0k}N}7l`L75EJK=pRI<31
z+9eCbjjIXiXvt!wJtnwKTPhjA;yQe_oUziXm^ntmh$TvjXFtxRzWD?41&Ts}=l<2D
zm|}%=FPy2&#Zc&5&Tc~cqE28}yGGDl%{;zKMU`3kqoG683(;7IK6(bK1ML<vy;%fd
zJrnP=>P2lx?W?&O7L|%>SY-uPA@=YTL{Dc}Cssh$0J&QttOh7dSeb{{yd|<4Af(yk
z^0T0QQNU;ADFKi7aZU*NKph4A5SQ>`H2~%ke(M50#$F5PHe?RvIw&QqGK3mN7?h1#
zSX8S*7oc}H;8_aOn~E{{ERaIAfhrdw3AG}uITy<ja&!T*Q7oD>#3hxl&;SX=t9gKw
zDw)xo7gdP_Rdar%l&<_00yM_T2TVMyX>0vUH|+<Okb)@I2bc1(m7!CFLE(x3Kh(^f
zD^u9}#5g)Av6XTrpDa)|>*hu;bu;Jd4%xZ~<%Jh_y^5sOG!&}~Au(iz#?2QCm~PmR
z!lCL*oZ*6YgO^}CIcQ;A^EQfsMhsxyU2;hI4nkc>jkIUU0S7M*XltM}G9AnLdLnOn
zNReYn+OsLb@7ZE;6ACb7xfn(7MaxDzK5hk$helCge2a{>4UHbG?V^)I$|3Z-2GS5x
zRgPX@oE*Fb#!5xfujh$WUSA?y;X}%^g7>hz+lNy%T>Q|es>h5)BEjat^~GA5WP@Pf
zM}`vA+rkt~|4<OOv5FVdf%~&d<b7p>yf1R#j^=R?eHRNcR5^a?_}l8(BL^Mlv$Y_L
zdTNrqx`a!NOW*<0%6Wy5dW#C^;hdH(XF{l{TT;L=lntn-o2|fRG#MD`1^{t}0nS@b
zA2w<q-o*;uSy>b?fH-d|6$I7H>MfkEBS?K_1wI3*FX_f=nsXf5b^bxHg7<%Hhe&NS
zBn#T6)TZ!4cNYrf5DMiG1==wNM|V3ZK`)-gVM`|v{U~%-911jQJ9^Vf2a-n^gbRFu
z7m;2}oA9D4Z4ud`e~R?SD<$7y%%->;ws?v1b4kA=h=wzo@J=jkUaj<6*r0+G;Eqh@
ziQDi3WME;j@NzCrB$i(fh`(pkCA_jp+@a#2*o#q;oYXC%C&W6sZ5^h}Qe+D$$buBF
z!@cnELBR8U&OAXHf)Mfu0X6{L0V4G>1x_TZ<YOZS2fpkUA`d0@Uz9PR1M%YA1HV1+
z6JoO2&&%VKPyt{I3LfT?n3a+_N<pp}GDW=61I;Bu^yI%n$q1SOvwBvTo~$ND#{Cci
zu0g;Pct)|}m?<F-S29DaCAJ-S9Y-xJWF8QNyFnHfV4!7frOR-reeY)E8nu@8EZJkN
z)yw<JJQ*fbqipnmkP(-nQY=C;Z>Sm81e-MX9Ei7+M{E^QO9nX7<Gv_G-WS)3yBIx?
zB~+aEFudo{TND*0{@#_0r0NL8i4NjiR7wcrnHcFmh6O_;ie+Nf{A$!ZCx}g;vxLF$
zPSyE(1D-$`8(^qE2*q1eDQ1*kv-8MSAjAar&+J3|FA*zUB=cgTAcO7&5f5D%DLA)S
zUc4dn9b>;rTP!$qqBC-}KosdmC}qK9+Ts+%Mhx!p!LkRsJKJ~Kq9uZ(w1u@Kp0?N}
zLqS4p1c_Rdb^|@oC!~LYa4jz8jXF|;oZ5^l=B!v`*ck7h6^w@s`HMv+%u%0|!bW|O
z!hd7bM|>2q7{PHDd&5|nO%j)F1oZ?tw_dKsh#53@E7U`*)T71b<5-V;EY<wT{COVp
zf@on=Xp~`<NGWYzx=hg={gqFi^l4Ui7u74DV%*upaciDaz^EqY6k?qM69)%ZS<R`%
zW5ohlTFfO;REhIBuGk>c(4K#Ig0VzI39F&U7?%xEEnK*Sm<Y`=nnY6n*c^jrA%h#O
zi2ZBk7^UR1W$8ag2GOufnKLBXT>48T8)>CY(x<uf^O{5i<_r@KPk$WW%*|i6nTGK;
zo$u#IxkpM<nWxu>q1T*VA4b1Dy*_+;efaeH@agqosB9@>E{O$?)9b^Q^Yr>~vo3df
zeHgmd>Gk2ztnu{vuvi&6y*|trZ|dFs>Gff}Z#}&}%%xhqrbfGH_5JMW^<n4f^<nOz
zIK4iMeKJ^)6)XOy*N0E9535%s`fai8t*~5sJ-t4BdVTnRa($R*w&1Weta@9?*oa$;
zw?9}?$L`a5E3p0x9QSLUXoU4-hR2EKG58I`sWwg?K7(^+@bzj(yw9}<muJG@tkA>+
z9Pcm#=PV`;!{K8%?2Xu<PI%F?9nL99aQfcI)5!P(_~;HFBj9;<mjigWgBMd9@p-{p
zz;UWLPKxt=ibs66a2&qAwRk=WY!{Ac!n@>Sc-#u;1f1h|s^jP+zDpwYb$~YmxtmFM
z9MXxNv4XZ2XFT~f!JXBGV7!@QafTBP{Yp&0As;hR@HdRdgAIx$HYmM_8Gw6=6K<Z*
zmN>E#zBJs0g2A~%PMnI8IJ+PAQF@zzZpEo7;-DV@;`o&P0B~%W6Nk5WaW<H57mm`n
z0`6mYMquv=xW^&Qj-#DSXcN=GL9Je%ih4IW=r|;uzN`l%#gl1$JlHp@AJI-Ys^{OO
z{P7}>b-7-aKbVsx??cIZS@OeK9jz2U&rHAYD9ifaqWqB!R_1zH{>>%tL&<wt^21pl
z&Bc%J0!06EVh0g+)mu({@j3`=ZP@gEIo@C6g`)n<=ZNu{4|b&B<terpego0B${O_^
zzxKh)RPIstZ9^NVv(7ps_SoaA)ZJF#6TE)yEy(4|P4~(8A{{%elJU}$8|%e)30A(`
zm5RvD_eJD-c3L{$r4!#w<8{8!)z2V<-06vPqxiw|=yZPlj03Cqodfe@c98!+^7nJg
zTjUbeH!Q{SjdWSwVFjp4JagkNK9RqG<&WyZ##-+;_<WfkM2y1gS#IvfMmY*pR4Dhq
zAGe(N7E8SKhM|GOG#BB-p55diPLRh|wLB2vC+hDY4EEDv`$;mYlGY6PfjYLICS&92
zPQ3F)d*Id*ydn2t8;Wy1+GPXV3*HT{=WdBr_>*;xyJ!OV3hSUHn+P6v0v64MI|)Dl
zTl9qG4Dpc|p79pc`Ia^6LnMKHHs1Hd%WoJh@L{VrwrF4%xcc-!wyXTBL<{7rCG4j-
z+1w;0?!~4me2sPzLJ~Q_w_=BIa1ola_%7H=>%lDIvj$`W>81^F;fI37fYjW8@g23<
zh}G`vOJ&2COnA354mNXPJ2vbL(4GgrLD(dl_3%72W%R?XVmAx6=CWO96`PG&jc+C-
z+xO}E|NpuE-PhyD$hUdi<VPO=(d%<7UgGW}oZW>FM|gPJThB$5{H!sn4#%YWas2r%
zkAFW-%5&csybtG*s_)Di54>^e)K}ZYct(25@#4kt!z8;j?yqe@GJYLNx*Uy9lim);
zf#}|ut^EG3u)jO>p?NACy9+Pv|3h)<_XF`o8g2NOYWmv$rn1SI{K?cErRVHlJpjLz
zI(_9HpyR3Vt?^%~_zz3ppfCJMn>1UYFXAEm-xWV1JZS1W9z)Ni|68`qF|>wknWx6V
z0lVQ3(Kh(PDA*CNjJDYtkNQ>287+*A(`Y&AjK`1HPoZ;jZ0mF$Mf?{871({F(ZtsH
z$xcY07n7aPudMyxguJMCU_HDQ)92Y8qK3BalLMdVR^z~DRD5g19OR4kjCuV#@s$XE
zK2v;MEVz@ln7Q+n*G2LsYzszXCm=6=YtM^u#Yx7v*AT!(tg8z5sz4;h-J!gy<xPYe
zp|EuZHc??><OfcAvswvP)k?T$6t+Y0&bZ533HNR*;kZSWmrmD-=YtBnh90mxUl}=w
zHtz@1=KTbi+?OWub2~6>3YI;1HHXb<C9E)~F9X{C$@2fCO?uk6Gi}m~J_d{?RcSL`
zUoQ195O>p7y~e%3Nhh6s9hasv1JA?@l(9w8#(^zRcqt1Bm&TvMGwC-F#8n{vW&02|
z?3G*NbpGQu>|yq#m8&(I=nNL+IHN`AjRJ`g?a=hxs&Et2&r!l^qISvq4%}!HeaQgn
zzF2u5P<Xw<b${bwp$sq1|3nzw=ej7oR*guy-}O>>sY?>peQ&73U2ci%{#T@MN0!8O
zADpdlZ6i~+j717>u!Vm~;Y+WUbaeQ&3a?eT?vvYfcs4LxI{Z-`Ug0`?Qlu=0rGvx6
zcT;$+!bQJBkRb|B^vLiUmxjr;F-6lgrfAZ%=_SQur%xX<Va9|h%@)%Or%plmDJ9d#
zOqw#IH8^BFU6M5jO&veJIfC+?Hlc9Lq*l?QX0$oY>9@h~dfOhFC0dlH5&3kLB@xRo
z`!31!7!$J1E`g${)9xHQZG5lh1Z=o1{5aU0Z#6=*hb@Lmp<f#*^s{rM3AKpO?9qA(
z-TSnW1UzgpG^v(R44>vW!t&;rl4)b7Oh;~R8)MXys?%maS+fquv$EMi7CIc9++h=~
zxc*|pGn8BRM|=47MtF);O=pvVpKaijBfLn*#PbQ@;?1O^$2Og==I4YF-sFFf8sF$`
z(qGjE-aneDw&dqY12_43+Q3cvQ)8OxnEc#r!!`d87`RFQK?67G?=^6feuwtW>6+mu
zsv*T*p7*qY|E3N6`8M!34V?Vwe0|gg{)rlgbv^2KvekxbI$zmv9X>)0efE6C8Ms*v
zm$ZTVX;2-Poz4~Ut>bP3H|ZZ@=5gtKY5s+=Ujx_tPf)g?8Q!Gx_criBRETid!~fpE
z&Gzt&4cGj<VBltZd(FU2e$J&*s?s&-_cU-b{DO|n;Z6Flwt=s11OKoM{A&X@+uQLr
z@S|h`m%ZFB;9RQ}{5#{U1~XqXyR;tuytHQA%<olQTgT6#0*j0MnBgDmA>bBx=d9N8
zi3ZMc_)G?KOts-(D}0U(*WoK{xDJ2Bz|C^-DE%lBbeZlfMgJNbzChvQZMY79s}0xT
z8x7nnpZl^UL7lGVf4&XZ{6AvDb@;_LT!-Ij!;dTe_t@}w=zYA7*>D{`Qt5-5PaS@s
z4X0krE6;{^RrpvNuES5X;W~Vyft&Sp-yoTe=4Y04I_BGOUH*^Qa2<ZJ4cFm!+VCe8
z{XI7P1%)58;W~Vz(u*~pI{ZKzUZd#b+3<H2KGufo@Dpvg4xggpF<t)jHBmiiT+bhu
z+HgI8)Mviw@Ou8(pz2fOdOmf~hU@uMtYSik*Yl|^HeAoA(rtLDOEQ>k!}Wa1Ys2+?
zYO)R2^QlrBuIE#a+HgIedd`OH`BaS!U#i+`y$!Eb_!l-@&!-OCa6O+&jgtAMH-CCQ
z^{+T{gU4+A-=3`DaHJqlY%($UnU_mB#&}Fru0xr~`EW$z=Q2S&&%90nO3#Xilj$h+
z?MKhn_=7E7MNQDq&%AfVk2F;^5fax=jo%%i$(O2r85#deRQfJ@8kbT}LP%R@Yc%A@
zASt1c@<GSfcZv@tk!}m;7$O1Y&%87{`kn_*{cMKos^Xg%pY34_fVIcZp8s?eKLvi~
zrRz^)Gx0#0_V{@!zQ<NE_W08gx7GIJlPNSPhS`43z)#2TB)wZrn_iC%l>ug|^r=(u
zb*Vi*o2QLM5i#*vadQ;eGG;G7s)wz`_p0+G9*|^0|MvKWQ^!xhF>&HtixgZxdbUN+
ze*ON@vn}%SL&-Z_;?ae5JPTj`w!hB3A_5w^+h6ag#yebh3f?4p{GB|Nd2vPA{=C?L
z>kMZ_0_{})p&f!P&zMKcYo0UV>FMBJk&U<kccc@}junGq=hj+t`-*U>BAmM&JaVE2
z&aEwJA8>eJq$t8&Z8;r|H4a#1$cCpkx?)&7$=9M0xK|`e^0DyA!xp8fpoOJtnaWwt
zBEK73lp-nyMKhsch_Ngl5ppYY&Vkq2k{!3;XCV1WWU$?+o#mvPQwet5V23&N;#IZp
zh`U;lMjnx8auZuIEQ#DqLnQZ#u7bG8Y+i?D<lDU_0tK`VWZcmegF2CZdjwt9MbOWQ
zrALgr>~O^F_JIiGw=P0vH-ei6iv0PA)P<2AL923C_d+aBrJK3UkMV#PC+tiFz6iKt
zUj{!Q6KOr<MT5}=u^h%49Rb%hTRhd^0((q89_aCV`+F*jukuvGkQrX}<NMf-t~eb5
zzI1oi?lrC)*!u$`o}qx#G#nUow&kfDmK+$A<P8iu2V21gC4+ZtOOh4fS(f}6>Mq&}
zc>M0XWgqRsE6k1&+80IdMJ`5L9o?ZAcuvv&cuqH-UC3AL9kBuTXb)oDpncG&yvRga
z$y4c`E7y9`)`9mtt79ndl|yEOWg`KXk$}rcz~#FC$SJfMYyNlmFWUpV_MXuz@4$0;
zUcRfm*n5M=Z((ayHr@bNd+^=d(84aI0oS!#P>h*KG>(PqgAGsL=WultjRoh7_zu9;
znJypBDe)eM>mr8qzK7pQxY9(Wj7>x@f~&iy`fb(`5`e3}ry4fyL<Pa+_Ee*qXcaRb
zt{jv{v^9sH)_YcZu&2jk`GRX)<FI!EEO|zG{KdmPE72}CV7CZeqXVv@mk}w(nllQK
z0<N1;ke-RbVQ|PjK4p7iR&v_yUtgX#y1e*?x?p16o=)W+AHv;Ko_FJlxmbz|FO5<J
zca$}jUL}fd-YT}w<*%WAqHX5HRNU0Xzp*?&wyrU;{I0mVuR4{x&p_qHm*;l~W()hM
zw)p>rR=ey;MrVgWPx04TuYcw)cY{PHu;VVzk8_vjoRL$R6A!dQa4ps^s>*_;s_y1R
zfPv*NkO1TiI;GqHcDd(FH=>qLOsETX$}aa<xs{%d<v68a9O8=lZ04V<=GBkN^z)-C
zh9z3BzlHSLA*~qyTEs_A;Esj6J>)`Mun~6jtD56yA-=nMfy{sY*^oGHe_grz9KpcY
zWMm?{Jm*{pp3dd&PQfx^gx}>b_`e7QmJ0^_9u!}^e^a?PmfX)2#pglsdE(2x9fIc?
zw*8yae-O1%{fx*y*uN>3rQc4JevByn_9*>0R7miTH1e-)&xX>UF~MD3BboPhsJM`c
zpD7YgU~CUMJ4()*7`zg(v1ewP4`ERiak{opEV04f;7qosW#a`W?kI@(a(6qV9TRjS
zYV|CEBZ&l54u~cMgGUU0s&nCwLQ00@V;Q0VgQG-}bJ*O_!bXq)hLRYJ0_C1qkcbPm
zlUYLlbT6N*{MiIh-ApDpJ><VkL`Mq)W2jDdJ5*&%@Pd%Pdqs(ie=V9|7vzbh7W_@f
zALE}G!HWEe0av86A{hs*%pEL5b37tq5ajsfjdBawkH<(g#_i;ok|Ba*MHl&pMCwru
zU49dzL|2TAC$YOj;|Pe!QEg2mhCVBh741~xSTc#p@iMDDh_%xq(Y>5Qj%WE(gRC^)
zQ;FFgzsHn_EI02@m5CK26Uoml%-k}`d7cjxT%+aanuRf(*-PT+#P{3oYQ++ZV6-ZN
zH8ewVh|!S2P9*qywsRR0`9no|mO&oUuF`R(9b|EEjYOQ`v1;(Ul$~pNS6O`2B!+a*
zSzhrt^kC1*$6?Pt&G&f9n$opsCdmGHkAG<`9zE^e$G_FFv<D=Zj#T8&I|(SufZ!Y+
z?i?5HoEGMEJ)mi!JRdQhpE90bF`ic$&l`;A9UgysZnTu`yKGPC{YRxhtz(fcUm%;K
zC#oXBzpmWVu5J&+Sq#Kcc^-OnUL3kW@S(QksS<}n&=<P+*Om`Clk(se3d8a&^ysXP
z<wFvKugXE7Mf#MdZYfdFJki}ck%3sjK%8Iz5-AUTutV^Q=5Zu69#EdTmp>!f9|D1z
zL=J>Rx<&Ax1^%<*%7?@THyHdk=idziD{2_upEnxvn5p|j>NkqiZ$jz?<(@IY+eCE5
ze<=Nnnf?lu{_u7c!=jmfdr<-qt0;juq~D=@$Qi+vVl*g4Gr`~=Dt|wm?qx5?47ez6
z+RXnAAfU!kmHvz=%OvYrkrl&YnR2v9xt&Nk1}VpancyKI$|<*&O_m8|b)ekIf@sW2
z2zElwB;97Y76Um^o(H*<7Z=>wEN^Ftf<kJLw^`AM-7feze3whyGa9|QJP)IJ-i^Vl
zM0RC)xvR&^bVs+Nv=wy{8~jU%?h4@z%^ICor<!ZVmjTdKAyAXbvmoiSVuBH&@^G)n
zmGtwXDw50d;GGu}+y-%~_Q*k=N}08W7N8=~vptX#v6K`2h2ZIh=!ir?=!yv!Jn<Af
zTPQB8qeZAJDR^|}TH{I#e1UDTt|TY4LMf^zGx)MpN%UM2Jx5AvN-{A=I71N3kOZR<
zVsxY?=oaaFmhZLlu@&`;m)De*vyE8!UwHiGrFb-yPed$TAfRRqG*G$_2$*SG5!Fc{
zOrG&vU_2Kq!Q2t$vpin#S@kgJ7WvB`!VQB3w!+H6Ld-V)EIOjfP<!w-q}>WRp+bpo
zLgzw(cVU6M7_d4?mJJKs8zYA?G5&Dsm4(TO5ceM=p_PgxWcaku#4AwB-CF6McE2Sn
zu49nHPeHQY8YVkS?+J6dDvaDcVLU%?JXagf?-<WpjOVYEC`ErV`tf>{(~8%SD`XYD
zIJV7RycoT$S)W>g($IYv(S-C8+UzYt_+>%(WyO~d=@5(%nQ1A%JG1>tg-)s~UD`}p
zoablLN4cwGd5vIu2&7l1Hl^3E+K}HoBHg`W0V!nrtJpN85G`k8p&Whg5k5c?R{aJK
zp2{lLS$R1tF!;KVsnAbD@{~sxxg|qrLXc`gp0+C0-W39(6m-mbxF1d_kNtV)$kFUv
zA<m)QL$-8AFF(KBb6#+=6dvL7d`6Lb`5c)QjwSJJjwPocx}cvbxi<t_9HbJYWma6!
zi##ot^2~D%rO~-avQzN56n07;uE>_DP@ctWdG=o+dFU1jDhl*5QJ@g2qVJ*bSeU{4
zM9$Iv+$&}&I+QC~BEQ;9XT=EVPs!RvOV-L}xw%Zm&w^aWSjDk2HkcjqUtYrK)PhIG
z>e!z{thz@H?0!m`|GNG<o(j?bkhSnJu;xGJ_J8m8f9&5{c|%mTe^0jm`?-6qoUL`o
zBK_MM-$EOnySMV*_F0ulzqE2rz}G(_HQw!iv+h)se^zR2<I@O%ljLG?9$`Fu-4Sm_
z9Bo_(P>}L(tK0K4Vtdvf&5qd9cn3TxN8IL^yTR?Rt;~wDa#qzHj&w)7b#-MnY}rSw
z$?ku^dWcrfXvr+wf7r12Zx}Nxs_OsAebJbC3!7%yl{2E;*XQ_8xJS7CA6@T%?>hg1
z?8<xEGn;e2&$e>Dch{Ya1apl65T3imT^Zjk+rI-I$8!DqaR6ex8@b8$pKN><{y0((
z8QB3RJhLN?G(HT_zezyf*9CtDG-A{GBN4&IneeELzCrrTt*!J#!OzNBTX!TfA{eo^
z(F=c1;a*SShdC7k9Yr2%%6t5S!Z*2&Mz$f#n{xaETb^uycH*h9{Ob;FF56+19gVdX
z{so<EC6&Q|I~G0C9p~|PFFP7nl3KPWuIy<0J3D$Rxhm=K6ed0FiC8z_)D92U#UOi-
zPOO%MXpBLIS&46X@Bzdv#evdBxK~3VZou6bq`=uvS{~7qe`r){4pUl+R19ZTxN}Lk
zb7{D<Hr!bcr^kOTnt<$|0hBM61OtvL-hy!`dLcJgqK#mkE0P5fRpc)|5!^?ySjEzD
z5sVIQ1t8qnbhit*PC0^a0~7JPzr6-5|129mG3Zt&gD(jmNB0e+P|6sXXyu&{v3?``
zO2Ha<TN^ACPRZYOxfSjG2i*R3*6V99adwxxBK=L!{M_Zm5i7YM4J0b3($y|4;0`#J
zm+i9L<=*JBld-eUTh4lS2V7_3uoYKAj^ER+>||U?x3Yb4WhYVdxs_sNGpDfEQk()g
z;M6X&=DX?g(X&bn?P*w@qW<EI&aQ~Wda5<&K8{F`=<fRu%PGb}kd!JqCCPbitQmM?
z<$8mg>Z-RxcDd_}+)7uxTFS)Xz|Mo1@?$cWCA1~J+pDG`FtpOcrA(>}Tp-m;su2}g
zVu`maXRK5$nwM0UW98})R<3q)!Pw)EUCwwY``GM?ta$P}_YLwZbURGsIqUPbvZR_N
zmTUzdRECCfany~)QMaJyUPG2YFB%BF5@j3XtjH1y%tz>G%ex4Eu=eTpd)m9pjz+h#
z<h))kIfJK8f~N|qhs%2*CW?hhl1FN#NC=Cu$3@wwgv7G#`3Nuc%^a?eqvYD_p5d<k
z{$&UbOn!x}Tj!drelYBb^-|bm?-<fKk;3Q<d|+tw)2tNm+79cyC>~90P1a5P5)7ln
z00bVOVLT5*(@oWG40z+*E3RCNbkjZo>AV6%8dMIO?eXJd#(!KU=;hOeC{?+9I$kZG
z7Gn8yoLD|(?_$wW_YM_Z@R=5R#%58#H3-E@i$OcXI-2$RyXZJv3FLB`yWA5uu-tP7
z+8mYwT^)kupd#rv>nmK3uYQtb&??(kj7~z2L|?-iqU#L*M)Xb5E4_jMI^3DTV<2Lz
z=Y7k@w@j!{1GylXm58BC@F7+t**~xx2tM54!!4I2oB0UqX7fQxXfJ5S9nr!&RE);Z
zioSMc`OE|o0ypSr7T4F#7X@PKU5IR~OX4$H<|ALOONR21-@GpQbeR5Rtm}?1FP?zX
z6YIJ~ST~zko_Dia*KMvQJugG=be<eqIKC7u3nw2t(Xui<)p%dPQ?z{^|BwNm<*{g5
z(O8y-YmjF-maAze8C`kwFd!oXS>slrp|;187V;FxD)fNH_~7lVtB|~Et}o8l<!Xbd
zudFWqRkGIOxc<9`m41f1Jg)=#(V3{j_~2{EXH{4^k71{NjaANuGrE(MXF`&RHB>Fk
zxQIF!u`y=Y#*@E^nJL<7%kktXt^u0k$-@Bb<H@}`ev9#>Q202yZ!yOcuN+SX3%^o~
z4rp;gLFHr@QX34%qOZgSLv*D##Yja~7Ze4G1m1rR>6OO%W)_=-T;Hrh0;>NlqkhYE
zQS^RS2Z~xGa%S)u!Ga6oHW&VA&~fN}`J&ZxVfQ8B&HlSgD4vqPEVf$7;9(VCtP2Y<
z2SH2T(6!D8E@@6*EC8e9l0Fud(7Qn*xCU#$LO$@@OWLo^fr=?rJ?eQL|A&=HeLVMn
zy$nS(9_Kw>id9aGy>n8Z!@YZLW%OThcSL*wr8Q|#Wm39-4Gwlntc>r2TXt4u^mp{m
zPHTcckC5$zpATy7_-y|nKoh%f08rxp96Dkb{BpAartohBT$A}QYfW~lOAtZGJWx%=
z&kL}CAfAh};W4vu)<NisvmBA0!c(5Q-BHxLz-M-9EdTm>I11m0y=2{SRQjyce*VqS
zR&)Hdjpuv^iH!54`W0FGYv=A&M>#VL#sVmUV(9lki{_c%{c`+sH-c_X#oV=ue0t-n
zg2IT@bRLc6uRpZ0tii%tGi%|ZM6FV0#X52;?}kd*qwH{8$)#nDab<^vN_mdpvM~Aq
zPelEIQw=$5+Ebm#DO(d;wSs$6v(t`h1=XdgxdVe^p{NXw%klT6hWhiLq@h+d4@65J
zL2vb}fJx3?^C3q(mCyZ#K)Xlrj}?Q-P^xrbIA?`B7lb>X40pZ|?yL!Sz8miREZq5Z
zxbt|pGamY;!Bv-VXV-9NMz}LK-03r%9)I4~D}+{wj2pOrJzv4(9c2L+aJlEW{~L_c
z3p6W-)|VF_E8A{Cv;%%p!{vD=pon&Ky`X_Lu7lBdRSKv^Ljl*3xp2SR>^^M4{aL_u
za<!O}Jf)jhU{FL6+bB*iEskI>N)I?JJMc3*FvkvH$`elGp8?8_#Fb3&R7_4yN3-)(
z4vgS9Gk+ZBMlB~u=rQSmfgx7&z&pB0z@b?U0WOROin$O50EsXKc3{%E-X7#SJK(kh
z!|gze5I;_1#E;Tw!B~q_M*b*`QSOSlTrJ>P#p&7VaQJM0gQs#x$3W!r@^O)z7Gz-(
z-5_3Qj*H;Z#<HHZnD*pWJOtgf-7;<sN?W@o>rBchDTQWtmXq47&ZwUu=Xw0oI#L8(
z)}{#Ztcjoq!j1yWYN<DNuk}C_r}NY6z;if1eHOI4oU*?yK@t%-oiEkZdBdR%WjkUT
zZ`jI%)#uV9x6+qdK#@E5=a6E#{?FZIZ$>vh7?h$mJ*T3Qp`a2+c({Qh2m9Gn(`Zq0
zx}AsGh0|V?o$Wxb9jLGag9Q+9q_DL6N#BYdP|VQQkXO&#@2xq$XyzmJ{DIxy9@u@$
z!0s3=HV2=eh{U8+aPL+vGh#U@fDFd~o;*Ax!c*56DM*z^N|+c)L$NA9BOLPAQO$z1
zvEhs=$daQ9^n|-WqN)3aoI~H{RQ$FIEbuVITF+x3x9V_CVV$RLN0g_3eaVg3AA}(`
zJv(h}AR@<u<(BlyrR4eklbn2dDsKlN2oY<3IW*irM41Q@f&&q?0w|AA>4TI=|C$I-
z*%~l~10ee!EWuLDVUdi7$>7{YBvaOi*9?j}qm%QH`=W5SKfrA7OLUcY5BE^%R(NAQ
z6|vdsu=w2m>#+pof5%f%jM>0?G@V2l3lar`JL>BN<YnO8E!G?^3V1O2Wr5W-L_+r{
z@m5wb|AJlll8+mHM16%SX8r-xZ6Feb!D`fSAaa!eLX{C-7&-mFwaSW+U{2v`PyZ%s
z&S*TdTE3Tc!)izifW3uRq1%+5MF05=?J5vaCmS7mfRu^b1>h=2QHvy4^Od^9(}R&>
zG!*kVbfp|wlh(YygP7=qE0BBSuO!vk_~9nzpHFYYB7*9XLjNg4n1Pk4bCx5PBe;Tb
z<%kgCLJu<`phfp<A-l8fG+Gb^hp(lUBFXL>8v6+5#4wZ>5o~{)%I6$-u}tm{ZwI;^
zHn*UN@Brl(g}Z;OMaj^zCk1<p*<LoQ9O<@J<)JoU@5AL6he!AaI&knE&cTFS#Tp~(
zo+Dd?+g^lY>_CeUH~nac8-Fx}??*%2&_)O@M+@mK+wN$4uom$oGXckHrItfnJSk=n
z<2^mQh$)3NcLhg*``=uKwqUMF>-JH0@{~2_Khb7&=Q;Gb!9p-tBifS7ftHUIn<<z^
zz>L?F_eJ-D8}kPI`%*h$ZF<3)IjNMNy!@}F^0f<Mbt$sOt;26MAdbX<lDLp;rOin#
zg&Q!xQ^49`cy`Mnr&NO$iIqFU-TrlW%fi<!kXT{i5D)E6j@tEl(y#<cgj!x85*;$U
zZK6Yl1L%<90QA0a03Gtb4?t_M=JZ4vOU=QWzl!3c(l&wNygp)mcs>J;?F2ABHV<UM
z9&+mu`kxeEvCOs*Ur8Tp@pZ;$;o|E}%(aF1qMn1Zf>R5!{ks~Y_nbNv{1eUr*J>h)
zY%(Qyv*K74LV`;%Sqng$#%9m%wSfq%W!FUr)kciY)Nh`|j5&Bd=(z*!i!c+5z`ct1
z;4R-GZulzre*$u^2wMTi(%y;SL%^|sjcu1)*sjCE_O&O2uY3>wyKixGl2^(}UVF^M
z$DF`?4(s7(l2Q#R!5vB8DEQudB6#o{crc2KV07>cxCeHxH5N0lBp_EaI+F&2W;19z
zN%%E{@4go>c>x@QHG-)pgZ}|om)Jie<b%1GZ)*ZAhmEgZkn1>Zz89#AD)QfcBKQi*
z-gql;D>$Ijcq!l&c{5)KV3qe|(EWqF23H+u=H$n-QfhCVEyt^10;YT@VraspWFPMz
z+$REV)Y5)rny`y-bg)4=nT5J`BE)-un5WkIL9k^%xD0JVjt6Ynx&F<|*|>B4zEme0
z_b+y1$*2O3;2TD9|9B?#+%GhaJA&<yC#Y<}_50DQ{GSHz#@${NE$Xo<=sMgo*pH|0
zvQu^h53!4e3Bcw_Y=}(=GgCwc96zPD?)j%EJAxDbsd%jvEnh>5fvtOO@Dd0%)ZqUY
zSE<USxNF-_w);=Ejb+lpv$koKY0wMNB8DL1r~OrY+S(G~xS<>C;#@c1jJ?<mYV)|>
zE{?@>u~A%a5$EzcemrbaYzu!ImK(9>8z;lz*wp-uqy)o!Tkv-8GeA6~@@*Of+zC1W
zeQ)CqLwm6E=le>ghD5PZ31Q#oT7Ikr+ocG19BCZ1l6PBy5BA}VMA(hQIf=<I(-+vg
z4}ah9!(PfRkK=E=*j72~@f2*R%zZo^NYYdI%YBNwEhj&fl7)@u;{6p;<L+~qJ*44t
zFz17b$?w2tXJiFuhvO>+9!7Txtav|x@8|gO$oCHfTL`y-d+Kq22juGa;cwGEq?v?#
z13Za*?FHXof>V~oTe!nQ?x<sy6KVLsv-f*IZ^H{mdnV)fH0O4ZIEAut=BM*51qunJ
zMH7qpIF-=58_##SWQcqiTg=CCZ7|vhrnX_zJr9nA5kwwcjIuz~O>nG7dRt&$5CON~
z3`5^$kb!wdFUsEc4vuPPnN5C*MO4CaNlRk+<${w>5TpJJkX(noXt*6@PDvevks0qH
zq`DiK1jkO6{tnoR9L4kgxO?CE8GOKk7sc#D+O(I5v;gc7C5LMJ3gy)GaRi-|4%b11
zM-y_6bs?*JQK8^9|7!#(c{~vb!S_qNrUCEBD9?_hf!Ux1D~h8Ivz%C2vfN++a#W+N
ztTm#(`1OHv1Ab^}s3QDr0;kSz!RcO>+(C<nrL#D;g8Y}@1(`_RgrrA^_Ty`AKiHZ~
zj#y64ESKe^;m0e$S_<fWfD*!E9uB&0vhugHT7(%?o>33urA<8U&$&+|_celNrC6g5
zpqf}#Y#QT=!OD9uq^lZQz)HIU<|VU08a5{JH@pBLi<#2sJfy=KnV60;;=zj8gpA6?
zAI)s$Z^aL-&j%mxcL;wN&j=2SmNYau1a>8d!?>s9(76YB{y?0s>D$eNWnn%QX`x1(
z`(%C7;wD>^m*zE*LmyhY7daS(??$|v;Qj{BI0ad}bklFv^!qmbZY^jfqwk6a|2|Ib
zW%<L%E}Kp=PCQNi5{;kdx^80!dJo5Ru4gCe#g;fr^yKlPCbC6MbYV{}5jBx3TI>jX
zN7J*`Q?WUxqA9mB{=uP@N&cagU2spW7+TqN!O+T*O2^R3jD<)w0jc(S0J#I_&Wti3
zgV4P4(XI1g-}8GmIvmc(?#`~>+@lb1Ha-~hu179lvbJj(qO)VB&f%v?0Op|-wjk6D
z)QN8c$e^gadr%XoP$#UgI^R;&gGL7w#!~Ys0#I{aND1exx!iA#@XqM@h`=*lhvj+7
z0tH}VKf)m`cK-Z&JiiIbUz16+AP{BuckV!nI4?FICP{t!$tP&0-Va}hHdxISIrF}Q
z$dIuxTJ1yU=8>ti2m1}OgoJ74brgiU7ZzK6q`Ma<hA^LLsrTbA=>bH>D^v={w0OeN
za|WUw$ah-my#FoRgQ*y(x~ReGC_m)QPDm_^j5+|TeX+2_*DeD0Ut`r2uX}`jK?cnc
zZ=odSC4ehWVNf+4lLdl}M!XB#fdkvJWtg3i7x_#=KNu5qCpcm35JnHBML5!VP0`uH
z`;d@L#6h%)I55Gvpq;Qmcr|Sh4gsC#S~D6*zK<(9w5P?3c}`T%S5m=si#(qbg}e^w
z5cMBA&w23br^5aEY&c-QPT6G~7~;WAJBWgZp$Sb9QIq0ZHEK9SnREe>)-NXWKWV#;
z>6^CeL|p@8-3y%MfJ@iy<#;BZ$Gu9pbp6uA8u2CkcomnZW88<~N4%H@GH{7H!+jin
zf)2|Mm#zaow`Pm(0pJXuBBt^Vho~Q5+?i_Hx?|n&`V)S{dF(nbjq|xR8*&nl<E_Qj
zn(aCUigL89y;ntSRcHn!t{3sEXtq1^`kQdHcchpgo>wV6-G;AGc!LdpPvI^bzCqz;
z{t11m@B(pf{`*Yf#m$g-+OF_Y8@@~7RW|%9g)gz;-za>k4L_vtS{r^s;q^8=O4Mt6
zN0SYYQ#dL=?6MTD`?vNyN8u^9@aHRBw;vszAK|ecUAFMQP<WmVzg*!3HoT|8i*0y+
zg_qj!Oodn3@PP_nV#9x}@TE4Ko4(0^tqmWg@Om45qrw|(c%j0ZZ1^OFqqBxxQx%?Q
z!|zadiVeR<;o4@1CU(EVUAFM^6`p6qD-~W~!?~A$<xp(H`SBU?QXBpUg;&||KPh~P
z4S!zYOKtee3a_={Zz#OphW}0B4K{qW!kcXP+X{zp3cEf~c%lveSm7x)e2c=<ZTNPD
zyKMMwh3DDuy$Ua|;e3rH%HM_`QFy5hKc(<08{SUjzrACL4ey}vr8b-&#fbKA!_QTC
zy$!!W;SDzY5`{O}@HB;E&<?wPrSL=>-do`*HvCG3r`vF>R0{rGHk_ZBGGBQ%e5k?;
zZ1{BwFSg;*rXkwD4IiVzSK06h3SVNwZ&mnG8-BaOYi;<Q3a_`}_bR->hL<V4$%gwC
z9x{H)V!`#0!V{G~Dp7~yw+c_O;ZG<$-G={B;Vv8gp9<Idkf#3^g%{YuzpC(J8@^oO
zr8fNU3a_%^Zz+6<4Sz@BOKmtmb`tI1hJUK?dK<n~;SDx?r^1_Tcu?VX<3IZqp2)d`
zaeb%o6dTSUQ6bAJ%@K|vJZ<+DsL6@Yzu|p0`e9;ou+URl8+dvfc<(lFN64s(1Ey6b
zOdDG=Va((SV~fU2m|zE{+JLDOr2W{EX%i-lDV%!8loIKlFnR2_se%xFnj=l0boYc_
zW5$i0j?nn)Hfj2_vE9;}{YA1<r#hxHfN4iJ9Fdl5wGG=IboS(5_g?%Lf4%txgHgFx
z_vK$d{`Kcy_jK_utlxGQQM=2yA_i?tA&iLAT}0|GV)YQQdWcv(M64bnRu7R_4-vPA
zh}%QN?IGg!5OI5oxIIPOo|0J+x2K5PQ^f5l;`S7Ady2R{MciH@ZZ8qHmx$X-#O)<>
zD&qDMaeIlly+qtzB5rRHx3`GfTg2@x;`WvWB;xiKaeIrny+z#KB5ofMw~vV1N5t(T
z;`R}7`-r%GMBF|iZXXf1kBHk>#O*8M_7!painx76+`b}iUlF&jh}&1h?JMH;6LI^A
zxcx-jej;u^5x1X++fT&pC*t-Kar=q5{YBjVB5r>Xx4($nU&QS%;`SGD`-`~!`^m-{
zYV<{SOesXO8$ZD@W^nE`S?=61*9;nT-M|rJM!2(b2aa*jNb=M|M-iL!m@%;BTX^f3
z!il$zDH=P8^Z%z+&t$jf)DN#}#r1+1-#N!~IFbx}tATUsr^8FVLWZY#OpVjd059HX
z9#i9~2HwfQvkbhmfsZus^9+2PfuC>SQw*G<NvF%bOT2iy06&eJw&yN1aDLLli#NPw
z4TWEA;3)?Ffq`FS;7_P2D7`N>@D&DriGjak;Hd`wsexZ=;5!Zc=LWuzugGyR-Cy9R
z^Sj!>od*7ifu|Yxmj-^Bfgd#RUmAFnnz}RHt_E%yIM*&TKj#}b&75ld7Y6<-1Mg|z
z=?0!@;N1=U*9P9hz@Jm=CgiiHfgdsOUIu<Cwyg4E_}=(wKKmPZ9|O-d@V*A_HSm50
zKHk7-cTUqUHt;J9e3pS<Y2aNF;J`&bGw{=NG7bDH10QDKTszX?M;ka^oQ3cr19utt
zGy``V_-q5uGVlit{AvSVWZ>BbUPd8>i}~VekLGiUfe$qBbp}4jz&|nY!3Ms=z&!@O
z&%kpG{HTEsG4L1)O<d&j*Z65Z&ob~_1HaI~hZ^|h20qNdGYtG11OK&w=Nb5D1Hab5
zCmHx~1HV_T>*;b<>YZaSg*Yza*Wsu0b(?{YF!0$1e!YP|YTzRc{CNW(W#H8Y&b4fv
z?nVQr2|SJOG4Rm_{=I?UVBnD)gm5umKKwMD1Ova(z|%Q+;No=WF8p-(Neb8H_89}=
znrXwe?cGmoIQuHEOF0PP(salZuWSRq32vS4?FK%^z)KChz`!3e@UaHI*ucjb_+JdX
z(7<a9e7u3bW8f1Ee6xWU8F<jZZ#MA520qci+jB6+#qybipDu@F1HZ+<FE#L64ZN3u
zPd4zY4g59(A8z1N4Ez=YpK9Ru8+fsSKVsmw8~C3Me42r;F!1RH{+@xC82DBLzr(<v
zNr3~GZdbFU(~*C%e9-ul3SZ-t4{Yam<EP8{cT@s!>F_nu=~%)+8W+RggP#shj2FZ6
z_oR}W498336-pi{wIYO<D_jlBA^aJI>$#?;Q>$=w3x)qk0jgVv@Fs<Kk?e%<cqIo@
zZ1@s|r`qr>R8DcJVhhpPsPL}RJA@xpICU6an*T&9v$%T6TL@28ct0ENQh0_9_bJ?E
z!<&@c$+qG5Qi;Rmk+)E~cPX4>7B9``euZ<a;id6-mC+Fr4&hfS+-t)}DBNemcPRP=
zHr%D;!gw40unIrXhA&k3WE=h`g|lz+()@p-_$iTa2oEZJrVV#oE`eD#+^NEs+VG<(
z5}0Gd`zn6QZFri(7ufI)D%~m@zA8lmkJ|8?6+e&J@D~*Rqz%79(O+W2XI?CUXKeWO
ziqGe4_;7{4V8d%vx=U^N?^U|5+3*(>zRZTdt?1X-@C7P-tqrf{Bnj7>@)oM!N!=u{
z)`t5O{;mzbOv$@?8(yKpe`Le&R_SiE;kPOLGaKHZ=%m<i<!sSUvE(c+pDnz8KWuO3
z`u(uIJoWowhcVx0-^f#PJIRLY_rMu8T(1lM6SmZ`9@lN!)bV3W#yZr!TjBJQ|6_yd
zDJTPGF$O>Law*62sy%4GME#)d6vxr{xs+Sq3Ow>@;u-ROCT>h8Ryu#wmikw=bX7E6
zecIWkE%l!opL1LC#kOExO)C9TRsY)0o`25CTg(4iPCjtC+Q~cf&qsUy2LNLdXW?gF
z#UzX?-BvL=eSObG7{*U$VB=cSS2~n93i;^l=sV{y`Z*K`5AUk@CdOxbSn9&;qW1Eu
zSMgKeXI?u08oL({q-l@eq~e#`612ykiMXwmf2>!gP%CfZe3lM)*6}+jytTCHHIs`K
vxRx*yt_1w<#Lpg|%P>*+*{@CmB{In7Y0HkIBZAq2h=^+y|MP8x?eYH~+;X(2

diff --git a/src/include/cpu/rpp_loongarch_common.hpp b/src/include/cpu/rpp_loongarch_common.hpp
new file mode 100644
index 00000000..d2a551f7
--- /dev/null
+++ b/src/include/cpu/rpp_loongarch_common.hpp
@@ -0,0 +1,6736 @@
+/*
+MIT License
+
+Copyright (c) 2019 - 2024 Advanced Micro Devices, Inc.
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
+*/
+
+#ifndef RPP_CPU_COMMON_H
+#define RPP_CPU_COMMON_H
+
+#include <math.h>
+#include <algorithm>
+#include <typeinfo>
+#include <cstring>
+#include <rppdefs.h>
+#include <omp.h>
+#include "rpp_loongarch_simd.hpp"
+
+#define PI                              3.14159265
+#define PI_OVER_180                     0.0174532925
+#define ONE_OVER_255                    0.00392156862745f
+#define ONE_OVER_256                    0.00390625f
+#define RPP_128_OVER_255                0.50196078431f
+#define RAD(deg)                        (deg * PI / 180)
+#define RPPABS(a)                       ((a < 0) ? (-a) : (a))
+#define RPPMIN2(a,b)                    ((a < b) ? a : b)
+#define RPPMIN3(a,b,c)                  ((a < b) && (a < c) ?  a : ((b < c) ? b : c))
+#define RPPMAX2(a,b)                    ((a > b) ? a : b)
+#define RPPMAX3(a,b,c)                  ((a > b) && (a > c) ?  a : ((b > c) ? b : c))
+#define RPPINRANGE(a, x, y)             ((a >= x) && (a <= y) ? 1 : 0)
+#define RPPPRANGECHECK(value, a, b)     (value < (Rpp32f) a) ? ((Rpp32f) a) : ((value < (Rpp32f) b) ? value : ((Rpp32f) b))
+#define RPPFLOOR(a)                     ((int) a)
+#define RPPCEIL(a)                      ((int) (a + 1.0))
+#define RPPISEVEN(a)                    ((a % 2 == 0) ? 1 : 0)
+#define RPPPIXELCHECK(pixel)            (pixel < (Rpp32f) 0) ? ((Rpp32f) 0) : ((pixel < (Rpp32f) 255) ? pixel : ((Rpp32f) 255))
+#define RPPPIXELCHECKF32(pixel)         (pixel < (Rpp32f) 0) ? ((Rpp32f) 0) : ((pixel < (Rpp32f) 1) ? pixel : ((Rpp32f) 1))
+#define RPPPIXELCHECKI8(pixel)          (pixel < (Rpp32f) -128) ? ((Rpp32f) -128) : ((pixel < (Rpp32f) 127) ? pixel : ((Rpp32f) 127))
+#define RPPISGREATER(pixel, value)      ((pixel > value) ? 1 : 0)
+#define RPPISLESSER(pixel, value)       ((pixel < value) ? 1 : 0)
+#define XORWOW_COUNTER_INC              0x587C5     // Hex 0x587C5 = Dec 362437U - xorwow counter increment
+#define XORWOW_EXPONENT_MASK            0x3F800000  // Hex 0x3F800000 = Bin 0b111111100000000000000000000000 - 23 bits of mantissa set to 0, 01111111 for the exponent, 0 for the sign bit
+#define RGB_TO_GREY_WEIGHT_RED          0.299f
+#define RGB_TO_GREY_WEIGHT_GREEN        0.587f
+#define RGB_TO_GREY_WEIGHT_BLUE         0.114f
+#define INTERP_BILINEAR_KERNEL_SIZE     2           // Kernel size needed for Bilinear Interpolation
+#define INTERP_BILINEAR_KERNEL_RADIUS   1.0f        // Kernel radius needed for Bilinear Interpolation
+#define INTERP_BILINEAR_NUM_COEFFS      4           // Number of coefficents needed for Bilinear Interpolation
+#define NEWTON_METHOD_INITIAL_GUESS     0x5f3759df          // Initial guess for Newton Raphson Inverse Square Root
+#define RPP_2POW32                      0x100000000         // (2^32)
+#define RPP_2POW32_INV                  2.3283064e-10f      // (1 / 2^32)
+#define RPP_2POW32_INV_DIV_2            1.164153218e-10f    // RPP_2POW32_INV / 2
+#define RPP_2POW32_INV_MUL_2PI          1.46291812e-09f     // (1 / 2^32) * 2PI
+#define RPP_2POW32_INV_MUL_2PI_DIV_2    7.3145906e-10f      // RPP_2POW32_INV_MUL_2PI / 2
+#define RPP_255_OVER_1PT57              162.3380757272f     // (255 / 1.570796) - multiplier used in phase computation
+#define ONE_OVER_1PT57                  0.6366199048f       // (1 / 1.570796) i.e. 2/pi - multiplier used in phase computation
+
+const __m128 xmm_p2Pow32 = ((__m128)((v4f32){RPP_2POW32, RPP_2POW32, RPP_2POW32, RPP_2POW32}));
+const __m128 xmm_p2Pow32Inv = lsx_set1_f32(RPP_2POW32_INV);
+const __m128 xmm_p2Pow32InvDiv2 = lsx_set1_f32(RPP_2POW32_INV_DIV_2);
+const __m128 xmm_p2Pow32InvMul2Pi = lsx_set1_f32(RPP_2POW32_INV_MUL_2PI);
+const __m128 xmm_p2Pow32InvMul2PiDiv2 = lsx_set1_f32(RPP_2POW32_INV_MUL_2PI_DIV_2);
+const __m128i xmm_newtonMethodInitialGuess = __lsx_vreplgr2vr_w(NEWTON_METHOD_INITIAL_GUESS);
+
+const __m256 avx_p2Pow32 = lasx_set1_f32(RPP_2POW32);
+const __m256 avx_p2Pow32Inv = lasx_set1_f32(RPP_2POW32_INV);
+const __m256 avx_p2Pow32InvDiv2 = lasx_set1_f32(RPP_2POW32_INV_DIV_2);
+const __m256 avx_p2Pow32InvMul2Pi = lasx_set1_f32(RPP_2POW32_INV_MUL_2PI);
+const __m256 avx_p2Pow32InvMul2PiDiv2 = lasx_set1_f32(RPP_2POW32_INV_MUL_2PI_DIV_2);
+const __m256i avx_newtonMethodInitialGuess = __lasx_xvreplgr2vr_w(NEWTON_METHOD_INITIAL_GUESS);
+
+#if defined(__AVX2__)
+    #define SIMD_FLOAT_VECTOR_LENGTH 8
+#elif defined(__loongarch_asx)
+    #define SIMD_FLOAT_VECTOR_LENGTH 8   // LASX 256-bit
+#else
+    #define SIMD_FLOAT_VECTOR_LENGTH 4
+#endif
+
+/*Constants used for Gaussian interpolation*/
+// Here sigma is considered as 0.5f
+#define GAUSSCONSTANT1                 -2.0f          // 1 / (sigma * sigma * -1 * 2);
+#define GAUSSCONSTANT2                  0.7978845608028654f // 1 / ((2 * PI)*(1/2) * sigma)
+static uint16_t wyhash16_x;
+
+alignas(64) const Rpp32f sch_mat[16] = {0.701f, -0.299f, -0.300f, 0.0f, -0.587f, 0.413f, -0.588f, 0.0f, -0.114f, -0.114f, 0.886f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f};
+alignas(64) const Rpp32f ssh_mat[16] = {0.168f, -0.328f, 1.250f, 0.0f, 0.330f, 0.035f, -1.050f, 0.0f, -0.497f, 0.292f, -0.203f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f};
+alignas(64) const Rpp32u multiseedStreamOffset[8] = {0x15E975, 0x2359A3, 0x42CC61, 0x1925A7, 0x123AA3, 0x21F149, 0x2DDE23, 0x2A93BB};    // Prime numbers for multiseed stream initialization
+
+inline uint32_t hash16(uint32_t input, uint32_t key) {
+  uint32_t hash = input * key;
+  return ((hash >> 16) ^ hash) & 0xFFFF;
+}
+
+inline uint16_t wyhash16() {
+  wyhash16_x += 0xfc15;
+  return hash16(wyhash16_x, 0x2ab);
+}
+
+inline uint16_t rand_range16(const uint16_t s) {
+    uint16_t x = wyhash16();
+    uint32_t m = (uint32_t)x * (uint32_t)s;
+    uint16_t l = (uint16_t)m;
+    if (l < s) {
+        uint16_t t = -s % s;
+        while (l < t) {
+            x = wyhash16();
+            m = (uint32_t)x * (uint32_t)s;
+            l = (uint16_t)m;
+        }
+    }
+    return m >> 16;
+}
+
+static unsigned int g_seed;
+
+inline void fast_srand( int seed )
+{
+    g_seed = seed;
+}
+
+inline int fastrand()
+{
+    g_seed = (214013*g_seed+2531011);
+    return (g_seed>>16)&0x7FFF;
+}
+
+#if !GPU_SUPPORT
+enum class RPPTensorDataType
+{
+    U8 = 0,
+    FP32,
+    FP16,
+    I8,
+};
+
+struct RPPTensorFunctionMetaData
+{
+    RPPTensorDataType _in_type = RPPTensorDataType::U8;
+    RPPTensorDataType _out_type = RPPTensorDataType::U8;
+    RppiChnFormat _in_format = RppiChnFormat::RPPI_CHN_PACKED;
+    RppiChnFormat _out_format = RppiChnFormat::RPPI_CHN_PLANAR;
+    Rpp32u _in_channels = 3;
+
+    RPPTensorFunctionMetaData(RppiChnFormat in_chn_format, RPPTensorDataType in_tensor_type,
+                              RPPTensorDataType out_tensor_type, Rpp32u in_channels,
+                              bool out_format_change) : _in_format(in_chn_format), _in_type(in_tensor_type),
+                                                        _out_type(out_tensor_type), _in_channels(in_channels)
+    {
+        if (out_format_change)
+        {
+            if (_in_format == RPPI_CHN_PLANAR)
+                _out_format = RppiChnFormat::RPPI_CHN_PACKED;
+            else
+                _out_format = RppiChnFormat::RPPI_CHN_PLANAR;
+        }
+        else
+            _out_format = _in_format;
+    }
+};
+#endif // GPU_SUPPORT
+
+// Computes strides for ND Tensor
+inline void compute_strides(Rpp32u *strides, Rpp32u *shape, Rpp32u tensorDim)
+{
+    if (tensorDim > 0)
+    {
+        Rpp32u v = 1;
+        for (Rpp32u i = tensorDim - 1; i > 0; i--)
+        {
+            strides[i] = v;
+            v *= shape[i];
+        }
+        strides[0] = v;
+    }
+}
+
+// Uses fast inverse square root algorithm from Lomont, C., 2003. FAST INVERSE SQUARE ROOT. [online] lomont.org. Available at: <http://www.lomont.org/papers/2003/InvSqrt.pdf>
+inline float rpp_host_math_inverse_sqrt_1(float x)
+{
+    float xHalf = 0.5f * x;
+    int i = *(int*)&x;                              // float bits in int
+    i = NEWTON_METHOD_INITIAL_GUESS - (i >> 1);     // initial guess for Newton's method
+    x = *(float*)&i;                                // new bits to float
+    x = x * (1.5f - xHalf * x * x);                 // One round of Newton's method
+
+    return x;
+}
+
+// SSE implementation of fast inverse square root algorithm from Lomont, C., 2003. FAST INVERSE SQUARE ROOT. [online] lomont.org. Available at: <http://www.lomont.org/papers/2003/InvSqrt.pdf>
+inline __m128 rpp_host_math_inverse_sqrt_4_sse(__m128 p)
+{
+    __m128 pHalfNeg;
+    __m128i pxI;
+    pHalfNeg = __lsx_vfmul_s(_ps_n0p5, p);                                         // float xHalfNeg = -0.5f * x;
+    pxI = *(__m128i *)&p;                                                       // int i = *(int*)&x;
+    pxI = __lsx_vsub_w(xmm_newtonMethodInitialGuess, lsx_srli_i32(pxI, 1));  // i = NEWTON_METHOD_INITIAL_GUESS - (i >> 1);
+    p = *(__m128 *)&pxI;                                                        // x = *(float*)&i;
+    p = __lsx_vfmul_s(p, __lsx_vfmadd_s(p, __lsx_vfmul_s(p, pHalfNeg), _ps_1p5));       // x = x * (1.5f - xHalf * x * x);
+
+    return p;
+}
+
+// AVX2 implementation of fast inverse square root algorithm from Lomont, C., 2003. FAST INVERSE SQUARE ROOT. [online] lomont.org. Available at: <http://www.lomont.org/papers/2003/InvSqrt.pdf>
+inline __m256 rpp_host_math_inverse_sqrt_8_avx(__m256 p)
+{
+    __m256 pHalfNeg;
+    __m256i pxI;
+    pHalfNeg = __lasx_xvfmul_s(_ps_n0p5_avx, p);                                          // float xHalfNeg = -0.5f * x;
+    pxI = *(__m256i *)&p;                                                               // int i = *(int*)&x;
+    pxI = __lasx_xvsub_w(avx_newtonMethodInitialGuess, lasx_srli_i32(pxI, 1));    // i = NEWTON_METHOD_INITIAL_GUESS - (i >> 1);
+    p = *(__m256 *)&pxI;                                                                // x = *(float*)&i;
+    p = __lasx_xvfmul_s(p, __lasx_xvfmadd_s(p, __lasx_xvfmul_s(p, pHalfNeg), _ps_1p5_avx));  // x = x * (1.5f - xHalf * x * x);
+
+    return p;
+}
+
+inline Rpp32f rpp_host_math_exp_lim256approx(Rpp32f x)
+{
+  x = 1.0 + x * ONE_OVER_256;
+  x *= x; x *= x; x *= x; x *= x;
+  x *= x; x *= x; x *= x; x *= x;
+
+  return x;
+}
+
+inline void rpp_host_math_fmadd8(__m256 *p, __m256 *pFmaddParams)
+{
+    p[0] = __lasx_xvfmadd_s(p[0], pFmaddParams[0], pFmaddParams[1]);    // fmadd adjustment
+}
+
+template<Rpp32s STREAM_SIZE>
+inline void rpp_host_rng_xorwow_f32_initialize_multiseed_stream(RpptXorwowState *xorwowInitialState, Rpp32u seed)
+{
+    Rpp32u xorwowSeedStream[STREAM_SIZE];
+
+    // Loop to initialize seed stream of size STREAM_SIZE based on user seed and offset
+    for (int i = 0; i < STREAM_SIZE; i++)
+        xorwowSeedStream[i] = seed + multiseedStreamOffset[i];
+
+    // Loop to initialize STREAM_SIZE xorwow initial states for multi-stream random number generation
+    for (int i = 0; i < STREAM_SIZE; i++)
+    {
+        xorwowInitialState[i].x[0] = 0x75BCD15 + xorwowSeedStream[i];      // state param x[0] offset 123456789U from Marsaglia, G. (2003). Xorshift RNGs. Journal of Statistical Software, 8(14), 16. https://doi.org/10.18637/jss.v008.i14
+        xorwowInitialState[i].x[1] = 0x159A55E5 + xorwowSeedStream[i];     // state param x[1] offset 362436069U from Marsaglia, G. (2003). Xorshift RNGs. Journal of Statistical Software, 8(14), 16. https://doi.org/10.18637/jss.v008.i14
+        xorwowInitialState[i].x[2] = 0x1F123BB5 + xorwowSeedStream[i];     // state param x[2] offset 521288629U from Marsaglia, G. (2003). Xorshift RNGs. Journal of Statistical Software, 8(14), 16. https://doi.org/10.18637/jss.v008.i14
+        xorwowInitialState[i].x[3] = 0x5491333 + xorwowSeedStream[i];      // state param x[3] offset 88675123U from Marsaglia, G. (2003). Xorshift RNGs. Journal of Statistical Software, 8(14), 16. https://doi.org/10.18637/jss.v008.i14
+        xorwowInitialState[i].x[4] = 0x583F19 + xorwowSeedStream[i];       // state param x[4] offset 5783321U from Marsaglia, G. (2003). Xorshift RNGs. Journal of Statistical Software, 8(14), 16. https://doi.org/10.18637/jss.v008.i14
+        xorwowInitialState[i].counter = 0x64F0C9 + xorwowSeedStream[i];    // state param counter offset 6615241U from Marsaglia, G. (2003). Xorshift RNGs. Journal of Statistical Software, 8(14), 16. https://doi.org/10.18637/jss.v008.i14
+    }
+}
+
+template<Rpp32s STREAM_SIZE>
+inline void rpp_host_rng_xorwow_f32_initialize_multiseed_stream_boxmuller(RpptXorwowStateBoxMuller *xorwowInitialState, Rpp32u seed)
+{
+    Rpp32u xorwowSeedStream[STREAM_SIZE];
+
+    // Loop to initialize seed stream of size STREAM_SIZE based on user seed and offset
+    for (int i = 0; i < STREAM_SIZE; i++)
+        xorwowSeedStream[i] = seed + multiseedStreamOffset[i];
+
+    // Loop to initialize STREAM_SIZE xorwow initial states for multi-stream random number generation
+    for (int i = 0; i < STREAM_SIZE; i++)
+    {
+        xorwowInitialState[i].x[0] = 0x75BCD15 + xorwowSeedStream[i];      // state param x[0] offset 123456789U from Marsaglia, G. (2003). Xorshift RNGs. Journal of Statistical Software, 8(14), 16. https://doi.org/10.18637/jss.v008.i14
+        xorwowInitialState[i].x[1] = 0x159A55E5 + xorwowSeedStream[i];     // state param x[1] offset 362436069U from Marsaglia, G. (2003). Xorshift RNGs. Journal of Statistical Software, 8(14), 16. https://doi.org/10.18637/jss.v008.i14
+        xorwowInitialState[i].x[2] = 0x1F123BB5 + xorwowSeedStream[i];     // state param x[2] offset 521288629U from Marsaglia, G. (2003). Xorshift RNGs. Journal of Statistical Software, 8(14), 16. https://doi.org/10.18637/jss.v008.i14
+        xorwowInitialState[i].x[3] = 0x5491333 + xorwowSeedStream[i];      // state param x[3] offset 88675123U from Marsaglia, G. (2003). Xorshift RNGs. Journal of Statistical Software, 8(14), 16. https://doi.org/10.18637/jss.v008.i14
+        xorwowInitialState[i].x[4] = 0x583F19 + xorwowSeedStream[i];       // state param x[4] offset 5783321U from Marsaglia, G. (2003). Xorshift RNGs. Journal of Statistical Software, 8(14), 16. https://doi.org/10.18637/jss.v008.i14
+        xorwowInitialState[i].counter = 0x64F0C9 + xorwowSeedStream[i];    // state param counter offset 6615241U from Marsaglia, G. (2003). Xorshift RNGs. Journal of Statistical Software, 8(14), 16. https://doi.org/10.18637/jss.v008.i14
+        xorwowInitialState[i].boxMullerFlag = 0;
+        xorwowInitialState[i].boxMullerExtra = 0.0f;
+    }
+}
+
+template<typename T>
+inline void rpp_host_rng_xorwow_state_offsetted_avx(T *xorwowInitialStatePtr, T &xorwowState, Rpp32u offset, __m256i *pxXorwowStateX, __m256i *pxXorwowStateCounter)
+{
+    xorwowState = xorwowInitialStatePtr[0];
+    xorwowState.x[0] = xorwowInitialStatePtr[0].x[0] + offset;
+
+    __m256i pxOffset = __lasx_xvreplgr2vr_w(offset);
+    pxXorwowStateX[0] = __lasx_xvadd_w(lasx_setr_i32(xorwowInitialStatePtr[0].x[0], xorwowInitialStatePtr[1].x[0], xorwowInitialStatePtr[2].x[0], xorwowInitialStatePtr[3].x[0], xorwowInitialStatePtr[4].x[0], xorwowInitialStatePtr[5].x[0], xorwowInitialStatePtr[6].x[0], xorwowInitialStatePtr[7].x[0]), pxOffset);
+    pxXorwowStateX[1] = lasx_setr_i32(xorwowInitialStatePtr[0].x[1], xorwowInitialStatePtr[1].x[1], xorwowInitialStatePtr[2].x[1], xorwowInitialStatePtr[3].x[1], xorwowInitialStatePtr[4].x[1], xorwowInitialStatePtr[5].x[1], xorwowInitialStatePtr[6].x[1], xorwowInitialStatePtr[7].x[1]);
+    pxXorwowStateX[2] = lasx_setr_i32(xorwowInitialStatePtr[0].x[2], xorwowInitialStatePtr[1].x[2], xorwowInitialStatePtr[2].x[2], xorwowInitialStatePtr[3].x[2], xorwowInitialStatePtr[4].x[2], xorwowInitialStatePtr[5].x[2], xorwowInitialStatePtr[6].x[2], xorwowInitialStatePtr[7].x[2]);
+    pxXorwowStateX[3] = lasx_setr_i32(xorwowInitialStatePtr[0].x[3], xorwowInitialStatePtr[1].x[3], xorwowInitialStatePtr[2].x[3], xorwowInitialStatePtr[3].x[3], xorwowInitialStatePtr[4].x[3], xorwowInitialStatePtr[5].x[3], xorwowInitialStatePtr[6].x[3], xorwowInitialStatePtr[7].x[3]);
+    pxXorwowStateX[4] = lasx_setr_i32(xorwowInitialStatePtr[0].x[4], xorwowInitialStatePtr[1].x[4], xorwowInitialStatePtr[2].x[4], xorwowInitialStatePtr[3].x[4], xorwowInitialStatePtr[4].x[4], xorwowInitialStatePtr[5].x[4], xorwowInitialStatePtr[6].x[4], xorwowInitialStatePtr[7].x[4]);
+    *pxXorwowStateCounter = lasx_setr_i32(xorwowInitialStatePtr[0].counter, xorwowInitialStatePtr[1].counter, xorwowInitialStatePtr[2].counter, xorwowInitialStatePtr[3].counter, xorwowInitialStatePtr[4].counter, xorwowInitialStatePtr[5].counter, xorwowInitialStatePtr[6].counter, xorwowInitialStatePtr[7].counter);
+}
+
+template<typename T>
+inline void rpp_host_rng_xorwow_state_offsetted_sse(T *xorwowInitialStatePtr, T &xorwowState, Rpp32u offset, __m128i *pxXorwowStateX, __m128i *pxXorwowStateCounter)
+{
+    xorwowState = xorwowInitialStatePtr[0];
+    xorwowState.x[0] = xorwowInitialStatePtr[0].x[0] + offset;
+
+    __m128i pxOffset = __lsx_vreplgr2vr_w(offset);
+    pxXorwowStateX[0] = __lsx_vadd_w(lsx_setr_i32(xorwowInitialStatePtr[0].x[0], xorwowInitialStatePtr[1].x[0], xorwowInitialStatePtr[2].x[0], xorwowInitialStatePtr[3].x[0]), pxOffset);
+    pxXorwowStateX[1] = lsx_setr_i32(xorwowInitialStatePtr[0].x[1], xorwowInitialStatePtr[1].x[1], xorwowInitialStatePtr[2].x[1], xorwowInitialStatePtr[3].x[1]);
+    pxXorwowStateX[2] = lsx_setr_i32(xorwowInitialStatePtr[0].x[2], xorwowInitialStatePtr[1].x[2], xorwowInitialStatePtr[2].x[2], xorwowInitialStatePtr[3].x[2]);
+    pxXorwowStateX[3] = lsx_setr_i32(xorwowInitialStatePtr[0].x[3], xorwowInitialStatePtr[1].x[3], xorwowInitialStatePtr[2].x[3], xorwowInitialStatePtr[3].x[3]);
+    pxXorwowStateX[4] = lsx_setr_i32(xorwowInitialStatePtr[0].x[4], xorwowInitialStatePtr[1].x[4], xorwowInitialStatePtr[2].x[4], xorwowInitialStatePtr[3].x[4]);
+    *pxXorwowStateCounter = lsx_setr_i32(xorwowInitialStatePtr[0].counter, xorwowInitialStatePtr[1].counter, xorwowInitialStatePtr[2].counter, xorwowInitialStatePtr[3].counter);
+}
+
+inline void rpp_host_rng_xorwow_8_state_update_avx(__m256i *pxXorwowStateXParam, __m256i *pxXorwowStateCounterParam)
+{
+    // Initialize avx-xorwow specific constants
+    __m256i pxXorwowCounterInc = __lasx_xvreplgr2vr_w(XORWOW_COUNTER_INC);
+
+    // Save current first and last x-params of xorwow state and compute pxT
+    __m256i pxT = pxXorwowStateXParam[0];                                                           // uint t  = xorwowState->x[0];
+    __m256i pxS = pxXorwowStateXParam[4];                                                           // uint s  = xorwowState->x[4];
+    pxT = __lasx_xvxor_v(pxT, lasx_srli_i32(pxT, 2));                                         // t ^= t >> 2;
+    pxT = __lasx_xvxor_v(pxT, lasx_slli_i32(pxT, 1));                                         // t ^= t << 1;
+    pxT = __lasx_xvxor_v(pxT, __lasx_xvxor_v(pxS, lasx_slli_i32(pxS, 4)));                  // t ^= s ^ (s << 4);
+
+    // Update all 6 xorwow state params
+    pxXorwowStateXParam[0] = pxXorwowStateXParam[1];                                                // xorwowState->x[0] = xorwowState->x[1];
+    pxXorwowStateXParam[1] = pxXorwowStateXParam[2];                                                // xorwowState->x[1] = xorwowState->x[2];
+    pxXorwowStateXParam[2] = pxXorwowStateXParam[3];                                                // xorwowState->x[2] = xorwowState->x[3];
+    pxXorwowStateXParam[3] = pxXorwowStateXParam[4];                                                // xorwowState->x[3] = xorwowState->x[4];
+    pxXorwowStateXParam[4] = pxT;                                                                   // xorwowState->x[4] = t;
+    *pxXorwowStateCounterParam = __lasx_xvadd_w(*pxXorwowStateCounterParam, pxXorwowCounterInc);  // xorwowState->counter += XORWOW_COUNTER_INC;
+}
+
+inline __m256i rpp_host_rng_xorwow_8_u32_avx(__m256i *pxXorwowStateXParam, __m256i *pxXorwowStateCounterParam)
+{
+    // Update xorwow state
+    rpp_host_rng_xorwow_8_state_update_avx(pxXorwowStateXParam, pxXorwowStateCounterParam);
+
+    // Return u32 random number
+    return  __lasx_xvadd_w(pxXorwowStateXParam[4], *pxXorwowStateCounterParam);   // return x[4] + counter
+}
+
+inline __m256 rpp_host_rng_xorwow_8_f32_avx(__m256i *pxXorwowStateXParam, __m256i *pxXorwowStateCounterParam)
+{
+    // Update xorwow state
+    rpp_host_rng_xorwow_8_state_update_avx(pxXorwowStateXParam, pxXorwowStateCounterParam);
+
+    // Initialize avx-xorwow specific constants
+    __m256i px7FFFFF = __lasx_xvreplgr2vr_w(0x7FFFFF);
+    __m256i pxExponentFloat = __lasx_xvreplgr2vr_w(XORWOW_EXPONENT_MASK);
+
+    // Create float representation and return 0 <= pxS < 1
+    __m256i pxS = __lasx_xvor_v(pxExponentFloat, __lasx_xvand_v(__lasx_xvadd_w(pxXorwowStateXParam[4], *pxXorwowStateCounterParam), px7FFFFF));   // uint out = (XORWOW_EXPONENT_MASK | ((xorwowState->x[4] + xorwowState->counter) & 0x7FFFFF));
+    return __lasx_xvfsub_s(*(__m256 *)&pxS, avx_p1);                                                                                                      // return  *(float *)&out - 1;
+}
+
+inline void rpp_host_rng_xorwow_4_state_update_sse(__m128i *pxXorwowStateXParam, __m128i *pxXorwowStateCounterParam)
+{
+    // Initialize sse-xorwow specific constants
+    __m128i pxXorwowCounterInc = __lsx_vreplgr2vr_w(XORWOW_COUNTER_INC);
+
+    // Save current first and last x-params of xorwow state and compute pxT
+    __m128i pxT = pxXorwowStateXParam[0];                                                       // uint t  = xorwowState->x[0];
+    __m128i pxS = pxXorwowStateXParam[4];                                                       // uint s  = xorwowState->x[4];
+    pxT = __lsx_vxor_v(pxT, lsx_srli_i32(pxT, 2));                                           // t ^= t >> 2;
+    pxT = __lsx_vxor_v(pxT, lsx_slli_i32(pxT, 1));                                           // t ^= t << 1;
+    pxT = __lsx_vxor_v(pxT, __lsx_vxor_v(pxS, lsx_slli_i32(pxS, 4)));                       // t ^= s ^ (s << 4);
+
+    // Update all 6 xorwow state params
+    pxXorwowStateXParam[0] = pxXorwowStateXParam[1];                                            // xorwowState->x[0] = xorwowState->x[1];
+    pxXorwowStateXParam[1] = pxXorwowStateXParam[2];                                            // xorwowState->x[1] = xorwowState->x[2];
+    pxXorwowStateXParam[2] = pxXorwowStateXParam[3];                                            // xorwowState->x[2] = xorwowState->x[3];
+    pxXorwowStateXParam[3] = pxXorwowStateXParam[4];                                            // xorwowState->x[3] = xorwowState->x[4];
+    pxXorwowStateXParam[4] = pxT;                                                               // xorwowState->x[4] = t;
+    *pxXorwowStateCounterParam = __lsx_vadd_w(*pxXorwowStateCounterParam, pxXorwowCounterInc); // xorwowState->counter += XORWOW_COUNTER_INC;
+}
+
+inline __m128i rpp_host_rng_xorwow_4_u32_sse(__m128i *pxXorwowStateXParam, __m128i *pxXorwowStateCounterParam)
+{
+    // Update xorwow state
+    rpp_host_rng_xorwow_4_state_update_sse(pxXorwowStateXParam, pxXorwowStateCounterParam);
+
+    // Return u32 random number
+    return  __lsx_vadd_w(pxXorwowStateXParam[4], *pxXorwowStateCounterParam);   // return x[4] + counter
+}
+
+inline __m128 rpp_host_rng_xorwow_4_f32_sse(__m128i *pxXorwowStateXParam, __m128i *pxXorwowStateCounterParam)
+{
+    // Update xorwow state
+    rpp_host_rng_xorwow_4_state_update_sse(pxXorwowStateXParam, pxXorwowStateCounterParam);
+
+    // Initialize sse-xorwow specific constants
+    __m128i px7FFFFF = __lsx_vreplgr2vr_w(0x7FFFFF);
+    __m128i pxExponentFloat = __lsx_vreplgr2vr_w(XORWOW_EXPONENT_MASK);
+
+    // Create float representation and return 0 <= pxS < 1
+    __m128i pxS = __lsx_vor_v(pxExponentFloat, __lsx_vand_v(__lsx_vadd_w(pxXorwowStateXParam[4], *pxXorwowStateCounterParam), px7FFFFF));    // uint out = (XORWOW_EXPONENT_MASK | ((xorwowState->x[4] + xorwowState->counter) & 0x7FFFFF));
+    return __lsx_vfsub_s(*(__m128 *)&pxS, xmm_p1);                                                                                                 // return  *(float *)&out - 1;
+}
+
+template<typename T>
+inline void rpp_host_rng_xorwow_state_update(T *xorwowState)
+{
+    // Save current first and last x-params of xorwow state and compute t
+    Rpp32s t  = xorwowState->x[0];
+    Rpp32s s  = xorwowState->x[4];
+    t ^= t >> 2;
+    t ^= t << 1;
+    t ^= s ^ (s << 4);
+
+    // Update all 6 xorwow state params
+    xorwowState->x[0] = xorwowState->x[1];                              // set new state param x[0]
+    xorwowState->x[1] = xorwowState->x[2];                              // set new state param x[1]
+    xorwowState->x[2] = xorwowState->x[3];                              // set new state param x[2]
+    xorwowState->x[3] = xorwowState->x[4];                              // set new state param x[3]
+    xorwowState->x[4] = t;                                              // set new state param x[4]
+    xorwowState->counter = xorwowState->counter + XORWOW_COUNTER_INC;   // set new state param counter
+}
+
+template<typename T>
+inline Rpp32u rpp_host_rng_xorwow_u32(T *xorwowState)
+{
+    // Update xorwow state
+    rpp_host_rng_xorwow_state_update(xorwowState);
+
+    // Return u32 random number
+    return  xorwowState->x[4] + xorwowState->counter;   // return x[4] + counter
+}
+
+template<typename T>
+inline Rpp32f rpp_host_rng_xorwow_f32(T *xorwowState)
+{
+    // Update xorwow state
+    rpp_host_rng_xorwow_state_update(xorwowState);
+
+    // Create float representation and return 0 <= outFloat < 1
+    Rpp32u out = (XORWOW_EXPONENT_MASK | ((xorwowState->x[4] + xorwowState->counter) & 0x7FFFFF));  // bitmask 23 mantissa bits, OR with exponent
+    Rpp32f outFloat = *(Rpp32f *)&out;                                                              // reinterpret out as float
+    return  outFloat - 1;                                                                           // return 0 <= outFloat < 1
+}
+
+inline void rpp_host_rng_16_gaussian_f32_avx(__m256 *pRngVals, __m256i *pxXorwowStateX, __m256i *pxXorwowStateCounter)
+{
+    __m256 pU, pV, pS;                                                                                  // Rpp32f u, v, s;
+    pU = __lasx_xvffint_s_w(rpp_host_rng_xorwow_8_u32_avx(pxXorwowStateX, pxXorwowStateCounter));       // u = (Rpp32f)rpp_host_rng_xorwow_u32(xorwowState);
+    pV = __lasx_xvffint_s_w(rpp_host_rng_xorwow_8_u32_avx(pxXorwowStateX, pxXorwowStateCounter));       // v = (Rpp32f)rpp_host_rng_xorwow_u32(xorwowState);
+    pS = (__m256)__lasx_xvfcmp_xxx_s(pU, avx_p0, _CMP_LT_OQ);                                                         // Adjust int32 out of bound values in float for u
+    pU = (__m256)__lasx_xvor_v((__m256i)__lasx_xvandn_v((__m256i)pS, (__m256i)pU), (__m256i)__lasx_xvand_v((__m256i)pS, (__m256i)__lasx_xvfadd_s(avx_p2Pow32, pU)));     // Adjust int32 out of bound values in float for u
+    pS = (__m256)__lasx_xvfcmp_xxx_s(pV, avx_p0, _CMP_LT_OQ);                                                         // Adjust int32 out of bound values in float for v
+    pV = (__m256)__lasx_xvor_v((__m256i)__lasx_xvandn_v((__m256i)pS, (__m256i)pV), (__m256i)__lasx_xvand_v((__m256i)pS, (__m256i)__lasx_xvfadd_s(avx_p2Pow32, pV)));     // Adjust int32 out of bound values in float for v
+    pU = __lasx_xvfmadd_s(pU, avx_p2Pow32Inv, avx_p2Pow32InvDiv2);                                       // u = u * RPP_2POW32_INV + RPP_2POW32_INV_DIV_2;
+    pV = __lasx_xvfmadd_s(pV, avx_p2Pow32InvMul2Pi, avx_p2Pow32InvMul2PiDiv2);                           // v = v * RPP_2POW32_INV_MUL_2PI + RPP_2POW32_INV_MUL_2PI_DIV_2;
+    pS = __lasx_xvfsqrt_s(__lasx_xvfmul_s(avx_pm2, log_ps(pU)));                                            // s = sqrt(-2.0f * std::log(u));
+    sincos_ps(pV, &pU, &pV);                                                                            // std::sin(v) and std::cos(v) computation
+    pRngVals[0] = __lasx_xvfmul_s(pU, pS);                                                                // u = std::sin(v) * s;
+    pRngVals[1] = __lasx_xvfmul_s(pV, pS);                                                                // v = std::cos(v) * s;
+}
+
+inline void rpp_host_rng_8_gaussian_f32_sse(__m128 *pRngVals, __m128i *pxXorwowStateX, __m128i *pxXorwowStateCounter)
+{
+    __m128 pU, pV, pS;                                                                          // Rpp32f u, v, s;
+    pU = __lsx_vffint_s_w(rpp_host_rng_xorwow_4_u32_sse(pxXorwowStateX, pxXorwowStateCounter));  // u = (Rpp32f)rpp_host_rng_xorwow_u32(xorwowState);
+    pV = __lsx_vffint_s_w(rpp_host_rng_xorwow_4_u32_sse(pxXorwowStateX, pxXorwowStateCounter));  // v = (Rpp32f)rpp_host_rng_xorwow_u32(xorwowState);
+    pS = (__m128)__lsx_vfcmp_clt_s(pU, xmm_p0);                                                              // Adjust int32 out of bound values in float for u
+    pU = (__m128)__lsx_vor_v((__m128i)__lsx_vandn_v((__m128i)pS, (__m128i)pU), (__m128i)__lsx_vand_v((__m128i)pS, (__m128i)__lsx_vfadd_s(xmm_p2Pow32, pU)));         // Adjust int32 out of bound values in float for u
+    pS = (__m128)__lsx_vfcmp_clt_s(pV, xmm_p0);                                                              // Adjust int32 out of bound values in float for v
+    pV = (__m128)__lsx_vor_v((__m128i)__lsx_vandn_v((__m128i)pS, (__m128i)pV), (__m128i)__lsx_vand_v((__m128i)pS, (__m128i)__lsx_vfadd_s(xmm_p2Pow32, pV)));         // Adjust int32 out of bound values in float for v
+    pU = __lsx_vfmadd_s(pU, xmm_p2Pow32Inv, xmm_p2Pow32InvDiv2);                                  // u = u * RPP_2POW32_INV + RPP_2POW32_INV_DIV_2;
+    pV = __lsx_vfmadd_s(pV, xmm_p2Pow32InvMul2Pi, xmm_p2Pow32InvMul2PiDiv2);                      // v = v * RPP_2POW32_INV_MUL_2PI + RPP_2POW32_INV_MUL_2PI_DIV_2;
+    pS = __lsx_vfsqrt_s(__lsx_vfmul_s(xmm_pm2, log_ps(pU)));                                          // s = sqrt(-2.0f * std::log(u));
+    sincos_ps(pV, &pU, &pV);                                                                    // std::sin(v) and std::cos(v) computation
+    pRngVals[0] = __lsx_vfmul_s(pU, pS);                                                           // u = std::sin(v) * s;
+    pRngVals[1] = __lsx_vfmul_s(pV, pS);                                                           // v = std::cos(v) * s;
+}
+
+inline float rpp_host_rng_1_gaussian_f32(RpptXorwowStateBoxMuller *xorwowState)
+{
+    if(!xorwowState->boxMullerFlag)
+    {
+        Rpp32f u, v, s;
+        u = (Rpp32f)rpp_host_rng_xorwow_u32(xorwowState) * RPP_2POW32_INV + RPP_2POW32_INV_DIV_2;
+        v = (Rpp32f)rpp_host_rng_xorwow_u32(xorwowState) * RPP_2POW32_INV_MUL_2PI + RPP_2POW32_INV_MUL_2PI_DIV_2;
+        s = sqrt(-2.0f * std::log(u));
+        u = std::sin(v) * s;
+        v = std::cos(v) * s;
+        xorwowState->boxMullerExtra = v;
+        xorwowState->boxMullerFlag = 1;
+        return u;
+    }
+    xorwowState->boxMullerFlag = 0;
+
+    return xorwowState->boxMullerExtra;
+}
+
+inline int power_function(int a, int b)
+{
+    int product = 1;
+    for(int i = 0; i < b; i++)
+        product *= product * a;
+    return product;
+}
+
+inline void saturate_pixel(Rpp32f &pixel, Rpp8u* dst)
+{
+    *dst = static_cast<Rpp8u>(RPPPIXELCHECK(std::nearbyintf(pixel)));
+}
+
+inline void saturate_pixel(Rpp32f &pixel, Rpp8s* dst)
+{
+    *dst = static_cast<Rpp8s>(RPPPIXELCHECKI8(std::nearbyintf(pixel) - 128));
+}
+
+inline void saturate_pixel(Rpp32f &pixel, Rpp32f* dst)
+{
+    *dst = RPPPIXELCHECKF32(pixel);
+}
+
+inline void saturate_pixel(Rpp32f &pixel, Rpp16f* dst)
+{
+    *dst = static_cast<Rpp16f>(RPPPIXELCHECKF32(pixel));
+}
+
+template <typename T>
+RppStatus subtract_host_batch(T* srcPtr1, T* srcPtr2, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                              RppiROI *roiPoints, Rpp32u nbatchSize,
+                              RppiChnFormat chnFormat, Rpp32u channel);
+template <typename T>
+RppStatus add_host_batch(T* srcPtr1, T* srcPtr2, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                              RppiROI *roiPoints, Rpp32u nbatchSize,
+                              RppiChnFormat chnFormat, Rpp32u channel);
+template <typename T>
+RppStatus multiply_host_batch(T* srcPtr1, T* srcPtr2, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                              RppiROI *roiPoints, Rpp32u nbatchSize,
+                              RppiChnFormat chnFormat, Rpp32u channel);
+template <typename T>
+RppStatus min_host_batch(T* srcPtr1, T* srcPtr2, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                              RppiROI *roiPoints, Rpp32u nbatchSize,
+                              RppiChnFormat chnFormat, Rpp32u channel);
+template <typename T>
+RppStatus max_host_batch(T* srcPtr1, T* srcPtr2, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                              RppiROI *roiPoints, Rpp32u nbatchSize,
+                              RppiChnFormat chnFormat, Rpp32u channel);
+
+template <typename T>
+RppStatus bitwise_AND_host_batch(T* srcPtr1, T* srcPtr2, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                              RppiROI *roiPoints, Rpp32u nbatchSize,
+                              RppiChnFormat chnFormat, Rpp32u channel);
+template <typename T>
+RppStatus inclusive_OR_host_batch(T* srcPtr1, T* srcPtr2, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                              RppiROI *roiPoints, Rpp32u nbatchSize,
+                              RppiChnFormat chnFormat, Rpp32u channel);
+template <typename T>
+RppStatus exclusive_OR_host_batch(T* srcPtr1, T* srcPtr2, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                              RppiROI *roiPoints, Rpp32u nbatchSize,
+                              RppiChnFormat chnFormat, Rpp32u channel);
+
+
+// Specific Helper Functions
+
+inline Rpp32f gaussian_2d_relative(Rpp32s locI, Rpp32s locJ, Rpp32f std_dev)
+{
+    Rpp32f relativeGaussian;
+    Rpp32f exp1, exp2;
+    exp1 = -(locJ * locJ) / (2 * std_dev * std_dev);
+    exp2 = -(locI * locI) / (2 * std_dev * std_dev);
+    relativeGaussian = exp(exp1 + exp2);
+
+    return relativeGaussian;
+}
+
+// Generate Functions
+
+inline void generate_gaussian_kernel_host(Rpp32f stdDev, Rpp32f* kernel, Rpp32u kernelSize)
+{
+    Rpp32f s, sum = 0.0, multiplier;
+    int bound = ((kernelSize - 1) / 2);
+    Rpp32u c = 0;
+    s = 1 / (2 * stdDev * stdDev);
+    multiplier = (1 / M_PI) * (s);
+    for (int i = -bound; i <= bound; i++)
+    {
+        for (int j = -bound; j <= bound; j++)
+        {
+            kernel[c] = multiplier * exp((-1) * (s) * (i*i + j*j));
+            sum += kernel[c];
+            c += 1;
+        }
+    }
+    for (int i = 0; i < (kernelSize * kernelSize); i++)
+    {
+        kernel[i] /= sum;
+    }
+}
+
+inline RppStatus generate_gaussian_kernel_asymmetric_host(Rpp32f stdDev, Rpp32f* kernel, Rpp32u kernelSizeX, Rpp32u kernelSizeY)
+{
+    Rpp32f s, sum = 0.0, multiplier;
+    if (kernelSizeX % 2 == 0)
+    {
+        return RPP_ERROR;
+    }
+    if (kernelSizeY % 2 == 0)
+    {
+        return RPP_ERROR;
+    }
+    int boundX = ((kernelSizeX - 1) / 2);
+    int boundY = ((kernelSizeY - 1) / 2);
+    Rpp32u c = 0;
+    s = 1 / (2 * stdDev * stdDev);
+    multiplier = (1 / M_PI) * (s);
+    for (int i = -boundY; i <= boundY; i++)
+    {
+        for (int j = -boundX; j <= boundX; j++)
+        {
+            kernel[c] = multiplier * exp((-1) * (s) * (i*i + j*j));
+            sum += kernel[c];
+            c += 1;
+        }
+    }
+    for (int i = 0; i < (kernelSizeX * kernelSizeY); i++)
+    {
+        kernel[i] /= sum;
+    }
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+inline void generate_bilateral_kernel_host(Rpp32f multiplierI, Rpp32f multiplierS, Rpp32f multiplier, Rpp32f* kernel, Rpp32u kernelSize, int bound,
+                                         T* srcPtrWindow, RppiSize srcSizeMod, Rpp32u remainingElementsInRow, Rpp32u incrementToWindowCenter,
+                                         RppiChnFormat chnFormat, Rpp32u channel)
+{
+    Rpp32f sum = 0.0;
+    Rpp32f* kernelTemp;
+    kernelTemp = kernel;
+
+    T *srcPtrWindowTemp, *srcPtrWindowCenter;
+    srcPtrWindowTemp = srcPtrWindow;
+    srcPtrWindowCenter = srcPtrWindow + incrementToWindowCenter;
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        for (int i = -bound; i <= bound; i++)
+        {
+            for (int j = -bound; j <= bound; j++)
+            {
+                T pixel = *srcPtrWindowCenter - *srcPtrWindowTemp;
+                pixel = RPPABS(pixel);
+                pixel = pixel * pixel;
+                *kernelTemp = multiplier * exp((multiplierS * (i*i + j*j)) + (multiplierI * pixel));
+                sum = sum + *kernelTemp;
+                kernelTemp++;
+                srcPtrWindowTemp++;
+            }
+            srcPtrWindowTemp += remainingElementsInRow;
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        for (int i = -bound; i <= bound; i++)
+        {
+            for (int j = -bound; j <= bound; j++)
+            {
+                T pixel = *srcPtrWindowCenter - *srcPtrWindowTemp;
+                pixel = RPPABS(pixel);
+                pixel = pixel * pixel;
+                *kernelTemp = multiplier * exp((multiplierS * (i*i + j*j)) + (multiplierI * pixel));
+                sum = sum + *kernelTemp;
+                kernelTemp++;
+                srcPtrWindowTemp += channel;
+            }
+            srcPtrWindowTemp += remainingElementsInRow;
+        }
+    }
+
+    kernelTemp = kernel;
+    for (int i = 0; i < (kernelSize * kernelSize); i++)
+    {
+        *kernelTemp = *kernelTemp / sum;
+        kernelTemp++;
+    }
+}
+
+template <typename T>
+inline RppStatus generate_evenly_padded_image_host(T* srcPtr, RppiSize srcSize, T* srcPtrMod, RppiSize srcSizeMod,
+                                     RppiChnFormat chnFormat, Rpp32u channel)
+{
+    if (RPPISEVEN(srcSize.height) != RPPISEVEN(srcSizeMod.height)
+        || RPPISEVEN(srcSize.width) != RPPISEVEN(srcSizeMod.width)
+        || srcSizeMod.height < srcSize.height
+        || srcSizeMod.width < srcSize.width)
+    {
+        return RPP_ERROR;
+    }
+    T *srcPtrTemp, *srcPtrModTemp;
+    srcPtrTemp = srcPtr;
+    srcPtrModTemp = srcPtrMod;
+    int bound = (srcSizeMod.height - srcSize.height) / 2;
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        for (int c = 0; c < channel; c++)
+        {
+            memset (srcPtrModTemp,(T) 0,bound * srcSizeMod.width * sizeof(T));
+            srcPtrModTemp += (bound * srcSizeMod.width);
+            for (int i = 0; i < srcSize.height; i++)
+            {
+                memset (srcPtrModTemp,(T) 0,bound * sizeof(T));
+                srcPtrModTemp += bound;
+
+                memcpy(srcPtrModTemp, srcPtrTemp, srcSize.width * sizeof(T));
+                srcPtrModTemp += srcSize.width;
+                srcPtrTemp += srcSize.width;
+
+                memset (srcPtrModTemp,(T) 0,bound * sizeof(T));
+                srcPtrModTemp += bound;
+            }
+            memset (srcPtrModTemp,(T) 0,bound * srcSizeMod.width * sizeof(T));
+            srcPtrModTemp += (bound * srcSizeMod.width);
+        }
+    }
+    else if(chnFormat == RPPI_CHN_PACKED)
+    {
+        Rpp32u elementsInRow = channel * srcSize.width;
+        Rpp32u numOfPixelsVtBorder = bound * channel;
+        Rpp32u numOfPixelsHrBorder = numOfPixelsVtBorder * srcSizeMod.width;
+
+        memset (srcPtrModTemp,(T) 0,numOfPixelsHrBorder * sizeof(T));
+        srcPtrModTemp += (numOfPixelsHrBorder);
+
+        for (int i = 0; i < srcSize.height; i++)
+        {
+            memset (srcPtrModTemp,(T) 0,numOfPixelsVtBorder * sizeof(T));
+            srcPtrModTemp += (numOfPixelsVtBorder);
+
+            memcpy(srcPtrModTemp, srcPtrTemp, elementsInRow * sizeof(T));
+            srcPtrModTemp += elementsInRow;
+            srcPtrTemp += elementsInRow;
+
+            memset (srcPtrModTemp,(T) 0,numOfPixelsVtBorder * sizeof(T));
+            srcPtrModTemp += (numOfPixelsVtBorder);
+        }
+
+        memset (srcPtrModTemp,(T) 0,numOfPixelsHrBorder * sizeof(T));
+        srcPtrModTemp += (numOfPixelsHrBorder);
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+inline void generate_corner_padded_image_host(T* srcPtr, RppiSize srcSize, T* srcPtrMod, RppiSize srcSizeMod, Rpp32u padType,
+                                     RppiChnFormat chnFormat, Rpp32u channel)
+{
+    T *srcPtrTemp, *srcPtrModTemp;
+    srcPtrTemp = srcPtr;
+    srcPtrModTemp = srcPtrMod;
+    Rpp32u boundY = srcSizeMod.height - srcSize.height;
+    Rpp32u boundX = srcSizeMod.width - srcSize.width;
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        for (int c = 0; c < channel; c++)
+        {
+            if (padType == 1 || padType == 2)
+            {
+                memset (srcPtrModTemp,(T) 0,boundY * srcSizeMod.width * sizeof(T));
+                srcPtrModTemp += (boundY * srcSizeMod.width);
+            }
+
+            if (padType == 1 || padType == 3)
+            {
+                for (int i = 0; i < srcSize.height; i++)
+                {
+                    memset (srcPtrModTemp,(T) 0,boundX * sizeof(T));
+                    srcPtrModTemp += boundX;
+
+                    memcpy(srcPtrModTemp, srcPtrTemp, srcSize.width * sizeof(T));
+                    srcPtrModTemp += srcSize.width;
+                    srcPtrTemp += srcSize.width;
+                }
+            }
+
+            if (padType == 2 || padType == 4)
+            {
+                for (int i = 0; i < srcSize.height; i++)
+                {
+                    memcpy(srcPtrModTemp, srcPtrTemp, srcSize.width * sizeof(T));
+                    srcPtrModTemp += srcSize.width;
+                    srcPtrTemp += srcSize.width;
+
+                    memset (srcPtrModTemp,(T) 0,boundX * sizeof(T));
+                    srcPtrModTemp += boundX;
+                }
+            }
+
+            if (padType == 3 || padType == 4)
+            {
+                memset (srcPtrModTemp,(T) 0,boundY * srcSizeMod.width * sizeof(T));
+                srcPtrModTemp += (boundY * srcSizeMod.width);
+            }
+        }
+    }
+    else if(chnFormat == RPPI_CHN_PACKED)
+    {
+        Rpp32u elementsInRow = channel * srcSize.width;
+        Rpp32u numOfPixelsVtBorder = boundX * channel;
+        Rpp32u numOfPixelsHrBorder = boundY * channel * srcSizeMod.width;
+
+        if (padType == 1 || padType == 2)
+        {
+            memset (srcPtrModTemp,(T) 0,numOfPixelsHrBorder * sizeof(T));
+            srcPtrModTemp += (numOfPixelsHrBorder);
+        }
+
+        if (padType == 1 || padType == 3)
+        {
+            for (int i = 0; i < srcSize.height; i++)
+            {
+                memset (srcPtrModTemp,(T) 0,numOfPixelsVtBorder * sizeof(T));
+                srcPtrModTemp += (numOfPixelsVtBorder);
+
+                memcpy(srcPtrModTemp, srcPtrTemp, elementsInRow * sizeof(T));
+                srcPtrModTemp += elementsInRow;
+                srcPtrTemp += elementsInRow;
+            }
+        }
+
+        if (padType == 2 || padType == 4)
+        {
+            for (int i = 0; i < srcSize.height; i++)
+            {
+                memcpy(srcPtrModTemp, srcPtrTemp, elementsInRow * sizeof(T));
+                srcPtrModTemp += elementsInRow;
+                srcPtrTemp += elementsInRow;
+
+                memset (srcPtrModTemp,(T) 0,numOfPixelsVtBorder * sizeof(T));
+                srcPtrModTemp += (numOfPixelsVtBorder);
+            }
+        }
+
+        if (padType == 3 || padType == 4)
+        {
+            memset (srcPtrModTemp,(T) 0,numOfPixelsHrBorder * sizeof(T));
+            srcPtrModTemp += (numOfPixelsHrBorder);
+        }
+    }
+}
+
+inline void generate_box_kernel_host(Rpp32f* kernel, Rpp32u kernelSize)
+{
+    Rpp32f* kernelTemp;
+    kernelTemp = kernel;
+    Rpp32f kernelValue = 1.0 / (Rpp32f) (kernelSize * kernelSize);
+    for (int i = 0; i < (kernelSize * kernelSize); i++)
+    {
+        *kernelTemp = kernelValue;
+        kernelTemp++;
+    }
+}
+
+template <typename T>
+inline void generate_crop_host(T* srcPtr, RppiSize srcSize, T* srcPtrSubImage, RppiSize srcSizeSubImage, T* dstPtr,
+                             RppiChnFormat chnFormat, Rpp32u channel)
+{
+    T *srcPtrSubImageTemp, *dstPtrTemp;
+    srcPtrSubImageTemp = srcPtrSubImage;
+    dstPtrTemp = dstPtr;
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        Rpp32u remainingElementsInRow = (srcSize.width - srcSizeSubImage.width);
+        for (int c = 0; c < channel; c++)
+        {
+            srcPtrSubImageTemp = srcPtrSubImage + (c * srcSize.height * srcSize.width);
+            for (int i = 0; i < srcSizeSubImage.height; i++)
+            {
+                Rpp32u bufferLength = srcSizeSubImage.width;
+                Rpp32u alignedLength = bufferLength & ~15;
+
+                __m128i px0;
+
+                int vectorLoopCount = 0;
+                for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+                {
+                    px0 =  __lsx_vld((__m128i *)srcPtrSubImageTemp, 0);
+                    __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+                    srcPtrSubImageTemp +=16;
+                    dstPtrTemp +=16;
+                }
+                for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                {
+                    *dstPtrTemp++ = *srcPtrSubImageTemp++;
+                }
+                srcPtrSubImageTemp += remainingElementsInRow;
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        Rpp32u remainingElementsInRow = channel * (srcSize.width - srcSizeSubImage.width);
+        Rpp32u elementsInRowCrop = channel * srcSizeSubImage.width;
+        for (int i = 0; i < srcSizeSubImage.height; i++)
+        {
+            Rpp32u bufferLength = elementsInRowCrop;
+            Rpp32u alignedLength = bufferLength & ~15;
+
+            __m128i px0;
+
+            int vectorLoopCount = 0;
+            for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+            {
+                px0 =  __lsx_vld((__m128i *)srcPtrSubImageTemp, 0);
+                __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+                srcPtrSubImageTemp +=16;
+                dstPtrTemp +=16;
+            }
+            for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+            {
+                *dstPtrTemp++ = *srcPtrSubImageTemp++;
+            }
+            srcPtrSubImageTemp += remainingElementsInRow;
+        }
+    }
+}
+
+inline RppStatus generate_sobel_kernel_host(Rpp32f* kernel, Rpp32u type)
+{
+    Rpp32f* kernelTemp;
+    kernelTemp = kernel;
+
+    if (type == 1)
+    {
+        Rpp32f kernelX[9] = {-1, 0, 1, -2, 0, 2, -1, 0, 1};
+        Rpp32f* kernelXTemp;
+        kernelXTemp = kernelX;
+
+        for (int i = 0; i < 9; i++)
+        {
+            *kernelTemp = *kernelXTemp;
+            kernelTemp++;
+            kernelXTemp++;
+        }
+    }
+    else if (type == 2)
+    {
+        Rpp32f kernelY[9] = {-1, -2, -1, 0, 0, 0, 1, 2, 1};
+        Rpp32f* kernelYTemp;
+        kernelYTemp = kernelY;
+
+        for (int i = 0; i < 9; i++)
+        {
+            *kernelTemp = *kernelYTemp;
+            kernelTemp++;
+            kernelYTemp++;
+        }
+    }
+    else
+    {
+        return RPP_ERROR;
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+inline void generate_bressenham_line_host(T *dstPtr, RppiSize dstSize, Rpp32u *endpoints, Rpp32u *rasterCoordinates)
+{
+    Rpp32u *rasterCoordinatesTemp;
+    rasterCoordinatesTemp = rasterCoordinates;
+
+    Rpp32s x0 = *endpoints;
+    Rpp32s y0 = *(endpoints + 1);
+    Rpp32s x1 = *(endpoints + 2);
+    Rpp32s y1 = *(endpoints + 3);
+
+    Rpp32s dx, dy;
+    Rpp32s stepX, stepY;
+
+    dx = x1 - x0;
+    dy = y1 - y0;
+
+    if (dy < 0)
+    {
+        dy = -dy;
+        stepY = -1;
+    }
+    else
+    {
+        stepY = 1;
+    }
+
+    if (dx < 0)
+    {
+        dx = -dx;
+        stepX = -1;
+    }
+    else
+    {
+        stepX = 1;
+    }
+
+    dy <<= 1;
+    dx <<= 1;
+
+    if ((0 <= x0) && (x0 < dstSize.width) && (0 <= y0) && (y0 < dstSize.height))
+    {
+        *(dstPtr + (y0 * dstSize.width) + x0) = (T) 255;
+        *rasterCoordinatesTemp = y0;
+        rasterCoordinatesTemp++;
+        *rasterCoordinatesTemp = x0;
+        rasterCoordinatesTemp++;
+    }
+
+    if (dx > dy)
+    {
+        Rpp32s fraction = dy - (dx >> 1);
+        while (x0 != x1)
+        {
+            x0 += stepX;
+            if (fraction >= 0)
+            {
+                y0 += stepY;
+                fraction -= dx;
+            }
+            fraction += dy;
+            if ((0 <= x0) && (x0 < dstSize.width) && (0 <= y0) && (y0 < dstSize.height))
+            {
+                *(dstPtr + (y0 * dstSize.width) + x0) = (T) 255;
+                *rasterCoordinatesTemp = y0;
+                rasterCoordinatesTemp++;
+                *rasterCoordinatesTemp = x0;
+                rasterCoordinatesTemp++;
+            }
+        }
+    }
+    else
+    {
+        int fraction = dx - (dy >> 1);
+        while (y0 != y1)
+        {
+            if (fraction >= 0)
+            {
+                x0 += stepX;
+                fraction -= dy;
+            }
+            y0 += stepY;
+            fraction += dx;
+            if ((0 <= x0) && (x0 < dstSize.width) && (0 <= y0) && (y0 < dstSize.height))
+            {
+                *(dstPtr + (y0 * dstSize.width) + x0) = (T) 255;
+                *rasterCoordinatesTemp = y0;
+                rasterCoordinatesTemp++;
+                *rasterCoordinatesTemp = x0;
+                rasterCoordinatesTemp++;
+            }
+        }
+    }
+}
+
+// copy ROI of voxel data from input to output
+template<typename T>
+void copy_3d_host_tensor(T *srcPtr,
+                         RpptGenericDescPtr srcGenericDescPtr,
+                         T *dstPtr,
+                         RpptGenericDescPtr dstGenericDescPtr,
+                         RpptROI3D *roi,
+                         RppLayoutParams layoutParams)
+{
+    if((srcGenericDescPtr->layout == RpptLayout::NDHWC) && (dstGenericDescPtr->layout == RpptLayout::NDHWC))
+    {
+        T *srcPtrDepth = srcPtr + (roi->xyzwhdROI.xyz.z * srcGenericDescPtr->strides[1]) + (roi->xyzwhdROI.xyz.y * srcGenericDescPtr->strides[2]) + (roi->xyzwhdROI.xyz.x * layoutParams.bufferMultiplier);
+        T *dstPtrDepth = dstPtr;
+        Rpp32u width = roi->xyzwhdROI.roiWidth * srcGenericDescPtr->dims[4];
+        for(int i = 0; i < roi->xyzwhdROI.roiDepth; i++)
+        {
+            T *srcPtrRow = srcPtrDepth;
+            T *dstPtrRow = dstPtrDepth;
+            for(int j = 0; j < roi->xyzwhdROI.roiHeight; j++)
+            {
+                memcpy(dstPtrRow, srcPtrRow, width * sizeof(T));
+                srcPtrRow += srcGenericDescPtr->strides[2];
+                dstPtrRow += dstGenericDescPtr->strides[2];
+            }
+            srcPtrDepth += srcGenericDescPtr->strides[1];
+            dstPtrDepth += dstGenericDescPtr->strides[1];
+        }
+    }
+    else if ((srcGenericDescPtr->layout == RpptLayout::NCDHW) && (dstGenericDescPtr->layout == RpptLayout::NCDHW))
+    {
+        T *srcPtrChannel = srcPtr + (roi->xyzwhdROI.xyz.z * srcGenericDescPtr->strides[2]) + (roi->xyzwhdROI.xyz.y * srcGenericDescPtr->strides[3]) + (roi->xyzwhdROI.xyz.x * layoutParams.bufferMultiplier);
+        T *dstPtrChannel = dstPtr;
+        int channels = srcGenericDescPtr->dims[1];
+        for(int c = 0; c < channels; c++)
+        {
+            T *srcPtrDepth = srcPtrChannel;
+            T *dstPtrDepth = dstPtrChannel;
+            for(int i = 0; i < roi->xyzwhdROI.roiDepth; i++)
+            {
+                T *srcPtrRow = srcPtrDepth;
+                T *dstPtrRow = dstPtrDepth;
+                for(int j = 0; j < roi->xyzwhdROI.roiHeight; j++)
+                {
+                    memcpy(dstPtrRow, srcPtrRow, roi->xyzwhdROI.roiWidth * sizeof(T));
+                    srcPtrRow += srcGenericDescPtr->strides[3];
+                    dstPtrRow += dstGenericDescPtr->strides[3];
+                }
+                srcPtrDepth += srcGenericDescPtr->strides[2];
+                dstPtrDepth += dstGenericDescPtr->strides[2];
+            }
+            srcPtrChannel += srcGenericDescPtr->strides[1];
+            dstPtrChannel += dstGenericDescPtr->strides[1];
+        }
+    }
+}
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+// Kernels for functions
+
+template<typename T, typename U>
+inline void convolution_kernel_host(T* srcPtrWindow, U* dstPtrPixel, RppiSize srcSize,
+                                       Rpp32f* kernel, RppiSize kernelSize, Rpp32u remainingElementsInRow, U maxVal, U minVal,
+                                       RppiChnFormat chnFormat, Rpp32u channel)
+{
+    Rpp32f pixel = 0.0;
+
+    T* srcPtrWindowTemp;
+    srcPtrWindowTemp = srcPtrWindow;
+
+    Rpp32f* kernelPtrTemp;
+    kernelPtrTemp = kernel;
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        for (int m = 0; m < kernelSize.height; m++)
+        {
+            for (int n = 0; n < kernelSize.width; n++)
+            {
+                pixel += ((*kernelPtrTemp) * (Rpp32f)(*srcPtrWindowTemp));
+                kernelPtrTemp++;
+                srcPtrWindowTemp++;
+            }
+            srcPtrWindowTemp += remainingElementsInRow;
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        for (int m = 0; m < kernelSize.height; m++)
+        {
+            for (int n = 0; n < kernelSize.width; n++)
+            {
+                pixel += ((*kernelPtrTemp) * (Rpp32f)(*srcPtrWindowTemp));
+                kernelPtrTemp++;
+                srcPtrWindowTemp += channel;
+            }
+            srcPtrWindowTemp += remainingElementsInRow;
+        }
+    }
+    (pixel < (Rpp32f) minVal) ? pixel = (Rpp32f) minVal : ((pixel < (Rpp32f) maxVal) ? pixel : pixel = (Rpp32f) maxVal);
+    *dstPtrPixel = (U) round(pixel);
+}
+
+template<typename T>
+inline void histogram_kernel_host(T* srcPtr, RppiSize srcSize, Rpp32u* histogram,
+                                Rpp8u bins,
+                                Rpp32u channel)
+{
+    if (bins == 0)
+    {
+        *histogram = channel * srcSize.height * srcSize.width;
+    }
+    else
+    {
+        Rpp8u rangeInBin = 256 / (bins + 1);
+        T *srcPtrTemp;
+        srcPtrTemp = srcPtr;
+        for (int i = 0; i < (channel * srcSize.height * srcSize.width); i++)
+        {
+            *(histogram + (*srcPtrTemp / rangeInBin)) += 1;
+            srcPtrTemp++;
+        }
+    }
+}
+
+template <typename T, typename U>
+inline void accumulate_kernel_host(T* srcPtr1, U* srcPtr2, RppiSize srcSize,
+                                        RppiChnFormat chnFormat, Rpp32u channel)
+{
+    T *srcPtr1Temp;
+    U *srcPtr2Temp;
+    srcPtr1Temp = srcPtr1;
+    srcPtr2Temp = srcPtr2;
+
+    Rpp32s pixel;
+
+    for (int i = 0; i < (channel * srcSize.height * srcSize.width); i++)
+    {
+        pixel = ((Rpp32s) (*srcPtr1Temp)) + ((Rpp32s) (*srcPtr2Temp));
+        pixel = RPPPIXELCHECK(pixel);
+        *srcPtr1Temp =(T) pixel;
+        srcPtr1Temp++;
+        srcPtr2Temp++;
+    }
+}
+
+template <typename U>
+inline void normalize_kernel_host(U* dstPtrROI, RppiSize dstSize, Rpp32u channel)
+{
+    U* dstPtrROITemp;
+    dstPtrROITemp = dstPtrROI;
+
+    U multiplier = (U) (1.0 / 255.0);
+
+    Rpp32u imageDim = dstSize.height * dstSize.width * channel;
+
+    for (int i = 0; i < imageDim; i++)
+    {
+        *dstPtrROITemp = *dstPtrROITemp * multiplier;
+        dstPtrROITemp++;
+    }
+}
+
+template <typename T, typename U>
+inline RppStatus resize_kernel_host(T* srcPtr, RppiSize srcSize, U* dstPtr, RppiSize dstSize,
+                           RppiChnFormat chnFormat, Rpp32u channel)
+{
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        if (dstSize.height < 0 || dstSize.width < 0)
+        {
+            return RPP_ERROR;
+        }
+
+        Rpp32f hRatio = (((Rpp32f) (dstSize.height - 1)) / ((Rpp32f) (srcSize.height - 1)));
+        Rpp32f wRatio = (((Rpp32f) (dstSize.width - 1)) / ((Rpp32f) (srcSize.width - 1)));
+        Rpp32f srcLocationRow, srcLocationColumn, pixel;
+        Rpp32s srcLocationRowFloor, srcLocationColumnFloor;
+        T *srcPtrTemp, *srcPtrTopRow, *srcPtrBottomRow;
+        U *dstPtrTemp;
+        srcPtrTemp = srcPtr;
+        dstPtrTemp = dstPtr;
+
+        if ((typeid(Rpp16f) == typeid(T)) || (typeid(Rpp16f) == typeid(U)))
+        {
+            for (int c = 0; c < channel; c++)
+            {
+                for (int i = 0; i < dstSize.height; i++)
+                {
+                    srcLocationRow = ((Rpp32f) i) / hRatio;
+                    srcLocationRowFloor = (Rpp32s) RPPFLOOR(srcLocationRow);
+                    Rpp32f weightedHeight = srcLocationRow - srcLocationRowFloor;
+                    if (srcLocationRowFloor > (srcSize.height - 2))
+                    {
+                        srcLocationRowFloor = srcSize.height - 2;
+                    }
+
+                    srcPtrTopRow = srcPtrTemp + srcLocationRowFloor * srcSize.width;
+                    srcPtrBottomRow  = srcPtrTopRow + srcSize.width;
+
+                    for (int j = 0; j < dstSize.width; j++)
+                    {
+                        srcLocationColumn = ((Rpp32f) j) / wRatio;
+                        srcLocationColumnFloor = (Rpp32s) RPPFLOOR(srcLocationColumn);
+                        Rpp32f weightedWidth = srcLocationColumn - srcLocationColumnFloor;
+
+                        if (srcLocationColumnFloor > (srcSize.width - 2))
+                        {
+                            srcLocationColumnFloor = srcSize.width - 2;
+                        }
+                        pixel = ((*(srcPtrTopRow + srcLocationColumnFloor)) * (1 - weightedHeight) * (1 - weightedWidth))
+                                + ((*(srcPtrTopRow + srcLocationColumnFloor + 1)) * (1 - weightedHeight) * (weightedWidth))
+                                + ((*(srcPtrBottomRow + srcLocationColumnFloor)) * (weightedHeight) * (1 - weightedWidth))
+                                + ((*(srcPtrBottomRow + srcLocationColumnFloor + 1)) * (weightedHeight) * (weightedWidth));
+
+                        *dstPtrTemp = (U) pixel;
+                        dstPtrTemp ++;
+                    }
+                }
+                srcPtrTemp += srcSize.height * srcSize.width;
+            }
+        }
+        else
+        {
+            for (int c = 0; c < channel; c++)
+            {
+                for (int i = 0; i < dstSize.height; i++)
+                {
+                    srcLocationRow = ((Rpp32f) i) / hRatio;
+                    srcLocationRowFloor = (Rpp32s) RPPFLOOR(srcLocationRow);
+                    Rpp32f weightedHeight = srcLocationRow - srcLocationRowFloor;
+                    if (srcLocationRowFloor > (srcSize.height - 2))
+                    {
+                        srcLocationRowFloor = srcSize.height - 2;
+                    }
+
+                    srcPtrTopRow = srcPtrTemp + srcLocationRowFloor * srcSize.width;
+                    srcPtrBottomRow  = srcPtrTopRow + srcSize.width;
+#pragma omp simd
+                    for (int j = 0; j < dstSize.width; j++)
+                    {
+                        srcLocationColumn = ((Rpp32f) j) / wRatio;
+                        srcLocationColumnFloor = (Rpp32s) RPPFLOOR(srcLocationColumn);
+                        Rpp32f weightedWidth = srcLocationColumn - srcLocationColumnFloor;
+
+                        if (srcLocationColumnFloor > (srcSize.width - 2))
+                        {
+                            srcLocationColumnFloor = srcSize.width - 2;
+                        }
+                        pixel = ((*(srcPtrTopRow + srcLocationColumnFloor)) * (1 - weightedHeight) * (1 - weightedWidth))
+                                + ((*(srcPtrTopRow + srcLocationColumnFloor + 1)) * (1 - weightedHeight) * (weightedWidth))
+                                + ((*(srcPtrBottomRow + srcLocationColumnFloor)) * (weightedHeight) * (1 - weightedWidth))
+                                + ((*(srcPtrBottomRow + srcLocationColumnFloor + 1)) * (weightedHeight) * (weightedWidth));
+
+                        *dstPtrTemp = (U) pixel;
+                        dstPtrTemp ++;
+                    }
+                }
+                srcPtrTemp += srcSize.height * srcSize.width;
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        if (dstSize.height < 0 || dstSize.width < 0)
+        {
+            return RPP_ERROR;
+        }
+
+        Rpp32f hRatio = (((Rpp32f) (dstSize.height - 1)) / ((Rpp32f) (srcSize.height - 1)));
+        Rpp32f wRatio = (((Rpp32f) (dstSize.width - 1)) / ((Rpp32f) (srcSize.width - 1)));
+        Rpp32f srcLocationRow, srcLocationColumn, pixel;
+        Rpp32s srcLocationRowFloor, srcLocationColumnFloor;
+        T *srcPtrTemp;
+        U *dstPtrTemp;
+        srcPtrTemp = srcPtr;
+        dstPtrTemp = dstPtr;
+
+        Rpp32u heightLimit = srcSize.height - 2;
+        Rpp32u widthLimit = srcSize.width - 2;
+
+        Rpp32s elementsInRow = srcSize.width * channel;
+        for (int i = 0; i < dstSize.height; i++)
+        {
+            srcLocationRow = ((Rpp32f) i) / hRatio;
+            srcLocationRowFloor = (Rpp32s) RPPFLOOR(srcLocationRow);
+            Rpp32f weightedHeight = srcLocationRow - srcLocationRowFloor;
+
+            if (srcLocationRowFloor > heightLimit)
+            {
+                srcLocationRowFloor = heightLimit;
+            }
+
+            T *srcPtrTopRow, *srcPtrBottomRow;
+            srcPtrTopRow = srcPtrTemp + srcLocationRowFloor * elementsInRow;
+            srcPtrBottomRow  = srcPtrTopRow + elementsInRow;
+
+            Rpp32u bufferLength = dstSize.width;
+            Rpp32u alignedLength = (bufferLength / 4) * 4;
+
+            Rpp32u srcLocCF[4] = {0};
+            Rpp32f param1[4] = {0};
+            Rpp32f param2[4] = {0};
+            Rpp32f param3[4] = {0};
+            Rpp32f param4[4] = {0};
+
+            __m128 pWRatio = lsx_set1_f32(1.0 / wRatio);
+            __m128 p0, p2, p4, p5, p6, p7, pColFloor;
+            __m128 p1 = lsx_set1_f32(weightedHeight);
+            __m128 p3 = lsx_set1_f32(1 - weightedHeight);
+            __m128 pOne = lsx_set1_f32(1.0);
+            __m128i pxColFloor;
+
+            Rpp64u vectorLoopCount = 0;
+            for (; vectorLoopCount < alignedLength; vectorLoopCount+=4)
+            {
+                p0 = lsx_setr_f32(vectorLoopCount, vectorLoopCount + 1, vectorLoopCount + 2, vectorLoopCount + 3);
+                p0 = __lsx_vfmul_s(p0, pWRatio);
+                pColFloor = __lsx_vfrintrm_s(p0);
+                pxColFloor = __lsx_vftint_w_s(pColFloor);
+                p0 = __lsx_vfsub_s(p0, pColFloor);
+                p2  = __lsx_vfsub_s(pOne, p0);
+
+                p4 = __lsx_vfmul_s(p3, p2);
+                p5 = __lsx_vfmul_s(p3, p0);
+                p6 = __lsx_vfmul_s(p1, p2);
+                p7 = __lsx_vfmul_s(p1, p0);
+
+                __lsx_vst(pxColFloor, (__m128i*) srcLocCF, 0);
+                __lsx_vst(p4, param1, 0);
+                __lsx_vst(p5, param2, 0);
+                __lsx_vst(p6, param3, 0);
+                __lsx_vst(p7, param4, 0);
+
+                for (int pos = 0; pos < 4; pos++)
+                {
+                    if (srcLocCF[pos] > widthLimit)
+                    {
+                        srcLocCF[pos] = widthLimit;
+                    }
+                    srcLocCF[pos] *= channel;
+
+                    for (int c = 0; c < channel; c++)
+                    {
+                        *dstPtrTemp++ = (U) ((*(srcPtrTopRow + c + srcLocCF[pos])) * param1[pos])
+                                            + ((*(srcPtrTopRow + c + srcLocCF[pos] + channel)) * param2[pos])
+                                            + ((*(srcPtrBottomRow + c + srcLocCF[pos])) * param3[pos])
+                                            + ((*(srcPtrBottomRow + c + srcLocCF[pos] + channel)) * param4[pos]);
+                    }
+                }
+            }
+            for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+            {
+                srcLocationColumn = ((Rpp32f) vectorLoopCount) / wRatio;
+                srcLocationColumnFloor = (Rpp32s) RPPFLOOR(srcLocationColumn);
+                Rpp32f weightedWidth = srcLocationColumn - srcLocationColumnFloor;
+
+                if (srcLocationColumnFloor > (srcSize.width - 2))
+                {
+                    srcLocationColumnFloor = srcSize.width - 2;
+                }
+
+                Rpp32s srcLocColFloorChanneled = channel * srcLocationColumnFloor;
+
+                for (int c = 0; c < channel; c++)
+                {
+                    pixel = ((*(srcPtrTopRow + c + srcLocColFloorChanneled)) * (1 - weightedHeight) * (1 - weightedWidth))
+                            + ((*(srcPtrTopRow + c + srcLocColFloorChanneled + channel)) * (1 - weightedHeight) * (weightedWidth))
+                            + ((*(srcPtrBottomRow + c + srcLocColFloorChanneled)) * (weightedHeight) * (1 - weightedWidth))
+                            + ((*(srcPtrBottomRow + c + srcLocColFloorChanneled + channel)) * (weightedHeight) * (weightedWidth));
+
+                    *dstPtrTemp = (U) pixel;
+                    dstPtrTemp ++;
+                }
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+inline void resize_crop_kernel_host(T* srcPtr, RppiSize srcSize, T* dstPtr, RppiSize dstSize,
+                           Rpp32u x1, Rpp32u y1, Rpp32u x2, Rpp32u y2,
+                           RppiChnFormat chnFormat, Rpp32u channel)
+{
+    RppiSize srcSizeSubImage;
+    T *srcPtrSubImage;
+
+    compute_subimage_location_host(srcPtr, &srcPtrSubImage, srcSize, &srcSizeSubImage, x1, y1, x2, y2, chnFormat, channel);
+
+    T *srcPtrResize = (T*) calloc(channel * srcSizeSubImage.height * srcSizeSubImage.width, sizeof(T));
+
+    generate_crop_host(srcPtr, srcSize, srcPtrSubImage, srcSizeSubImage, srcPtrResize, chnFormat, channel);
+
+    resize_kernel_host(srcPtrResize, srcSizeSubImage, dstPtr, dstSize, chnFormat, channel);
+
+    free(srcPtrResize);
+}
+
+template<typename T>
+inline void erode_kernel_host(T* srcPtrWindow, T* dstPtrPixel, RppiSize srcSize,
+                                       Rpp32u kernelSize, Rpp32u remainingElementsInRow,
+                                       RppiChnFormat chnFormat, Rpp32u channel)
+{
+    T pixel;
+
+    T* srcPtrWindowTemp;
+    srcPtrWindowTemp = srcPtrWindow;
+    pixel = *srcPtrWindowTemp;
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        for (int m = 0; m < kernelSize; m++)
+        {
+            for (int n = 0; n < kernelSize; n++)
+            {
+                if (*srcPtrWindowTemp < pixel)
+                {
+                    pixel = *srcPtrWindowTemp;
+                }
+                srcPtrWindowTemp++;
+            }
+            srcPtrWindowTemp += remainingElementsInRow;
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        for (int m = 0; m < kernelSize; m++)
+        {
+            for (int n = 0; n < kernelSize; n++)
+            {
+                if (*srcPtrWindowTemp < pixel)
+                {
+                    pixel = *srcPtrWindowTemp;
+                }
+                srcPtrWindowTemp += channel;
+            }
+            srcPtrWindowTemp += remainingElementsInRow;
+        }
+    }
+    *dstPtrPixel = pixel;
+}
+
+template<typename T>
+inline void dilate_kernel_host(T* srcPtrWindow, T* dstPtrPixel, RppiSize srcSize,
+                                       Rpp32u kernelSize, Rpp32u remainingElementsInRow,
+                                       RppiChnFormat chnFormat, Rpp32u channel)
+{
+    T pixel;
+
+    T* srcPtrWindowTemp;
+    srcPtrWindowTemp = srcPtrWindow;
+    pixel = *srcPtrWindowTemp;
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        for (int m = 0; m < kernelSize; m++)
+        {
+            for (int n = 0; n < kernelSize; n++)
+            {
+                if (*srcPtrWindowTemp > pixel)
+                {
+                    pixel = *srcPtrWindowTemp;
+                }
+                srcPtrWindowTemp++;
+            }
+            srcPtrWindowTemp += remainingElementsInRow;
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        for (int m = 0; m < kernelSize; m++)
+        {
+            for (int n = 0; n < kernelSize; n++)
+            {
+                if (*srcPtrWindowTemp > pixel)
+                {
+                    pixel = *srcPtrWindowTemp;
+                }
+                srcPtrWindowTemp += channel;
+            }
+            srcPtrWindowTemp += remainingElementsInRow;
+        }
+    }
+    *dstPtrPixel = pixel;
+}
+
+template<typename T>
+inline void median_filter_kernel_host(T* srcPtrWindow, T* dstPtrPixel, RppiSize srcSize,
+                                       Rpp32u kernelSize, Rpp32u remainingElementsInRow,
+                                       RppiChnFormat chnFormat, Rpp32u channel)
+{
+    T *kernel = (T*)calloc(kernelSize * kernelSize, sizeof(T));
+    T *kernelTemp;
+    kernelTemp = kernel;
+
+    T* srcPtrWindowTemp;
+    srcPtrWindowTemp = srcPtrWindow;
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        for (int m = 0; m < kernelSize; m++)
+        {
+            for (int n = 0; n < kernelSize; n++)
+            {
+                *kernelTemp = *srcPtrWindowTemp;
+                srcPtrWindowTemp++;
+                kernelTemp++;
+            }
+            srcPtrWindowTemp += remainingElementsInRow;
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        for (int m = 0; m < kernelSize; m++)
+        {
+            for (int n = 0; n < kernelSize; n++)
+            {
+                *kernelTemp = *srcPtrWindowTemp;
+                srcPtrWindowTemp += channel;
+                kernelTemp++;
+            }
+            srcPtrWindowTemp += remainingElementsInRow;
+        }
+    }
+
+    std::sort(kernel, kernel + (kernelSize * kernelSize));
+
+    *dstPtrPixel = *(kernel + (((kernelSize * kernelSize) - 1) / 2));
+
+    free(kernel);
+}
+
+template<typename T>
+inline void local_binary_pattern_kernel_host(T* srcPtrWindow, T* dstPtrPixel, RppiSize srcSize,
+                                       Rpp32u remainingElementsInRow, T* centerPixelPtr,
+                                       RppiChnFormat chnFormat, Rpp32u channel)
+{
+    int pixel = (int) 0;
+    T *srcPtrWindowTemp;
+    srcPtrWindowTemp = srcPtrWindow;
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        if (*srcPtrWindowTemp - *centerPixelPtr >= 0)
+        {
+            pixel += pow(2, 0);
+        }
+        srcPtrWindowTemp++;
+
+        if (*srcPtrWindowTemp - *centerPixelPtr >= 0)
+        {
+            pixel += pow(2, 1);
+        }
+        srcPtrWindowTemp++;
+
+        if (*srcPtrWindowTemp - *centerPixelPtr >= 0)
+        {
+            pixel += pow(2, 2);
+        }
+        srcPtrWindowTemp++;
+        srcPtrWindowTemp += remainingElementsInRow;
+
+        if (*srcPtrWindowTemp - *centerPixelPtr >= 0)
+        {
+            pixel += pow(2, 7);
+        }
+        srcPtrWindowTemp += 2;
+
+        if (*srcPtrWindowTemp - *centerPixelPtr >= 0)
+        {
+            pixel += pow(2, 3);
+        }
+        srcPtrWindowTemp++;
+        srcPtrWindowTemp += remainingElementsInRow;
+
+        if (*srcPtrWindowTemp - *centerPixelPtr >= 0)
+        {
+            pixel += pow(2, 6);
+        }
+        srcPtrWindowTemp++;
+
+        if (*srcPtrWindowTemp - *centerPixelPtr >= 0)
+        {
+            pixel += pow(2, 5);
+        }
+        srcPtrWindowTemp++;
+
+        if (*srcPtrWindowTemp - *centerPixelPtr >= 0)
+        {
+            pixel += pow(2, 4);
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        if (*srcPtrWindowTemp - *centerPixelPtr >= 0)
+        {
+            pixel += pow(2, 0);
+        }
+        srcPtrWindowTemp += channel;
+
+        if (*srcPtrWindowTemp - *centerPixelPtr >= 0)
+        {
+            pixel += pow(2, 1);
+        }
+        srcPtrWindowTemp += channel;
+
+        if (*srcPtrWindowTemp - *centerPixelPtr >= 0)
+        {
+            pixel += pow(2, 2);
+        }
+        srcPtrWindowTemp += channel;
+        srcPtrWindowTemp += remainingElementsInRow;
+
+        if (*srcPtrWindowTemp - *centerPixelPtr >= 0)
+        {
+            pixel += pow(2, 7);
+        }
+        srcPtrWindowTemp += (2 * channel);
+
+        if (*srcPtrWindowTemp - *centerPixelPtr >= 0)
+        {
+            pixel += pow(2, 3);
+        }
+        srcPtrWindowTemp += channel;
+        srcPtrWindowTemp += remainingElementsInRow;
+
+        if (*srcPtrWindowTemp - *centerPixelPtr >= 0)
+        {
+            pixel += pow(2, 6);
+        }
+        srcPtrWindowTemp += channel;
+
+        if (*srcPtrWindowTemp - *centerPixelPtr >= 0)
+        {
+            pixel += pow(2, 5);
+        }
+        srcPtrWindowTemp += channel;
+
+        if (*srcPtrWindowTemp - *centerPixelPtr >= 0)
+        {
+            pixel += pow(2, 4);
+        }
+    }
+
+    *dstPtrPixel = (T) RPPPIXELCHECK(pixel);
+}
+
+template<typename T>
+inline void non_max_suppression_kernel_host(T* srcPtrWindow, T* dstPtrPixel, RppiSize srcSize,
+                                       Rpp32u kernelSize, Rpp32u remainingElementsInRow, T windowCenter,
+                                       RppiChnFormat chnFormat, Rpp32u channel)
+{
+    T pixel;
+
+    T* srcPtrWindowTemp;
+    srcPtrWindowTemp = srcPtrWindow;
+    pixel = *srcPtrWindowTemp;
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        for (int m = 0; m < kernelSize; m++)
+        {
+            for (int n = 0; n < kernelSize; n++)
+            {
+                if (*srcPtrWindowTemp > pixel)
+                {
+                    pixel = *srcPtrWindowTemp;
+                }
+                srcPtrWindowTemp++;
+            }
+            srcPtrWindowTemp += remainingElementsInRow;
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        for (int m = 0; m < kernelSize; m++)
+        {
+            for (int n = 0; n < kernelSize; n++)
+            {
+                if (*srcPtrWindowTemp > pixel)
+                {
+                    pixel = *srcPtrWindowTemp;
+                }
+                srcPtrWindowTemp += channel;
+            }
+            srcPtrWindowTemp += remainingElementsInRow;
+        }
+    }
+    if (windowCenter >= pixel)
+    {
+        *dstPtrPixel = windowCenter;
+    }
+    else
+    {
+        *dstPtrPixel = (T) 0;
+    }
+}
+
+template<typename T>
+inline void canny_non_max_suppression_kernel_host(T* dstPtrPixel, T windowCenter, T *position1Ptr, T *position2Ptr)
+{
+    if ((windowCenter >= *position1Ptr) && (windowCenter >= *position2Ptr))
+    {
+        *dstPtrPixel = windowCenter;
+    }
+    else
+    {
+        *dstPtrPixel = (T) 0;
+    }
+}
+
+template<typename T>
+inline void canny_hysterisis_edge_tracing_kernel_host(T* srcPtrWindow, T* dstPtrPixel, RppiSize srcSize,
+                                       Rpp32u kernelSize, Rpp32u remainingElementsInRow, T windowCenter, Rpp32u bound,
+                                       RppiChnFormat chnFormat, Rpp32u channel)
+{
+    T* srcPtrWindowTemp;
+    srcPtrWindowTemp = srcPtrWindow;
+
+    for (int m = 0; m < kernelSize; m++)
+    {
+        for (int n = 0; n < kernelSize; n++)
+        {
+            if (*srcPtrWindowTemp == (T) 255)
+            {
+                *dstPtrPixel = (T) 255;
+            }
+            srcPtrWindowTemp++;
+        }
+        srcPtrWindowTemp += remainingElementsInRow;
+    }
+    *dstPtrPixel = (T) 0;
+}
+
+template<typename T, typename U>
+inline void harris_corner_detector_kernel_host(T* srcPtrWindowX, T* srcPtrWindowY, U* dstPtrPixel, RppiSize srcSize,
+                                             Rpp32u kernelSize, Rpp32u remainingElementsInRow, Rpp32f kValue, Rpp32f threshold,
+                                             RppiChnFormat chnFormat, Rpp32u channel)
+{
+    Rpp32f pixel;
+
+    T *srcPtrWindowTempX, *srcPtrWindowTempY;
+    srcPtrWindowTempX = srcPtrWindowX;
+    srcPtrWindowTempY = srcPtrWindowY;
+
+    Rpp32f sumXX = 0, sumYY = 0, sumXY = 0;
+
+    for (int m = 0; m < kernelSize; m++)
+    {
+        for (int n = 0; n < kernelSize; n++)
+        {
+            Rpp32f valX = (Rpp32f) *srcPtrWindowTempX;
+            Rpp32f valY = (Rpp32f) *srcPtrWindowTempY;
+            sumXX += (valX * valX);
+            sumYY += (valY * valY);
+            sumXY += (valX * valY);
+
+            srcPtrWindowTempX++;
+            srcPtrWindowTempY++;
+        }
+        srcPtrWindowTempX += remainingElementsInRow;
+        srcPtrWindowTempY += remainingElementsInRow;
+    }
+    Rpp32f det = (sumXX * sumYY) - (sumXY * sumXY);
+    Rpp32f trace = sumXX + sumYY;
+    pixel = (det) - (kValue * trace * trace);
+
+    if (pixel > threshold)
+    {
+        *dstPtrPixel = (U) pixel;
+    }
+    else
+    {
+        *dstPtrPixel = (U) 0;
+    }
+}
+
+template<typename T>
+inline void harris_corner_set_maximum_kernel_host(T* dstPtrWindow, Rpp32u kernelSize, Rpp32u remainingElementsInRow,
+                                                  RppiChnFormat chnFormat, Rpp32u channel)
+{
+    T* dstPtrWindowTemp;
+    dstPtrWindowTemp = dstPtrWindow;
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        for (int m = 0; m < kernelSize; m++)
+        {
+            for (int n = 0; n < kernelSize; n++)
+            {
+                *dstPtrWindowTemp = (T) 255;
+                dstPtrWindowTemp++;
+            }
+            dstPtrWindowTemp += remainingElementsInRow;
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        for (int m = 0; m < kernelSize; m++)
+        {
+            for (int n = 0; n < kernelSize; n++)
+            {
+                *dstPtrWindowTemp = (T) 255;
+                dstPtrWindowTemp += channel;
+            }
+            dstPtrWindowTemp += remainingElementsInRow;
+        }
+    }
+}
+
+template<typename T>
+inline void harris_corner_set_minimum_kernel_host(T* dstPtrWindow, Rpp32u kernelSize, Rpp32u remainingElementsInRow,
+                                                  RppiChnFormat chnFormat, Rpp32u channel)
+{
+    T* dstPtrWindowTemp;
+    dstPtrWindowTemp = dstPtrWindow;
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        for (int m = 0; m < kernelSize; m++)
+        {
+            for (int n = 0; n < kernelSize; n++)
+            {
+                *dstPtrWindowTemp = (T) 0;
+                dstPtrWindowTemp++;
+            }
+            dstPtrWindowTemp += remainingElementsInRow;
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        for (int m = 0; m < kernelSize; m++)
+        {
+            for (int n = 0; n < kernelSize; n++)
+            {
+                *dstPtrWindowTemp = (T) 0;
+                dstPtrWindowTemp += channel;
+            }
+            dstPtrWindowTemp += remainingElementsInRow;
+        }
+    }
+}
+
+inline void tensor_index_exchange_kernel_host(Rpp32u *loopCount, Rpp32u *loopCountTransposed, Rpp32u tensorDimension, Rpp32u dimension1, Rpp32u dimension2)
+{
+    memcpy(loopCountTransposed, loopCount, tensorDimension * sizeof(Rpp32u));
+
+    loopCountTransposed[dimension2] = loopCount[dimension1];
+    loopCountTransposed[dimension1] = loopCount[dimension2];
+}
+
+template<typename T>
+inline void tensor_transpose_iterate_kernel_host(T* srcPtr, T* dstPtr,
+                                               Rpp32u tensorDimensionTemp, Rpp32u tensorDimension,
+                                               Rpp32u *tensorDimensionValues, Rpp32u *tensorDimensionValuesProduct,
+                                               Rpp32u *loopCount, Rpp32u *loopCountTransposed,
+                                               Rpp32u dimension1, Rpp32u dimension2)
+{
+    if (tensorDimensionTemp >= tensorDimension)
+    {
+        Rpp32u dstPtrLoc = 0;
+        for (int i = tensorDimension - 1; i > 0 ; i--)
+        {
+            dstPtrLoc = dstPtrLoc + (loopCount[i] * tensorDimensionValuesProduct[i - 1]);
+        }
+        dstPtrLoc += loopCount[0];
+
+        tensor_index_exchange_kernel_host(loopCount, loopCountTransposed, tensorDimension, dimension1, dimension2);
+
+        Rpp32u srcPtrLoc = 0;
+        for (int i = tensorDimension - 1; i > 0 ; i--)
+        {
+            srcPtrLoc = srcPtrLoc + (loopCountTransposed[i] * tensorDimensionValuesProduct[i - 1]);
+        }
+        srcPtrLoc += loopCountTransposed[0];
+
+        *(dstPtr + dstPtrLoc) = *(srcPtr + srcPtrLoc);
+    }
+    for (int i = 0; i < *(tensorDimensionValues + tensorDimensionTemp); i++)
+    {
+        *(loopCount + tensorDimensionTemp) = i;
+        tensor_transpose_iterate_kernel_host(srcPtr, dstPtr,
+                                             tensorDimensionTemp + 1, tensorDimension,
+                                             tensorDimensionValues, tensorDimensionValuesProduct,
+                                             loopCount, loopCountTransposed,
+                                             dimension1, dimension2);
+    }
+}
+
+template<typename T>
+inline void fast_corner_detector_kernel_host(T* srcPtrWindow, T* dstPtrPixel, RppiSize srcSize,
+                                           Rpp32u* bresenhamCirclePositions, T threshold, Rpp32u numOfPixels)
+{
+    T centerPixel = *(srcPtrWindow + (3 * srcSize.width) + 3);
+    T max = (T) (RPPPIXELCHECK((Rpp32s) centerPixel + (Rpp32s) threshold));
+    T min = (T) (RPPPIXELCHECK((Rpp32s) centerPixel - (Rpp32s) threshold));
+
+    // Find Bresenham Circle for the pixel
+
+    Rpp32u *bresenhamCirclePositionsTemp;
+    bresenhamCirclePositionsTemp = bresenhamCirclePositions;
+
+    T *bresenhamCircle = (T*) calloc(16, sizeof(T));
+    T *bresenhamCircleTemp;
+    bresenhamCircleTemp = bresenhamCircle;
+
+    T* bresenhamCircleOutput = (T*) calloc(16, sizeof(T));
+
+    for (int i = 0; i < 16; i++)
+    {
+        *bresenhamCircleTemp = *(srcPtrWindow + *bresenhamCirclePositionsTemp);
+        bresenhamCircleTemp++;
+        bresenhamCirclePositionsTemp++;
+    }
+
+    Rpp32u flag = 0;
+
+    *bresenhamCircleOutput = (T) RPPISLESSER(*bresenhamCircle, min);
+    *(bresenhamCircleOutput + 8) = (T) RPPISLESSER(*(bresenhamCircle + 8), min);
+
+    if (*bresenhamCircleOutput == 1)
+    {
+        *(bresenhamCircleOutput + 4) = (T) RPPISLESSER(*(bresenhamCircle + 4), min);
+        *(bresenhamCircleOutput + 12) = (T) RPPISLESSER(*(bresenhamCircle + 12), min);
+        if (*(bresenhamCircleOutput + 8) == 1)
+        {
+            if (*(bresenhamCircleOutput + 4) == 1 || *(bresenhamCircleOutput + 12) == 1)
+            {
+                flag = 1;
+            }
+        }
+        else if (*(bresenhamCircleOutput + 4) == 1 && *(bresenhamCircleOutput + 12) == 1)
+        {
+            flag = 1;
+        }
+    }
+    else if (*(bresenhamCircleOutput + 8) == 1)
+    {
+        *(bresenhamCircleOutput + 4) = (T) RPPISLESSER(*(bresenhamCircle + 4), min);
+        *(bresenhamCircleOutput + 12) = (T) RPPISLESSER(*(bresenhamCircle + 12), min);
+        if (*(bresenhamCircleOutput + 4) == 1 && *(bresenhamCircleOutput + 12) == 1)
+        {
+            flag = 1;
+        }
+    }
+    if (flag == 0)
+    {
+        *bresenhamCircleOutput = (T) RPPISGREATER(*bresenhamCircle, max);
+        *(bresenhamCircleOutput + 8) = (T) RPPISGREATER(*(bresenhamCircle + 8), max);
+
+        if (*bresenhamCircleOutput == 1)
+        {
+            *(bresenhamCircleOutput + 4) = (T) RPPISGREATER(*(bresenhamCircle + 4), max);
+            *(bresenhamCircleOutput + 12) = (T) RPPISGREATER(*(bresenhamCircle + 12), max);
+            if (*(bresenhamCircleOutput + 8) == 1)
+            {
+                if (*(bresenhamCircleOutput + 4) == 1 || *(bresenhamCircleOutput + 12) == 1)
+                {
+                    flag = 2;
+                }
+            }
+            else if (*(bresenhamCircleOutput + 4) == 1 && *(bresenhamCircleOutput + 12) == 1)
+            {
+                flag = 2;
+            }
+        }
+        else if (*(bresenhamCircleOutput + 8) == 1)
+        {
+            *(bresenhamCircleOutput + 4) = (T) RPPISGREATER(*(bresenhamCircle + 4), max);
+            *(bresenhamCircleOutput + 12) = (T) RPPISGREATER(*(bresenhamCircle + 12), max);
+            if (*(bresenhamCircleOutput + 4) == 1 && *(bresenhamCircleOutput + 12) == 1)
+            {
+                flag = 2;
+            }
+        }
+    }
+    if (flag == 0)
+    {
+        *dstPtrPixel = (T) 0;
+    }
+    else if (flag == 1)
+    {
+        *(bresenhamCircleOutput + 1) = (T) RPPISLESSER(*(bresenhamCircle + 1), min);
+        *(bresenhamCircleOutput + 2) = (T) RPPISLESSER(*(bresenhamCircle + 2), min);
+        *(bresenhamCircleOutput + 3) = (T) RPPISLESSER(*(bresenhamCircle + 3), min);
+        *(bresenhamCircleOutput + 5) = (T) RPPISLESSER(*(bresenhamCircle + 5), min);
+        *(bresenhamCircleOutput + 6) = (T) RPPISLESSER(*(bresenhamCircle + 6), min);
+        *(bresenhamCircleOutput + 7) = (T) RPPISLESSER(*(bresenhamCircle + 7), min);
+        *(bresenhamCircleOutput + 9) = (T) RPPISLESSER(*(bresenhamCircle + 9), min);
+        *(bresenhamCircleOutput + 10) = (T) RPPISLESSER(*(bresenhamCircle + 10), min);
+        *(bresenhamCircleOutput + 11) = (T) RPPISLESSER(*(bresenhamCircle + 11), min);
+        *(bresenhamCircleOutput + 13) = (T) RPPISLESSER(*(bresenhamCircle + 13), min);
+        *(bresenhamCircleOutput + 14) = (T) RPPISLESSER(*(bresenhamCircle + 14), min);
+        *(bresenhamCircleOutput + 15) = (T) RPPISLESSER(*(bresenhamCircle + 15), min);
+    }
+    else if (flag == 2)
+    {
+        *(bresenhamCircleOutput + 1) = (T) RPPISGREATER(*(bresenhamCircle + 1), max);
+        *(bresenhamCircleOutput + 2) = (T) RPPISGREATER(*(bresenhamCircle + 2), max);
+        *(bresenhamCircleOutput + 3) = (T) RPPISGREATER(*(bresenhamCircle + 3), max);
+        *(bresenhamCircleOutput + 5) = (T) RPPISGREATER(*(bresenhamCircle + 5), max);
+        *(bresenhamCircleOutput + 6) = (T) RPPISGREATER(*(bresenhamCircle + 6), max);
+        *(bresenhamCircleOutput + 7) = (T) RPPISGREATER(*(bresenhamCircle + 7), max);
+        *(bresenhamCircleOutput + 9) = (T) RPPISGREATER(*(bresenhamCircle + 9), max);
+        *(bresenhamCircleOutput + 10) = (T) RPPISGREATER(*(bresenhamCircle + 10), max);
+        *(bresenhamCircleOutput + 11) = (T) RPPISGREATER(*(bresenhamCircle + 11), max);
+        *(bresenhamCircleOutput + 13) = (T) RPPISGREATER(*(bresenhamCircle + 13), max);
+        *(bresenhamCircleOutput + 14) = (T) RPPISGREATER(*(bresenhamCircle + 14), max);
+        *(bresenhamCircleOutput + 15) = (T) RPPISGREATER(*(bresenhamCircle + 15), max);
+    }
+
+    // Find maximum contiguous pixels in bresenhamCircleOutput with value 1
+
+    Rpp32u count = 0;
+    Rpp32u maxLength = 0;
+
+    for (int i = 0; i < 32; i++)
+    {
+        if (*(bresenhamCircleOutput + (i % 16)) == 0)
+        {
+            count = 0;
+            if (i >= 16)
+            {
+                break;
+            }
+        }
+        else
+        {
+            count++;
+            maxLength = RPPMAX2(maxLength, count);
+        }
+    }
+
+    // Corner Classification
+
+    if (maxLength >= numOfPixels)
+    {
+        *dstPtrPixel = (T) 255;
+    }
+    else
+    {
+        *dstPtrPixel = (T) 0;
+    }
+
+    free(bresenhamCircle);
+    free(bresenhamCircleOutput);
+}
+
+template<typename T, typename U>
+inline void fast_corner_detector_score_function_kernel_host(T* srcPtrWindow, U* dstPtrPixel, RppiSize srcSize,
+                                                          Rpp32u* bresenhamCirclePositions, U centerPixel)
+{
+    U* bresenhamCircle = (U*) calloc(16, sizeof(U));
+    U *bresenhamCircleTemp;
+    bresenhamCircleTemp = bresenhamCircle;
+    Rpp32u *bresenhamCirclePositionsTemp;
+    bresenhamCirclePositionsTemp = bresenhamCirclePositions;
+
+    for (int i = 0; i < 16; i++)
+    {
+        *bresenhamCircleTemp = (U) *(srcPtrWindow + *bresenhamCirclePositionsTemp);
+        bresenhamCircleTemp++;
+        bresenhamCirclePositionsTemp++;
+    }
+
+    U score = 0;
+    bresenhamCircleTemp = bresenhamCircle;
+    for (int i = 0; i < 16; i++)
+    {
+        score += RPPABS(centerPixel - *bresenhamCircleTemp);
+        bresenhamCircleTemp++;
+    }
+
+    *dstPtrPixel = score;
+
+    free(bresenhamCircle);
+}
+
+template<typename T, typename U, typename V>
+inline void hog_single_channel_gradient_computations_kernel_host(T* srcPtr, RppiSize srcSize, U* gradientX, U* gradientY, U* gradientMagnitude, V* gradientDirection,
+                                                               Rpp32f* gradientKernel, RppiSize rppiGradientKernelSizeX, RppiSize rppiGradientKernelSizeY)
+{
+    custom_convolve_image_host(srcPtr, srcSize, gradientX, gradientKernel, rppiGradientKernelSizeX, RPPI_CHN_PLANAR, 1);
+    custom_convolve_image_host(srcPtr, srcSize, gradientY, gradientKernel, rppiGradientKernelSizeY, RPPI_CHN_PLANAR, 1);
+    compute_magnitude_host(gradientX, gradientY, srcSize, gradientMagnitude, RPPI_CHN_PLANAR, 1);
+    compute_gradient_direction_host(gradientX, gradientY, srcSize, gradientDirection, RPPI_CHN_PLANAR, 1);
+}
+
+template<typename T, typename U, typename V>
+inline void hog_three_channel_gradient_computations_kernel_host(T* srcPtr, T* srcPtrSingleChannel, RppiSize srcSize,
+                                                              U* gradientX0, U* gradientY0, U* gradientX1, U* gradientY1, U* gradientX2, U* gradientY2,
+                                                              U* gradientX, U* gradientY,
+                                                              U* gradientMagnitude, V* gradientDirection,
+                                                              Rpp32f* gradientKernel, RppiSize rppiGradientKernelSizeX, RppiSize rppiGradientKernelSizeY,
+                                                              RppiChnFormat chnFormat, Rpp32u channel)
+{
+    Rpp32u imageDim = srcSize.height * srcSize.width;
+
+    compute_channel_extract_host(srcPtr, srcSize, srcPtrSingleChannel, 0, chnFormat, channel);
+    custom_convolve_image_host(srcPtrSingleChannel, srcSize, gradientX0, gradientKernel, rppiGradientKernelSizeX, RPPI_CHN_PLANAR, 1);
+    custom_convolve_image_host(srcPtrSingleChannel, srcSize, gradientY0, gradientKernel, rppiGradientKernelSizeY, RPPI_CHN_PLANAR, 1);
+
+    compute_channel_extract_host(srcPtr, srcSize, srcPtrSingleChannel, 1, chnFormat, channel);
+    custom_convolve_image_host(srcPtrSingleChannel, srcSize, gradientX1, gradientKernel, rppiGradientKernelSizeX, RPPI_CHN_PLANAR, 1);
+    custom_convolve_image_host(srcPtrSingleChannel, srcSize, gradientY1, gradientKernel, rppiGradientKernelSizeY, RPPI_CHN_PLANAR, 1);
+
+    compute_channel_extract_host(srcPtr, srcSize, srcPtrSingleChannel, 2, chnFormat, channel);
+    custom_convolve_image_host(srcPtrSingleChannel, srcSize, gradientX2, gradientKernel, rppiGradientKernelSizeX, RPPI_CHN_PLANAR, 1);
+    custom_convolve_image_host(srcPtrSingleChannel, srcSize, gradientY2, gradientKernel, rppiGradientKernelSizeY, RPPI_CHN_PLANAR, 1);
+
+    compute_max_host(gradientX0, gradientX1, srcSize, gradientX, channel);
+    memcpy(gradientX0, gradientX, imageDim * sizeof(Rpp32s));
+    compute_max_host(gradientX0, gradientX2, srcSize, gradientX, channel);
+
+    compute_max_host(gradientY0, gradientY1, srcSize, gradientY, channel);
+    memcpy(gradientY0, gradientY, imageDim * sizeof(Rpp32s));
+    compute_max_host(gradientY0, gradientY2, srcSize, gradientY, channel);
+
+    compute_magnitude_host(gradientX, gradientY, srcSize, gradientMagnitude, RPPI_CHN_PLANAR, 1);
+    compute_gradient_direction_host(gradientX, gradientY, srcSize, gradientDirection, RPPI_CHN_PLANAR, 1);
+}
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+// Convolution Functions
+
+template<typename T>
+inline void convolve_image_host_batch(T* srcPtrImage, RppiSize srcSize, RppiSize srcSizeMax, T* dstPtrImage,
+                                           T* srcPtrBoundedROI, RppiSize srcSizeBoundedROI,
+                                           Rpp32f* kernel, RppiSize kernelSize,
+                                           Rpp32f x1, Rpp32f y1, Rpp32f x2, Rpp32f y2,
+                                           RppiChnFormat chnFormat, Rpp32u channel)
+{
+    Rpp32u imageDimMax = srcSizeMax.height * srcSizeMax.width;
+    Rpp32u imageDimROI = srcSizeBoundedROI.height * srcSizeBoundedROI.width;
+
+    T maxVal = (T)(std::numeric_limits<T>::max());
+    T minVal = (T)(std::numeric_limits<T>::min());
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        Rpp32u remainingElementsInRow = srcSizeBoundedROI.width - kernelSize.width;
+
+        for(int c = 0; c < channel; c++)
+        {
+            T *srcPtrBoundedROIChannel, *srcPtrChannel, *dstPtrChannel;
+            srcPtrBoundedROIChannel = srcPtrBoundedROI + (c * imageDimROI);
+            srcPtrChannel = srcPtrImage + (c * imageDimMax);
+            dstPtrChannel = dstPtrImage + (c * imageDimMax);
+
+            Rpp32u roiRowCount = 0;
+
+
+            for(int i = 0; i < srcSize.height; i++)
+            {
+                T *srcPtrTemp, *dstPtrTemp;
+                srcPtrTemp = srcPtrChannel + (i * srcSizeMax.width);
+                dstPtrTemp = dstPtrChannel + (i * srcSizeMax.width);
+
+                if (!((y1 <= i) && (i <= y2)))
+                {
+                    memcpy(dstPtrTemp, srcPtrTemp, srcSize.width * sizeof(T));
+
+                    dstPtrTemp += srcSizeMax.width;
+                    srcPtrTemp += srcSizeMax.width;
+                }
+                else
+                {
+                    T *srcPtrWindow;
+                    srcPtrWindow = srcPtrBoundedROIChannel + (roiRowCount * srcSizeBoundedROI.width);
+                    for(int j = 0; j < srcSize.width; j++)
+                    {
+                        if((x1 <= j) && (j <= x2 ))
+                        {
+                            convolution_kernel_host(srcPtrWindow, dstPtrTemp, srcSize,
+                                                    kernel, kernelSize, remainingElementsInRow, maxVal, minVal,
+                                                    chnFormat, channel);
+
+                            srcPtrWindow++;
+                            srcPtrTemp++;
+                            dstPtrTemp++;
+                        }
+                        else
+                        {
+                            *dstPtrTemp = *srcPtrTemp;
+
+                            srcPtrTemp++;
+                            dstPtrTemp++;
+                        }
+                    }
+                    #pragma omp critical
+                    roiRowCount++;
+                }
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        Rpp32u remainingElementsInRow = (srcSizeBoundedROI.width - kernelSize.width) * channel;
+        Rpp32u elementsInRowBoundedROI = channel * srcSizeBoundedROI.width;
+        Rpp32u elementsInRowMax = channel * srcSizeMax.width;
+        Rpp32u elementsInRow = channel * srcSize.width;
+
+        Rpp32u roiRowCount = 0;
+
+
+        for(int i = 0; i < srcSize.height; i++)
+        {
+            T *srcPtrTemp, *dstPtrTemp;
+            srcPtrTemp = srcPtrImage + (i * elementsInRowMax);
+            dstPtrTemp = dstPtrImage + (i * elementsInRowMax);
+
+            if (!((y1 <= i) && (i <= y2)))
+            {
+                memcpy(dstPtrTemp, srcPtrTemp, elementsInRow * sizeof(T));
+
+                dstPtrTemp += elementsInRowMax;
+                srcPtrTemp += elementsInRowMax;
+            }
+            else
+            {
+                T *srcPtrWindow;
+                srcPtrWindow = srcPtrBoundedROI + (roiRowCount * elementsInRowBoundedROI);
+                for(int j = 0; j < srcSize.width; j++)
+                {
+                    if (!((x1 <= j) && (j <= x2 )))
+                    {
+                        memcpy(dstPtrTemp, srcPtrTemp, channel * sizeof(T));
+
+                        dstPtrTemp += channel;
+                        srcPtrTemp += channel;
+                    }
+                    else
+                    {
+                        for(int c = 0; c < channel; c++)
+                        {
+
+                            convolution_kernel_host(srcPtrWindow, dstPtrTemp, srcSize,
+                                                    kernel, kernelSize, remainingElementsInRow, maxVal, minVal,
+                                                    chnFormat, channel);
+
+                            srcPtrWindow++;
+                            srcPtrTemp++;
+                            dstPtrTemp++;
+                        }
+                    }
+                }
+                #pragma omp critical
+                roiRowCount++;
+            }
+        }
+    }
+}
+
+template<typename T, typename U>
+inline void convolve_image_host(T* srcPtrMod, RppiSize srcSizeMod, U* dstPtr, RppiSize srcSize,
+                        Rpp32f* kernel, RppiSize kernelSize,
+                        RppiChnFormat chnFormat, Rpp32u channel)
+{
+    T *srcPtrWindow;
+    U *dstPtrTemp;
+    srcPtrWindow = srcPtrMod;
+    dstPtrTemp = dstPtr;
+
+    U maxVal = (U)(std::numeric_limits<U>::max());
+    U minVal = (U)(std::numeric_limits<U>::min());
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        Rpp32u remainingElementsInRow = srcSizeMod.width - kernelSize.width;
+
+        for (int c = 0; c < channel; c++)
+        {
+            for (int i = 0; i < srcSize.height; i++)
+            {
+                for (int j = 0; j < srcSize.width; j++)
+                {
+                    convolution_kernel_host(srcPtrWindow, dstPtrTemp, srcSize,
+                                                 kernel, kernelSize, remainingElementsInRow, maxVal, minVal,
+                                                 chnFormat, channel);
+                    srcPtrWindow++;
+                    dstPtrTemp++;
+                }
+                srcPtrWindow += (kernelSize.width - 1);
+            }
+            srcPtrWindow += ((kernelSize.height - 1) * srcSizeMod.width);
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        Rpp32u remainingElementsInRow = (srcSizeMod.width - kernelSize.width) * channel;
+
+        for (int i = 0; i < srcSize.height; i++)
+        {
+            for (int j = 0; j < srcSize.width; j++)
+            {
+                for (int c = 0; c < channel; c++)
+                {
+                    convolution_kernel_host(srcPtrWindow, dstPtrTemp, srcSize,
+                                                 kernel, kernelSize, remainingElementsInRow, maxVal, minVal,
+                                                 chnFormat, channel);
+                    srcPtrWindow++;
+                    dstPtrTemp++;
+                }
+            }
+            srcPtrWindow += ((kernelSize.width - 1) * channel);
+        }
+    }
+}
+
+template<typename T>
+inline void convolve_subimage_host(T* srcPtrMod, RppiSize srcSizeMod, T* dstPtr, RppiSize srcSizeSubImage, RppiSize srcSize,
+                        Rpp32f* kernel, RppiSize kernelSize,
+                        RppiChnFormat chnFormat, Rpp32u channel)
+{
+    int widthDiffPlanar = srcSize.width - srcSizeSubImage.width;
+    int widthDiffPacked = (srcSize.width - srcSizeSubImage.width) * channel;
+
+    T *srcPtrWindow, *dstPtrTemp;
+
+    T maxVal = (T)(std::numeric_limits<T>::max());
+    T minVal = (T)(std::numeric_limits<T>::min());
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        Rpp32u remainingElementsInRow = srcSize.width - kernelSize.width;
+
+        for (int c = 0; c < channel; c++)
+        {
+            srcPtrWindow = srcPtrMod + (c * srcSize.height * srcSize.width);
+            dstPtrTemp = dstPtr + (c * srcSize.height * srcSize.width);
+            for (int i = 0; i < srcSizeSubImage.height; i++)
+            {
+                for (int j = 0; j < srcSizeSubImage.width; j++)
+                {
+                    convolution_kernel_host(srcPtrWindow, dstPtrTemp, srcSize,
+                                                 kernel, kernelSize, remainingElementsInRow, maxVal, minVal,
+                                                 chnFormat, channel);
+                    srcPtrWindow++;
+                    dstPtrTemp++;
+                }
+                srcPtrWindow += widthDiffPlanar;
+                dstPtrTemp += widthDiffPlanar;
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        Rpp32u remainingElementsInRow = (srcSize.width - kernelSize.width) * channel;
+
+        srcPtrWindow = srcPtrMod;
+        dstPtrTemp = dstPtr;
+        for (int i = 0; i < srcSizeSubImage.height; i++)
+        {
+            for (int j = 0; j < srcSizeSubImage.width; j++)
+            {
+                for (int c = 0; c < channel; c++)
+                {
+                    convolution_kernel_host(srcPtrWindow, dstPtrTemp, srcSize,
+                                                 kernel, kernelSize, remainingElementsInRow, maxVal, minVal,
+                                                 chnFormat, channel);
+                    srcPtrWindow++;
+                    dstPtrTemp++;
+                }
+            }
+            srcPtrWindow += widthDiffPacked;
+            dstPtrTemp += widthDiffPacked;
+        }
+    }
+}
+
+template <typename T, typename U>
+inline RppStatus custom_convolve_image_host(T* srcPtr, RppiSize srcSize, U* dstPtr,
+                                  Rpp32f *kernel, RppiSize rppiKernelSize,
+                                  RppiChnFormat chnFormat, Rpp32u channel)
+{
+    if (rppiKernelSize.height % 2 == 0 || rppiKernelSize.width % 2 == 0)
+    {
+        return RPP_ERROR;
+    }
+
+    int boundY = ((rppiKernelSize.height - 1) / 2);
+    int boundX = ((rppiKernelSize.width - 1) / 2);
+
+    RppiSize srcSizeMod1, srcSizeMod2;
+
+    srcSizeMod1.height = srcSize.height + boundY;
+    srcSizeMod1.width = srcSize.width + boundX;
+    T *srcPtrMod1 = (T *)calloc(srcSizeMod1.height * srcSizeMod1.width * channel, sizeof(T));
+    generate_corner_padded_image_host(srcPtr, srcSize, srcPtrMod1, srcSizeMod1, 1, chnFormat, channel);
+
+    srcSizeMod2.height = srcSizeMod1.height + boundY;
+    srcSizeMod2.width = srcSizeMod1.width + boundX;
+    T *srcPtrMod2 = (T *)calloc(srcSizeMod2.height * srcSizeMod2.width * channel, sizeof(T));
+    generate_corner_padded_image_host(srcPtrMod1, srcSizeMod1, srcPtrMod2, srcSizeMod2, 4, chnFormat, channel);
+
+    convolve_image_host(srcPtrMod2, srcSizeMod2, dstPtr, srcSize, kernel, rppiKernelSize, chnFormat, channel);
+
+    free(srcPtrMod1);
+    free(srcPtrMod2);
+
+    return RPP_SUCCESS;
+}
+
+// Compute Functions for RPP Tensor API
+
+inline void compute_multiply_16_host(__m256 *p, __m256 *pMulParam)
+{
+    p[0] = __lasx_xvfmul_s(p[0], pMulParam[0]);    // multiply adjustment
+    p[1] = __lasx_xvfmul_s(p[1], pMulParam[0]);    // multiply adjustment
+}
+
+inline void compute_subtract_16_host(__m256 *p, __m256 *pSubtractParam)
+{
+    p[0] = __lasx_xvfsub_s(p[0], pSubtractParam[0]);    // subtract adjustment
+    p[1] = __lasx_xvfsub_s(p[1], pSubtractParam[0]);    // subtract adjustment
+}
+
+inline void compute_add_16_host(__m256 *p, __m256 *pAddParam)
+{
+    p[0] = __lasx_xvfadd_s(p[0], pAddParam[0]);    // add adjustment
+    p[1] = __lasx_xvfadd_s(p[1], pAddParam[0]);    // add adjustment
+}
+
+inline void compute_rmn_24_host(__m256 *p, __m256 *pRMNParams)
+{
+    p[0] = __lasx_xvfmul_s(__lasx_xvfsub_s(p[0], pRMNParams[0]), pRMNParams[1]);
+    p[1] = __lasx_xvfmul_s(__lasx_xvfsub_s(p[1], pRMNParams[2]), pRMNParams[3]);
+    p[2] = __lasx_xvfmul_s(__lasx_xvfsub_s(p[2], pRMNParams[4]), pRMNParams[5]);
+}
+
+inline void compute_rmn_8_host(__m256 *p, __m256 *pRMNParams)
+{
+    p[0] = __lasx_xvfmul_s(__lasx_xvfsub_s(p[0], pRMNParams[0]), pRMNParams[1]);
+}
+
+inline void compute_color_48_to_greyscale_16_host(__m256 *p, __m256 *pChannelWeights)
+{
+    p[0] = __lasx_xvfmadd_s(p[0], pChannelWeights[0], __lasx_xvfmadd_s(p[2], pChannelWeights[1], __lasx_xvfmul_s(p[4], pChannelWeights[2])));
+    p[1] = __lasx_xvfmadd_s(p[1], pChannelWeights[0], __lasx_xvfmadd_s(p[3], pChannelWeights[1], __lasx_xvfmul_s(p[5], pChannelWeights[2])));
+}
+
+inline void compute_color_48_to_greyscale_16_host(__m128 *p, __m128 *pChannelWeights)
+{
+    p[0] = __lsx_vfmadd_s(p[0], pChannelWeights[0], __lsx_vfmadd_s(p[4], pChannelWeights[1], __lsx_vfmul_s(p[8], pChannelWeights[2])));
+    p[1] = __lsx_vfmadd_s(p[1], pChannelWeights[0], __lsx_vfmadd_s(p[5], pChannelWeights[1], __lsx_vfmul_s(p[9], pChannelWeights[2])));
+    p[2] = __lsx_vfmadd_s(p[2], pChannelWeights[0], __lsx_vfmadd_s(p[6], pChannelWeights[1], __lsx_vfmul_s(p[10], pChannelWeights[2])));
+    p[3] = __lsx_vfmadd_s(p[3], pChannelWeights[0], __lsx_vfmadd_s(p[7], pChannelWeights[1], __lsx_vfmul_s(p[11], pChannelWeights[2])));
+}
+
+inline void compute_color_24_to_greyscale_8_host(__m256 *p, __m256 *pChannelWeights)
+{
+    p[0] = __lasx_xvfmadd_s(p[0], pChannelWeights[0], __lasx_xvfmadd_s(p[1], pChannelWeights[1], __lasx_xvfmul_s(p[2], pChannelWeights[2])));
+}
+
+inline void compute_color_12_to_greyscale_4_host(__m128 *p, __m128 *pChannelWeights)
+{
+    p[0] = __lsx_vfmadd_s(p[0], pChannelWeights[0], __lsx_vfmadd_s(p[1], pChannelWeights[1], __lsx_vfmul_s(p[2], pChannelWeights[2])));
+}
+
+inline void compute_contrast_48_host(__m256 *p, __m256 *pContrastParams)
+{
+    p[0] = __lasx_xvfmadd_s(__lasx_xvfsub_s(p[0], pContrastParams[1]), pContrastParams[0], pContrastParams[1]);    // contrast adjustment
+    p[1] = __lasx_xvfmadd_s(__lasx_xvfsub_s(p[1], pContrastParams[1]), pContrastParams[0], pContrastParams[1]);    // contrast adjustment
+    p[2] = __lasx_xvfmadd_s(__lasx_xvfsub_s(p[2], pContrastParams[1]), pContrastParams[0], pContrastParams[1]);    // contrast adjustment
+    p[3] = __lasx_xvfmadd_s(__lasx_xvfsub_s(p[3], pContrastParams[1]), pContrastParams[0], pContrastParams[1]);    // contrast adjustment
+    p[4] = __lasx_xvfmadd_s(__lasx_xvfsub_s(p[4], pContrastParams[1]), pContrastParams[0], pContrastParams[1]);    // contrast adjustment
+    p[5] = __lasx_xvfmadd_s(__lasx_xvfsub_s(p[5], pContrastParams[1]), pContrastParams[0], pContrastParams[1]);    // contrast adjustment
+}
+
+inline void compute_contrast_24_host(__m256 *p, __m256 *pContrastParams)
+{
+    p[0] = __lasx_xvfmadd_s(__lasx_xvfsub_s(p[0], pContrastParams[1]), pContrastParams[0], pContrastParams[1]);    // contrast adjustment
+    p[1] = __lasx_xvfmadd_s(__lasx_xvfsub_s(p[1], pContrastParams[1]), pContrastParams[0], pContrastParams[1]);    // contrast adjustment
+    p[2] = __lasx_xvfmadd_s(__lasx_xvfsub_s(p[2], pContrastParams[1]), pContrastParams[0], pContrastParams[1]);    // contrast adjustment
+}
+
+inline void compute_contrast_16_host(__m256 *p, __m256 *pContrastParams)
+{
+    p[0] = __lasx_xvfmadd_s(__lasx_xvfsub_s(p[0], pContrastParams[1]), pContrastParams[0], pContrastParams[1]);    // contrast adjustment
+    p[1] = __lasx_xvfmadd_s(__lasx_xvfsub_s(p[1], pContrastParams[1]), pContrastParams[0], pContrastParams[1]);    // contrast adjustment
+}
+
+inline void compute_contrast_8_host(__m256 *p, __m256 *pContrastParams)
+{
+    p[0] = __lasx_xvfmadd_s(__lasx_xvfsub_s(p[0], pContrastParams[1]), pContrastParams[0], pContrastParams[1]);    // contrast adjustment
+}
+
+inline void compute_brightness_48_host(__m256 *p, __m256 *pBrightnessParams)
+{
+    p[0] = __lasx_xvfmadd_s(p[0], pBrightnessParams[0], pBrightnessParams[1]);    // brightness adjustment
+    p[1] = __lasx_xvfmadd_s(p[1], pBrightnessParams[0], pBrightnessParams[1]);    // brightness adjustment
+    p[2] = __lasx_xvfmadd_s(p[2], pBrightnessParams[0], pBrightnessParams[1]);    // brightness adjustment
+    p[3] = __lasx_xvfmadd_s(p[3], pBrightnessParams[0], pBrightnessParams[1]);    // brightness adjustment
+    p[4] = __lasx_xvfmadd_s(p[4], pBrightnessParams[0], pBrightnessParams[1]);    // brightness adjustment
+    p[5] = __lasx_xvfmadd_s(p[5], pBrightnessParams[0], pBrightnessParams[1]);    // brightness adjustment
+}
+
+inline void compute_brightness_48_host(__m128 *p, __m128 *pBrightnessParams)
+{
+    p[0] = __lsx_vfmadd_s(p[0], pBrightnessParams[0], pBrightnessParams[1]);    // brightness adjustment
+    p[1] = __lsx_vfmadd_s(p[1], pBrightnessParams[0], pBrightnessParams[1]);    // brightness adjustment
+    p[2] = __lsx_vfmadd_s(p[2], pBrightnessParams[0], pBrightnessParams[1]);    // brightness adjustment
+    p[3] = __lsx_vfmadd_s(p[3], pBrightnessParams[0], pBrightnessParams[1]);    // brightness adjustment
+    p[4] = __lsx_vfmadd_s(p[4], pBrightnessParams[0], pBrightnessParams[1]);    // brightness adjustment
+    p[5] = __lsx_vfmadd_s(p[5], pBrightnessParams[0], pBrightnessParams[1]);    // brightness adjustment
+    p[6] = __lsx_vfmadd_s(p[6], pBrightnessParams[0], pBrightnessParams[1]);    // brightness adjustment
+    p[7] = __lsx_vfmadd_s(p[7], pBrightnessParams[0], pBrightnessParams[1]);    // brightness adjustment
+    p[8] = __lsx_vfmadd_s(p[8], pBrightnessParams[0], pBrightnessParams[1]);    // brightness adjustment
+    p[9] = __lsx_vfmadd_s(p[9], pBrightnessParams[0], pBrightnessParams[1]);    // brightness adjustment
+    p[10] = __lsx_vfmadd_s(p[10], pBrightnessParams[0], pBrightnessParams[1]);    // brightness adjustment
+    p[11] = __lsx_vfmadd_s(p[11], pBrightnessParams[0], pBrightnessParams[1]);    // brightness adjustment
+}
+
+inline void compute_brightness_24_host(__m256 *p, __m256 *pBrightnessParams)
+{
+    p[0] = __lasx_xvfmadd_s(p[0], pBrightnessParams[0], pBrightnessParams[1]);    // brightness adjustment
+    p[1] = __lasx_xvfmadd_s(p[1], pBrightnessParams[0], pBrightnessParams[1]);    // brightness adjustment
+    p[2] = __lasx_xvfmadd_s(p[2], pBrightnessParams[0], pBrightnessParams[1]);    // brightness adjustment
+}
+
+inline void compute_brightness_24_host(__m128 *p, __m128 *pBrightnessParams)
+{
+    p[0] = __lsx_vfmadd_s(p[0], pBrightnessParams[0], pBrightnessParams[1]);    // brightness adjustment
+    p[1] = __lsx_vfmadd_s(p[1], pBrightnessParams[0], pBrightnessParams[1]);    // brightness adjustment
+    p[2] = __lsx_vfmadd_s(p[2], pBrightnessParams[0], pBrightnessParams[1]);    // brightness adjustment
+    p[3] = __lsx_vfmadd_s(p[3], pBrightnessParams[0], pBrightnessParams[1]);    // brightness adjustment
+    p[4] = __lsx_vfmadd_s(p[4], pBrightnessParams[0], pBrightnessParams[1]);    // brightness adjustment
+    p[5] = __lsx_vfmadd_s(p[5], pBrightnessParams[0], pBrightnessParams[1]);    // brightness adjustment
+}
+
+inline void compute_brightness_16_host(__m256 *p, __m256 *pBrightnessParams)
+{
+    p[0] = __lasx_xvfmadd_s(p[0], pBrightnessParams[0], pBrightnessParams[1]);    // brightness adjustment
+    p[1] = __lasx_xvfmadd_s(p[1], pBrightnessParams[0], pBrightnessParams[1]);    // brightness adjustment
+}
+
+inline void compute_brightness_16_host(__m128 *p, __m128 *pBrightnessParams)
+{
+    p[0] = __lsx_vfmadd_s(p[0], pBrightnessParams[0], pBrightnessParams[1]);    // brightness adjustment
+    p[1] = __lsx_vfmadd_s(p[1], pBrightnessParams[0], pBrightnessParams[1]);    // brightness adjustment
+    p[2] = __lsx_vfmadd_s(p[2], pBrightnessParams[0], pBrightnessParams[1]);    // brightness adjustment
+    p[3] = __lsx_vfmadd_s(p[3], pBrightnessParams[0], pBrightnessParams[1]);    // brightness adjustment
+}
+
+inline void compute_brightness_12_host(__m128 *p, __m128 *pBrightnessParams)
+{
+    p[0] = __lsx_vfmadd_s(p[0], pBrightnessParams[0], pBrightnessParams[1]);    // brightness adjustment
+    p[1] = __lsx_vfmadd_s(p[1], pBrightnessParams[0], pBrightnessParams[1]);    // brightness adjustment
+    p[2] = __lsx_vfmadd_s(p[2], pBrightnessParams[0], pBrightnessParams[1]);    // brightness adjustment
+}
+
+inline void compute_brightness_8_host(__m256 *p, __m256 *pBrightnessParams)
+{
+    p[0] = __lasx_xvfmadd_s(p[0], pBrightnessParams[0], pBrightnessParams[1]);    // brightness adjustment
+}
+
+inline void compute_brightness_8_host(__m128 *p, __m128 *pBrightnessParams)
+{
+    p[0] = __lsx_vfmadd_s(p[0], pBrightnessParams[0], pBrightnessParams[1]);    // brightness adjustment
+    p[1] = __lsx_vfmadd_s(p[1], pBrightnessParams[0], pBrightnessParams[1]);    // brightness adjustment
+}
+
+inline void compute_brightness_4_host(__m128 *p, __m128 *pBrightnessParams)
+{
+    p[0] = __lsx_vfmadd_s(p[0], pBrightnessParams[0], pBrightnessParams[1]);    // brightness adjustment
+}
+
+inline void compute_exposure_48_host(__m256 *p, __m256 &pExposureParam)
+{
+    p[0] = __lasx_xvfmul_s(p[0], pExposureParam);    // exposure adjustment
+    p[1] = __lasx_xvfmul_s(p[1], pExposureParam);    // exposure adjustment
+    p[2] = __lasx_xvfmul_s(p[2], pExposureParam);    // exposure adjustment
+    p[3] = __lasx_xvfmul_s(p[3], pExposureParam);    // exposure adjustment
+    p[4] = __lasx_xvfmul_s(p[4], pExposureParam);    // exposure adjustment
+    p[5] = __lasx_xvfmul_s(p[5], pExposureParam);    // exposure adjustment
+}
+
+inline void compute_exposure_24_host(__m256 *p, __m256 &pExposureParam)
+{
+    p[0] = __lasx_xvfmul_s(p[0], pExposureParam);    // exposure adjustment
+    p[1] = __lasx_xvfmul_s(p[1], pExposureParam);    // exposure adjustment
+    p[2] = __lasx_xvfmul_s(p[2], pExposureParam);    // exposure adjustment
+}
+
+inline void compute_exposure_16_host(__m256 *p, __m256 &pExposureParam)
+{
+    p[0] = __lasx_xvfmul_s(p[0], pExposureParam);    // exposure adjustment
+    p[1] = __lasx_xvfmul_s(p[1], pExposureParam);    // exposure adjustment
+}
+
+inline void compute_exposure_8_host(__m256 *p, __m256 &pExposureParam)
+{
+    p[0] = __lasx_xvfmul_s(p[0], pExposureParam);    // exposure adjustment
+}
+
+inline void compute_spatter_48_host(__m256 *p, __m256 *pSpatterMaskInv, __m256 *pSpatterMask, __m256 *pSpatterValue)
+{
+    p[0] = __lasx_xvfmadd_s(p[0], pSpatterMaskInv[0], __lasx_xvfmul_s(pSpatterValue[0], pSpatterMask[0]));    // spatter adjustment
+    p[1] = __lasx_xvfmadd_s(p[1], pSpatterMaskInv[1], __lasx_xvfmul_s(pSpatterValue[0], pSpatterMask[1]));    // spatter adjustment
+    p[2] = __lasx_xvfmadd_s(p[2], pSpatterMaskInv[0], __lasx_xvfmul_s(pSpatterValue[1], pSpatterMask[0]));    // spatter adjustment
+    p[3] = __lasx_xvfmadd_s(p[3], pSpatterMaskInv[1], __lasx_xvfmul_s(pSpatterValue[1], pSpatterMask[1]));    // spatter adjustment
+    p[4] = __lasx_xvfmadd_s(p[4], pSpatterMaskInv[0], __lasx_xvfmul_s(pSpatterValue[2], pSpatterMask[0]));    // spatter adjustment
+    p[5] = __lasx_xvfmadd_s(p[5], pSpatterMaskInv[1], __lasx_xvfmul_s(pSpatterValue[2], pSpatterMask[1]));    // spatter adjustment
+}
+
+inline void compute_spatter_24_host(__m256 *p, __m256 *pSpatterMaskInv, __m256 *pSpatterMask, __m256 *pSpatterValue)
+{
+    p[0] = __lasx_xvfmadd_s(p[0], pSpatterMaskInv[0], __lasx_xvfmul_s(pSpatterValue[0], pSpatterMask[0]));    // spatter adjustment
+    p[1] = __lasx_xvfmadd_s(p[1], pSpatterMaskInv[0], __lasx_xvfmul_s(pSpatterValue[1], pSpatterMask[0]));    // spatter adjustment
+    p[2] = __lasx_xvfmadd_s(p[2], pSpatterMaskInv[0], __lasx_xvfmul_s(pSpatterValue[2], pSpatterMask[0]));    // spatter adjustment
+}
+
+inline void compute_spatter_16_host(__m256 *p, __m256 *pSpatterMaskInv, __m256 *pSpatterMask, __m256 pSpatterValue)
+{
+    p[0] = __lasx_xvfmadd_s(p[0], pSpatterMaskInv[0], __lasx_xvfmul_s(pSpatterValue, pSpatterMask[0]));    // spatter adjustment
+    p[1] = __lasx_xvfmadd_s(p[1], pSpatterMaskInv[1], __lasx_xvfmul_s(pSpatterValue, pSpatterMask[1]));    // spatter adjustment
+}
+
+inline void compute_spatter_8_host(__m256 *p, __m256 *pSpatterMaskInv, __m256 *pSpatterMask, __m256 *pSpatterValue)
+{
+    p[0] = __lasx_xvfmadd_s(p[0], pSpatterMaskInv[0], __lasx_xvfmul_s(pSpatterValue[0], pSpatterMask[0]));    // spatter adjustment
+}
+
+inline void compute_spatter_48_host(__m128 *p, __m128 *pSpatterMaskInv, __m128 *pSpatterMask, __m128 *pSpatterValue)
+{
+    p[0] = __lsx_vfmadd_s(p[0], pSpatterMaskInv[0], __lsx_vfmul_s(pSpatterValue[0], pSpatterMask[0]));    // spatter adjustment
+    p[1] = __lsx_vfmadd_s(p[1], pSpatterMaskInv[1], __lsx_vfmul_s(pSpatterValue[0], pSpatterMask[1]));    // spatter adjustment
+    p[2] = __lsx_vfmadd_s(p[2], pSpatterMaskInv[2], __lsx_vfmul_s(pSpatterValue[0], pSpatterMask[2]));    // spatter adjustment
+    p[3] = __lsx_vfmadd_s(p[3], pSpatterMaskInv[3], __lsx_vfmul_s(pSpatterValue[0], pSpatterMask[3]));    // spatter adjustment
+    p[4] = __lsx_vfmadd_s(p[4], pSpatterMaskInv[0], __lsx_vfmul_s(pSpatterValue[1], pSpatterMask[0]));    // spatter adjustment
+    p[5] = __lsx_vfmadd_s(p[5], pSpatterMaskInv[1], __lsx_vfmul_s(pSpatterValue[1], pSpatterMask[1]));    // spatter adjustment
+    p[6] = __lsx_vfmadd_s(p[6], pSpatterMaskInv[2], __lsx_vfmul_s(pSpatterValue[1], pSpatterMask[2]));    // spatter adjustment
+    p[7] = __lsx_vfmadd_s(p[7], pSpatterMaskInv[3], __lsx_vfmul_s(pSpatterValue[1], pSpatterMask[3]));    // spatter adjustment
+    p[8] = __lsx_vfmadd_s(p[8], pSpatterMaskInv[0], __lsx_vfmul_s(pSpatterValue[2], pSpatterMask[0]));    // spatter adjustment
+    p[9] = __lsx_vfmadd_s(p[9], pSpatterMaskInv[1], __lsx_vfmul_s(pSpatterValue[2], pSpatterMask[1]));    // spatter adjustment
+    p[10] = __lsx_vfmadd_s(p[10], pSpatterMaskInv[2], __lsx_vfmul_s(pSpatterValue[2], pSpatterMask[2]));    // spatter adjustment
+    p[11] = __lsx_vfmadd_s(p[11], pSpatterMaskInv[3], __lsx_vfmul_s(pSpatterValue[2], pSpatterMask[3]));    // spatter adjustment
+}
+
+inline void compute_spatter_16_host(__m128 *p, __m128 *pSpatterMaskInv, __m128 *pSpatterMask, __m128 pSpatterValue)
+{
+    p[0] = __lsx_vfmadd_s(p[0], pSpatterMaskInv[0], __lsx_vfmul_s(pSpatterValue, pSpatterMask[0]));    // spatter adjustment
+    p[1] = __lsx_vfmadd_s(p[1], pSpatterMaskInv[1], __lsx_vfmul_s(pSpatterValue, pSpatterMask[1]));    // spatter adjustment
+    p[2] = __lsx_vfmadd_s(p[2], pSpatterMaskInv[2], __lsx_vfmul_s(pSpatterValue, pSpatterMask[2]));    // spatter adjustment
+    p[3] = __lsx_vfmadd_s(p[3], pSpatterMaskInv[3], __lsx_vfmul_s(pSpatterValue, pSpatterMask[3]));    // spatter adjustment
+}
+
+inline void compute_spatter_12_host(__m128 *p, __m128 *pSpatterMaskInv, __m128 *pSpatterMask, __m128 *pSpatterValue)
+{
+    p[0] = __lsx_vfmadd_s(p[0], pSpatterMaskInv[0], __lsx_vfmul_s(pSpatterValue[0], pSpatterMask[0]));    // spatter adjustment
+    p[1] = __lsx_vfmadd_s(p[1], pSpatterMaskInv[0], __lsx_vfmul_s(pSpatterValue[1], pSpatterMask[0]));    // spatter adjustment
+    p[2] = __lsx_vfmadd_s(p[2], pSpatterMaskInv[0], __lsx_vfmul_s(pSpatterValue[2], pSpatterMask[0]));    // spatter adjustment
+}
+
+inline void compute_spatter_4_host(__m128 *p, __m128 *pSpatterMaskInv, __m128 *pSpatterMask, __m128 *pSpatterValue)
+{
+    p[0] = __lsx_vfmadd_s(p[0], pSpatterMaskInv[0], __lsx_vfmul_s(pSpatterValue[0], pSpatterMask[0]));    // spatter adjustment
+}
+
+inline void compute_cmn_48_host(__m256 *p, __m256 *pCMNParams)
+{
+    p[0] = __lasx_xvfmadd_s(p[0], pCMNParams[0], pCMNParams[1]);
+    p[1] = __lasx_xvfmadd_s(p[1], pCMNParams[0], pCMNParams[1]);
+    p[2] = __lasx_xvfmadd_s(p[2], pCMNParams[2], pCMNParams[3]);
+    p[3] = __lasx_xvfmadd_s(p[3], pCMNParams[2], pCMNParams[3]);
+    p[4] = __lasx_xvfmadd_s(p[4], pCMNParams[4], pCMNParams[5]);
+    p[5] = __lasx_xvfmadd_s(p[5], pCMNParams[4], pCMNParams[5]);
+}
+
+inline void compute_cmn_48_rgb_host(__m256 *p, __m256 *pCMNParams)
+{
+    p[0] = __lasx_xvfmadd_s(p[0], pCMNParams[0], pCMNParams[1]);
+    p[1] = __lasx_xvfmadd_s(p[1], pCMNParams[0], pCMNParams[1]);
+    p[2] = __lasx_xvfmadd_s(p[2], pCMNParams[0], pCMNParams[1]);
+    p[3] = __lasx_xvfmadd_s(p[3], pCMNParams[0], pCMNParams[1]);
+    p[4] = __lasx_xvfmadd_s(p[4], pCMNParams[0], pCMNParams[1]);
+    p[5] = __lasx_xvfmadd_s(p[5], pCMNParams[0], pCMNParams[1]);
+    p[6] = __lasx_xvfmadd_s(p[6], pCMNParams[0], pCMNParams[1]);
+    p[7] = __lasx_xvfmadd_s(p[7], pCMNParams[0], pCMNParams[1]);
+}
+
+inline void compute_cmn_24_host(__m256 *p, __m256 *pCMNParams)
+{
+    p[0] = __lasx_xvfmadd_s(p[0], pCMNParams[0], pCMNParams[1]);
+    p[1] = __lasx_xvfmadd_s(p[1], pCMNParams[2], pCMNParams[3]);
+    p[2] = __lasx_xvfmadd_s(p[2], pCMNParams[4], pCMNParams[5]);
+}
+
+inline void compute_cmn_16_host(__m256 *p, __m256 *pCMNParams)
+{
+    p[0] = __lasx_xvfmadd_s(p[0], pCMNParams[0], pCMNParams[1]);
+    p[1] = __lasx_xvfmadd_s(p[1], pCMNParams[0], pCMNParams[1]);
+}
+
+inline void compute_cmn_8_host(__m256 *p, __m256 *pCMNParams)
+{
+    p[0] = __lasx_xvfmadd_s(p[0], pCMNParams[0], pCMNParams[1]);
+}
+
+inline void compute_gridmask_masks_16_host(__m128 *pCol, __m128 *pGridRowRatio, __m128 pCosRatio, __m128 pSinRatio, __m128 pGridRatio, __m128 *pMask)
+{
+    __m128 pCalc[2];
+
+    pCalc[0] = __lsx_vfmadd_s(pCol[0], pCosRatio, pGridRowRatio[0]);
+    pCalc[1] = __lsx_vfmadd_s(pCol[0], pSinRatio, pGridRowRatio[1]);
+    pCalc[0] = (__m128)__lsx_vfcmp_cle_s(__lsx_vfsub_s(pCalc[0], __lsx_vfrintrm_s(pCalc[0])), pGridRatio);
+    pCalc[1] = (__m128)__lsx_vfcmp_cle_s(__lsx_vfsub_s(pCalc[1], __lsx_vfrintrm_s(pCalc[1])), pGridRatio);
+    pMask[0] = (__m128)__lsx_vor_v((__m128i)pCalc[0], (__m128i)pCalc[1]);
+
+    pCalc[0] = __lsx_vfmadd_s(pCol[1], pCosRatio, pGridRowRatio[0]);
+    pCalc[1] = __lsx_vfmadd_s(pCol[1], pSinRatio, pGridRowRatio[1]);
+    pCalc[0] = (__m128)__lsx_vfcmp_cle_s(__lsx_vfsub_s(pCalc[0], __lsx_vfrintrm_s(pCalc[0])), pGridRatio);
+    pCalc[1] = (__m128)__lsx_vfcmp_cle_s(__lsx_vfsub_s(pCalc[1], __lsx_vfrintrm_s(pCalc[1])), pGridRatio);
+    pMask[1] = (__m128)__lsx_vor_v((__m128i)pCalc[0], (__m128i)pCalc[1]);
+
+    pCalc[0] = __lsx_vfmadd_s(pCol[2], pCosRatio, pGridRowRatio[0]);
+    pCalc[1] = __lsx_vfmadd_s(pCol[2], pSinRatio, pGridRowRatio[1]);
+    pCalc[0] = (__m128)__lsx_vfcmp_cle_s(__lsx_vfsub_s(pCalc[0], __lsx_vfrintrm_s(pCalc[0])), pGridRatio);
+    pCalc[1] = (__m128)__lsx_vfcmp_cle_s(__lsx_vfsub_s(pCalc[1], __lsx_vfrintrm_s(pCalc[1])), pGridRatio);
+    pMask[2] = (__m128)__lsx_vor_v((__m128i)pCalc[0], (__m128i)pCalc[1]);
+
+    pCalc[0] = __lsx_vfmadd_s(pCol[3], pCosRatio, pGridRowRatio[0]);
+    pCalc[1] = __lsx_vfmadd_s(pCol[3], pSinRatio, pGridRowRatio[1]);
+    pCalc[0] = (__m128)__lsx_vfcmp_cle_s(__lsx_vfsub_s(pCalc[0], __lsx_vfrintrm_s(pCalc[0])), pGridRatio);
+    pCalc[1] = (__m128)__lsx_vfcmp_cle_s(__lsx_vfsub_s(pCalc[1], __lsx_vfrintrm_s(pCalc[1])), pGridRatio);
+    pMask[3] = (__m128)__lsx_vor_v((__m128i)pCalc[0], (__m128i)pCalc[1]);
+
+    pCol[0] = __lsx_vfadd_s(pCol[0], xmm_p16);
+    pCol[1] = __lsx_vfadd_s(pCol[1], xmm_p16);
+    pCol[2] = __lsx_vfadd_s(pCol[2], xmm_p16);
+    pCol[3] = __lsx_vfadd_s(pCol[3], xmm_p16);
+}
+
+inline void compute_gridmask_masks_4_host(__m128 &pCol, __m128 *pGridRowRatio, __m128 pCosRatio, __m128 pSinRatio, __m128 pGridRatio, __m128 &pMask)
+{
+    __m128 pCalc[2];
+
+    pCalc[0] = __lsx_vfmadd_s(pCol, pCosRatio, pGridRowRatio[0]);
+    pCalc[1] = __lsx_vfmadd_s(pCol, pSinRatio, pGridRowRatio[1]);
+    pCalc[0] = (__m128)__lsx_vfcmp_cle_s(__lsx_vfsub_s(pCalc[0], __lsx_vfrintrm_s(pCalc[0])), pGridRatio);
+    pCalc[1] = (__m128)__lsx_vfcmp_cle_s(__lsx_vfsub_s(pCalc[1], __lsx_vfrintrm_s(pCalc[1])), pGridRatio);
+    pMask = (__m128)__lsx_vor_v((__m128i)pCalc[0], (__m128i)pCalc[1]);
+    pCol = __lsx_vfadd_s(pCol, xmm_p4);
+}
+
+inline void compute_gridmask_result_48_host(__m128 *p, __m128 *pMask)
+{
+    p[0] = (__m128)__lsx_vand_v((__m128i)p[0],(__m128i)pMask[0]);
+    p[1] = (__m128)__lsx_vand_v((__m128i)p[1],(__m128i)pMask[1]);
+    p[2] = (__m128)__lsx_vand_v((__m128i)p[2],(__m128i)pMask[2]);
+    p[3] = (__m128)__lsx_vand_v((__m128i)p[3],(__m128i)pMask[3]);
+    p[4] = (__m128)__lsx_vand_v((__m128i)p[4],(__m128i)pMask[0]);
+    p[5] = (__m128)__lsx_vand_v((__m128i)p[5],(__m128i)pMask[1]);
+    p[6] = (__m128)__lsx_vand_v((__m128i)p[6],(__m128i)pMask[2]);
+    p[7] = (__m128)__lsx_vand_v((__m128i)p[7],(__m128i)pMask[3]);
+    p[8] = (__m128)__lsx_vand_v((__m128i)p[8],(__m128i)pMask[0]);
+    p[9] = (__m128)__lsx_vand_v((__m128i)p[9],(__m128i)pMask[1]);
+    p[10] = (__m128)__lsx_vand_v((__m128i)p[10], (__m128i)pMask[2]);
+    p[11] = (__m128)__lsx_vand_v((__m128i)p[11], (__m128i)pMask[3]);
+}
+
+inline void compute_gridmask_result_16_host(__m128 *p, __m128 *pMask)
+{
+    p[0] = (__m128)__lsx_vand_v((__m128i)p[0], (__m128i)pMask[0]);
+    p[1] = (__m128)__lsx_vand_v((__m128i)p[1], (__m128i)pMask[1]);
+    p[2] = (__m128)__lsx_vand_v((__m128i)p[2], (__m128i)pMask[2]);
+    p[3] = (__m128)__lsx_vand_v((__m128i)p[3], (__m128i)pMask[3]);
+}
+
+inline void compute_gridmask_result_12_host(__m128 *p, __m128 pMask)
+{
+    p[0] = (__m128)__lsx_vand_v((__m128i)p[0], (__m128i)pMask);
+    p[1] = (__m128)__lsx_vand_v((__m128i)p[1], (__m128i)pMask);
+    p[2] = (__m128)__lsx_vand_v((__m128i)p[2], (__m128i)pMask);
+}
+
+inline void compute_gridmask_result_4_host(__m128 *p, __m128 pMask)
+{
+    p[0] = (__m128)__lsx_vand_v((__m128i)p[0], (__m128i)pMask);
+}
+
+inline void compute_color_twist_host(RpptFloatRGB *pixel, Rpp32f brightnessParam, Rpp32f contrastParam, Rpp32f hueParam, Rpp32f saturationParam)
+{
+    // RGB to HSV
+
+    Rpp32f hue, sat, v, add;
+    Rpp32f rf, gf, bf, cmax, cmin, delta;
+    rf = pixel->R;
+    gf = pixel->G;
+    bf = pixel->B;
+    cmax = RPPMAX3(rf, gf, bf);
+    cmin = RPPMIN3(rf, gf, bf);
+    delta = cmax - cmin;
+    hue = 0.0f;
+    sat = 0.0f;
+    add = 0.0f;
+    if ((delta != 0) && (cmax != 0))
+    {
+        sat = delta / cmax;
+        if (cmax == rf)
+        {
+            hue = gf - bf;
+            add = 0.0f;
+        }
+        else if (cmax == gf)
+        {
+            hue = bf - rf;
+            add = 2.0f;
+        }
+        else
+        {
+            hue = rf - gf;
+            add = 4.0f;
+        }
+        hue /= delta;
+    }
+    v = cmax;
+
+    // Modify Hue and Saturation
+
+    hue += hueParam + add;
+    if (hue >= 6.0f) hue -= 6.0f;
+    if (hue < 0) hue += 6.0f;
+    sat *= saturationParam;
+    sat = std::max(0.0f, std::min(1.0f, sat));
+
+    // HSV to RGB with brightness/contrast adjustment
+
+    Rpp32s hueIntegerPart = (Rpp32s) hue;
+    Rpp32f hueFractionPart = hue - hueIntegerPart;
+    Rpp32f vsat = v * sat;
+    Rpp32f vsatf = vsat * hueFractionPart;
+    Rpp32f p = v - vsat;
+    Rpp32f q = v - vsatf;
+    Rpp32f t = v - vsat + vsatf;
+    switch (hueIntegerPart)
+    {
+        case 0: rf = v; gf = t; bf = p; break;
+        case 1: rf = q; gf = v; bf = p; break;
+        case 2: rf = p; gf = v; bf = t; break;
+        case 3: rf = p; gf = q; bf = v; break;
+        case 4: rf = t; gf = p; bf = v; break;
+        case 5: rf = v; gf = p; bf = q; break;
+    }
+    pixel->R = std::fma(rf, brightnessParam, contrastParam);
+    pixel->G = std::fma(gf, brightnessParam, contrastParam);
+    pixel->B = std::fma(bf, brightnessParam, contrastParam);
+}
+
+inline void compute_color_twist_12_host(__m128 &pVecR, __m128 &pVecG, __m128 &pVecB, __m128 *pColorTwistParams)
+{
+    __m128 pA, pH, pS, pV, pDelta, pAdd, pIntH;
+    __m128 pMask[4];
+    __m128i pxIntH;
+
+    // RGB to HSV
+    pV = __lsx_vfmax_s(pVecR, __lsx_vfmax_s(pVecG, pVecB));                                                               // cmax = RPPMAX3(rf, gf, bf);
+    pS = __lsx_vfmin_s(pVecR, __lsx_vfmin_s(pVecG, pVecB));                                                               // cmin = RPPMIN3(rf, gf, bf);
+    pDelta = __lsx_vfsub_s(pV, pS);                                                                                    // delta = cmax - cmin;
+    pH = xmm_p0;                                                                                                    // hue = 0.0f;
+    pS = xmm_p0;                                                                                                    // sat = 0.0f;
+    pAdd = xmm_p0;                                                                                                  // add = 0.0f;
+    pMask[0] = (__m128)__lsx_vand_v((__m128i)__lsx_vfcmp_cune_s(pDelta, xmm_p0), (__m128i)__lsx_vfcmp_cune_s(pV, xmm_p0));                                // if ((delta != 0) && (cmax != 0)) {
+    pS = __lsx_vfdiv_s((__m128)__lsx_vand_v((__m128i)pMask[0], (__m128i)pDelta), pV);                                                              //     sat = delta / cmax;
+    pMask[1] = (__m128)__lsx_vfcmp_ceq_s(pV, pVecR);                                                                             //     Temporarily store cmax == rf comparison
+    pMask[2] = (__m128)__lsx_vand_v((__m128i)pMask[0], (__m128i)pMask[1]);                                                                      //     if (cmax == rf)
+    pH = (__m128)__lsx_vand_v((__m128i)pMask[2], (__m128i)__lsx_vfsub_s(pVecG, pVecB));                                                            //         hue = gf - bf;
+    pAdd = (__m128)__lsx_vand_v((__m128i)pMask[2], (__m128i)xmm_p0);                                                                            //         add = 0.0f;
+    pMask[3] = (__m128)__lsx_vfcmp_ceq_s(pV, pVecG);                                                                             //     Temporarily store cmax == gf comparison
+    pMask[2] = (__m128)__lsx_vandn_v((__m128i)pMask[1], (__m128i)pMask[3]);                                                                   //     else if (cmax == gf)
+    pH = (__m128)__lsx_vor_v((__m128i)__lsx_vandn_v((__m128i)pMask[2], (__m128i)pH), (__m128i)__lsx_vand_v((__m128i)pMask[2], (__m128i)__lsx_vfsub_s(pVecB, pVecR)));                    //         hue = bf - rf;
+    pAdd = (__m128)__lsx_vor_v((__m128i)__lsx_vandn_v((__m128i)pMask[2], (__m128i)pAdd),(__m128i)__lsx_vand_v((__m128i)pMask[2], (__m128i)xmm_p2));                                  //         add = 2.0f;
+    pMask[3] = (__m128)__lsx_vandn_v((__m128i)pMask[3],(__m128i)__lsx_vandn_v((__m128i)pMask[1], (__m128i)pMask[0]));                                          //     else
+    pH = (__m128)__lsx_vor_v((__m128i)__lsx_vandn_v((__m128i)pMask[3], (__m128i)pH), (__m128i)__lsx_vand_v((__m128i)pMask[3],(__m128i)__lsx_vfsub_s(pVecR, pVecG)));                    //         hue = rf - gf;
+    pAdd = (__m128)__lsx_vor_v((__m128i)__lsx_vandn_v((__m128i)pMask[3], (__m128i)pAdd), (__m128i)__lsx_vand_v((__m128i)pMask[3], (__m128i)xmm_p4));                                  //         add = 4.0f;
+    pH = (__m128)__lsx_vor_v((__m128i)__lsx_vandn_v((__m128i)pMask[0], (__m128i)pH), (__m128i)__lsx_vand_v((__m128i)pMask[0], (__m128i)__lsx_vfdiv_s(pH, pDelta)));                      //     hue /= delta; }
+
+    // Modify Hue and Saturation
+    pH = __lsx_vfadd_s(pH, __lsx_vfadd_s(pColorTwistParams[2], pAdd));                                                    // hue += hueParam + add;
+    pH = __lsx_vfsub_s(pH, (__m128)__lsx_vand_v((__m128i)(__m128)__lsx_vfcmp_cle_s(pH, xmm_p6), (__m128i)xmm_p6));                                              // if (hue >= 6.0f) hue -= 6.0f;
+    pH = __lsx_vfadd_s(pH, (__m128)__lsx_vand_v((__m128i)(__m128)__lsx_vfcmp_clt_s(pH, xmm_p0), (__m128i)xmm_p6));                                              // if (hue < 0) hue += 6.0f;
+    pS = __lsx_vfmul_s(pS, pColorTwistParams[3]);                                                                      // sat *= saturationParam;
+    pS = __lsx_vfmax_s(xmm_p0, __lsx_vfmin_s(xmm_p1, pS));                                                                // sat = std::max(0.0f, std::min(1.0f, sat));
+
+    // HSV to RGB with brightness/contrast adjustment
+    pIntH = __lsx_vfrintrm_s(pH);                                                                                       // Rpp32s hueIntegerPart = (Rpp32s) hue;
+    pxIntH = __lsx_vftint_w_s(pIntH);                                                                                // Convert to epi32
+    pH = __lsx_vfsub_s(pH, pIntH);                                                                                     // Rpp32f hueFractionPart = hue - hueIntegerPart;
+    pS = __lsx_vfmul_s(pV, pS);                                                                                        // Rpp32f vsat = v * sat;
+    pAdd = __lsx_vfmul_s(pS, pH);                                                                                      // Rpp32f vsatf = vsat * hueFractionPart;
+    pA = __lsx_vfsub_s(pV, pS);                                                                                        // Rpp32f p = v - vsat;
+    pH = __lsx_vfsub_s(pV, pAdd);                                                                                      // Rpp32f q = v - vsatf;
+    pS = __lsx_vfadd_s(pA, pAdd);                                                                                      // Rpp32f t = v - vsat + vsatf;
+    pVecR = xmm_p0;                                                                                                 // Reset dstPtrR
+    pVecG = xmm_p0;                                                                                                 // Reset dstPtrG
+    pVecB = xmm_p0;                                                                                                 // Reset dstPtrB
+    pMask[0] = (__m128)(__lsx_vseq_w(pxIntH, xmm_px0));                                                  // switch (hueIntegerPart) {case 0:
+    pVecR = (__m128)__lsx_vand_v((__m128i)pMask[0],(__m128i)pV);                                                                               //     rf = v;
+    pVecG = (__m128)__lsx_vand_v((__m128i)pMask[0],(__m128i)pS);                                                                               //     gf = t;
+    pVecB = (__m128)__lsx_vand_v((__m128i)pMask[0],(__m128i)pA);                                                                               //     bf = p; break;
+    pMask[0] = (__m128)(__lsx_vseq_w(pxIntH, xmm_px1));                                                  // case 1:
+    pVecR = (__m128)__lsx_vor_v((__m128i)__lsx_vandn_v((__m128i)pMask[0], (__m128i)pVecR),(__m128i)__lsx_vand_v((__m128i)pMask[0], (__m128i)pH));  //     rf = q;
+    pVecG = (__m128)__lsx_vor_v((__m128i)__lsx_vandn_v((__m128i)pMask[0], (__m128i)pVecG),(__m128i)__lsx_vand_v((__m128i)pMask[0], (__m128i)pV));  //     gf = v;
+    pVecB = (__m128)__lsx_vor_v((__m128i)__lsx_vandn_v((__m128i)pMask[0], (__m128i)pVecB),(__m128i)__lsx_vand_v((__m128i)pMask[0], (__m128i)pA));  //     bf = p; break;
+    pMask[0] = (__m128)(__lsx_vseq_w(pxIntH, xmm_px2));                                                  // case 2:
+    pVecR = (__m128)__lsx_vor_v((__m128i)__lsx_vandn_v((__m128i)pMask[0], (__m128i)pVecR), (__m128i)__lsx_vand_v((__m128i)pMask[0], (__m128i)pA)); //     rf = p;
+    pVecG = (__m128)__lsx_vor_v((__m128i)__lsx_vandn_v((__m128i)pMask[0], (__m128i)pVecG), (__m128i)__lsx_vand_v((__m128i)pMask[0], (__m128i)pV)); //     gf = v;
+    pVecB = (__m128)__lsx_vor_v((__m128i)__lsx_vandn_v((__m128i)pMask[0], (__m128i)pVecB), (__m128i)__lsx_vand_v((__m128i)pMask[0], (__m128i)pS)); //     bf = t; break;
+    pMask[0] = (__m128)(__lsx_vseq_w(pxIntH, xmm_px3));                                                  // case 3:
+    pVecR = (__m128)__lsx_vor_v((__m128i)__lsx_vandn_v((__m128i)pMask[0], (__m128i)pVecR),(__m128i)__lsx_vand_v((__m128i)pMask[0], (__m128i)pA)); //     rf = p;
+    pVecG = (__m128)__lsx_vor_v((__m128i)__lsx_vandn_v((__m128i)pMask[0], (__m128i)pVecG),(__m128i)__lsx_vand_v((__m128i)pMask[0], (__m128i)pH)); //     gf = q;
+    pVecB = (__m128)__lsx_vor_v((__m128i)__lsx_vandn_v((__m128i)pMask[0], (__m128i)pVecB),(__m128i)__lsx_vand_v((__m128i)pMask[0], (__m128i)pV)); //     bf = v; break;
+    pMask[0] = (__m128)(__lsx_vseq_w(pxIntH, xmm_px4));                                                  // c(__m128)ase 4:
+    pVecR = (__m128)__lsx_vor_v((__m128i)__lsx_vandn_v((__m128i)pMask[0], (__m128i)pVecR), (__m128i)__lsx_vand_v((__m128i)pMask[0], (__m128i)pS)); //     rf = t;
+    pVecG = (__m128)__lsx_vor_v((__m128i)__lsx_vandn_v((__m128i)pMask[0], (__m128i)pVecG), (__m128i)__lsx_vand_v((__m128i)pMask[0], (__m128i)pA)); //     gf = p;
+    pVecB = (__m128)__lsx_vor_v((__m128i)__lsx_vandn_v((__m128i)pMask[0], (__m128i)pVecB), (__m128i)__lsx_vand_v((__m128i)pMask[0], (__m128i)pV)); //     bf = v; break;
+    pMask[0] = (__m128)(__lsx_vseq_w(pxIntH, xmm_px5));                                                  // c(__m128)ase 5:
+    pVecR = (__m128)__lsx_vor_v((__m128i)__lsx_vandn_v((__m128i)pMask[0], (__m128i)pVecR), (__m128i)__lsx_vand_v((__m128i)pMask[0], (__m128i)pV));  //     rf = v;
+    pVecG = (__m128)__lsx_vor_v((__m128i)__lsx_vandn_v((__m128i)pMask[0], (__m128i)pVecG), (__m128i)__lsx_vand_v((__m128i)pMask[0], (__m128i)pA)); //     gf = p;
+    pVecB = (__m128)__lsx_vor_v((__m128i)__lsx_vandn_v((__m128i)pMask[0], (__m128i)pVecB), (__m128i)__lsx_vand_v((__m128i)pMask[0], (__m128i)pH)); //     bf = q; break;}
+    pVecR = __lsx_vfmadd_s(pVecR, pColorTwistParams[0], pColorTwistParams[1]);                                        // dstPtrR = rf * brightnessParam + contrastParam;
+    pVecG = __lsx_vfmadd_s(pVecG, pColorTwistParams[0], pColorTwistParams[1]);                                        // dstPtrG = gf * brightnessParam + contrastParam;
+    pVecB = __lsx_vfmadd_s(pVecB, pColorTwistParams[0], pColorTwistParams[1]);                                        // dstPtrB = bf * brightnessParam + contrastParam;
+}
+
+inline void compute_color_twist_24_host(__m256 &pVecR, __m256 &pVecG, __m256 &pVecB, __m256 *pColorTwistParams)
+{
+    __m256 pA, pH, pS, pV, pDelta, pAdd, pIntH;
+    __m256 pMask[4];
+    __m256i pxIntH;
+
+    // RGB to HSV
+    pV = __lasx_xvfmax_s(pVecR, __lasx_xvfmax_s(pVecG, pVecB));                                                            // cmax = RPPMAX3(rf, gf, bf);
+    pS = __lasx_xvfmin_s(pVecR, __lasx_xvfmin_s(pVecG, pVecB));                                                            // cmin = RPPMIN3(rf, gf, bf);
+    pDelta = __lasx_xvfsub_s(pV, pS);                                                                                    // delta = cmax - cmin;
+    pH = avx_p0;                                                                                                       // hue = 0.0f;
+    pS = avx_p0;                                                                                                       // sat = 0.0f;
+    pAdd = avx_p0;                                                                                                     // add = 0.0f;
+    pMask[0] = (__m256)__lasx_xvand_v((__m256i)(__m256)__lasx_xvfcmp_xxx_s(pDelta, avx_p0, _CMP_NEQ_OQ), (__m256i)(__m256)__lasx_xvfcmp_xxx_s(pV, avx_p0, _CMP_NEQ_OQ));      // if ((delta != 0) && (cmax != 0)) {
+    pS = __lasx_xvfdiv_s((__m256)__lasx_xvand_v((__m256i)pMask[0], pDelta), (__m256i)pV);                                                           //     sat = delta / cmax;
+    pMask[1] = (__m256)__lasx_xvfcmp_xxx_s(pV, pVecR, _CMP_EQ_OQ);                                                                   //     Temporarily store cmax == rf comparison
+    pMask[2] = (__m256)__lasx_xvand_v((__m256i)pMask[0], (__m256i)pMask[1]);                                                                      //     if (cmax == rf)
+    pH = (__m256)__lasx_xvand_v((__m256i)pMask[2], (__m256i)__lasx_xvfsub_s(pVecG, pVecB));                                                         //         hue = gf - bf;
+    pAdd = (__m256)__lasx_xvand_v((__m256i)pMask[2], (__m256i)avx_p0);                                                                            //         add = 0.0f;
+    pMask[3] = (__m256)__lasx_xvfcmp_xxx_s(pV, pVecG, _CMP_EQ_OQ);                                                                   //     Temporarily store cmax == gf comparison
+    pMask[2] = (__m256)__lasx_xvandn_v((__m256i)pMask[1], (__m256i)pMask[3]);                                                                   //     else if (cmax == gf)
+    pH = (__m256)__lasx_xvor_v((__m256i)__lasx_xvandn_v((__m256i)pMask[2], (__m256i)pH), (__m256i)__lasx_xvand_v((__m256i)pMask[2], (__m256i)__lasx_xvfsub_s(pVecB, pVecR))); //   hue = bf - rf;
+    pAdd = (__m256)__lasx_xvor_v((__m256i)__lasx_xvandn_v((__m256i)pMask[2], (__m256i)pAdd), (__m256i)__lasx_xvand_v((__m256i)pMask[2], (__m256i)avx_p2));                    //   add = 2.0f;
+    pMask[3] = (__m256)__lasx_xvandn_v((__m256i)pMask[3], (__m256i)__lasx_xvandn_v(pMask[1], pMask[0]));                                       //     else
+    pH = (__m256)__lasx_xvor_v((__m256i)__lasx_xvandn_v((__m256i)pMask[3], (__m256i)pH), (__m256i)__lasx_xvand_v((__m256i)pMask[3], (__m256i)__lasx_xvfsub_s(pVecR, pVecG)));  //   hue = rf - gf;
+    pAdd = (__m256)__lasx_xvor_v((__m256i)__lasx_xvandn_v((__m256i)pMask[3], (__m256i)pAdd), (__m256i)__lasx_xvand_v((__m256i)pMask[3], (__m256i)avx_p4));            //         add = 4.0f;
+    pH = (__m256)__lasx_xvor_v((__m256i)__lasx_xvandn_v((__m256i)pMask[0], (__m256i)pH), (__m256i)__lasx_xvand_v((__m256i)pMask[0], (__m256i)__lasx_xvfdiv_s(pH, pDelta)));    //     hue /= delta; }
+
+    // Modify Hue and Saturation
+    pH = __lasx_xvfadd_s(pH, __lasx_xvfadd_s(pColorTwistParams[2], pAdd));                                                 // hue += hueParam + add;
+    pH = __lasx_xvfsub_s(pH, (__m256)__lasx_xvand_v((__m256i)(__m256)__lasx_xvfcmp_xxx_s(pH, avx_p6, _CMP_GE_OQ), (__m256i)avx_p6));                              // if (hue >= 6.0f) hue -= 6.0f;
+    pH = __lasx_xvfadd_s(pH, (__m256)__lasx_xvand_v((__m256i)(__m256)__lasx_xvfcmp_xxx_s(pH, avx_p0, _CMP_LT_OQ), (__m256i)avx_p6));                              // if (hue < 0) hue += 6.0f;
+    pS = __lasx_xvfmul_s(pS, pColorTwistParams[3]);                                                                      // sat *= saturationParam;
+    pS = __lasx_xvfmax_s(avx_p0, __lasx_xvfmin_s(avx_p1, pS));                                                             // sat = std::max(0.0f, std::min(1.0f, sat));
+
+    // HSV to RGB with brightness/contrast adjustment
+    pIntH = __lasx_xvfrintrm_s(pH);                                                                                       // Rpp32s hueIntegerPart = (Rpp32s) hue;
+    pxIntH = __lasx_xvftint_w_s(pIntH);                                                                                // Convert to epi32
+    pH = __lasx_xvfsub_s(pH, pIntH);                                                                                     // Rpp32f hueFractionPart = hue - hueIntegerPart;
+    pS = __lasx_xvfmul_s(pV, pS);                                                                                        // Rpp32f vsat = v * sat;
+    pAdd = __lasx_xvfmul_s(pS, pH);                                                                                      // Rpp32f vsatf = vsat * hueFractionPart;
+    pA = __lasx_xvfsub_s(pV, pS);                                                                                        // Rpp32f p = v - vsat;
+    pH = __lasx_xvfsub_s(pV, pAdd);                                                                                      // Rpp32f q = v - vsatf;
+    pS = __lasx_xvfadd_s(pA, pAdd);                                                                                      // Rpp32f t = v - vsat + vsatf;
+    pVecR = avx_p0;                                                                                                    // Reset dstPtrR
+    pVecG = avx_p0;                                                                                                    // Reset dstPtrG
+    pVecB = avx_p0;                                                                                                    // Reset dstPtrB
+    pMask[0] = (__m256)(__lasx_xvseq_w(pxIntH, avx_px0));                                               // switch (hueIntegerPart) {case 0:
+    pVecR = (__m256)__lasx_xvand_v((__m256i)pMask[0], (__m256i)pV);                                                                               //     rf = v;
+    pVecG = (__m256)__lasx_xvand_v((__m256i)pMask[0], (__m256i)pS);                                                                               //     gf = t;
+    pVecB = (__m256)__lasx_xvand_v((__m256i)pMask[0], (__m256i)pA);                                                                               //     bf = p; break;
+    pMask[0] = (__m256)(__lasx_xvseq_w(pxIntH, avx_px1));                                               // case 1:
+    pVecR = (__m256)__lasx_xvor_v((__m256i)__lasx_xvandn_v((__m256i)pMask[0], (__m256i)pVecR), (__m256i)__lasx_xvand_v((__m256i)pMask[0], (__m256i)pH));       //     rf = q;
+    pVecG = (__m256)__lasx_xvor_v((__m256i)__lasx_xvandn_v((__m256i)pMask[0], (__m256i)pVecG), (__m256i)__lasx_xvand_v((__m256i)pMask[0], (__m256i)pV));       //     gf = v;
+    pVecB = (__m256)__lasx_xvor_v((__m256i)__lasx_xvandn_v((__m256i)pMask[0], (__m256i)pVecB), (__m256i)__lasx_xvand_v((__m256i)pMask[0], (__m256i)pA));       //     bf = p; break;
+    pMask[0] = (__m256)(__lasx_xvseq_w(pxIntH, avx_px2));                                               // case 2:
+    pVecR = (__m256)__lasx_xvor_v((__m256i)__lasx_xvandn_v((__m256i)pMask[0], (__m256i)pVecR), (__m256i)__lasx_xvand_v((__m256i)pMask[0], (__m256i)pA));       //     rf = p;
+    pVecG = (__m256)__lasx_xvor_v((__m256i)__lasx_xvandn_v((__m256i)pMask[0], (__m256i)pVecG), (__m256i)__lasx_xvand_v((__m256i)pMask[0], (__m256i)pV));       //     gf = v;
+    pVecB = (__m256)__lasx_xvor_v((__m256i)__lasx_xvandn_v((__m256i)pMask[0], (__m256i)pVecB), (__m256i)__lasx_xvand_v((__m256i)pMask[0], (__m256i)pS));       //     bf = t; break;
+    pMask[0] = (__m256)(__lasx_xvseq_w(pxIntH, avx_px3));                                               // case 3:
+    pVecR = (__m256)__lasx_xvor_v((__m256i)__lasx_xvandn_v(pMask[0], pVecR), (__m256i)__lasx_xvand_v((__m256i)pMask[0], (__m256i)pA));                         //     rf = p;
+    pVecG = (__m256)__lasx_xvor_v((__m256i)__lasx_xvandn_v(pMask[0], pVecG), (__m256i)__lasx_xvand_v((__m256i)pMask[0], (__m256i)pH));                         //     gf = q;
+    pVecB = (__m256)__lasx_xvor_v((__m256i)__lasx_xvandn_v(pMask[0], pVecB), (__m256i)__lasx_xvand_v((__m256i)pMask[0], (__m256i)pV));                         //     bf = v; break;
+    pMask[0] = (__m256)(__lasx_xvseq_w(pxIntH, avx_px4));                                               // case 4:
+    pVecR = (__m256)__lasx_xvor_v((__m256i)__lasx_xvandn_v(pMask[0], pVecR), (__m256i)__lasx_xvand_v((__m256i)pMask[0], (__m256i)pS));                         //     rf = t;
+    pVecG = (__m256)__lasx_xvor_v((__m256i)__lasx_xvandn_v(pMask[0], pVecG), (__m256i)__lasx_xvand_v((__m256i)pMask[0], (__m256i)pA));                         //     gf = p;
+    pVecB = (__m256)__lasx_xvor_v((__m256i)__lasx_xvandn_v(pMask[0], pVecB), (__m256i)__lasx_xvand_v((__m256i)pMask[0], (__m256i)pV));                         //     bf = v; break;
+    pMask[0] = (__m256)(__lasx_xvseq_w(pxIntH, avx_px5));                                               // case 5:
+    pVecR = (__m256)__lasx_xvor_v((__m256i)__lasx_xvandn_v(pMask[0], pVecR), (__m256i)__lasx_xvand_v((__m256i)pMask[0], (__m256i)pV));                         //     rf = v;
+    pVecG = (__m256)__lasx_xvor_v((__m256i)__lasx_xvandn_v(pMask[0], pVecG), (__m256i)__lasx_xvand_v((__m256i)pMask[0], (__m256i)pA));                         //     gf = p;
+    pVecB = (__m256)__lasx_xvor_v((__m256i)__lasx_xvandn_v(pMask[0], pVecB), (__m256i)__lasx_xvand_v((__m256i)pMask[0], (__m256i)pH));                         //     bf = q; break;}
+    pVecR = __lasx_xvfmadd_s(pVecR, pColorTwistParams[0], pColorTwistParams[1]);                                        // dstPtrR = rf * brightnessParam + contrastParam;
+    pVecG = __lasx_xvfmadd_s(pVecG, pColorTwistParams[0], pColorTwistParams[1]);                                        // dstPtrG = gf * brightnessParam + contrastParam;
+    pVecB = __lasx_xvfmadd_s(pVecB, pColorTwistParams[0], pColorTwistParams[1]);                                        // dstPtrB = bf * brightnessParam + contrastParam;
+}
+
+inline void compute_color_cast_48_host(__m128 *p, __m128 pMul, __m128 *pAdd)
+{
+    p[0] = __lsx_vfmadd_s(__lsx_vfsub_s(p[0], pAdd[0]), pMul, pAdd[0]);    // color_cast adjustment Rs
+    p[1] = __lsx_vfmadd_s(__lsx_vfsub_s(p[1], pAdd[0]), pMul, pAdd[0]);    // color_cast adjustment Rs
+    p[2] = __lsx_vfmadd_s(__lsx_vfsub_s(p[2], pAdd[0]), pMul, pAdd[0]);    // color_cast adjustment Rs
+    p[3] = __lsx_vfmadd_s(__lsx_vfsub_s(p[3], pAdd[0]), pMul, pAdd[0]);    // color_cast adjustment Rs
+    p[4] = __lsx_vfmadd_s(__lsx_vfsub_s(p[4], pAdd[1]), pMul, pAdd[1]);    // color_cast adjustment Gs
+    p[5] = __lsx_vfmadd_s(__lsx_vfsub_s(p[5], pAdd[1]), pMul, pAdd[1]);    // color_cast adjustment Gs
+    p[6] = __lsx_vfmadd_s(__lsx_vfsub_s(p[6], pAdd[1]), pMul, pAdd[1]);    // color_cast adjustment Gs
+    p[7] = __lsx_vfmadd_s(__lsx_vfsub_s(p[7], pAdd[1]), pMul, pAdd[1]);    // color_cast adjustment Gs
+    p[8] = __lsx_vfmadd_s(__lsx_vfsub_s(p[8], pAdd[2]), pMul, pAdd[2]);    // color_cast adjustment Bs
+    p[9] = __lsx_vfmadd_s(__lsx_vfsub_s(p[9], pAdd[2]), pMul, pAdd[2]);    // color_cast adjustment Bs
+    p[10] = __lsx_vfmadd_s(__lsx_vfsub_s(p[10], pAdd[2]), pMul, pAdd[2]);    // color_cast adjustment Bs
+    p[11] = __lsx_vfmadd_s(__lsx_vfsub_s(p[11], pAdd[2]), pMul, pAdd[2]);    // color_cast adjustment Bs
+}
+
+inline void compute_color_cast_12_host(__m128 *p, __m128 pMul, __m128 *pAdd)
+{
+    p[0] = __lsx_vfmadd_s(__lsx_vfsub_s(p[0], pAdd[0]), pMul, pAdd[0]);    // color_cast adjustment Rs
+    p[1] = __lsx_vfmadd_s(__lsx_vfsub_s(p[1], pAdd[1]), pMul, pAdd[1]);    // color_cast adjustment Rs
+    p[2] = __lsx_vfmadd_s(__lsx_vfsub_s(p[2], pAdd[2]), pMul, pAdd[2]);    // color_cast adjustment Rs
+}
+
+inline void compute_color_temperature_48_host(__m256 *p, __m256 pAdj)
+{
+    p[0] = __lasx_xvfadd_s(p[0], pAdj);    // color_temperature adjustment Rs
+    p[1] = __lasx_xvfadd_s(p[1], pAdj);    // color_temperature adjustment Rs
+    // no color_temperature adjustment Gs
+    p[4] = __lasx_xvfsub_s(p[4], pAdj);    // color_temperature adjustment Bs
+    p[5] = __lasx_xvfsub_s(p[5], pAdj);    // color_temperature adjustment Bs
+}
+
+inline void compute_color_temperature_24_host(__m256 *p, __m256 pAdj)
+{
+    p[0] = __lasx_xvfadd_s(p[0], pAdj);    // color_temperature adjustment Rs
+    // no color_temperature adjustment Gs
+    p[2] = __lasx_xvfsub_s(p[2], pAdj);    // color_temperature adjustment Bs
+}
+
+inline void compute_fog_48_host(__m256 *p, __m256 *pFogAlphaMask, __m256 *pFogIntensityMask, __m256 pIntensityFactor, __m256 pGrayFactor, __m256 *pConversionFactor)
+{
+    __m256 pAlphaMaskFactor[2], pIntensityMaskFactor[2], pGray[2], pOneMinusGrayFactor;
+    pGray[0] = __lasx_xvfmadd_s(pConversionFactor[0], p[0], __lasx_xvfmadd_s(pConversionFactor[1], p[2], __lasx_xvfmul_s(pConversionFactor[2], p[4]))); 
+    pGray[1] = __lasx_xvfmadd_s(pConversionFactor[0], p[1], __lasx_xvfmadd_s(pConversionFactor[1], p[3], __lasx_xvfmul_s(pConversionFactor[2], p[5]))); 
+    pOneMinusGrayFactor = __lasx_xvfsub_s(avx_p1 , pGrayFactor);
+    pGray[0] = __lasx_xvfmul_s(pGray[0], pGrayFactor);
+    pGray[1] = __lasx_xvfmul_s(pGray[1], pGrayFactor);
+    p[0] = __lasx_xvfmadd_s(p[0], pOneMinusGrayFactor, pGray[0]);  
+    p[1] = __lasx_xvfmadd_s(p[1], pOneMinusGrayFactor, pGray[0]);  
+    p[2] = __lasx_xvfmadd_s(p[2], pOneMinusGrayFactor, pGray[0]);  
+    p[3] = __lasx_xvfmadd_s(p[3], pOneMinusGrayFactor, pGray[1]);  
+    p[4] = __lasx_xvfmadd_s(p[4], pOneMinusGrayFactor, pGray[1]);  
+    p[5] = __lasx_xvfmadd_s(p[5], pOneMinusGrayFactor, pGray[1]);  
+    pAlphaMaskFactor[0] = __lasx_xvfsub_s(avx_p1, __lasx_xvfadd_s(pFogAlphaMask[0], pIntensityFactor));
+    pAlphaMaskFactor[1] = __lasx_xvfsub_s(avx_p1, __lasx_xvfadd_s(pFogAlphaMask[1], pIntensityFactor));
+    pIntensityMaskFactor[0] = __lasx_xvfmul_s(pFogIntensityMask[0], __lasx_xvfadd_s(pFogAlphaMask[0], pIntensityFactor));
+    pIntensityMaskFactor[1] = __lasx_xvfmul_s(pFogIntensityMask[1], __lasx_xvfadd_s(pFogAlphaMask[1], pIntensityFactor));
+    p[0] = __lasx_xvfmadd_s(p[0],  pAlphaMaskFactor[0], pIntensityMaskFactor[0]);    // fog adjustment
+    p[1] = __lasx_xvfmadd_s(p[1],  pAlphaMaskFactor[1], pIntensityMaskFactor[1]);    // fog adjustment
+    p[2] = __lasx_xvfmadd_s(p[2],  pAlphaMaskFactor[0], pIntensityMaskFactor[0]);    // fog adjustment
+    p[3] = __lasx_xvfmadd_s(p[3],  pAlphaMaskFactor[1], pIntensityMaskFactor[1]);    // fog adjustment
+    p[4] = __lasx_xvfmadd_s(p[4],  pAlphaMaskFactor[0], pIntensityMaskFactor[0]);    // fog adjustment
+    p[5] = __lasx_xvfmadd_s(p[5],  pAlphaMaskFactor[1], pIntensityMaskFactor[1]);    // fog adjustment
+}
+
+inline void compute_fog_24_host(__m256 *p, __m256 *pFogAlphaMask, __m256 *pFogIntensityMask, __m256 pIntensityFactor, __m256 pGrayFactor, __m256 *pConversionFactor)
+{
+    __m256 pAlphaMaskFactor, pIntensityMaskFactor, pGray, pOneMinusGrayFactor;
+    pGray = __lasx_xvfmadd_s(pConversionFactor[0], p[0], __lasx_xvfmadd_s(pConversionFactor[1], p[1], __lasx_xvfmul_s(pConversionFactor[2], p[2]))); 
+    pOneMinusGrayFactor = __lasx_xvfsub_s(avx_p1 , pGrayFactor);
+    pGray = __lasx_xvfmul_s(pGray, pGrayFactor);
+    p[0] = __lasx_xvfmadd_s(p[0], pOneMinusGrayFactor, pGray);  
+    p[1] = __lasx_xvfmadd_s(p[1], pOneMinusGrayFactor, pGray);  
+    p[2] = __lasx_xvfmadd_s(p[2], pOneMinusGrayFactor, pGray);  
+    pAlphaMaskFactor = __lasx_xvfsub_s(avx_p1, __lasx_xvfadd_s(pFogAlphaMask[0], pIntensityFactor));
+    pIntensityMaskFactor = __lasx_xvfmul_s(pFogIntensityMask[0], __lasx_xvfadd_s(pFogAlphaMask[0], pIntensityFactor));
+    p[0] = __lasx_xvfmadd_s(p[0],  pAlphaMaskFactor, pIntensityMaskFactor);    // fog adjustment
+    p[1] = __lasx_xvfmadd_s(p[1],  pAlphaMaskFactor, pIntensityMaskFactor);    // fog adjustment
+    p[2] = __lasx_xvfmadd_s(p[2],  pAlphaMaskFactor, pIntensityMaskFactor);    // fog adjustment
+}
+
+inline void compute_fog_16_host(__m256 *p, __m256 *pFogAlphaMask, __m256 *pFogIntensityMask, __m256 pIntensityFactor)
+{
+    __m256 pAlphaMaskFactor[2], pIntensityMaskFactor[2];
+    pAlphaMaskFactor[0] = __lasx_xvfsub_s(avx_p1, __lasx_xvfadd_s(pFogAlphaMask[0], pIntensityFactor));
+    pAlphaMaskFactor[1] = __lasx_xvfsub_s(avx_p1, __lasx_xvfadd_s(pFogAlphaMask[1], pIntensityFactor));
+    pIntensityMaskFactor[0] = __lasx_xvfmul_s(pFogIntensityMask[0], __lasx_xvfadd_s(pFogAlphaMask[0], pIntensityFactor));
+    pIntensityMaskFactor[1] = __lasx_xvfmul_s(pFogIntensityMask[1], __lasx_xvfadd_s(pFogAlphaMask[1], pIntensityFactor));
+    p[0] = __lasx_xvfmadd_s(p[0],  pAlphaMaskFactor[0], pIntensityMaskFactor[0]);    // fog adjustment
+    p[1] = __lasx_xvfmadd_s(p[1],  pAlphaMaskFactor[1], pIntensityMaskFactor[1]);    // fog adjustment
+}
+
+inline void compute_fog_8_host(__m256 *p, __m256 *pFogAlphaMask, __m256 *pFogIntensityMask, __m256 pIntensityFactor)
+{
+    __m256 pAlphaMaskFactor, pIntensityMaskFactor;
+    pAlphaMaskFactor = __lasx_xvfsub_s(avx_p1, __lasx_xvfadd_s(pFogAlphaMask[0], pIntensityFactor));
+    pIntensityMaskFactor = __lasx_xvfmul_s(pFogIntensityMask[0], __lasx_xvfadd_s(pFogAlphaMask[0], pIntensityFactor));
+    p[0] = __lasx_xvfmadd_s(p[0],  pAlphaMaskFactor, pIntensityMaskFactor);    // fog adjustment
+}
+
+inline void compute_xywh_from_ltrb_host(RpptROIPtr roiPtrInput, RpptROIPtr roiPtrImage)
+{
+    roiPtrImage->xywhROI.xy.x = roiPtrInput->ltrbROI.lt.x;
+    roiPtrImage->xywhROI.xy.y = roiPtrInput->ltrbROI.lt.y;
+    roiPtrImage->xywhROI.roiWidth = roiPtrInput->ltrbROI.rb.x - roiPtrInput->ltrbROI.lt.x + 1;
+    roiPtrImage->xywhROI.roiHeight = roiPtrInput->ltrbROI.rb.y - roiPtrInput->ltrbROI.lt.y + 1;
+}
+
+inline void compute_xyzwhd_from_ltfrbb_host(RpptROI3DPtr roiPtrInput, RpptROI3DPtr roiPtrImage)
+{
+    roiPtrImage->xyzwhdROI.xyz.x = roiPtrInput->ltfrbbROI.ltf.x;
+    roiPtrImage->xyzwhdROI.xyz.y = roiPtrInput->ltfrbbROI.ltf.y;
+    roiPtrImage->xyzwhdROI.xyz.z = roiPtrInput->ltfrbbROI.ltf.z;
+    roiPtrImage->xyzwhdROI.roiWidth = roiPtrInput->ltfrbbROI.rbb.x - roiPtrInput->ltfrbbROI.ltf.x + 1;
+    roiPtrImage->xyzwhdROI.roiHeight = roiPtrInput->ltfrbbROI.rbb.y - roiPtrInput->ltfrbbROI.ltf.y + 1;
+    roiPtrImage->xyzwhdROI.roiDepth = roiPtrInput->ltfrbbROI.rbb.z - roiPtrInput->ltfrbbROI.ltf.z + 1;
+}
+
+inline void compute_ltrb_from_xywh_host(RpptROIPtr roiPtrInput, RpptROIPtr roiPtrImage)
+{
+    roiPtrImage->ltrbROI.lt.x = roiPtrInput->xywhROI.xy.x;
+    roiPtrImage->ltrbROI.lt.y = roiPtrInput->xywhROI.xy.y;
+    roiPtrImage->ltrbROI.rb.x = roiPtrInput->xywhROI.xy.x + roiPtrInput->xywhROI.roiWidth - 1;
+    roiPtrImage->ltrbROI.rb.y = roiPtrInput->xywhROI.xy.y + roiPtrInput->xywhROI.roiHeight - 1;
+}
+
+inline void compute_roi_boundary_check_host(RpptROIPtr roiPtrImage, RpptROIPtr roiPtr, RpptROIPtr roiPtrDefault)
+{
+    roiPtr->xywhROI.xy.x = std::max(roiPtrDefault->xywhROI.xy.x, roiPtrImage->xywhROI.xy.x);
+    roiPtr->xywhROI.xy.y = std::max(roiPtrDefault->xywhROI.xy.y, roiPtrImage->xywhROI.xy.y);
+    roiPtr->xywhROI.roiWidth = std::min(roiPtrDefault->xywhROI.roiWidth - roiPtrImage->xywhROI.xy.x, roiPtrImage->xywhROI.roiWidth);
+    roiPtr->xywhROI.roiHeight = std::min(roiPtrDefault->xywhROI.roiHeight - roiPtrImage->xywhROI.xy.y, roiPtrImage->xywhROI.roiHeight);
+}
+
+inline void compute_roi3D_boundary_check_host(RpptROI3DPtr roiPtrImage, RpptROI3DPtr roiPtr, RpptROI3DPtr roiPtrDefault)
+{
+    roiPtr->xyzwhdROI.xyz.x = std::max(roiPtrDefault->xyzwhdROI.xyz.x, roiPtrImage->xyzwhdROI.xyz.x);
+    roiPtr->xyzwhdROI.xyz.y = std::max(roiPtrDefault->xyzwhdROI.xyz.y, roiPtrImage->xyzwhdROI.xyz.y);
+    roiPtr->xyzwhdROI.xyz.z = std::max(roiPtrDefault->xyzwhdROI.xyz.z, roiPtrImage->xyzwhdROI.xyz.z);
+    roiPtr->xyzwhdROI.roiWidth = std::min(roiPtrDefault->xyzwhdROI.roiWidth - roiPtrImage->xyzwhdROI.xyz.x, roiPtrImage->xyzwhdROI.roiWidth);
+    roiPtr->xyzwhdROI.roiHeight = std::min(roiPtrDefault->xyzwhdROI.roiHeight - roiPtrImage->xyzwhdROI.xyz.y, roiPtrImage->xyzwhdROI.roiHeight);
+    roiPtr->xyzwhdROI.roiDepth = std::min(roiPtrDefault->xyzwhdROI.roiDepth - roiPtrImage->xyzwhdROI.xyz.z, roiPtrImage->xyzwhdROI.roiDepth);
+}
+
+inline void compute_roi_validation_host(RpptROIPtr roiPtrInput, RpptROIPtr roiPtr, RpptROIPtr roiPtrDefault, RpptRoiType roiType)
+{
+    if (roiPtrInput == NULL)
+    {
+        roiPtr = roiPtrDefault;
+    }
+    else
+    {
+        RpptROI roiImage;
+        RpptROIPtr roiPtrImage = &roiImage;
+        if (roiType == RpptRoiType::LTRB)
+            compute_xywh_from_ltrb_host(roiPtrInput, roiPtrImage);
+        else if (roiType == RpptRoiType::XYWH)
+            roiPtrImage = roiPtrInput;
+        compute_roi_boundary_check_host(roiPtrImage, roiPtr, roiPtrDefault);
+    }
+}
+
+inline void compute_roi3D_validation_host(RpptROI3DPtr roiPtrInput, RpptROI3DPtr roiPtr, RpptROI3DPtr roiPtrDefault, RpptRoi3DType roiType)
+{
+    if (roiPtrInput == NULL)
+    {
+        roiPtr = roiPtrDefault;
+    }
+    else
+    {
+        RpptROI3D roiImage;
+        RpptROI3DPtr roiPtrImage = &roiImage;
+        if (roiType == RpptRoi3DType::LTFRBB)
+            compute_xyzwhd_from_ltfrbb_host(roiPtrInput, roiPtrImage);
+        else if (roiType == RpptRoi3DType::XYZWHD)
+            roiPtrImage = roiPtrInput;
+        compute_roi3D_boundary_check_host(roiPtrImage, roiPtr, roiPtrDefault);
+    }
+}
+
+inline void compute_color_jitter_ctm_host(Rpp32f brightnessParam, Rpp32f contrastParam, Rpp32f hueParam, Rpp32f saturationParam, Rpp32f *ctm)
+{
+    contrastParam += 1.0f;
+
+    alignas(64) Rpp32f hue_saturation_matrix[16] = {RGB_TO_GREY_WEIGHT_RED, RGB_TO_GREY_WEIGHT_RED, RGB_TO_GREY_WEIGHT_RED, 0.0f, RGB_TO_GREY_WEIGHT_GREEN, RGB_TO_GREY_WEIGHT_GREEN, RGB_TO_GREY_WEIGHT_GREEN, 0.0f, RGB_TO_GREY_WEIGHT_BLUE, RGB_TO_GREY_WEIGHT_BLUE, RGB_TO_GREY_WEIGHT_BLUE, 0.0f, 0.0f, 0.0f, 0.0f, 1.0f};
+    alignas(64) Rpp32f brightness_contrast_matrix[16] = {contrastParam, 0.0f, 0.0f, 0.0f, 0.0f, contrastParam, 0.0f, 0.0f, 0.0f, 0.0f, contrastParam, 0.0f, brightnessParam, brightnessParam, brightnessParam, 1.0f};
+
+    Rpp32f sch = saturationParam * cos(hueParam * PI_OVER_180);
+    Rpp32f ssh = saturationParam * sin(hueParam * PI_OVER_180);
+
+    __m128 psch = lsx_set1_f32(sch);
+    __m128 pssh = lsx_set1_f32(ssh);
+    __m128 p0, p1, p2;
+
+    for (int i = 0; i < 16; i+=4)
+    {
+        p0 = (__m128)__lsx_vld(hue_saturation_matrix + i, 0);
+        p1 = (__m128)__lsx_vld(sch_mat + i, 0);
+        p2 = (__m128)__lsx_vld(ssh_mat + i, 0);
+        p0 = __lsx_vfmadd_s(psch, p1, __lsx_vfmadd_s(pssh, p2, p0));
+        __lsx_vst(p0, hue_saturation_matrix + i, 0);
+    }
+
+    fast_matmul4x4_sse(hue_saturation_matrix, brightness_contrast_matrix, ctm);
+}
+
+inline void compute_color_jitter_48_host(__m128 *p, __m128 *pCtm)
+{
+    __m128 pResult[3];
+
+    pResult[0] = __lsx_vfrintrxxx_s(__lsx_vfmadd_s(p[0], pCtm[0], __lsx_vfmadd_s(p[4], pCtm[1], __lsx_vfmadd_s(p[8], pCtm[2], pCtm[3]))), (const int)(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC));    // color_jitter adjustment R0-R3
+    pResult[1] = __lsx_vfrintrxxx_s(__lsx_vfmadd_s(p[0], pCtm[4], __lsx_vfmadd_s(p[4], pCtm[5], __lsx_vfmadd_s(p[8], pCtm[6], pCtm[7]))), (const int)(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC));    // color_jitter adjustment G0-G3
+    pResult[2] = __lsx_vfrintrxxx_s(__lsx_vfmadd_s(p[0], pCtm[8], __lsx_vfmadd_s(p[4], pCtm[9], __lsx_vfmadd_s(p[8], pCtm[10], pCtm[11]))), (const int)(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC));    // color_jitter adjustment B0-B3
+    p[0] = pResult[0];    // color_jitter adjustment R0-R3
+    p[4] = pResult[1];    // color_jitter adjustment G0-G3
+    p[8] = pResult[2];    // color_jitter adjustment B0-B3
+    pResult[0] = __lsx_vfrintrxxx_s(__lsx_vfmadd_s(p[1], pCtm[0], __lsx_vfmadd_s(p[5], pCtm[1], __lsx_vfmadd_s(p[9], pCtm[2], pCtm[3]))), (const int)(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC));    // color_jitter adjustment R4-R7
+    pResult[1] = __lsx_vfrintrxxx_s(__lsx_vfmadd_s(p[1], pCtm[4], __lsx_vfmadd_s(p[5], pCtm[5], __lsx_vfmadd_s(p[9], pCtm[6], pCtm[7]))), (const int)(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC));    // color_jitter adjustment G4-G7
+    pResult[2] = __lsx_vfrintrxxx_s(__lsx_vfmadd_s(p[1], pCtm[8], __lsx_vfmadd_s(p[5], pCtm[9], __lsx_vfmadd_s(p[9], pCtm[10], pCtm[11]))), (const int)(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC));    // color_jitter adjustment B4-B7
+    p[1] = pResult[0];    // color_jitter adjustment R4-R7
+    p[5] = pResult[1];    // color_jitter adjustment G4-G7
+    p[9] = pResult[2];    // color_jitter adjustment B4-B7
+    pResult[0] = __lsx_vfrintrxxx_s(__lsx_vfmadd_s(p[2], pCtm[0], __lsx_vfmadd_s(p[6], pCtm[1], __lsx_vfmadd_s(p[10], pCtm[2], pCtm[3]))), (const int)(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC));    // color_jitter adjustment R8-R11
+    pResult[1] = __lsx_vfrintrxxx_s(__lsx_vfmadd_s(p[2], pCtm[4], __lsx_vfmadd_s(p[6], pCtm[5], __lsx_vfmadd_s(p[10], pCtm[6], pCtm[7]))), (const int)(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC));    // color_jitter adjustment G8-G11
+    pResult[2] = __lsx_vfrintrxxx_s(__lsx_vfmadd_s(p[2], pCtm[8], __lsx_vfmadd_s(p[6], pCtm[9], __lsx_vfmadd_s(p[10], pCtm[10], pCtm[11]))), (const int)(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC));    // color_jitter adjustment B8-B11
+    p[2] = pResult[0];    // color_jitter adjustment R8-R11
+    p[6] = pResult[1];    // color_jitter adjustment G8-G11
+    p[10] = pResult[2];    // color_jitter adjustment B8-B11
+    pResult[0] = __lsx_vfrintrxxx_s(__lsx_vfmadd_s(p[3], pCtm[0], __lsx_vfmadd_s(p[7], pCtm[1], __lsx_vfmadd_s(p[11], pCtm[2], pCtm[3]))), (const int)(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC));    // color_jitter adjustment R12-R15
+    pResult[1] = __lsx_vfrintrxxx_s(__lsx_vfmadd_s(p[3], pCtm[4], __lsx_vfmadd_s(p[7], pCtm[5], __lsx_vfmadd_s(p[11], pCtm[6], pCtm[7]))), (const int)(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC));    // color_jitter adjustment G12-G15
+    pResult[2] = __lsx_vfrintrxxx_s(__lsx_vfmadd_s(p[3], pCtm[8], __lsx_vfmadd_s(p[7], pCtm[9], __lsx_vfmadd_s(p[11], pCtm[10], pCtm[11]))), (const int)(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC));    // color_jitter adjustment B12-B15
+    p[3] = pResult[0];    // color_jitter adjustment R12-R15
+    p[7] = pResult[1];    // color_jitter adjustment G12-G15
+    p[11] = pResult[2];    // color_jitter adjustment B12-B15
+}
+
+inline void compute_color_jitter_12_host(__m128 *p, __m128 *pCtm)
+{
+    __m128 pResult[3];
+
+    pResult[0] = __lsx_vfmadd_s(p[0], pCtm[0], __lsx_vfmadd_s(p[1], pCtm[1], __lsx_vfmadd_s(p[2], pCtm[2], pCtm[3])));    // color_jitter adjustment R0-R3
+    pResult[1] = __lsx_vfmadd_s(p[0], pCtm[4], __lsx_vfmadd_s(p[1], pCtm[5], __lsx_vfmadd_s(p[2], pCtm[6], pCtm[7])));    // color_jitter adjustment G0-G3
+    pResult[2] = __lsx_vfmadd_s(p[0], pCtm[8], __lsx_vfmadd_s(p[1], pCtm[9], __lsx_vfmadd_s(p[2], pCtm[10], pCtm[11])));    // color_jitter adjustment B0-B3
+    p[0] = pResult[0];    // color_jitter adjustment R0-R3
+    p[1] = pResult[1];    // color_jitter adjustment G0-G3
+    p[2] = pResult[2];    // color_jitter adjustment B0-B3
+}
+
+inline void compute_salt_and_pepper_noise_8_host(__m256 *p, __m256i *pxXorwowStateX, __m256i *pxXorwowStateCounter, __m256 *pSaltAndPepperNoiseParams)
+{
+    __m256 pMask[3];
+    __m256 pRandomNumbers = rpp_host_rng_xorwow_8_f32_avx(pxXorwowStateX, pxXorwowStateCounter);
+    pMask[0] = (__m256)__lasx_xvfcmp_xxx_s(pRandomNumbers, pSaltAndPepperNoiseParams[0], _CMP_GT_OQ);
+    pMask[1] = (__m256)__lasx_xvandn_v(pMask[0], (__m256)__lasx_xvfcmp_xxx_s(pRandomNumbers, pSaltAndPepperNoiseParams[1], _CMP_LE_OQ));
+    pMask[2] = (__m256)__lasx_xvandn_v(pMask[0], (__m256)__lasx_xvfcmp_xxx_s(pRandomNumbers, pSaltAndPepperNoiseParams[1], _CMP_GT_OQ));
+    p[0] = (__m256)__lasx_xvand_v((__m256i)pMask[0], (__m256i)p[0]);
+    p[0] = (__m256)__lasx_xvor_v((__m256i)__lasx_xvandn_v(pMask[1], p[0]), (__m256i)__lasx_xvand_v((__m256i)pMask[1], (__m256i)pSaltAndPepperNoiseParams[2]));
+    p[0] = (__m256)__lasx_xvor_v((__m256i)__lasx_xvandn_v(pMask[2], p[0]), (__m256i)__lasx_xvand_v((__m256i)pMask[2], (__m256i)pSaltAndPepperNoiseParams[3]));
+}
+
+inline void compute_salt_and_pepper_noise_4_host(__m128 *p, __m128i *pxXorwowStateX, __m128i *pxXorwowStateCounter, __m128 *pSaltAndPepperNoiseParams)
+{
+    __m128 pMask[3];
+    __m128 pRandomNumbers = rpp_host_rng_xorwow_4_f32_sse(pxXorwowStateX, pxXorwowStateCounter);
+    pMask[0] = (__m128)__lsx_vfcmp_clt_s(pRandomNumbers, pSaltAndPepperNoiseParams[0]);
+    pMask[1] = (__m128)__lsx_vandn_v((__m128i)pMask[0], (__m128i)(__m128)__lsx_vfcmp_cle_s(pRandomNumbers, pSaltAndPepperNoiseParams[1]));
+    pMask[2] = (__m128)__lsx_vandn_v((__m128i)pMask[0], (__m128i)(__m128)__lsx_vfcmp_clt_s(pRandomNumbers, pSaltAndPepperNoiseParams[1]));
+    p[0] = (__m128)__lsx_vand_v((__m128i)pMask[0], (__m128i)p[0]);
+    p[0] = (__m128)__lsx_vor_v((__m128i)__lsx_vandn_v((__m128i)pMask[1], (__m128i)p[0]), (__m128i)__lsx_vand_v((__m128i)pMask[1], (__m128i)pSaltAndPepperNoiseParams[2]));
+    p[0] = (__m128)__lsx_vor_v((__m128i)__lsx_vandn_v((__m128i)pMask[2], (__m128i)p[0]), (__m128i)__lsx_vand_v((__m128i)pMask[2], (__m128i)pSaltAndPepperNoiseParams[3]));
+}
+
+inline void compute_salt_and_pepper_noise_16_host(__m256 *p, __m256i *pxXorwowStateX, __m256i *pxXorwowStateCounter, __m256 *pSaltAndPepperNoiseParams)
+{
+    compute_salt_and_pepper_noise_8_host(p    , pxXorwowStateX, pxXorwowStateCounter, pSaltAndPepperNoiseParams);
+    compute_salt_and_pepper_noise_8_host(p + 1, pxXorwowStateX, pxXorwowStateCounter, pSaltAndPepperNoiseParams);
+}
+
+inline void compute_salt_and_pepper_noise_16_host(__m128 *p, __m128i *pxXorwowStateX, __m128i *pxXorwowStateCounter, __m128 *pSaltAndPepperNoiseParams)
+{
+    compute_salt_and_pepper_noise_4_host(p    , pxXorwowStateX, pxXorwowStateCounter, pSaltAndPepperNoiseParams);
+    compute_salt_and_pepper_noise_4_host(p + 1, pxXorwowStateX, pxXorwowStateCounter, pSaltAndPepperNoiseParams);
+    compute_salt_and_pepper_noise_4_host(p + 2, pxXorwowStateX, pxXorwowStateCounter, pSaltAndPepperNoiseParams);
+    compute_salt_and_pepper_noise_4_host(p + 3, pxXorwowStateX, pxXorwowStateCounter, pSaltAndPepperNoiseParams);
+}
+
+inline void compute_salt_and_pepper_noise_24_host(__m256 *pR, __m256 *pG, __m256 *pB, __m256i *pxXorwowStateX, __m256i *pxXorwowStateCounter, __m256 *pSaltAndPepperNoiseParams)
+{
+    __m256 pMask[3];
+    __m256 pRandomNumbers = rpp_host_rng_xorwow_8_f32_avx(pxXorwowStateX, pxXorwowStateCounter);
+    pMask[0] = (__m256)__lasx_xvfcmp_xxx_s(pRandomNumbers, pSaltAndPepperNoiseParams[0], _CMP_GT_OQ);
+    pMask[1] = (__m256)__lasx_xvandn_v(pMask[0], (__m256)__lasx_xvfcmp_xxx_s(pRandomNumbers, pSaltAndPepperNoiseParams[1], _CMP_LE_OQ));
+    pMask[2] = (__m256)__lasx_xvandn_v(pMask[0], (__m256)__lasx_xvfcmp_xxx_s(pRandomNumbers, pSaltAndPepperNoiseParams[1], _CMP_GT_OQ));
+    pR[0] = (__m256)__lasx_xvand_v((__m256i)pMask[0], (__m256i)pR[0]);
+    pR[0] = (__m256)__lasx_xvor_v((__m256i)__lasx_xvandn_v(pMask[1], pR[0]), (__m256i)__lasx_xvand_v((__m256i)pMask[1], (__m256i)pSaltAndPepperNoiseParams[2]));
+    pR[0] = (__m256)__lasx_xvor_v((__m256i)__lasx_xvandn_v(pMask[2], pR[0]), (__m256i)__lasx_xvand_v((__m256i)pMask[2], (__m256i)pSaltAndPepperNoiseParams[3]));
+    pG[0] = (__m256)__lasx_xvand_v((__m256i)pMask[0], (__m256i)pG[0]);
+    pG[0] = (__m256)__lasx_xvor_v((__m256i)__lasx_xvandn_v(pMask[1], pG[0]), (__m256i)__lasx_xvand_v((__m256i)pMask[1], (__m256i)pSaltAndPepperNoiseParams[2]));
+    pG[0] = (__m256)__lasx_xvor_v((__m256i)__lasx_xvandn_v(pMask[2], pG[0]), (__m256i)__lasx_xvand_v((__m256i)pMask[2], (__m256i)pSaltAndPepperNoiseParams[3]));
+    pB[0] = (__m256)__lasx_xvand_v((__m256i)pMask[0], (__m256i)pB[0]);
+    pB[0] = (__m256)__lasx_xvor_v((__m256i)__lasx_xvandn_v(pMask[1], pB[0]), (__m256i)__lasx_xvand_v((__m256i)pMask[1], (__m256i)pSaltAndPepperNoiseParams[2]));
+    pB[0] = (__m256)__lasx_xvor_v((__m256i)__lasx_xvandn_v(pMask[2], pB[0]), (__m256i)__lasx_xvand_v((__m256i)pMask[2], (__m256i)pSaltAndPepperNoiseParams[3]));
+}
+
+inline void compute_salt_and_pepper_noise_12_host(__m128 *pR, __m128 *pG, __m128 *pB, __m128i *pxXorwowStateX, __m128i *pxXorwowStateCounter, __m128 *pSaltAndPepperNoiseParams)
+{
+    __m128 pMask[3];
+    __m128 pRandomNumbers = rpp_host_rng_xorwow_4_f32_sse(pxXorwowStateX, pxXorwowStateCounter);
+    pMask[0] = (__m128)__lsx_vfcmp_clt_s(pRandomNumbers, pSaltAndPepperNoiseParams[0]);
+    pMask[1] = (__m128)__lsx_vandn_v((__m128i)pMask[0], (__m128i)(__m128)__lsx_vfcmp_cle_s(pRandomNumbers, pSaltAndPepperNoiseParams[1]));
+    pMask[2] = (__m128)__lsx_vandn_v((__m128i)pMask[0], (__m128i)(__m128)__lsx_vfcmp_clt_s(pRandomNumbers, pSaltAndPepperNoiseParams[1]));
+    pR[0] = (__m128)__lsx_vand_v((__m128i)pMask[0], (__m128i)pR[0]);
+    pR[0] = (__m128)__lsx_vor_v((__m128i)__lsx_vandn_v((__m128i)pMask[1], (__m128i)pR[0]), (__m128i)__lsx_vand_v((__m128i)pMask[1], (__m128i)pSaltAndPepperNoiseParams[2]));
+    pR[0] = (__m128)__lsx_vor_v((__m128i)__lsx_vandn_v((__m128i)pMask[2], (__m128i)pR[0]), (__m128i)__lsx_vand_v((__m128i)pMask[2], (__m128i)pSaltAndPepperNoiseParams[3]));
+    pG[0] = (__m128)__lsx_vand_v((__m128i)pMask[0], (__m128i)pG[0]);
+    pG[0] = (__m128)__lsx_vor_v((__m128i)__lsx_vandn_v((__m128i)pMask[1], (__m128i)pG[0]), (__m128i)__lsx_vand_v((__m128i)pMask[1], (__m128i)pSaltAndPepperNoiseParams[2]));
+    pG[0] = (__m128)__lsx_vor_v((__m128i)__lsx_vandn_v((__m128i)pMask[2], (__m128i)pG[0]), (__m128i)__lsx_vand_v((__m128i)pMask[2], (__m128i)pSaltAndPepperNoiseParams[3]));
+    pB[0] = (__m128)__lsx_vand_v((__m128i)pMask[0],(__m128i)pB[0]);
+    pB[0] = (__m128)__lsx_vor_v((__m128i)__lsx_vandn_v((__m128i)pMask[1], (__m128i)pB[0]), (__m128i)__lsx_vand_v((__m128i)pMask[1], (__m128i)pSaltAndPepperNoiseParams[2]));
+    pB[0] = (__m128)__lsx_vor_v((__m128i)__lsx_vandn_v((__m128i)pMask[2], (__m128i)pB[0]), (__m128i)__lsx_vand_v((__m128i)pMask[2], (__m128i)pSaltAndPepperNoiseParams[3]));
+}
+
+inline void compute_salt_and_pepper_noise_48_host(__m256 *p, __m256i *pxXorwowStateX, __m256i *pxXorwowStateCounter, __m256 *pSaltAndPepperNoiseParams)
+{
+    compute_salt_and_pepper_noise_24_host(p    , p + 2, p +  4, pxXorwowStateX, pxXorwowStateCounter, pSaltAndPepperNoiseParams);
+    compute_salt_and_pepper_noise_24_host(p + 1, p + 3, p +  5, pxXorwowStateX, pxXorwowStateCounter, pSaltAndPepperNoiseParams);
+}
+
+inline void compute_salt_and_pepper_noise_48_host(__m128 *p, __m128i *pxXorwowStateX, __m128i *pxXorwowStateCounter, __m128 *pSaltAndPepperNoiseParams)
+{
+    compute_salt_and_pepper_noise_12_host(p    , p + 4, p +  8, pxXorwowStateX, pxXorwowStateCounter, pSaltAndPepperNoiseParams);
+    compute_salt_and_pepper_noise_12_host(p + 1, p + 5, p +  9, pxXorwowStateX, pxXorwowStateCounter, pSaltAndPepperNoiseParams);
+    compute_salt_and_pepper_noise_12_host(p + 2, p + 6, p + 10, pxXorwowStateX, pxXorwowStateCounter, pSaltAndPepperNoiseParams);
+    compute_salt_and_pepper_noise_12_host(p + 3, p + 7, p + 11, pxXorwowStateX, pxXorwowStateCounter, pSaltAndPepperNoiseParams);
+}
+
+inline void compute_shot_noise_8_host(__m256 *p, __m256i *pxXorwowStateX, __m256i *pxXorwowStateCounter, __m256 *pShotNoiseFactorInv, __m256 *pShotNoiseFactor)
+{
+    __m256 pShotNoiseValue = avx_p0;                                                                                                                                // Rpp32u shotNoiseValue = 0;
+    __m256 pFactValue = fast_exp_avx(__lasx_xvfmul_s(*p, *pShotNoiseFactorInv));                                                                                      // Rpp32f factValue = expf(lambda);
+    __m256 pFactValueMask = avx_p1;                                                                                                                                 // Set mask for pFactValue computation to 1
+    do
+    {
+        pShotNoiseValue = (__m256)__lasx_xvor_v((__m256i)__lasx_xvandn_v(pFactValueMask, pShotNoiseValue), (__m256i)__lasx_xvand_v((__m256i)pFactValueMask, (__m256i)__lasx_xvfadd_s(pShotNoiseValue, avx_p1)));   // shotNoiseValue++;
+        pFactValue = __lasx_xvfmul_s(pFactValue, rpp_host_rng_xorwow_8_f32_avx(pxXorwowStateX, pxXorwowStateCounter));                                                // factValue *= rpp_host_rng_xorwow_f32(xorwowStatePtr);
+        pFactValueMask = (__m256)__lasx_xvfcmp_xxx_s(pFactValue, avx_p1, _CMP_GT_OQ);                                                                                             // compute new pFactValueMask for loop exit condition
+    } while (lasx_movemask_i8(__lasx_xvftint_w_s(pFactValueMask)) != 0);                                                                                        // while (factValue > 1.0f);
+
+    pShotNoiseValue = __lasx_xvfsub_s(pShotNoiseValue, avx_p1);                                                                                                       // shotNoiseValue -= 1;
+    *p = __lasx_xvfmul_s(pShotNoiseValue, *pShotNoiseFactor);                                                                                                         // dst = pShotNoiseValue * shotNoiseFactor;
+}
+
+inline void compute_shot_noise_4_host(__m128 *p, __m128i *pxXorwowStateX, __m128i *pxXorwowStateCounter, __m128 *pShotNoiseFactorInv, __m128 *pShotNoiseFactor)
+{
+    __m128 pShotNoiseValue = xmm_p0;                                                                                                                    // Rpp32u shotNoiseValue = 0;
+    __m128 pFactValue = fast_exp_sse(__lsx_vfmul_s(*p, *pShotNoiseFactorInv));                                                                             // Rpp32f factValue = expf(lambda);
+    __m128 pFactValueMask = xmm_p1;                                                                                                                     // Set mask for pFactValue computation to 1
+    do
+    {
+        pShotNoiseValue = (__m128)__lsx_vor_v((__m128i)__lsx_vandn_v((__m128i)pFactValueMask, (__m128i)pShotNoiseValue), (__m128i)__lsx_vand_v((__m128i)pFactValueMask, (__m128i)__lsx_vfadd_s(pShotNoiseValue, xmm_p1)));   // shotNoiseValue++;
+        pFactValue = __lsx_vfmul_s(pFactValue, rpp_host_rng_xorwow_4_f32_sse(pxXorwowStateX, pxXorwowStateCounter));                                       // factValue *= rpp_host_rng_xorwow_f32(xorwowStatePtr);
+        pFactValueMask = (__m128)__lsx_vfcmp_clt_s(pFactValue, xmm_p1);                                                                                              // compute new pFactValueMask for loop exit condition
+    } while (lsx_movemask_i8(__lsx_vftint_w_s(pFactValueMask)) != 0);                                                                                  // while (factValue > 1.0f);
+
+    pShotNoiseValue = __lsx_vfsub_s(pShotNoiseValue, xmm_p1);                                                                                              // shotNoiseValue -= 1;
+    *p = __lsx_vfmul_s(pShotNoiseValue, *pShotNoiseFactor);                                                                                                // dst = pShotNoiseValue * shotNoiseFactor;
+}
+
+inline void compute_shot_noise_48_host(__m256 *p, __m256i *pxXorwowStateX, __m256i *pxXorwowStateCounter, __m256 *pShotNoiseFactorInv, __m256 *pShotNoiseFactor)
+{
+    compute_shot_noise_8_host(p     , pxXorwowStateX, pxXorwowStateCounter, pShotNoiseFactorInv, pShotNoiseFactor);
+    compute_shot_noise_8_host(p +  1, pxXorwowStateX, pxXorwowStateCounter, pShotNoiseFactorInv, pShotNoiseFactor);
+    compute_shot_noise_8_host(p +  2, pxXorwowStateX, pxXorwowStateCounter, pShotNoiseFactorInv, pShotNoiseFactor);
+    compute_shot_noise_8_host(p +  3, pxXorwowStateX, pxXorwowStateCounter, pShotNoiseFactorInv, pShotNoiseFactor);
+    compute_shot_noise_8_host(p +  4, pxXorwowStateX, pxXorwowStateCounter, pShotNoiseFactorInv, pShotNoiseFactor);
+    compute_shot_noise_8_host(p +  5, pxXorwowStateX, pxXorwowStateCounter, pShotNoiseFactorInv, pShotNoiseFactor);
+}
+
+inline void compute_shot_noise_48_host(__m128 *p, __m128i *pxXorwowStateX, __m128i *pxXorwowStateCounter, __m128 *pShotNoiseFactorInv, __m128 *pShotNoiseFactor)
+{
+    compute_shot_noise_4_host(p     , pxXorwowStateX, pxXorwowStateCounter, pShotNoiseFactorInv, pShotNoiseFactor);
+    compute_shot_noise_4_host(p +  1, pxXorwowStateX, pxXorwowStateCounter, pShotNoiseFactorInv, pShotNoiseFactor);
+    compute_shot_noise_4_host(p +  2, pxXorwowStateX, pxXorwowStateCounter, pShotNoiseFactorInv, pShotNoiseFactor);
+    compute_shot_noise_4_host(p +  3, pxXorwowStateX, pxXorwowStateCounter, pShotNoiseFactorInv, pShotNoiseFactor);
+    compute_shot_noise_4_host(p +  4, pxXorwowStateX, pxXorwowStateCounter, pShotNoiseFactorInv, pShotNoiseFactor);
+    compute_shot_noise_4_host(p +  5, pxXorwowStateX, pxXorwowStateCounter, pShotNoiseFactorInv, pShotNoiseFactor);
+    compute_shot_noise_4_host(p +  6, pxXorwowStateX, pxXorwowStateCounter, pShotNoiseFactorInv, pShotNoiseFactor);
+    compute_shot_noise_4_host(p +  7, pxXorwowStateX, pxXorwowStateCounter, pShotNoiseFactorInv, pShotNoiseFactor);
+    compute_shot_noise_4_host(p +  8, pxXorwowStateX, pxXorwowStateCounter, pShotNoiseFactorInv, pShotNoiseFactor);
+    compute_shot_noise_4_host(p +  9, pxXorwowStateX, pxXorwowStateCounter, pShotNoiseFactorInv, pShotNoiseFactor);
+    compute_shot_noise_4_host(p + 10, pxXorwowStateX, pxXorwowStateCounter, pShotNoiseFactorInv, pShotNoiseFactor);
+    compute_shot_noise_4_host(p + 11, pxXorwowStateX, pxXorwowStateCounter, pShotNoiseFactorInv, pShotNoiseFactor);
+}
+
+inline void compute_shot_noise_24_host(__m256 *p, __m256i *pxXorwowStateX, __m256i *pxXorwowStateCounter, __m256 *pShotNoiseFactorInv, __m256 *pShotNoiseFactor)
+{
+    compute_shot_noise_8_host(p     , pxXorwowStateX, pxXorwowStateCounter, pShotNoiseFactorInv, pShotNoiseFactor);
+    compute_shot_noise_8_host(p +  1, pxXorwowStateX, pxXorwowStateCounter, pShotNoiseFactorInv, pShotNoiseFactor);
+    compute_shot_noise_8_host(p +  2, pxXorwowStateX, pxXorwowStateCounter, pShotNoiseFactorInv, pShotNoiseFactor);
+}
+
+inline void compute_shot_noise_16_host(__m256 *p, __m256i *pxXorwowStateX, __m256i *pxXorwowStateCounter, __m256 *pShotNoiseFactorInv, __m256 *pShotNoiseFactor)
+{
+    compute_shot_noise_8_host(p     , pxXorwowStateX, pxXorwowStateCounter, pShotNoiseFactorInv, pShotNoiseFactor);
+    compute_shot_noise_8_host(p +  1, pxXorwowStateX, pxXorwowStateCounter, pShotNoiseFactorInv, pShotNoiseFactor);
+}
+
+inline void compute_shot_noise_16_host(__m128 *p, __m128i *pxXorwowStateX, __m128i *pxXorwowStateCounter, __m128 *pShotNoiseFactorInv, __m128 *pShotNoiseFactor)
+{
+    compute_shot_noise_4_host(p     , pxXorwowStateX, pxXorwowStateCounter, pShotNoiseFactorInv, pShotNoiseFactor);
+    compute_shot_noise_4_host(p +  1, pxXorwowStateX, pxXorwowStateCounter, pShotNoiseFactorInv, pShotNoiseFactor);
+    compute_shot_noise_4_host(p +  2, pxXorwowStateX, pxXorwowStateCounter, pShotNoiseFactorInv, pShotNoiseFactor);
+    compute_shot_noise_4_host(p +  3, pxXorwowStateX, pxXorwowStateCounter, pShotNoiseFactorInv, pShotNoiseFactor);
+}
+
+inline void compute_shot_noise_12_host(__m128 *p, __m128i *pxXorwowStateX, __m128i *pxXorwowStateCounter, __m128 *pShotNoiseFactorInv, __m128 *pShotNoiseFactor)
+{
+    compute_shot_noise_4_host(p     , pxXorwowStateX, pxXorwowStateCounter, pShotNoiseFactorInv, pShotNoiseFactor);
+    compute_shot_noise_4_host(p +  1, pxXorwowStateX, pxXorwowStateCounter, pShotNoiseFactorInv, pShotNoiseFactor);
+    compute_shot_noise_4_host(p +  2, pxXorwowStateX, pxXorwowStateCounter, pShotNoiseFactorInv, pShotNoiseFactor);
+}
+
+inline Rpp32u compute_shot_noise_1_host(RpptXorwowState *xorwowStatePtr, Rpp32f lambda)
+{
+    Rpp32u shotNoiseValue = 0;                                   // initialize shotNoiseValue to 0
+    Rpp32f factValue = rpp_host_math_exp_lim256approx(lambda);   // initialize factValue to e^lambda
+    do
+    {
+        shotNoiseValue++;                                        // additively cumulate shotNoiseValue by 1 until exit condition
+        factValue *= rpp_host_rng_xorwow_f32(xorwowStatePtr);    // multiplicatively cumulate factValue by the next uniform random number until exit condition
+    } while (factValue > 1.0f);                                  // loop while factValue >= 1.0f
+
+    return shotNoiseValue - 1;
+}
+
+inline void compute_gaussian_noise_16_host(__m256 *p, __m256i *pxXorwowStateX, __m256i *pxXorwowStateCounter, __m256 *pGaussianNoiseParams)
+{
+    __m256 pRngVals[2], pSqrt[2];
+
+    rpp_host_rng_16_gaussian_f32_avx(pRngVals, pxXorwowStateX, pxXorwowStateCounter);               // rngVal = rpp_host_rng_1_gaussian_f32(xorwowStatePtr);
+    pRngVals[0] = __lasx_xvfmadd_s(pRngVals[0], pGaussianNoiseParams[1], pGaussianNoiseParams[0]);   // rngVal = rngVal * stdDev + mean;
+    pRngVals[1] = __lasx_xvfmadd_s(pRngVals[1], pGaussianNoiseParams[1], pGaussianNoiseParams[0]);   // rngVal = rngVal * stdDev + mean;
+    pSqrt[0] = __lasx_xvfsqrt_s(p[0]);                                                                // pixSqrt = sqrt(pixVal);
+    pSqrt[1] = __lasx_xvfsqrt_s(p[1]);                                                                // pixSqrt = sqrt(pixVal);
+    p[0] = __lasx_xvfmadd_s(pSqrt[0], pRngVals[0], p[0]);                                            // return RPPPIXELCHECKF32(pixSqrt * rngVal + pixVal);
+    p[1] = __lasx_xvfmadd_s(pSqrt[1], pRngVals[1], p[1]);                                            // return RPPPIXELCHECKF32(pixSqrt * rngVal + pixVal);
+}
+
+inline void compute_gaussian_noise_voxel_16_host(__m256 *p, __m256i *pxXorwowStateX, __m256i *pxXorwowStateCounter, __m256 *pGaussianNoiseParams)
+{
+    __m256 pRngVals[2], pSqrt[2];
+
+    rpp_host_rng_16_gaussian_f32_avx(pRngVals, pxXorwowStateX, pxXorwowStateCounter);               // rngVal = rpp_host_rng_1_gaussian_f32(xorwowStatePtr);
+    pRngVals[0] = __lasx_xvfmadd_s(pRngVals[0], pGaussianNoiseParams[1], pGaussianNoiseParams[0]);   // rngVal = rngVal * stdDev + mean;
+    pRngVals[1] = __lasx_xvfmadd_s(pRngVals[1], pGaussianNoiseParams[1], pGaussianNoiseParams[0]);   // rngVal = rngVal * stdDev + mean;
+    p[0] = __lasx_xvfadd_s(p[0], pRngVals[0]);                                                        // return pixVal + rngVal;
+    p[1] = __lasx_xvfadd_s(p[1], pRngVals[1]);                                                        // return pixVal + rngVal;
+}
+
+inline void compute_gaussian_noise_8_host(__m128 *p, __m128i *pxXorwowStateX, __m128i *pxXorwowStateCounter, __m128 *pGaussianNoiseParams)
+{
+    __m128 pRngVals[2], pSqrt[2];
+
+    rpp_host_rng_8_gaussian_f32_sse(pRngVals, pxXorwowStateX, pxXorwowStateCounter);            // rngVal = rpp_host_rng_1_gaussian_f32(xorwowStatePtr);
+    pRngVals[0] = __lsx_vfmadd_s(pRngVals[0], pGaussianNoiseParams[1], pGaussianNoiseParams[0]);  // rngVal = rngVal * stdDev + mean;
+    pRngVals[1] = __lsx_vfmadd_s(pRngVals[1], pGaussianNoiseParams[1], pGaussianNoiseParams[0]);  // rngVal = rngVal * stdDev + mean;
+    pSqrt[0] = __lsx_vfsqrt_s(p[0]);                                                               // pixSqrt = sqrt(pixVal);
+    pSqrt[1] = __lsx_vfsqrt_s(p[1]);                                                               // pixSqrt = sqrt(pixVal);
+    p[0] = __lsx_vfmadd_s(pSqrt[0], pRngVals[0], p[0]);                                           // return RPPPIXELCHECKF32(pixSqrt * rngVal + pixVal);
+    p[1] = __lsx_vfmadd_s(pSqrt[1], pRngVals[1], p[1]);                                           // return RPPPIXELCHECKF32(pixSqrt * rngVal + pixVal);
+}
+
+inline void compute_gaussian_noise_voxel_8_host(__m128 *p, __m128i *pxXorwowStateX, __m128i *pxXorwowStateCounter, __m128 *pGaussianNoiseParams)
+{
+    __m128 pRngVals[2], pSqrt[2];
+
+    rpp_host_rng_8_gaussian_f32_sse(pRngVals, pxXorwowStateX, pxXorwowStateCounter);            // rngVal = rpp_host_rng_1_gaussian_f32(xorwowStatePtr);
+    pRngVals[0] = __lsx_vfmadd_s(pRngVals[0], pGaussianNoiseParams[1], pGaussianNoiseParams[0]);  // rngVal = rngVal * stdDev + mean;
+    pRngVals[1] = __lsx_vfmadd_s(pRngVals[1], pGaussianNoiseParams[1], pGaussianNoiseParams[0]);  // rngVal = rngVal * stdDev + mean;
+    p[0] = __lsx_vfadd_s(p[0], pRngVals[0]);                                                       // return (pixVal + rngVal);
+    p[1] = __lsx_vfadd_s(p[1], pRngVals[1]);                                                       // return (pixVal + rngVal);
+}
+
+inline void compute_gaussian_noise_16_host(__m128 *p, __m128i *pxXorwowStateX, __m128i *pxXorwowStateCounter, __m128 *pGaussianNoiseParams)
+{
+    compute_gaussian_noise_8_host(p, pxXorwowStateX, pxXorwowStateCounter, pGaussianNoiseParams);
+    compute_gaussian_noise_8_host(&p[2], pxXorwowStateX, pxXorwowStateCounter, pGaussianNoiseParams);
+}
+
+inline void compute_gaussian_noise_voxel_16_host(__m128 *p, __m128i *pxXorwowStateX, __m128i *pxXorwowStateCounter, __m128 *pGaussianNoiseParams)
+{
+    compute_gaussian_noise_voxel_8_host(p, pxXorwowStateX, pxXorwowStateCounter, pGaussianNoiseParams);
+    compute_gaussian_noise_voxel_8_host(&p[2], pxXorwowStateX, pxXorwowStateCounter, pGaussianNoiseParams);
+}
+
+inline void compute_gaussian_noise_48_host(__m256 *p, __m256i *pxXorwowStateX, __m256i *pxXorwowStateCounter, __m256 *pGaussianNoiseParams)
+{
+    compute_gaussian_noise_16_host(p, pxXorwowStateX, pxXorwowStateCounter, pGaussianNoiseParams);
+    compute_gaussian_noise_16_host(&p[2], pxXorwowStateX, pxXorwowStateCounter, pGaussianNoiseParams);
+    compute_gaussian_noise_16_host(&p[4], pxXorwowStateX, pxXorwowStateCounter, pGaussianNoiseParams);
+}
+
+inline void compute_gaussian_noise_voxel_48_host(__m256 *p, __m256i *pxXorwowStateX, __m256i *pxXorwowStateCounter, __m256 *pGaussianNoiseParams)
+{
+    compute_gaussian_noise_voxel_16_host(p, pxXorwowStateX, pxXorwowStateCounter, pGaussianNoiseParams);
+    compute_gaussian_noise_voxel_16_host(&p[2], pxXorwowStateX, pxXorwowStateCounter, pGaussianNoiseParams);
+    compute_gaussian_noise_voxel_16_host(&p[4], pxXorwowStateX, pxXorwowStateCounter, pGaussianNoiseParams);
+}
+
+inline void compute_gaussian_noise_48_host(__m128 *p, __m128i *pxXorwowStateX, __m128i *pxXorwowStateCounter, __m128 *pGaussianNoiseParams)
+{
+    compute_gaussian_noise_16_host(p, pxXorwowStateX, pxXorwowStateCounter, pGaussianNoiseParams);
+    compute_gaussian_noise_16_host(&p[4], pxXorwowStateX, pxXorwowStateCounter, pGaussianNoiseParams);
+    compute_gaussian_noise_16_host(&p[8], pxXorwowStateX, pxXorwowStateCounter, pGaussianNoiseParams);
+}
+
+inline void compute_gaussian_noise_voxel_48_host(__m128 *p, __m128i *pxXorwowStateX, __m128i *pxXorwowStateCounter, __m128 *pGaussianNoiseParams)
+{
+    compute_gaussian_noise_voxel_16_host(p, pxXorwowStateX, pxXorwowStateCounter, pGaussianNoiseParams);
+    compute_gaussian_noise_voxel_16_host(&p[4], pxXorwowStateX, pxXorwowStateCounter, pGaussianNoiseParams);
+    compute_gaussian_noise_voxel_16_host(&p[8], pxXorwowStateX, pxXorwowStateCounter, pGaussianNoiseParams);
+}
+
+inline void compute_gaussian_noise_24_host(__m256 *p, __m256i *pxXorwowStateX, __m256i *pxXorwowStateCounter, __m256 *pGaussianNoiseParams)
+{
+    compute_gaussian_noise_16_host(p, pxXorwowStateX, pxXorwowStateCounter, pGaussianNoiseParams);
+
+    __m256 pRngVals[2], pSqrt;
+    rpp_host_rng_16_gaussian_f32_avx(pRngVals, pxXorwowStateX, pxXorwowStateCounter);               // rngVal = rpp_host_rng_1_gaussian_f32(xorwowStatePtr);
+    pRngVals[0] = __lasx_xvfmadd_s(pRngVals[0], pGaussianNoiseParams[1], pGaussianNoiseParams[0]);   // rngVal = rngVal * stdDev + mean;
+    pSqrt = __lasx_xvfsqrt_s(p[2]);                                                                   // pixSqrt = sqrt(pixVal);
+    p[2] = __lasx_xvfmadd_s(pSqrt, pRngVals[0], p[2]);                                               // return RPPPIXELCHECKF32(pixSqrt * rngVal + pixVal);
+}
+
+inline void compute_gaussian_noise_voxel_24_host(__m256 *p, __m256i *pxXorwowStateX, __m256i *pxXorwowStateCounter, __m256 *pGaussianNoiseParams)
+{
+    compute_gaussian_noise_16_host(p, pxXorwowStateX, pxXorwowStateCounter, pGaussianNoiseParams);
+
+    __m256 pRngVals[2];
+    rpp_host_rng_16_gaussian_f32_avx(pRngVals, pxXorwowStateX, pxXorwowStateCounter);               // rngVal = rpp_host_rng_1_gaussian_f32(xorwowStatePtr);
+    pRngVals[0] = __lasx_xvfmadd_s(pRngVals[0], pGaussianNoiseParams[1], pGaussianNoiseParams[0]);   // rngVal = rngVal * stdDev + mean;
+    p[2] = __lasx_xvfadd_s(p[2], pRngVals[0]);                                                        // return pixVal + rngVal;
+}
+
+inline void compute_gaussian_noise_12_host(__m128 *p, __m128i *pxXorwowStateX, __m128i *pxXorwowStateCounter, __m128 *pGaussianNoiseParams)
+{
+    compute_gaussian_noise_8_host(p, pxXorwowStateX, pxXorwowStateCounter, pGaussianNoiseParams);
+
+    __m128 pRngVals[2], pSqrt;
+    rpp_host_rng_8_gaussian_f32_sse(pRngVals, pxXorwowStateX, pxXorwowStateCounter);            // rngVal = rpp_host_rng_1_gaussian_f32(xorwowStatePtr);
+    pRngVals[0] = __lsx_vfmadd_s(pRngVals[0], pGaussianNoiseParams[1], pGaussianNoiseParams[0]);  // rngVal = rngVal * stdDev + mean;
+    pSqrt = __lsx_vfsqrt_s(p[2]);                                                                  // pixSqrt = sqrt(pixVal);
+    p[2] = __lsx_vfmadd_s(pSqrt, pRngVals[0], p[2]);                                              // return RPPPIXELCHECKF32(pixSqrt * rngVal + pixVal);
+}
+
+inline void compute_gaussian_noise_voxel_12_host(__m128 *p, __m128i *pxXorwowStateX, __m128i *pxXorwowStateCounter, __m128 *pGaussianNoiseParams)
+{
+    compute_gaussian_noise_8_host(p, pxXorwowStateX, pxXorwowStateCounter, pGaussianNoiseParams);
+
+    __m128 pRngVals[2], pSqrt;
+    rpp_host_rng_8_gaussian_f32_sse(pRngVals, pxXorwowStateX, pxXorwowStateCounter);            // rngVal = rpp_host_rng_1_gaussian_f32(xorwowStatePtr);
+    pRngVals[0] = __lsx_vfmadd_s(pRngVals[0], pGaussianNoiseParams[1], pGaussianNoiseParams[0]);  // rngVal = rngVal * stdDev + mean;
+    p[2] = __lsx_vfadd_s(p[2], pRngVals[0]);                                                       // return pixVal + rngVal;
+}
+
+inline Rpp32f compute_gaussian_noise_1_host(Rpp32f pixVal, RpptXorwowStateBoxMuller *xorwowStatePtr, Rpp32f mean, Rpp32f stdDev)
+{
+    Rpp32f rngVal, pixSqrt;
+    rngVal = rpp_host_rng_1_gaussian_f32(xorwowStatePtr);
+    rngVal = rngVal * stdDev + mean;
+    pixSqrt = sqrt(pixVal);
+
+    return RPPPIXELCHECKF32(pixSqrt * rngVal + pixVal);
+}
+
+inline Rpp32f compute_gaussian_noise_voxel_1_host(Rpp32f pixVal, RpptXorwowStateBoxMuller *xorwowStatePtr, Rpp32f mean, Rpp32f stdDev)
+{
+    Rpp32f rngVal, pixSqrt;
+    rngVal = rpp_host_rng_1_gaussian_f32(xorwowStatePtr);
+    rngVal = rngVal * stdDev + mean;
+    return pixVal + rngVal;
+}
+
+inline void compute_offset_i8_1c_avx(__m256 &p)
+{
+    p = __lasx_xvfadd_s(p, avx_p128);
+}
+
+inline void compute_offset_i8_3c_avx(__m256 *p)
+{
+    compute_offset_i8_1c_avx(p[0]);
+    compute_offset_i8_1c_avx(p[1]);
+    compute_offset_i8_1c_avx(p[2]);
+}
+
+// Compute Functions for RPP Image API
+
+template<typename T>
+inline RppStatus compute_subimage_location_host(T* ptr, T** ptrSubImage,
+                                         RppiSize size, RppiSize *sizeSubImage,
+                                         Rpp32u x1, Rpp32u y1, Rpp32u x2, Rpp32u y2,
+                                         RppiChnFormat chnFormat, Rpp32u channel)
+{
+    if ((RPPINRANGE(x1, 0, size.width - 1) == 0)
+        || (RPPINRANGE(x2, 0, size.width - 1) == 0)
+        || (RPPINRANGE(y1, 0, size.height - 1) == 0)
+        || (RPPINRANGE(y2, 0, size.height - 1) == 0))
+    {
+        return RPP_ERROR;
+    }
+
+    int yDiff = (int) y2 - (int) y1;
+    int xDiff = (int) x2 - (int) x1;
+
+    sizeSubImage->height = (Rpp32u) RPPABS(yDiff) + 1;
+    sizeSubImage->width = (Rpp32u) RPPABS(xDiff) + 1;
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        *ptrSubImage = ptr + (RPPMIN2(y1, y2) * size.width) + RPPMIN2(x1, x2);
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        *ptrSubImage = ptr + (RPPMIN2(y1, y2) * size.width * channel) + (RPPMIN2(x1, x2) * channel);
+    }
+
+    return RPP_SUCCESS;
+}
+
+template<typename T>
+inline void compute_transpose_host(T* srcPtr, RppiSize srcSize, T* dstPtr, RppiSize dstSize,
+                                 RppiChnFormat chnFormat, Rpp32u channel)
+{
+    T *srcPtrTemp, *dstPtrTemp;
+    dstPtrTemp = dstPtr;
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        for (int c = 0; c < channel; c++)
+        {
+            srcPtrTemp = srcPtr + (c * srcSize.height * srcSize.width);
+            for (int i = 0; i < dstSize.height; i++)
+            {
+                for (int j = 0; j < dstSize.width; j++)
+                {
+                    *dstPtrTemp = *(srcPtrTemp + (j * srcSize.width) + i);
+                    dstPtrTemp++;
+                }
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        for (int i = 0; i < dstSize.height; i++)
+        {
+            for (int j = 0; j < dstSize.width; j++)
+            {
+                srcPtrTemp = srcPtr + (channel * ((j * srcSize.width) + i));
+                for (int c = 0; c < channel; c++)
+                {
+                    *dstPtrTemp = *(srcPtrTemp + c);
+                    dstPtrTemp++;
+                }
+            }
+        }
+    }
+}
+
+template <typename T, typename U>
+inline void compute_add_host(T* srcPtr1, U* srcPtr2, RppiSize srcSize, T* dstPtr,
+                   Rpp32u channel)
+{
+    T *srcPtr1Temp, *dstPtrTemp;
+    U *srcPtr2Temp;
+    srcPtr1Temp = srcPtr1;
+    srcPtr2Temp = srcPtr2;
+    dstPtrTemp = dstPtr;
+
+    Rpp32s pixel;
+
+    for (int i = 0; i < (channel * srcSize.height * srcSize.width); i++)
+    {
+        pixel = ((Rpp32s) (*srcPtr1Temp)) + ((Rpp32s) (*srcPtr2Temp));
+        pixel = RPPPIXELCHECK(pixel);
+        *dstPtrTemp =(T) pixel;
+        srcPtr1Temp++;
+        srcPtr2Temp++;
+        dstPtrTemp++;
+    }
+}
+
+template <typename T, typename U>
+inline void compute_subtract_host(T* srcPtr1, U* srcPtr2, RppiSize srcSize, T* dstPtr,
+                        Rpp32u channel)
+{
+    T *srcPtr1Temp, *dstPtrTemp;
+    U *srcPtr2Temp;
+    srcPtr1Temp = srcPtr1;
+    srcPtr2Temp = srcPtr2;
+    dstPtrTemp = dstPtr;
+
+    Rpp32s pixel;
+
+    for (int i = 0; i < (channel * srcSize.height * srcSize.width); i++)
+    {
+        pixel = ((Rpp32s) (*srcPtr1Temp)) - ((Rpp32s) (*srcPtr2Temp));
+        pixel = RPPPIXELCHECK(pixel);
+        *dstPtrTemp =(T) pixel;
+        srcPtr1Temp++;
+        srcPtr2Temp++;
+        dstPtrTemp++;
+    }
+}
+
+template <typename T, typename U>
+inline void compute_multiply_host(T* srcPtr1, U* srcPtr2, RppiSize srcSize, T* dstPtr,
+                                   Rpp32u channel)
+{
+    T *srcPtr1Temp, *dstPtrTemp;
+    U *srcPtr2Temp;
+    srcPtr1Temp = srcPtr1;
+    srcPtr2Temp = srcPtr2;
+    dstPtrTemp = dstPtr;
+
+    Rpp32f pixel;
+
+    for (int i = 0; i < (channel * srcSize.height * srcSize.width); i++)
+    {
+        pixel = ((Rpp32f) (*srcPtr1Temp)) * ((Rpp32f) (*srcPtr2Temp));
+        pixel = RPPPIXELCHECK(pixel);
+        *dstPtrTemp = (T) pixel;
+        srcPtr1Temp++;
+        srcPtr2Temp++;
+        dstPtrTemp++;
+    }
+}
+
+template <typename T, typename U>
+inline void compute_bitwise_AND_host(T* srcPtr1, U* srcPtr2, RppiSize srcSize, T* dstPtr,
+                           Rpp32u channel)
+{
+    T *srcPtr1Temp, *dstPtrTemp;
+    U *srcPtr2Temp;
+    srcPtr1Temp = srcPtr1;
+    srcPtr2Temp = srcPtr2;
+    dstPtrTemp = dstPtr;
+
+    Rpp8u pixel;
+
+    for (int i = 0; i < (channel * srcSize.height * srcSize.width); i++)
+    {
+        pixel = ((Rpp8u) (*srcPtr1Temp)) & ((Rpp8u) (*srcPtr2Temp));
+        pixel = RPPPIXELCHECK(pixel);
+        *dstPtrTemp =(T) pixel;
+        srcPtr1Temp++;
+        srcPtr2Temp++;
+        dstPtrTemp++;
+    }
+}
+
+template <typename T, typename U>
+inline void compute_inclusive_OR_host(T* srcPtr1, U* srcPtr2, RppiSize srcSize, T* dstPtr,
+                            Rpp32u channel)
+{
+    T *srcPtr1Temp, *dstPtrTemp;
+    U *srcPtr2Temp;
+    srcPtr1Temp = srcPtr1;
+    srcPtr2Temp = srcPtr2;
+    dstPtrTemp = dstPtr;
+
+    Rpp8u pixel;
+
+    for (int i = 0; i < (channel * srcSize.height * srcSize.width); i++)
+    {
+        pixel = ((Rpp8u) (*srcPtr1Temp)) | ((Rpp8u) (*srcPtr2Temp));
+        pixel = RPPPIXELCHECK(pixel);
+        *dstPtrTemp =(T) pixel;
+        srcPtr1Temp++;
+        srcPtr2Temp++;
+        dstPtrTemp++;
+    }
+}
+
+template <typename T, typename U>
+inline void compute_exclusive_OR_host(T* srcPtr1, U* srcPtr2, RppiSize srcSize, T* dstPtr,
+                            Rpp32u channel)
+{
+    T *srcPtr1Temp, *dstPtrTemp;
+    U *srcPtr2Temp;
+    srcPtr1Temp = srcPtr1;
+    srcPtr2Temp = srcPtr2;
+    dstPtrTemp = dstPtr;
+
+    Rpp8u pixel;
+
+    for (int i = 0; i < (channel * srcSize.height * srcSize.width); i++)
+    {
+        pixel = ((Rpp8u) (*srcPtr1Temp)) ^ ((Rpp8u) (*srcPtr2Temp));
+        pixel = RPPPIXELCHECK(pixel);
+        *dstPtrTemp =(T) pixel;
+        srcPtr1Temp++;
+        srcPtr2Temp++;
+        dstPtrTemp++;
+    }
+}
+
+template <typename T, typename U>
+inline void compute_min_host(T* srcPtr1, U* srcPtr2, RppiSize srcSize, T* dstPtr,
+                                   Rpp32u channel)
+{
+    T *srcPtr1Temp, *dstPtrTemp;
+    U *srcPtr2Temp;
+    srcPtr1Temp = srcPtr1;
+    srcPtr2Temp = srcPtr2;
+    dstPtrTemp = dstPtr;
+
+    for (int i = 0; i < (channel * srcSize.height * srcSize.width); i++)
+    {
+        *dstPtrTemp = RPPMIN2(*srcPtr1Temp, ((T)*srcPtr2Temp));
+        srcPtr1Temp++;
+        srcPtr2Temp++;
+        dstPtrTemp++;
+    }
+}
+
+template <typename T, typename U>
+inline void compute_max_host(T* srcPtr1, U* srcPtr2, RppiSize srcSize, T* dstPtr,
+                                   Rpp32u channel)
+{
+    T *srcPtr1Temp, *dstPtrTemp;
+    U *srcPtr2Temp;
+    srcPtr1Temp = srcPtr1;
+    srcPtr2Temp = srcPtr2;
+    dstPtrTemp = dstPtr;
+
+    for (int i = 0; i < (channel * srcSize.height * srcSize.width); i++)
+    {
+        *dstPtrTemp = RPPMAX2(*srcPtr1Temp, ((T)*srcPtr2Temp));
+        srcPtr1Temp++;
+        srcPtr2Temp++;
+        dstPtrTemp++;
+    }
+}
+
+template <typename T, typename U>
+inline void compute_rgb_to_hsv_host(T* srcPtr, RppiSize srcSize, U* dstPtr,
+                    RppiChnFormat chnFormat, Rpp32u channel)
+{
+    T *srcPtrTempR, *srcPtrTempG, *srcPtrTempB;
+    U *dstPtrTempH, *dstPtrTempS, *dstPtrTempV;
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        Rpp64u imageDim = srcSize.height * srcSize.width;
+        srcPtrTempR = srcPtr;
+        srcPtrTempG = srcPtr + (imageDim);
+        srcPtrTempB = srcPtr + (2 * imageDim);
+        dstPtrTempH = dstPtr;
+        dstPtrTempS = dstPtr + (imageDim);
+        dstPtrTempV = dstPtr + (2 * imageDim);
+
+        Rpp64u bufferLength = srcSize.height * srcSize.width;
+        Rpp64u alignedLength = bufferLength & ~3;
+
+        __m128i const zero = __lsx_vldi(0);
+        __m128 pDiv = lsx_set1_f32(255.0);
+        __m128 pMul = lsx_set1_f32(360.0);
+        __m128i px0, px1, px2;
+        __m128 xR, xG, xB;
+        __m128 xH, xS, xV, xC;
+        __m128 xX, xY, xZ;
+
+        Rpp64u vectorLoopCount = 0;
+        for (; vectorLoopCount < alignedLength; vectorLoopCount+=4)
+        {
+            px0 =  __lsx_vld((__m128i *)srcPtrTempR, 0);
+            px1 =  __lsx_vld((__m128i *)srcPtrTempG, 0);
+            px2 =  __lsx_vld((__m128i *)srcPtrTempB, 0);
+
+            px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+            px1 = __lsx_vilvl_b(zero, px1);    // pixels 0-7
+            px2 = __lsx_vilvl_b(zero, px2);    // pixels 0-7
+
+            xR = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+            xG = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 0-3
+            xB = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px2));    // pixels 0-3
+
+            xR = __lsx_vfdiv_s(xR, pDiv);
+            xG = __lsx_vfdiv_s(xG, pDiv);
+            xB = __lsx_vfdiv_s(xB, pDiv);
+
+            // Calculate Saturation, Value, Chroma
+            xS = __lsx_vfmax_s(xG, xB);                               // xS <- [max(G, B)]
+            xC = __lsx_vfmin_s(xG, xB);                               // xC <- [min(G, B)]
+
+            xS = __lsx_vfmax_s(xS, xR);                               // xS <- [max(G, B, R)]
+            xC = __lsx_vfmin_s(xC, xR);                               // xC <- [min(G, B, R)]
+
+            xV = xS;                                               // xV <- [V    ]
+            xS = __lsx_vfsub_s(xS, xC);                               // xS <- [V - m]
+            xS = __lsx_vfdiv_s(xS, xV);                               // xS <- [S    ]
+
+            xC = __lsx_vfsub_s(xC, xV);                               // xC <- [V + m]
+
+            // Calculate Hue
+            xZ = (__m128)__lsx_vfcmp_ceq_s(xV, xG);                             // xZ <- [V==G]
+            xX = (__m128)__lsx_vfcmp_cune_s(xV, xR);                            // xX <- [V!=R]
+
+            xY = (__m128)__lsx_vand_v((__m128i)xZ, (__m128i)xX);                               // xY <- [V!=R && V==G]
+            xZ = (__m128)__lsx_vandn_v((__m128i)xZ, (__m128i)xX);                            // xZ <- [V!=R && V!=G]
+
+            xY = (__m128)__lsx_vxor_v((__m128i)xY, (__m128i)SIMD_GET_PS(full));                // xY <- [V==R || V!=G]
+            xZ = (__m128)__lsx_vxor_v((__m128i)xZ, (__m128i)SIMD_GET_PS(full));                // xZ <- [V==R || V==G]
+
+            xR = (__m128)__lsx_vand_v((__m128i)xR, (__m128i)xX);                               // xR <- [X!=0 ? R : 0]
+            xB = (__m128)__lsx_vand_v((__m128i)xB, (__m128i)xZ);                               // xB <- [Z!=0 ? B : 0]
+            xG = (__m128)__lsx_vand_v((__m128i)xG, (__m128i)xY);                               // xG <- [Y!=0 ? G : 0]
+
+            xZ = (__m128)__lsx_vandn_v((__m128i)xZ, (__m128i)SIMD_GET_PS(sn));               // xZ <- [sign(!Z)]
+            xY = (__m128)__lsx_vandn_v((__m128i)xY, (__m128i)SIMD_GET_PS(sn));               // xY <- [sign(!Y)]
+
+            xG = (__m128)__lsx_vxor_v((__m128i)xG, (__m128i)xZ);                               // xG <- [Y!=0 ? (Z==0 ? G : -G) : 0]
+            xR = (__m128)__lsx_vxor_v((__m128i)xR, (__m128i)xY);                               // xR <- [X!=0 ? (Y==0 ? R : -R) : 0]
+
+            // G is the accumulator
+            xG = __lsx_vfadd_s(xG, xR);                               // xG <- [Rx + Gx]
+            xB = (__m128)__lsx_vxor_v((__m128i)xB, (__m128i)xY);                               // xB <- [Z!=0 ? (Y==0 ? B : -B) : 0]
+
+            xC = __lsx_vfmul_s(xC, SIMD_GET_PS(m6_m6_m6_m6));         // xC <- [C*6     ]
+            xG = __lsx_vfsub_s(xG, xB);                               // xG <- [Rx+Gx+Bx]
+
+            xH = (__m128)__lsx_vand_v((__m128i)xX, (__m128i)SIMD_GET_PS(m4o6_m4o6_m4o6_m4o6)); // xH <- [V==R ?0 :-4/6]
+            xG = __lsx_vfdiv_s(xG, xC);                               // xG <- [(Rx+Gx+Bx)/6C]
+
+            // Correct achromatic cases (H/S may be infinite due to zero division)
+            xH = (__m128)__lsx_vxor_v((__m128i)xH, (__m128i)xZ);                               // xH <- [V==R ? 0 : V==G ? -4/6 : 4/6]
+            xC = (__m128)__lsx_vfcmp_cle_s(SIMD_GET_PS(eps), xC);
+            xH = __lsx_vfadd_s(xH, SIMD_GET_PS(p1));                  // xH <- [V==R ? 1 : V==G ?  2/6 :10/6]
+
+            xG = __lsx_vfadd_s(xG, xH);
+
+            // Normalize H to fraction. If H >= 1 then H - 1
+            xH = (__m128)__lsx_vfcmp_cle_s(SIMD_GET_PS(p1), xG);
+
+            xH = (__m128)__lsx_vand_v((__m128i)xH, (__m128i)SIMD_GET_PS(p1));
+            xS = (__m128)__lsx_vand_v((__m128i)xS, (__m128i)xC);
+            xG = (__m128)__lsx_vand_v((__m128i)xG, (__m128i)xC);
+            xG = __lsx_vfsub_s(xG, xH);
+
+            // Multiply by 360
+            xG = __lsx_vfmul_s(xG, pMul);
+
+            __lsx_vst(xG, dstPtrTempH, 0);
+            __lsx_vst(xS, dstPtrTempS, 0);
+            __lsx_vst(xV, dstPtrTempV, 0);
+
+            srcPtrTempR += 4;
+            srcPtrTempG += 4;
+            srcPtrTempB += 4;
+            dstPtrTempH += 4;
+            dstPtrTempS += 4;
+            dstPtrTempV += 4;
+        }
+        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+        {
+            Rpp32f rf, gf, bf, cmax, cmin, delta;
+            rf = ((Rpp32f) *srcPtrTempR) / 255;
+            gf = ((Rpp32f) *srcPtrTempG) / 255;
+            bf = ((Rpp32f) *srcPtrTempB) / 255;
+            cmax = RPPMAX3(rf, gf, bf);
+            cmin = RPPMIN3(rf, gf, bf);
+            delta = cmax - cmin;
+
+            if (delta == 0)
+            {
+                *dstPtrTempH = 0;
+            }
+            else if (cmax == rf)
+            {
+                *dstPtrTempH = round(60 * fmod(((gf - bf) / delta),6));
+            }
+            else if (cmax == gf)
+            {
+                *dstPtrTempH = round(60 * (((bf - rf) / delta) + 2));
+            }
+            else if (cmax == bf)
+            {
+                *dstPtrTempH = round(60 * (((rf - gf) / delta) + 4));
+            }
+
+            while (*dstPtrTempH > 360)
+            {
+                *dstPtrTempH = *dstPtrTempH - 360;
+            }
+            while (*dstPtrTempH < 0)
+            {
+                *dstPtrTempH = 360 + *dstPtrTempH;
+            }
+
+            if (cmax == 0)
+            {
+                *dstPtrTempS = 0;
+            }
+            else
+            {
+                *dstPtrTempS = delta / cmax;
+            }
+
+            *dstPtrTempV = cmax;
+
+            srcPtrTempR++;
+            srcPtrTempG++;
+            srcPtrTempB++;
+            dstPtrTempH++;
+            dstPtrTempS++;
+            dstPtrTempV++;
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        srcPtrTempR = srcPtr;
+        srcPtrTempG = srcPtr + 1;
+        srcPtrTempB = srcPtr + 2;
+        dstPtrTempH = dstPtr;
+        dstPtrTempS = dstPtr + 1;
+        dstPtrTempV = dstPtr + 2;
+
+        for (int i = 0; i < (srcSize.height * srcSize.width); i++)
+        {
+            Rpp32f rf, gf, bf, cmax, cmin, delta;
+            rf = ((Rpp32f) *srcPtrTempR) / 255;
+            gf = ((Rpp32f) *srcPtrTempG) / 255;
+            bf = ((Rpp32f) *srcPtrTempB) / 255;
+            cmax = RPPMAX3(rf, gf, bf);
+            cmin = RPPMIN3(rf, gf, bf);
+            delta = cmax - cmin;
+
+            if (delta == 0)
+            {
+                *dstPtrTempH = 0;
+            }
+            else if (cmax == rf)
+            {
+                *dstPtrTempH = round(60 * fmod(((gf - bf) / delta),6));
+            }
+            else if (cmax == gf)
+            {
+                *dstPtrTempH = round(60 * (((bf - rf) / delta) + 2));
+            }
+            else if (cmax == bf)
+            {
+                *dstPtrTempH = round(60 * (((rf - gf) / delta) + 4));
+            }
+
+            while (*dstPtrTempH > 360)
+            {
+                *dstPtrTempH = *dstPtrTempH - 360;
+            }
+            while (*dstPtrTempH < 0)
+            {
+                *dstPtrTempH = 360 + *dstPtrTempH;
+            }
+
+            if (cmax == 0)
+            {
+                *dstPtrTempS = 0;
+            }
+            else
+            {
+                *dstPtrTempS = delta / cmax;
+            }
+
+            *dstPtrTempV = cmax;
+
+            srcPtrTempR += 3;
+            srcPtrTempG += 3;
+            srcPtrTempB += 3;
+            dstPtrTempH += 3;
+            dstPtrTempS += 3;
+            dstPtrTempV += 3;
+        }
+    }
+}
+
+template <typename T, typename U>
+inline void compute_hsv_to_rgb_host(T* srcPtr, RppiSize srcSize, U* dstPtr,
+                    RppiChnFormat chnFormat, Rpp32u channel)
+{
+    T *srcPtrTempH, *srcPtrTempS, *srcPtrTempV;
+    U *dstPtrTempR, *dstPtrTempG, *dstPtrTempB;
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        Rpp64u imageDim = srcSize.height * srcSize.width;
+        srcPtrTempH = srcPtr;
+        srcPtrTempS = srcPtr + (imageDim);
+        srcPtrTempV = srcPtr + (2 * imageDim);
+        dstPtrTempR = dstPtr;
+        dstPtrTempG = dstPtr + (imageDim);
+        dstPtrTempB = dstPtr + (2 * imageDim);
+
+        Rpp64u bufferLength = srcSize.height * srcSize.width;
+        Rpp64u alignedLength = bufferLength & ~3;
+
+        __m128 pDiv = lsx_set1_f32(360.0);
+        __m128 pMul = lsx_set1_f32(255.0);
+
+        __m128 h0, h1, h2, h3;
+        h0 = lsx_set1_f32(1.0);
+        __m128 x0, x1, x2, x3;
+        __m128 a0, a1;
+        __m128i px1, px2, px3;
+
+        Rpp8u arrayR[4];
+        Rpp8u arrayG[4];
+        Rpp8u arrayB[4];
+
+        Rpp64u vectorLoopCount = 0;
+        for (; vectorLoopCount < alignedLength; vectorLoopCount+=4)
+        {
+            // Load
+            h1 =  (__m128)__lsx_vld((float*)srcPtrTempH, 0);
+            h2 =  (__m128)__lsx_vld((float*)srcPtrTempS, 0);
+            h3 =  (__m128)__lsx_vld((float*)srcPtrTempV, 0);
+
+            h1 = __lsx_vfdiv_s(h1, pDiv);
+
+            _MM_TRANSPOSE4_PS (h0, h1, h2, h3);
+
+            // Prepare HUE for RGB components (per pixel).
+            x0 = SIMD_SHUFFLE_PS(h0, _MM_SHUFFLE(1, 1, 1, 3));     // x0 <- [H           |H           |H           |V          ]
+            x1 = SIMD_SHUFFLE_PS(h1, _MM_SHUFFLE(1, 1, 1, 3));     // x1 <- [H           |H           |H           |V          ]
+            x2 = SIMD_SHUFFLE_PS(h2, _MM_SHUFFLE(1, 1, 1, 3));     // x2 <- [H           |H           |H           |V          ]
+            x3 = SIMD_SHUFFLE_PS(h3, _MM_SHUFFLE(1, 1, 1, 3));     // x3 <- [H           |H           |H           |V          ]
+
+            // Calculate intervals from HUE.
+            x0 = __lsx_vfsub_s(x0, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x0 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+            x1 = __lsx_vfsub_s(x1, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x1 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+            x2 = __lsx_vfsub_s(x2, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x2 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+            x3 = __lsx_vfsub_s(x3, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x3 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+
+            x0 = (__m128)__lsx_vand_v((__m128i)x0, (__m128i)SIMD_GET_PS(abs));                 // x0 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+            x1 = (__m128)__lsx_vand_v((__m128i)x1, (__m128i)SIMD_GET_PS(abs));                 // x1 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+            x2 = (__m128)__lsx_vand_v((__m128i)x2, (__m128i)SIMD_GET_PS(abs));                 // x2 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+            x3 = (__m128)__lsx_vand_v((__m128i)x3, (__m128i)SIMD_GET_PS(abs));                 // x3 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+
+            x0 = __lsx_vfmul_s(x0, SIMD_GET_PS(m6_m6_p6_p0));         // x0 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+            x1 = __lsx_vfmul_s(x1, SIMD_GET_PS(m6_m6_p6_p0));         // x1 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+            x2 = __lsx_vfmul_s(x2, SIMD_GET_PS(m6_m6_p6_p0));         // x2 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+            x3 = __lsx_vfmul_s(x3, SIMD_GET_PS(m6_m6_p6_p0));         // x3 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+
+            x0 = __lsx_vfadd_s(x0, SIMD_GET_PS(p1_p1_m2_p0));         // x0 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+            x1 = __lsx_vfadd_s(x1, SIMD_GET_PS(p1_p1_m2_p0));         // x1 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+            x2 = __lsx_vfadd_s(x2, SIMD_GET_PS(p1_p1_m2_p0));         // x2 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+            x3 = __lsx_vfadd_s(x3, SIMD_GET_PS(p1_p1_m2_p0));         // x3 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+
+            // Bound intervals.
+            x0 = __lsx_vfmax_s(x0, SIMD_GET_PS(m1_m1_m1_p1));
+            x1 = __lsx_vfmax_s(x1, SIMD_GET_PS(m1_m1_m1_p1));
+            x2 = __lsx_vfmax_s(x2, SIMD_GET_PS(m1_m1_m1_p1));
+            x3 = __lsx_vfmax_s(x3, SIMD_GET_PS(m1_m1_m1_p1));
+
+            x0 = __lsx_vfmin_s(x0, SIMD_GET_PS(p0));                  // x0 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+            x1 = __lsx_vfmin_s(x1, SIMD_GET_PS(p0));                  // x1 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+            x2 = __lsx_vfmin_s(x2, SIMD_GET_PS(p0));                  // x2 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+            x3 = __lsx_vfmin_s(x3, SIMD_GET_PS(p0));                  // x3 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+
+            // Prepare S/V vectors.
+            a0 = SIMD_SHUFFLE_PS(h0, _MM_SHUFFLE(2, 2, 2, 2));     // a0 <- [S           |S           |S           |S          ]
+            a1 = SIMD_SHUFFLE_PS(h1, _MM_SHUFFLE(2, 2, 2, 2));     // a1 <- [S           |S           |S           |S          ]
+            h0 = SIMD_SHUFFLE_PS(h0, _MM_SHUFFLE(3, 3, 3, 0));     // h0 <- [V           |V           |V           |A          ]
+            h1 = SIMD_SHUFFLE_PS(h1, _MM_SHUFFLE(3, 3, 3, 0));     // h1 <- [V           |V           |V           |A          ]
+
+            // Multiply with 'S*V' and add 'V'.
+            x0 = __lsx_vfmul_s(x0, a0);                               // x0 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            x1 = __lsx_vfmul_s(x1, a1);                               // x1 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            a0 = SIMD_SHUFFLE_PS(h2, _MM_SHUFFLE(2, 2, 2, 2));     // a0 <- [S           |S           |S           |S          ]
+            a1 = SIMD_SHUFFLE_PS(h3, _MM_SHUFFLE(2, 2, 2, 2));     // a1 <- [S           |S           |S           |S          ]
+
+            x0 = __lsx_vfmul_s(x0, h0);                               // x0 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            x1 = __lsx_vfmul_s(x1, h1);                               // x1 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            h2 = SIMD_SHUFFLE_PS(h2, _MM_SHUFFLE(3, 3, 3, 0));     // h2 <- [V           |V           |V           |A          ]
+            h3 = SIMD_SHUFFLE_PS(h3, _MM_SHUFFLE(3, 3, 3, 0));     // h3 <- [V           |V           |V           |A          ]
+
+            x2 = __lsx_vfmul_s(x2, a0);                               // x2 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            x3 = __lsx_vfmul_s(x3, a1);                               // x3 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            x0 = __lsx_vfadd_s(x0, h0);                               // x0 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |A          ]
+
+            x2 = __lsx_vfmul_s(x2, h2);                               // x2 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            x3 = __lsx_vfmul_s(x3, h3);                               // x3 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            x1 = __lsx_vfadd_s(x1, h1);                               // x1 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |A          ]
+
+            x2 = __lsx_vfadd_s(x2, h2);                               // x2 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |A          ]
+            x3 = __lsx_vfadd_s(x3, h3);                               // x3 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |A          ]
+
+            // Store
+            _MM_TRANSPOSE4_PS (x0, x1, x2, x3);
+
+            x1 = __lsx_vfmul_s(x1, pMul);
+            x2 = __lsx_vfmul_s(x2, pMul);
+            x3 = __lsx_vfmul_s(x3, pMul);
+
+            px1 = __lsx_vftint_w_s(x1);
+            px2 = __lsx_vftint_w_s(x2);
+            px3 = __lsx_vftint_w_s(x3);
+
+            px1 = lsx_packs_i32(px1, px1);
+            px1 = lsx_packus_i16(px1, px1);
+            *((int*)arrayR) = __lsx_vpickve2gr_w(px1, 0);
+
+            px2 = lsx_packs_i32(px2, px2);
+            px2 = lsx_packus_i16(px2, px2);
+            *((int*)arrayG) = __lsx_vpickve2gr_w(px2, 0);
+
+            px3 = lsx_packs_i32(px3, px3);
+            px3 = lsx_packus_i16(px3, px3);
+            *((int*)arrayB) = __lsx_vpickve2gr_w(px3, 0);
+
+            memcpy(dstPtrTempR, arrayR, 4 * sizeof(Rpp8u));
+            memcpy(dstPtrTempG, arrayG, 4 * sizeof(Rpp8u));
+            memcpy(dstPtrTempB, arrayB, 4 * sizeof(Rpp8u));
+
+            srcPtrTempH += 4;
+            srcPtrTempS += 4;
+            srcPtrTempV += 4;
+            dstPtrTempR += 4;
+            dstPtrTempG += 4;
+            dstPtrTempB += 4;
+        }
+        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+        {
+            Rpp32f c, x, m, rf, gf, bf;
+            c = *srcPtrTempV * *srcPtrTempS;
+            x = c * (1 - abs(int(fmod((*srcPtrTempH / 60), 2)) - 1));
+            m = *srcPtrTempV - c;
+
+            if ((0 <= *srcPtrTempH) && (*srcPtrTempH < 60))
+            {
+                rf = c;
+                gf = x;
+                bf = 0;
+            }
+            else if ((60 <= *srcPtrTempH) && (*srcPtrTempH < 120))
+            {
+                rf = x;
+                gf = c;
+                bf = 0;
+            }
+            else if ((120 <= *srcPtrTempH) && (*srcPtrTempH < 180))
+            {
+                rf = 0;
+                gf = c;
+                bf = x;
+            }
+            else if ((180 <= *srcPtrTempH) && (*srcPtrTempH < 240))
+            {
+                rf = 0;
+                gf = x;
+                bf = c;
+            }
+            else if ((240 <= *srcPtrTempH) && (*srcPtrTempH < 300))
+            {
+                rf = x;
+                gf = 0;
+                bf = c;
+            }
+            else if ((300 <= *srcPtrTempH) && (*srcPtrTempH < 360))
+            {
+                rf = c;
+                gf = 0;
+                bf = x;
+            }
+
+            *dstPtrTempR = (Rpp8u) round((rf + m) * 255);
+            *dstPtrTempG = (Rpp8u) round((gf + m) * 255);
+            *dstPtrTempB = (Rpp8u) round((bf + m) * 255);
+
+            srcPtrTempH++;
+            srcPtrTempS++;
+            srcPtrTempV++;
+            dstPtrTempR++;
+            dstPtrTempG++;
+            dstPtrTempB++;
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        srcPtrTempH = srcPtr;
+        srcPtrTempS = srcPtr + 1;
+        srcPtrTempV = srcPtr + 2;
+        dstPtrTempR = dstPtr;
+        dstPtrTempG = dstPtr + 1;
+        dstPtrTempB = dstPtr + 2;
+
+        for (int i = 0; i < (srcSize.height * srcSize.width); i++)
+        {
+            Rpp32f c, x, m, rf, gf, bf;
+            c = *srcPtrTempV * *srcPtrTempS;
+            x = c * (1 - abs(int(fmod((*srcPtrTempH / 60), 2)) - 1));
+            m = *srcPtrTempV - c;
+
+            if ((0 <= *srcPtrTempH) && (*srcPtrTempH < 60))
+            {
+                rf = c;
+                gf = x;
+                bf = 0;
+            }
+            else if ((60 <= *srcPtrTempH) && (*srcPtrTempH < 120))
+            {
+                rf = x;
+                gf = c;
+                bf = 0;
+            }
+            else if ((120 <= *srcPtrTempH) && (*srcPtrTempH < 180))
+            {
+                rf = 0;
+                gf = c;
+                bf = x;
+            }
+            else if ((180 <= *srcPtrTempH) && (*srcPtrTempH < 240))
+            {
+                rf = 0;
+                gf = x;
+                bf = c;
+            }
+            else if ((240 <= *srcPtrTempH) && (*srcPtrTempH < 300))
+            {
+                rf = x;
+                gf = 0;
+                bf = c;
+            }
+            else if ((300 <= *srcPtrTempH) && (*srcPtrTempH < 360))
+            {
+                rf = c;
+                gf = 0;
+                bf = x;
+            }
+
+            *dstPtrTempR = (Rpp8u) round((rf + m) * 255);
+            *dstPtrTempG = (Rpp8u) round((gf + m) * 255);
+            *dstPtrTempB = (Rpp8u) round((bf + m) * 255);
+
+            srcPtrTempH += 3;
+            srcPtrTempS += 3;
+            srcPtrTempV += 3;
+            dstPtrTempR += 3;
+            dstPtrTempG += 3;
+            dstPtrTempB += 3;
+        }
+    }
+}
+
+template <typename T, typename U>
+inline void compute_magnitude_host(T* srcPtr1, T* srcPtr2, RppiSize srcSize, U* dstPtr,
+                         RppiChnFormat chnFormat, Rpp32u channel)
+{
+    T *srcPtr1Temp, *srcPtr2Temp;
+    U *dstPtrTemp;
+    srcPtr1Temp = srcPtr1;
+    srcPtr2Temp = srcPtr2;
+    dstPtrTemp = dstPtr;
+
+    Rpp32f pixel;
+    Rpp32s srcPtr1Value, srcPtr2Value;
+
+    for (int i = 0; i < (channel * srcSize.height * srcSize.width); i++)
+    {
+        srcPtr1Value = (Rpp32s) *srcPtr1Temp;
+        srcPtr2Value = (Rpp32s) *srcPtr2Temp;
+        pixel = sqrt((srcPtr1Value * srcPtr1Value) + (srcPtr2Value * srcPtr2Value));
+        pixel = RPPPIXELCHECK(pixel);
+        *dstPtrTemp =(U) round(pixel);
+        srcPtr1Temp++;
+        srcPtr2Temp++;
+        dstPtrTemp++;
+    }
+}
+
+template <typename T>
+inline void compute_magnitude_ROI_host(T* srcPtr1, T* srcPtr2, RppiSize srcSize, T* dstPtr,
+                                            Rpp32f x1, Rpp32f y1, Rpp32f x2, Rpp32f y2,
+                                            RppiChnFormat chnFormat, Rpp32u channel)
+{
+    Rpp32u imageDim = srcSize.height * srcSize.width;
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        for(int c = 0; c < channel; c++)
+        {
+            T *srcPtr1Channel, *srcPtr2Channel, *dstPtrChannel;
+            srcPtr1Channel = srcPtr1 + (c * imageDim);
+            srcPtr2Channel = srcPtr2 + (c * imageDim);
+            dstPtrChannel = dstPtr + (c * imageDim);
+
+
+            for(int i = 0; i < srcSize.height; i++)
+            {
+                Rpp32f pixel;
+
+                T *srcPtr1Temp, *srcPtr2Temp, *dstPtrTemp;
+                srcPtr1Temp = srcPtr1Channel + (i * srcSize.width);
+                srcPtr2Temp = srcPtr2Channel + (i * srcSize.width);
+                dstPtrTemp = dstPtrChannel + (i * srcSize.width);
+
+                if (!((y1 <= i) && (i <= y2)))
+                {
+                    memcpy(dstPtrTemp, srcPtr1Temp, srcSize.width * sizeof(T));
+
+                    srcPtr1Temp += srcSize.width;
+                    srcPtr2Temp += srcSize.width;
+                    dstPtrTemp += srcSize.width;
+                }
+                else
+                {
+                    for(int j = 0; j < srcSize.width; j++)
+                    {
+                        if((x1 <= j) && (j <= x2 ))
+                        {
+                            Rpp32s srcPtr1Value = (Rpp32s) *srcPtr1Temp;
+                            Rpp32s srcPtr2Value = (Rpp32s) *srcPtr2Temp;
+                            pixel = sqrt((srcPtr1Value * srcPtr1Value) + (srcPtr2Value * srcPtr2Value));
+                            pixel = RPPPIXELCHECK(pixel);
+                            *dstPtrTemp =(T) round(pixel);
+
+                            srcPtr1Temp++;
+                            srcPtr2Temp++;
+                            dstPtrTemp++;
+                        }
+                        else
+                        {
+                            *dstPtrTemp = *srcPtr1Temp;
+
+                            srcPtr1Temp++;
+                            srcPtr2Temp++;
+                            dstPtrTemp++;
+                        }
+                    }
+                }
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        Rpp32u elementsInRow = channel * srcSize.width;
+
+
+        for(int i = 0; i < srcSize.height; i++)
+        {
+            Rpp32f pixel;
+
+            T *srcPtr1Temp, *srcPtr2Temp, *dstPtrTemp;
+            srcPtr1Temp = srcPtr1 + (i * elementsInRow);
+            srcPtr2Temp = srcPtr2 + (i * elementsInRow);
+            dstPtrTemp = dstPtr + (i * elementsInRow);
+
+            if (!((y1 <= i) && (i <= y2)))
+            {
+                memcpy(dstPtrTemp, srcPtr1Temp, elementsInRow * sizeof(T));
+
+                srcPtr1Temp += elementsInRow;
+                srcPtr2Temp += elementsInRow;
+                dstPtrTemp += elementsInRow;
+            }
+            else
+            {
+                for(int j = 0; j < srcSize.width; j++)
+                {
+                    if (!((x1 <= j) && (j <= x2 )))
+                    {
+                        memcpy(dstPtrTemp, srcPtr1Temp, channel * sizeof(T));
+
+                        srcPtr1Temp += channel;
+                        srcPtr2Temp += channel;
+                        dstPtrTemp += channel;
+                    }
+                    else
+                    {
+                        for(int c = 0; c < channel; c++)
+                        {
+                            Rpp32s srcPtr1Value = (Rpp32s) *srcPtr1Temp;
+                            Rpp32s srcPtr2Value = (Rpp32s) *srcPtr2Temp;
+                            pixel = sqrt((srcPtr1Value * srcPtr1Value) + (srcPtr2Value * srcPtr2Value));
+                            pixel = RPPPIXELCHECK(pixel);
+                            *dstPtrTemp =(T) round(pixel);
+
+                            srcPtr1Temp++;
+                            srcPtr2Temp++;
+                            dstPtrTemp++;
+                        }
+                    }
+                }
+            }
+        }
+    }
+}
+
+template <typename T, typename U>
+inline void compute_threshold_host(T* srcPtr, RppiSize srcSize, U* dstPtr,
+                                 U min, U max, Rpp32u type,
+                                 RppiChnFormat chnFormat, Rpp32u channel)
+{
+    T *srcPtrTemp;
+    U *dstPtrTemp;
+    srcPtrTemp = srcPtr;
+    dstPtrTemp = dstPtr;
+
+    if (type == 1)
+    {
+        for (int i = 0; i < (channel * srcSize.height * srcSize.width); i++)
+        {
+            if (*srcPtrTemp < min)
+            {
+                *dstPtrTemp = (U) 0;
+            }
+            else if (*srcPtrTemp <= max)
+            {
+                *dstPtrTemp = (U) 255;
+            }
+            else
+            {
+                *dstPtrTemp = (U) 0;
+            }
+
+            srcPtrTemp++;
+            dstPtrTemp++;
+        }
+    }
+    else if (type == 2)
+    {
+        for (int i = 0; i < (channel * srcSize.height * srcSize.width); i++)
+        {
+            if (RPPABS(*srcPtrTemp) < min)
+            {
+                *dstPtrTemp = (U) 0;
+            }
+            else if (RPPABS(*srcPtrTemp) <= max)
+            {
+                *dstPtrTemp = (U) 255;
+            }
+            else
+            {
+                *dstPtrTemp = (U) 0;
+            }
+
+            srcPtrTemp++;
+            dstPtrTemp++;
+        }
+    }
+}
+
+template <typename T>
+inline void compute_data_object_copy_host(T* srcPtr, RppiSize srcSize, T* dstPtr,
+                    RppiChnFormat chnFormat, Rpp32u channel)
+{
+    memcpy(dstPtr, srcPtr, srcSize.height * srcSize.width * channel * sizeof(T));
+
+}
+
+template <typename T>
+inline void compute_downsampled_image_host(T* srcPtr, RppiSize srcSize, T* dstPtr, RppiSize dstSize,
+                                         RppiChnFormat chnFormat, Rpp32u channel)
+{
+    T *srcPtrTemp, *dstPtrTemp;
+    srcPtrTemp = srcPtr;
+    dstPtrTemp = dstPtr;
+
+    Rpp8u checkEven;
+    checkEven = (Rpp8u) RPPISEVEN(srcSize.width);
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        for (int c = 0; c < channel; c++)
+        {
+            srcPtrTemp = srcPtr + (c * srcSize.height * srcSize.width);
+
+            for (int i = 0; i < dstSize.height; i++)
+            {
+                for (int j = 0; j < dstSize.width; j++)
+                {
+                    *dstPtrTemp = *srcPtrTemp;
+                    srcPtrTemp += 2;
+                    dstPtrTemp++;
+                }
+                if (checkEven == 0)
+                {
+                    srcPtrTemp--;
+                }
+                srcPtrTemp += srcSize.width;
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        Rpp32u elementsInRow = srcSize.width * channel;
+
+        for (int i = 0; i < dstSize.height; i++)
+        {
+            for (int j = 0; j < dstSize.width; j++)
+            {
+                for (int c = 0; c < channel; c++)
+                {
+                    *dstPtrTemp = *srcPtrTemp;
+                    srcPtrTemp++;
+                    dstPtrTemp++;
+                }
+                srcPtrTemp += channel;
+            }
+            if (checkEven == 0)
+            {
+                srcPtrTemp -= channel;
+            }
+            srcPtrTemp += elementsInRow;
+        }
+    }
+}
+
+template <typename T>
+inline RppStatus compute_channel_extract_host(T* srcPtr, RppiSize srcSize, T* dstPtr,
+                                       Rpp32u extractChannelNumber,
+                                       RppiChnFormat chnFormat, Rpp32u channel)
+{
+    if (extractChannelNumber != 0 && extractChannelNumber != 1 && extractChannelNumber != 2)
+    {
+        return RPP_ERROR;
+    }
+
+    T *srcPtrTemp, *dstPtrTemp;
+    dstPtrTemp = dstPtr;
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        srcPtrTemp = srcPtr + (extractChannelNumber * srcSize.height * srcSize.width);
+        memcpy(dstPtrTemp, srcPtrTemp, srcSize.height * srcSize.width * sizeof(T));
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        srcPtrTemp = srcPtr + extractChannelNumber;
+        for (int i = 0; i < srcSize.height * srcSize.width; i++)
+        {
+            *dstPtrTemp = *srcPtrTemp;
+            srcPtrTemp = srcPtrTemp + channel;
+            dstPtrTemp++;
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T, typename U>
+inline void compute_gradient_direction_host(T* gradientX, T* gradientY, RppiSize srcSize, U* gradientDirection,
+                                          RppiChnFormat chnFormat, Rpp32u channel)
+{
+    T *gradientXTemp, *gradientYTemp;
+    U *gradientDirectionTemp;
+    gradientXTemp = gradientX;
+    gradientYTemp = gradientY;
+    gradientDirectionTemp = gradientDirection;
+
+    Rpp32f pixel;
+
+    for (int i = 0; i < (srcSize.height * srcSize.width * channel); i++)
+    {
+        if (*gradientXTemp != 0)
+        {
+            *gradientDirectionTemp = atan((Rpp32f) *gradientYTemp / (Rpp32f) *gradientXTemp);
+        }
+        else
+        {
+            if (*gradientYTemp > 0)
+            {
+                *gradientDirectionTemp = ((Rpp32f) PI) / 2.0;
+            }
+            else if (*gradientYTemp < 0)
+            {
+                *gradientDirectionTemp = ((Rpp32f) PI) / 2.0 * -1.0;
+            }
+            else if (*gradientYTemp == 0)
+            {
+                *gradientDirectionTemp = 0.0;
+            }
+        }
+        gradientDirectionTemp++;
+        gradientXTemp++;
+        gradientYTemp++;
+    }
+}
+
+inline Rpp32u fogGenerator(Rpp32u srcPtr, Rpp32f fogValue, int colour, int check)
+{
+    int fog = 0;
+    int range = 3;
+    if(check >= (240) && fogValue!=0);
+    else if(check>=(170))
+        range = 1;
+    else if(check<=(85))
+        range = 2;
+    else
+        range = 3;
+    switch(range)
+    {
+        case 1:
+            if(colour==1)
+            {
+                fog = srcPtr * (1.5 + fogValue) - (fogValue*4) + (7*fogValue);
+            }
+            else if(colour==2)
+            {
+                fog = srcPtr * (1.5 + fogValue) + (7*fogValue);
+            }
+            else
+            {
+                fog = srcPtr * (1.5 + fogValue) + (fogValue*4) + (7*fogValue);
+            }
+            break;
+        case 2:
+            if(colour==1)
+            {
+                fog = srcPtr * (1.5 + pow(fogValue,2)) - (fogValue*4) + (130*fogValue);
+            }
+            else if(colour==2)
+            {
+                fog = srcPtr * (1.5 + pow(fogValue,2)) + (130*fogValue);
+            }
+            else
+            {
+                fog = srcPtr * (1.5 + pow(fogValue,2)) + (fogValue*4) + 130*fogValue;
+            }
+            break;
+        case 3:
+            if(colour==1)
+            {
+                fog = srcPtr * (1.5 + pow(fogValue,1.5)) - (fogValue*4) + 20 + (100*fogValue);
+            }
+            else if(colour==2)
+            {
+                fog = srcPtr * (1.5 + pow(fogValue,1.5)) + 20 + (100*fogValue);
+            }
+            else
+            {
+                fog = srcPtr * (1.5 + pow(fogValue,1.5)) + (fogValue*4) + (100*fogValue);
+            }
+            break;
+    }
+    fog = RPPPIXELCHECK(fog);
+    return fog;
+}
+
+inline void compute_image_location_host(RppiSize *batch_srcSizeMax, int batchCount, Rpp32u *loc, Rpp32u channel)
+{
+    for (int m = 0; m < batchCount; m++)
+    {
+        *loc += (batch_srcSizeMax[m].height * batch_srcSizeMax[m].width);
+    }
+    *loc *= channel;
+}
+
+template <typename T>
+inline void compute_1_channel_minmax_host(T *srcPtr, RppiSize srcSize, RppiSize srcSizeMax,
+                                                    T *min, T *max,
+                                                    RppiChnFormat chnFormat, Rpp32u channel)
+{
+    T *srcPtrTemp;
+    srcPtrTemp = srcPtr;
+
+    __m128i pMin, pMax;
+
+    for (int i = 0; i < srcSize.height; i++)
+    {
+        pMin = __lsx_vreplgr2vr_b(*min);
+        pMax = __lsx_vreplgr2vr_b(*max);
+
+        int bufferLength = srcSize.width;
+        int alignedLength = bufferLength & ~15;
+
+        __m128i px0;
+
+        int vectorLoopCount = 0;
+        for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+        {
+            px0 =  __lsx_vld((__m128i *)srcPtrTemp, 0);
+            pMin = __lsx_vmin_bu(px0, pMin);
+            pMax = __lsx_vmax_bu(px0, pMax);
+            srcPtrTemp +=16;
+        }
+        *min = (T) HorMin(pMin);
+        *max = (T) HorMax(pMax);
+        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+        {
+            if (*srcPtrTemp < *min)
+            {
+                *min = *srcPtrTemp;
+            }
+            if (*srcPtrTemp > *max)
+            {
+                *max = *srcPtrTemp;
+            }
+            srcPtrTemp++;
+        }
+        srcPtrTemp += (srcSizeMax.width - srcSize.width);
+    }
+}
+
+template <typename T>
+inline void compute_3_channel_minmax_host(T *srcPtr, RppiSize srcSize, RppiSize srcSizeMax,
+                                               T *min, T *max,
+                                               RppiChnFormat chnFormat, Rpp32u channel)
+{
+    T *srcPtrTemp;
+    srcPtrTemp = srcPtr;
+
+    __m128i pMin, pMax;
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        for (int c = 0; c < channel; c++)
+        {
+            T minTemp, maxTemp;
+            minTemp = *(min + c);
+            maxTemp = *(max + c);
+            for (int i = 0; i < srcSize.height; i++)
+            {
+                pMin = __lsx_vreplgr2vr_b(minTemp);
+                pMax = __lsx_vreplgr2vr_b(maxTemp);
+
+                int bufferLength = srcSize.width;
+                int alignedLength = bufferLength & ~15;
+
+                __m128i px0;
+
+                int vectorLoopCount = 0;
+                for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+                {
+                    px0 =  __lsx_vld((__m128i *)srcPtrTemp, 0);
+                    pMin = __lsx_vmin_bu(px0, pMin);
+                    pMax = __lsx_vmax_bu(px0, pMax);
+                    srcPtrTemp +=16;
+                }
+                minTemp = (T) HorMin(pMin);
+                maxTemp = (T) HorMax(pMax);
+                for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                {
+                    if (*srcPtrTemp < minTemp)
+                    {
+                        minTemp = *srcPtrTemp;
+                    }
+                    if (*srcPtrTemp > maxTemp)
+                    {
+                        maxTemp = *srcPtrTemp;
+                    }
+                    srcPtrTemp++;
+                }
+                srcPtrTemp += (srcSizeMax.width - srcSize.width);
+            }
+            *(min + c) = minTemp;
+            *(max + c) = maxTemp;
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        Rpp32u elementsInRow = channel * srcSize.width;
+        Rpp32u elementsInRowMax = channel * srcSizeMax.width;
+
+        T minRTemp, maxRTemp, minGTemp, maxGTemp, minBTemp, maxBTemp;
+        minRTemp = *min;
+        maxRTemp = *max;
+        minGTemp = *(min + 1);
+        maxGTemp = *(max + 1);
+        minBTemp = *(min + 2);
+        maxBTemp = *(max + 2);
+
+        pMin = __lsx_vreplgr2vr_b(minRTemp);
+        pMax = __lsx_vreplgr2vr_b(maxRTemp);
+
+        for (int i = 0; i < srcSize.height; i++)
+        {
+            int bufferLength = elementsInRow;
+            int alignedLength = bufferLength & ~14;
+
+            __m128i px0;
+
+            int vectorLoopCount = 0;
+            for (; vectorLoopCount < alignedLength; vectorLoopCount+=15)
+            {
+                px0 =  __lsx_vld((__m128i *)srcPtrTemp, 0);
+                pMin = __lsx_vmin_bu(px0, pMin);
+                pMax = __lsx_vmax_bu(px0, pMax);
+                srcPtrTemp +=15;
+            }
+            for (; vectorLoopCount < bufferLength; vectorLoopCount+=3)
+            {
+                if (*srcPtrTemp < minRTemp)
+                {
+                    minRTemp = *srcPtrTemp;
+                }
+                if (*srcPtrTemp > maxRTemp)
+                {
+                    maxRTemp = *srcPtrTemp;
+                }
+                srcPtrTemp++;
+                if (*srcPtrTemp < minGTemp)
+                {
+                    minGTemp = *srcPtrTemp;
+                }
+                if (*srcPtrTemp > maxGTemp)
+                {
+                    maxGTemp = *srcPtrTemp;
+                }
+                srcPtrTemp++;
+                if (*srcPtrTemp < minBTemp)
+                {
+                    minBTemp = *srcPtrTemp;
+                }
+                if (*srcPtrTemp > maxBTemp)
+                {
+                    maxBTemp = *srcPtrTemp;
+                }
+                srcPtrTemp++;
+            }
+            srcPtrTemp += (elementsInRowMax - elementsInRow);
+        }
+
+        T minVector[16], maxVector[16];
+        __lsx_vst(pMin, (__m128i *)minVector, 0);
+        __lsx_vst(pMax, (__m128i *)maxVector, 0);
+
+        minRTemp = RPPMIN2(RPPMIN3(minVector[0], minVector[3], minVector[6]), RPPMIN3(minVector[9], minVector[12], minRTemp));
+        minGTemp = RPPMIN2(RPPMIN3(minVector[1], minVector[4], minVector[7]), RPPMIN3(minVector[10], minVector[13], minGTemp));
+        minBTemp = RPPMIN2(RPPMIN3(minVector[2], minVector[5], minVector[8]), RPPMIN3(minVector[11], minVector[14], minBTemp));
+
+        maxRTemp = RPPMAX2(RPPMAX3(maxVector[0], maxVector[3], maxVector[6]), RPPMAX3(maxVector[9], maxVector[12], maxRTemp));
+        maxGTemp = RPPMAX2(RPPMAX3(maxVector[1], maxVector[4], maxVector[7]), RPPMAX3(maxVector[10], maxVector[13], maxGTemp));
+        maxBTemp = RPPMAX2(RPPMAX3(maxVector[2], maxVector[5], maxVector[8]), RPPMAX3(maxVector[11], maxVector[14], maxBTemp));
+
+        *min = minRTemp;
+        *max = maxRTemp;
+        *(min + 1) = minGTemp;
+        *(max + 1) = maxGTemp;
+        *(min + 2) = minBTemp;
+        *(max + 2) = maxBTemp;
+    }
+}
+
+inline void compute_histogram_location_host(Rpp32u *batch_bins, int batchCount, Rpp32u *locHist)
+{
+    for (int m = 0; m < batchCount; m++)
+    {
+        *locHist += batch_bins[m];
+    }
+}
+
+template <typename T>
+inline void compute_unpadded_from_padded_host(T* srcPtrPadded, RppiSize srcSize, RppiSize srcSizeMax, T* dstPtrUnpadded,
+                                                   RppiChnFormat chnFormat, Rpp32u channel)
+{
+    T *srcPtrPaddedChannel, *srcPtrPaddedRow, *dstPtrUnpaddedRow;
+    Rpp32u imageDimMax = srcSizeMax.height * srcSizeMax.width;
+    dstPtrUnpaddedRow = dstPtrUnpadded;
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        for (int c = 0; c < channel; c++)
+        {
+            srcPtrPaddedChannel = srcPtrPadded + (c * imageDimMax);
+            for (int i = 0; i < srcSize.height; i++)
+            {
+                srcPtrPaddedRow = srcPtrPaddedChannel + (i * srcSizeMax.width);
+                memcpy(dstPtrUnpaddedRow, srcPtrPaddedRow, srcSize.width * sizeof(T));
+                dstPtrUnpaddedRow += srcSize.width;
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        Rpp32u elementsInRowMax = channel * srcSizeMax.width;
+        Rpp32u elementsInRow = channel * srcSize.width;
+        for (int i = 0; i < srcSize.height; i++)
+        {
+            srcPtrPaddedRow = srcPtrPadded + (i * elementsInRowMax);
+            memcpy(dstPtrUnpaddedRow, srcPtrPaddedRow, elementsInRow * sizeof(T));
+            dstPtrUnpaddedRow += elementsInRow;
+        }
+    }
+}
+
+template <typename T>
+inline void compute_padded_from_unpadded_host(T* srcPtrUnpadded, RppiSize srcSize, RppiSize dstSizeMax, T* dstPtrPadded,
+                                                   RppiChnFormat chnFormat, Rpp32u channel)
+{
+    T *dstPtrPaddedChannel, *dstPtrPaddedRow, *srcPtrUnpaddedRow;
+    Rpp32u imageDimMax = dstSizeMax.height * dstSizeMax.width;
+    srcPtrUnpaddedRow = srcPtrUnpadded;
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        for (int c = 0; c < channel; c++)
+        {
+            dstPtrPaddedChannel = dstPtrPadded + (c * imageDimMax);
+            for (int i = 0; i < srcSize.height; i++)
+            {
+                dstPtrPaddedRow = dstPtrPaddedChannel + (i * dstSizeMax.width);
+                memcpy(dstPtrPaddedRow, srcPtrUnpaddedRow, srcSize.width * sizeof(T));
+                srcPtrUnpaddedRow += srcSize.width;
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        Rpp32u elementsInRowMax = channel * dstSizeMax.width;
+        Rpp32u elementsInRow = channel * srcSize.width;
+        for (int i = 0; i < srcSize.height; i++)
+        {
+            dstPtrPaddedRow = dstPtrPadded + (i * elementsInRowMax);
+            memcpy(dstPtrPaddedRow, srcPtrUnpaddedRow, elementsInRow * sizeof(T));
+            srcPtrUnpaddedRow += elementsInRow;
+        }
+    }
+}
+
+template <typename T>
+inline void compute_planar_to_packed_host(T* srcPtr, RppiSize srcSize, T* dstPtr,
+                                        Rpp32u channel)
+{
+    T *srcPtrTemp, *dstPtrTemp;
+    srcPtrTemp = srcPtr;
+    dstPtrTemp = dstPtr;
+
+    for (int c = 0; c < channel; c++)
+    {
+        dstPtrTemp += c;
+        for (int i = 0; i < srcSize.height; i++)
+        {
+            for (int j = 0; j < srcSize.width; j++)
+            {
+                *dstPtrTemp = *srcPtrTemp;
+                srcPtrTemp++;
+                dstPtrTemp += 3;
+            }
+        }
+        dstPtrTemp = dstPtr;
+    }
+}
+
+template <typename T>
+inline void compute_packed_to_planar_host(T* srcPtr, RppiSize srcSize, T* dstPtr,
+                                        Rpp32u channel)
+{
+    T *srcPtrTemp, *dstPtrTemp;
+    srcPtrTemp = srcPtr;
+    dstPtrTemp = dstPtr;
+
+    for (int c = 0; c < channel; c++)
+    {
+        srcPtrTemp += c;
+        for (int i = 0; i < srcSize.height; i++)
+        {
+            for (int j = 0; j < srcSize.width; j++)
+            {
+                *dstPtrTemp = *srcPtrTemp;
+                dstPtrTemp++;
+                srcPtrTemp += 3;
+            }
+        }
+        srcPtrTemp = srcPtr;
+    }
+}
+
+/* Generic interpolation helper functions */
+
+template <typename T>
+inline void compute_generic_bilinear_srclocs_and_interpolate(T *srcPtrChannel, RpptDescPtr srcDescPtr, Rpp32f &srcY, Rpp32f &srcX, RpptROI* roiLTRB, T *dst)
+{
+    RppiPoint srcLT, srcRB;
+    Rpp32f weightParams[4], bilinearCoeffs[4];
+    Rpp32s srcLoc[4];
+    srcLT.y = (Rpp32s) srcY;                                    // Bilinear LT point y value
+    srcLT.y = std::min(srcLT.y, roiLTRB->ltrbROI.rb.y - 1);
+    srcRB.y = std::min(srcLT.y + 1, roiLTRB->ltrbROI.rb.y - 1); // Bilinear RB point y value
+    srcLT.x = (Rpp32s) srcX;                                    // Bilinear LT point x value
+    srcLT.x = std::min(srcLT.x, roiLTRB->ltrbROI.rb.x - 1);
+    srcRB.x = std::min(srcLT.x + 1, roiLTRB->ltrbROI.rb.x - 1); // Bilinear RB point x value
+    weightParams[0] = srcY - srcLT.y;                           // weightedHeight
+    weightParams[1] = 1 - weightParams[0];                      // 1 - weightedHeight
+    weightParams[2] = srcX - srcLT.x;                           // weightedWidth
+    weightParams[3] = 1 - weightParams[2];                      // 1 - weightedWidth
+    bilinearCoeffs[0] = weightParams[1] * weightParams[3];      // (1 - weightedHeight) * (1 - weightedWidth)
+    bilinearCoeffs[1] = weightParams[1] * weightParams[2];      // (1 - weightedHeight) * weightedWidth
+    bilinearCoeffs[2] = weightParams[0] * weightParams[3];      // weightedHeight * (1 - weightedWidth)
+    bilinearCoeffs[3] = weightParams[0] * weightParams[2];      // weightedHeight * weightedWidth
+    srcLT.y *= srcDescPtr->strides.hStride;                     // LT Row * hStride
+    srcRB.y *= srcDescPtr->strides.hStride;                     // RB Row * hStride
+    srcLT.x *= srcDescPtr->strides.wStride;                     // LT Col * wStride
+    srcRB.x *= srcDescPtr->strides.wStride;                     // LT Col * wStride
+    srcLoc[0] = srcLT.y + srcLT.x;                              // Left-Top pixel memory location
+    srcLoc[1] = srcLT.y + srcRB.x;                              // Right-Top pixel memory location
+    srcLoc[2] = srcRB.y + srcLT.x;                              // Left-Bottom pixel memory location
+    srcLoc[3] = srcRB.y + srcRB.x;                              // Right-Bottom pixel memory location
+
+    for (int c = 0; c < srcDescPtr->c; c++)
+    {
+        if constexpr (std::is_same<T, Rpp8s>::value || std::is_same<T, Rpp8u>::value)
+          dst[c] = (T)std::nearbyintf(((*(srcPtrChannel + srcLoc[0]) * bilinearCoeffs[0]) +        // TopRow R01 Pixel * coeff0
+                    (*(srcPtrChannel + srcLoc[1]) * bilinearCoeffs[1]) +                           // TopRow R02 Pixel * coeff1
+                    (*(srcPtrChannel + srcLoc[2]) * bilinearCoeffs[2]) +                           // BottomRow R01 Pixel * coeff2
+                    (*(srcPtrChannel + srcLoc[3]) * bilinearCoeffs[3])));                          // BottomRow R02 Pixel * coeff3
+        else if constexpr (std::is_same<T, Rpp32f>::value || std::is_same<T, Rpp16f>::value)
+          dst[c] = (T)(((*(srcPtrChannel + srcLoc[0]) * bilinearCoeffs[0]) +                       // TopRow R01 Pixel * coeff0
+                    (*(srcPtrChannel + srcLoc[1]) * bilinearCoeffs[1]) +                           // TopRow R02 Pixel * coeff1
+                    (*(srcPtrChannel + srcLoc[2]) * bilinearCoeffs[2]) +                           // BottomRow R01 Pixel * coeff2
+                    (*(srcPtrChannel + srcLoc[3]) * bilinearCoeffs[3])));                          // BottomRow R02 Pixel * coeff3
+
+        srcPtrChannel += srcDescPtr->strides.cStride;
+    }
+}
+
+inline void compute_generic_bilinear_srclocs_1c_avx(__m256 &pSrcY, __m256 &pSrcX, RpptBilinearNbhoodLocsVecLen8 &srcLocs, __m256 *pBilinearCoeffs, __m256 &pSrcStrideH, __m256i *pxSrcStridesCHW, __m256 *pRoiLTRB)
+{
+    __m256 pWeightParams[4], pSrcBilinearLTyx[4];
+    pSrcBilinearLTyx[0] = __lasx_xvfrintrm_s(pSrcY);                               // srcLT->y = (Rpp32s) srcY;
+    pSrcBilinearLTyx[1] = __lasx_xvfrintrm_s(pSrcX);                               // srcLT->x = (Rpp32s) srcX;
+    pWeightParams[0] = __lasx_xvfsub_s(pSrcY, pSrcBilinearLTyx[0]);               // weightParams[0] = srcY - srcLT->y;
+    pWeightParams[1] = __lasx_xvfsub_s(avx_p1, pWeightParams[0]);                 // weightParams[1] = 1 - weightParams[0];
+    pWeightParams[2] = __lasx_xvfsub_s(pSrcX, pSrcBilinearLTyx[1]);               // weightParams[2] = srcX - srcLT->x;
+    pWeightParams[3] = __lasx_xvfsub_s(avx_p1, pWeightParams[2]);                 // weightParams[3] = 1 - weightParams[2]
+    pBilinearCoeffs[0] = __lasx_xvfmul_s(pWeightParams[1], pWeightParams[3]);     // (1 - weightedHeight) * (1 - weightedWidth)
+    pBilinearCoeffs[1] = __lasx_xvfmul_s(pWeightParams[1], pWeightParams[2]);     // (1 - weightedHeight) * weightedWidth
+    pBilinearCoeffs[2] = __lasx_xvfmul_s(pWeightParams[0], pWeightParams[3]);     // weightedHeight * (1 - weightedWidth)
+    pBilinearCoeffs[3] = __lasx_xvfmul_s(pWeightParams[0], pWeightParams[2]);     // weightedHeight * weightedWidth
+    pSrcBilinearLTyx[0] = __lasx_xvfmin_s(__lasx_xvfmax_s(pSrcBilinearLTyx[0], pRoiLTRB[1]), __lasx_xvfsub_s(pRoiLTRB[3], avx_p1));
+    pSrcBilinearLTyx[1] = __lasx_xvfmin_s(__lasx_xvfmax_s(pSrcBilinearLTyx[1], pRoiLTRB[0]), __lasx_xvfsub_s(pRoiLTRB[2], avx_p1));
+    pSrcBilinearLTyx[2] = __lasx_xvfmin_s(__lasx_xvfmax_s(__lasx_xvfadd_s(pSrcBilinearLTyx[0], avx_p1), pRoiLTRB[1]), __lasx_xvfsub_s(pRoiLTRB[3], avx_p1));
+    pSrcBilinearLTyx[3] = __lasx_xvfmin_s(__lasx_xvfmax_s(__lasx_xvfadd_s(pSrcBilinearLTyx[1], avx_p1), pRoiLTRB[0]), __lasx_xvfsub_s(pRoiLTRB[2], avx_p1));
+    __m256i pxSrcLocsTL =  __lasx_xvftint_w_s(__lasx_xvfmadd_s(pSrcBilinearLTyx[0], pSrcStrideH, pSrcBilinearLTyx[1]));     // 8 Top-Left memory locations = 8 Top-Left srcYs * hStride + 8 Top-Left srcXs
+    __m256i pxSrcLocsTR =  __lasx_xvftint_w_s(__lasx_xvfmadd_s(pSrcBilinearLTyx[0], pSrcStrideH, pSrcBilinearLTyx[3]));     // 8 Top-Right memory locations = 8 Top-Left srcYs * hStride + 8 Bottom-right srcXs
+    __m256i pxSrcLocsBL = __lasx_xvftint_w_s(__lasx_xvfmadd_s(pSrcBilinearLTyx[2], pSrcStrideH, pSrcBilinearLTyx[1]));      // 8 Bottom-Left memory locations = 8 Bottom-right srcYs * hStride + 8 Top-Left srcXs
+    __m256i pxSrcLocsBR = __lasx_xvftint_w_s(__lasx_xvfmadd_s(pSrcBilinearLTyx[2], pSrcStrideH, pSrcBilinearLTyx[3]));      // 8 Bottom-Right memory locations = 8 Bottom-right srcYs * hStride + 8 Bottom-right srcXs
+    __lasx_xvst(pxSrcLocsTL, (__m256i*) &srcLocs.srcLocsTL.data[0], 0);    // Store precomputed bilinear Top-Left locations
+    __lasx_xvst(pxSrcLocsTR, (__m256i*) &srcLocs.srcLocsTR.data[0], 0);    // Store precomputed bilinear Top-Right locations
+    __lasx_xvst(pxSrcLocsBL, (__m256i*) &srcLocs.srcLocsBL.data[0], 0);    // Store precomputed bilinear Bottom-Left locations
+    __lasx_xvst(pxSrcLocsBR, (__m256i*) &srcLocs.srcLocsBR.data[0], 0);    // Store precomputed bilinear Bottom-Right locations
+}
+
+inline void compute_generic_bilinear_srclocs_3c_avx(__m256 &pSrcY, __m256 &pSrcX, RpptBilinearNbhoodLocsVecLen8 &srcLocs, __m256 *pBilinearCoeffs, __m256 &pSrcStrideH, __m256i *pxSrcStridesCHW, Rpp32s srcChannels, __m256 *pRoiLTRB, bool isSrcPKD3 = false)
+{
+    __m256 pWeightParams[4], pSrcBilinearLTyx[4];
+    pSrcBilinearLTyx[0] = __lasx_xvfrintrm_s(pSrcY);                               // srcLT->y = (Rpp32s) srcY;
+    pSrcBilinearLTyx[1] = __lasx_xvfrintrm_s(pSrcX);                               // srcLT->x = (Rpp32s) srcX;
+    pWeightParams[0] = __lasx_xvfsub_s(pSrcY, pSrcBilinearLTyx[0]);               // weightParams[0] = srcY - srcLT->y;
+    pWeightParams[1] = __lasx_xvfsub_s(avx_p1, pWeightParams[0]);                 // weightParams[1] = 1 - weightParams[0];
+    pWeightParams[2] = __lasx_xvfsub_s(pSrcX, pSrcBilinearLTyx[1]);               // weightParams[2] = srcX - srcLT->x;
+    pWeightParams[3] = __lasx_xvfsub_s(avx_p1, pWeightParams[2]);                 // weightParams[3] = 1 - weightParams[2]
+    pBilinearCoeffs[0] = __lasx_xvfmul_s(pWeightParams[1], pWeightParams[3]);     // (1 - weightedHeight) * (1 - weightedWidth)
+    pBilinearCoeffs[1] = __lasx_xvfmul_s(pWeightParams[1], pWeightParams[2]);     // (1 - weightedHeight) * weightedWidth
+    pBilinearCoeffs[2] = __lasx_xvfmul_s(pWeightParams[0], pWeightParams[3]);     // weightedHeight * (1 - weightedWidth)
+    pBilinearCoeffs[3] = __lasx_xvfmul_s(pWeightParams[0], pWeightParams[2]);     // weightedHeight * weightedWidth
+    pSrcBilinearLTyx[0] = __lasx_xvfmin_s(__lasx_xvfmax_s(pSrcBilinearLTyx[0], pRoiLTRB[1]), __lasx_xvfsub_s(pRoiLTRB[3], avx_p1));
+    pSrcBilinearLTyx[1] = __lasx_xvfmin_s(__lasx_xvfmax_s(pSrcBilinearLTyx[1], pRoiLTRB[0]), __lasx_xvfsub_s(pRoiLTRB[2], avx_p1));
+    pSrcBilinearLTyx[2] = __lasx_xvfmin_s(__lasx_xvfmax_s(__lasx_xvfadd_s(pSrcBilinearLTyx[0], avx_p1), pRoiLTRB[1]), __lasx_xvfsub_s(pRoiLTRB[3], avx_p1));
+    pSrcBilinearLTyx[3] = __lasx_xvfmin_s(__lasx_xvfmax_s(__lasx_xvfadd_s(pSrcBilinearLTyx[1], avx_p1), pRoiLTRB[0]), __lasx_xvfsub_s(pRoiLTRB[2], avx_p1));
+    if(isSrcPKD3)
+    {
+        pSrcBilinearLTyx[1] = __lasx_xvfmul_s(pSrcBilinearLTyx[1], avx_p3);       // if pkd3, multiply Left-Top column location by 3
+        pSrcBilinearLTyx[3] = __lasx_xvfmul_s(pSrcBilinearLTyx[3], avx_p3);       // if pkd3, multiply Right-Top column location by 3
+    }
+    __m256i pxSrcLocsTL =  __lasx_xvftint_w_s(__lasx_xvfmadd_s(pSrcBilinearLTyx[0], pSrcStrideH, pSrcBilinearLTyx[1]));    // 8 Top-Left memory locations = 8 Top-Left srcYs * hStride + 8 Top-Left srcXs
+    __m256i pxSrcLocsTR =  __lasx_xvftint_w_s(__lasx_xvfmadd_s(pSrcBilinearLTyx[0], pSrcStrideH, pSrcBilinearLTyx[3]));    // 8 Top-Right memory locations = 8 Top-Left srcYs * hStride + 8 Bottom-right srcXs
+    __m256i pxSrcLocsBL = __lasx_xvftint_w_s(__lasx_xvfmadd_s(pSrcBilinearLTyx[2], pSrcStrideH, pSrcBilinearLTyx[1]));     // 8 Bottom-Left memory locations = 8 Bottom-right srcYs * hStride + 8 Top-Left srcXs
+    __m256i pxSrcLocsBR = __lasx_xvftint_w_s(__lasx_xvfmadd_s(pSrcBilinearLTyx[2], pSrcStrideH, pSrcBilinearLTyx[3]));     // 8 Bottom-Right memory locations = 8 Bottom-right srcYs * hStride + 8 Bottom-right srcXs
+    for (int c = 0; c < srcChannels * 8; c += 8)
+    {
+        __lasx_xvst(pxSrcLocsTL, (__m256i*) &srcLocs.srcLocsTL.data[c], 0);    // Store precomputed bilinear Top-Left locations
+        __lasx_xvst(pxSrcLocsTR, (__m256i*) &srcLocs.srcLocsTR.data[c], 0);    // Store precomputed bilinear Top-Right locations
+        __lasx_xvst(pxSrcLocsBL, (__m256i*) &srcLocs.srcLocsBL.data[c], 0);    // Store precomputed bilinear Bottom-Left locations
+        __lasx_xvst(pxSrcLocsBR, (__m256i*) &srcLocs.srcLocsBR.data[c], 0);    // Store precomputed bilinear Bottom-Right locations
+        pxSrcLocsTL = __lasx_xvadd_w(pxSrcLocsTL, pxSrcStridesCHW[0]);            // Increment Top-Left locations by cStride
+        pxSrcLocsTR = __lasx_xvadd_w(pxSrcLocsTR, pxSrcStridesCHW[0]);            // Increment Top-Right locations by cStride
+        pxSrcLocsBL = __lasx_xvadd_w(pxSrcLocsBL, pxSrcStridesCHW[0]);            // Increment Bottom-Left locations by cStride
+        pxSrcLocsBR = __lasx_xvadd_w(pxSrcLocsBR, pxSrcStridesCHW[0]);            // Increment Bottom-Right locations by cStride
+    }
+}
+
+template <typename T>
+inline void compute_generic_bilinear_interpolation_pkd3_to_pln3(Rpp32f srcY, Rpp32f srcX, RpptROI *roiLTRB, T *dstPtrTempR, T *dstPtrTempG, T *dstPtrTempB, T *srcPtrChannel, RpptDescPtr srcDescPtr)
+{
+    Rpp32s srcXFloor = std::floor(srcX);
+    Rpp32s srcYFloor = std::floor(srcY);
+    if ((srcXFloor < roiLTRB->ltrbROI.lt.x) || (srcYFloor < roiLTRB->ltrbROI.lt.y) || (srcXFloor > roiLTRB->ltrbROI.rb.x) || (srcYFloor > roiLTRB->ltrbROI.rb.y))
+    {
+        *dstPtrTempR = 0;
+        *dstPtrTempG = 0;
+        *dstPtrTempB = 0;
+    }
+    else
+    {
+        T dst[3];
+        compute_generic_bilinear_srclocs_and_interpolate(srcPtrChannel, srcDescPtr, srcY, srcX, roiLTRB, dst);
+        *dstPtrTempR = dst[0];
+        *dstPtrTempG = dst[1];
+        *dstPtrTempB = dst[2];
+    }
+}
+
+template <typename T>
+inline void compute_generic_bilinear_interpolation_pln3pkd3_to_pkd3(Rpp32f srcY, Rpp32f srcX, RpptROI *roiLTRB, T *dstPtrTemp, T *srcPtrChannel, RpptDescPtr srcDescPtr)
+{
+    Rpp32s srcXFloor = std::floor(srcX);
+    Rpp32s srcYFloor = std::floor(srcY);
+    if ((srcXFloor < roiLTRB->ltrbROI.lt.x) || (srcYFloor < roiLTRB->ltrbROI.lt.y) || (srcXFloor > roiLTRB->ltrbROI.rb.x) || (srcYFloor > roiLTRB->ltrbROI.rb.y))
+    {
+        memset(dstPtrTemp, 0, 3 * sizeof(T));
+    }
+    else
+    {
+        compute_generic_bilinear_srclocs_and_interpolate(srcPtrChannel, srcDescPtr, srcY, srcX, roiLTRB, dstPtrTemp);
+    }
+}
+
+template <typename T>
+inline void compute_generic_bilinear_interpolation_pln_to_pln(Rpp32f srcY, Rpp32f srcX, RpptROI *roiLTRB, T *dstPtrTemp, T *srcPtrChannel, RpptDescPtr srcDescPtr, RpptDescPtr dstDescPtr)
+{
+    Rpp32s srcXFloor = std::floor(srcX);
+    Rpp32s srcYFloor = std::floor(srcY);
+    if ((srcXFloor < roiLTRB->ltrbROI.lt.x) || (srcYFloor < roiLTRB->ltrbROI.lt.y) || (srcXFloor > roiLTRB->ltrbROI.rb.x) || (srcYFloor > roiLTRB->ltrbROI.rb.y))
+    {
+        for(int c = 0; c < srcDescPtr->c; c++)
+        {
+            *dstPtrTemp = 0;
+            dstPtrTemp += dstDescPtr->strides.cStride;
+        }
+    }
+    else
+    {
+        T dst[3];
+        compute_generic_bilinear_srclocs_and_interpolate(srcPtrChannel, srcDescPtr, srcY, srcX, roiLTRB, dst);
+        for(int c = 0; c < srcDescPtr->c; c++)
+        {
+            *dstPtrTemp = dst[c];
+            dstPtrTemp += dstDescPtr->strides.cStride;
+        }
+    }
+}
+
+inline void compute_generic_nn_srclocs_and_validate_sse(__m128 pSrcY, __m128 pSrcX, __m128 *pRoiLTRB, __m128 pSrcStrideH, Rpp32s *srcLoc, Rpp32s *invalidLoad, bool hasRGBChannels = false)
+{
+    pSrcY = __lsx_vfrintrxxx_s(pSrcY, (const int)(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC));        // Nearest Neighbor Y location vector
+    pSrcX = __lsx_vfrintrxxx_s(pSrcX, (const int)(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC));        // Nearest Neighbor X location vector
+    __lsx_vst(__lsx_vftint_w_s((__m128)__lsx_vor_v(                 // Vectorized ROI boundary check
+        (__m128i)__lsx_vor_v((__m128i)(__m128)__lsx_vfcmp_clt_s(pSrcX, pRoiLTRB[0]), (__m128i)(__m128)__lsx_vfcmp_clt_s(pSrcY, pRoiLTRB[1])),
+        (__m128i)__lsx_vor_v((__m128i)(__m128)__lsx_vfcmp_clt_s(pSrcX, pRoiLTRB[2]), (__m128i)(__m128)__lsx_vfcmp_clt_s(pSrcY, pRoiLTRB[3]))
+    )), (__m128i*)invalidLoad, 0);
+    if (hasRGBChannels)
+        pSrcX = __lsx_vfmul_s(pSrcX, xmm_p3);
+    __m128i pxSrcLoc = __lsx_vftint_w_s(__lsx_vfmadd_s(pSrcY, pSrcStrideH, pSrcX));
+    __lsx_vst(pxSrcLoc, (__m128i*) srcLoc, 0);
+}
+
+inline void compute_generic_nn_srclocs_and_validate_avx(__m256 pSrcY, __m256 pSrcX, __m256 *pRoiLTRB, __m256 pSrcStrideH, Rpp32s *srcLoc, Rpp32s *invalidLoad, bool hasRGBChannels = false)
+{
+    pSrcY = __lasx_xvfrintrxxx_s(pSrcY, (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC));              // Nearest Neighbor Y location vector
+    pSrcX = __lasx_xvfrintrxxx_s(pSrcX, (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC));              // Nearest Neighbor X location vector
+    __lasx_xvst(__lasx_xvftint_w_s((__m256)__lasx_xvor_v(                 // Vectorized ROI boundary check
+        (__m256i)__lasx_xvor_v((__m256i)(__m256)__lasx_xvfcmp_xxx_s(pSrcX, pRoiLTRB[0], _CMP_LT_OQ), (__m256i)(__m256)__lasx_xvfcmp_xxx_s(pSrcY, pRoiLTRB[1],_CMP_LT_OQ)),
+        (__m256i)__lasx_xvor_v((__m256i)(__m256)__lasx_xvfcmp_xxx_s(pSrcX, pRoiLTRB[2], _CMP_GT_OQ), (__m256i)(__m256)__lasx_xvfcmp_xxx_s(pSrcY, pRoiLTRB[3], _CMP_GT_OQ))
+    )), (__m256i*) invalidLoad, 0);
+    if (hasRGBChannels)
+        pSrcX = __lasx_xvfmul_s(pSrcX, avx_p3);
+    __m256i pxSrcLoc = __lasx_xvftint_w_s(__lasx_xvfmadd_s(pSrcY, pSrcStrideH, pSrcX));
+    __lasx_xvst(pxSrcLoc, (__m256i*) srcLoc, 0);
+}
+
+template <typename T>
+inline void compute_generic_nn_interpolation_pkd3_to_pln3(Rpp32f srcY, Rpp32f srcX, RpptROI *roiLTRB, T *dstPtrTempR, T *dstPtrTempG, T *dstPtrTempB, T *srcPtrChannel, RpptDescPtr srcDescPtr)
+{
+    srcY = std::round(srcY);    // Nearest Neighbor Y location
+    srcX = std::round(srcX);    // Nearest Neighbor X location
+    if ((srcX < roiLTRB->ltrbROI.lt.x) || (srcY < roiLTRB->ltrbROI.lt.y) || (srcX > roiLTRB->ltrbROI.rb.x) || (srcY > roiLTRB->ltrbROI.rb.y))
+    {
+        *dstPtrTempR = 0;
+        *dstPtrTempG = 0;
+        *dstPtrTempB = 0;
+    }
+    else
+    {
+        T *srcPtrTemp;
+        srcPtrTemp = srcPtrChannel + ((Rpp32s)srcY * srcDescPtr->strides.hStride) + ((Rpp32s)srcX * srcDescPtr->strides.wStride);
+        *dstPtrTempR = *srcPtrTemp++;
+        *dstPtrTempG = *srcPtrTemp++;
+        *dstPtrTempB = *srcPtrTemp;
+    }
+}
+
+template <typename T>
+inline void compute_generic_nn_interpolation_pkd3_to_pkd3(Rpp32f srcY, Rpp32f srcX, RpptROI *roiLTRB, T *dstPtrTemp, T *srcPtrChannel, RpptDescPtr srcDescPtr)
+{
+    srcY = std::round(srcY);    // Nearest Neighbor Y location
+    srcX = std::round(srcX);    // Nearest Neighbor X location
+    if ((srcX < roiLTRB->ltrbROI.lt.x) || (srcY < roiLTRB->ltrbROI.lt.y) || (srcX > roiLTRB->ltrbROI.rb.x) || (srcY > roiLTRB->ltrbROI.rb.y))
+    {
+        memset(dstPtrTemp, 0, 3 * sizeof(T));
+    }
+    else
+    {
+        T *srcPtrTemp;
+        srcPtrTemp = srcPtrChannel + ((Rpp32s)srcY * srcDescPtr->strides.hStride) + ((Rpp32s)srcX * srcDescPtr->strides.wStride);
+        memcpy(dstPtrTemp, srcPtrTemp, 3 * sizeof(T));
+    }
+}
+
+template <typename T>
+inline void compute_generic_nn_interpolation_pln3_to_pkd3(Rpp32f srcY, Rpp32f srcX, RpptROI *roiLTRB, T *dstPtrTemp, T *srcPtrChannel, RpptDescPtr srcDescPtr)
+{
+    srcY = std::round(srcY);    // Nearest Neighbor Y location
+    srcX = std::round(srcX);    // Nearest Neighbor X location
+    if ((srcX < roiLTRB->ltrbROI.lt.x) || (srcY < roiLTRB->ltrbROI.lt.y) || (srcX > roiLTRB->ltrbROI.rb.x) || (srcY > roiLTRB->ltrbROI.rb.y))
+    {
+        memset(dstPtrTemp, 0, 3 * sizeof(T));
+    }
+    else
+    {
+        T *srcPtrTemp;
+        srcPtrTemp = srcPtrChannel + ((Rpp32s)srcY * srcDescPtr->strides.hStride) + ((Rpp32s)srcX * srcDescPtr->strides.wStride);
+        *dstPtrTemp++ = *srcPtrTemp;
+        srcPtrTemp += srcDescPtr->strides.cStride;
+        *dstPtrTemp++ = *srcPtrTemp;
+        srcPtrTemp += srcDescPtr->strides.cStride;
+        *dstPtrTemp = *srcPtrTemp;
+    }
+}
+
+template <typename T>
+inline void compute_generic_nn_interpolation_pln_to_pln(Rpp32f srcY, Rpp32f srcX, RpptROI *roiLTRB, T *dstPtrTemp, T *srcPtrChannel, RpptDescPtr srcDescPtr, RpptDescPtr dstDescPtr)
+{
+    srcY = std::round(srcY);    // Nearest Neighbor Y location
+    srcX = std::round(srcX);    // Nearest Neighbor X location
+    if ((srcX < roiLTRB->ltrbROI.lt.x) || (srcY < roiLTRB->ltrbROI.lt.y) || (srcX > roiLTRB->ltrbROI.rb.x) || (srcY > roiLTRB->ltrbROI.rb.y))
+    {
+        for(int c = 0; c < srcDescPtr->c; c++)
+        {
+            *dstPtrTemp = 0;
+            dstPtrTemp += dstDescPtr->strides.cStride;
+        }
+    }
+    else
+    {
+        T *srcPtrTemp;
+        srcPtrTemp = srcPtrChannel + ((Rpp32s)srcY * srcDescPtr->strides.hStride) + ((Rpp32s)srcX * srcDescPtr->strides.wStride);
+        for(int c = 0; c < srcDescPtr->c; c++)
+        {
+            *dstPtrTemp = *srcPtrTemp;
+            srcPtrTemp += srcDescPtr->strides.cStride;
+            dstPtrTemp += dstDescPtr->strides.cStride;
+        }
+    }
+}
+
+/* Resize helper functions */
+inline void compute_dst_size_cap_host(RpptImagePatchPtr dstImgSize, RpptDescPtr dstDescPtr)
+{
+    dstImgSize->width = std::min(dstImgSize->width, dstDescPtr->w);
+    dstImgSize->height = std::min(dstImgSize->height, dstDescPtr->h);
+}
+
+inline void compute_resize_src_loc(Rpp32s dstLocation, Rpp32f scale, Rpp32s &srcLoc, Rpp32f &weight, Rpp32f offset = 0, Rpp32u srcStride = 1)
+{
+    Rpp32f srcLocationFloat = ((Rpp32f) dstLocation) * scale + offset;
+    Rpp32s srcLocation = (Rpp32s) std::ceil(srcLocationFloat);
+    weight = srcLocation - srcLocationFloat;
+    srcLoc = srcLocation * srcStride;
+}
+
+inline void compute_resize_nn_src_loc(Rpp32s dstLocation, Rpp32f scale, Rpp32u limit, Rpp32s &srcLoc, Rpp32f offset = 0, Rpp32u srcStride = 1)
+{
+    Rpp32f srcLocation = ((Rpp32f) dstLocation) * scale + offset;
+    Rpp32s srcLocationFloor = (Rpp32s) RPPFLOOR(srcLocation);
+    srcLoc = ((srcLocationFloor > limit) ? limit : srcLocationFloor) * srcStride;
+}
+
+inline void compute_resize_bilinear_src_loc_and_weights(Rpp32s dstLocation, Rpp32f scale, Rpp32s &srcLoc, Rpp32f *weight, Rpp32f offset = 0, Rpp32u srcStride = 1)
+{
+    compute_resize_src_loc(dstLocation, scale, srcLoc, weight[1], offset, srcStride);
+    weight[0] = 1 - weight[1];
+}
+
+inline void compute_resize_nn_src_loc_sse(__m128 &pDstLoc, __m128 &pScale, __m128 &pLimit, Rpp32s *srcLoc, __m128 pOffset = xmm_p0, bool hasRGBChannels = false)
+{
+    __m128 pLoc = __lsx_vfmadd_s(pDstLoc, pScale, pOffset);
+    pDstLoc = __lsx_vfadd_s(pDstLoc, xmm_p4);
+    __m128 pLocFloor = __lsx_vfrintrm_s(pLoc);
+    pLocFloor = __lsx_vfmax_s(__lsx_vfmin_s(pLocFloor, pLimit), xmm_p0);
+    if(hasRGBChannels)
+        pLocFloor = __lsx_vfmul_s(pLocFloor, xmm_p3);
+    __m128i pxLocFloor = __lsx_vftint_w_s(pLocFloor);
+    __lsx_vst(pxLocFloor, (__m128i*) srcLoc, 0);
+}
+
+inline void compute_resize_bilinear_src_loc_and_weights_avx(__m256 &pDstLoc, __m256 &pScale, Rpp32s *srcLoc, __m256 *pWeight, __m256i &pxLoc, __m256 pOffset = avx_p0, bool hasRGBChannels = false)
+{
+    __m256 pLocFloat = __lasx_xvfmadd_s(pDstLoc, pScale, pOffset);
+    pDstLoc = __lasx_xvfadd_s(pDstLoc, avx_p8);
+    __m256 pLoc = __lasx_xvfrintrp_s(pLocFloat);
+    pWeight[1] = __lasx_xvfsub_s(pLoc, pLocFloat);
+    pWeight[0] = __lasx_xvfsub_s(avx_p1, pWeight[1]);
+    if(hasRGBChannels)
+        pLoc = __lasx_xvfmul_s(pLoc, avx_p3);
+    pxLoc = __lasx_xvftint_w_s(pLoc);
+    __lasx_xvst(pxLoc, (__m256i*) srcLoc, 0);
+}
+
+inline void compute_resize_bilinear_src_loc_and_weights_mirror_avx(__m256 &pDstLoc, __m256 &pScale, Rpp32s *srcLoc, __m256 *pWeight, __m256i &pxLoc, __m256 pOffset = avx_p0, bool hasRGBChannels = false)
+{
+    __m256 pLocFloat = __lasx_xvfmadd_s(pDstLoc, pScale, pOffset);
+    pDstLoc = __lasx_xvfsub_s(pDstLoc, avx_p8);
+    __m256 pLoc = __lasx_xvfrintrp_s(pLocFloat);
+    pWeight[1] = __lasx_xvfsub_s(pLoc, pLocFloat);
+    pWeight[0] = __lasx_xvfsub_s(avx_p1, pWeight[1]);
+    if (hasRGBChannels)
+        pLoc = __lasx_xvfmul_s(pLoc, avx_p3);
+    pxLoc = __lasx_xvftint_w_s(pLoc);
+    __lasx_xvst(pxLoc, (__m256i *)srcLoc, 0);
+}
+
+inline void compute_bicubic_coefficient(Rpp32f weight, Rpp32f &coeff)
+{
+    Rpp32f x = fabsf(weight);
+    coeff = (x >= 2) ? 0 : ((x > 1) ? (x * x * (-0.5f * x + 2.5f) - 4.0f * x + 2.0f) : (x * x * (1.5f * x - 2.5f) + 1.0f));
+}
+
+inline Rpp32f sinc(Rpp32f x)
+{
+    x *= M_PI;
+    return (std::abs(x) < 1e-5f) ? (1.0f - x * x * ONE_OVER_6) : std::sin(x) / x;
+}
+
+inline void compute_lanczos3_coefficient(Rpp32f weight, Rpp32f &coeff)
+{
+    coeff = fabs(weight) >= 3 ? 0.0f : (sinc(weight) * sinc(weight * 0.333333f));
+}
+
+inline void compute_gaussian_coefficient(Rpp32f weight, Rpp32f &coeff)
+{
+    coeff = expf(weight * weight * -4.0f);
+}
+
+inline void compute_triangular_coefficient(Rpp32f weight, Rpp32f &coeff)
+{
+    coeff = 1 - std::fabs(weight);
+    coeff = coeff < 0 ? 0 : coeff;
+}
+
+inline void compute_coefficient(RpptInterpolationType interpolationType, Rpp32f weight, Rpp32f &coeff)
+{
+    switch (interpolationType)
+    {
+    case RpptInterpolationType::BICUBIC:
+    {
+        compute_bicubic_coefficient(weight, coeff);
+        break;
+    }
+    case RpptInterpolationType::LANCZOS:
+    {
+        compute_lanczos3_coefficient(weight, coeff);
+        break;
+    }
+    case RpptInterpolationType::GAUSSIAN:
+    {
+        compute_gaussian_coefficient(weight, coeff);
+        break;
+    }
+    case RpptInterpolationType::TRIANGULAR:
+    {
+        compute_triangular_coefficient(weight, coeff);
+        break;
+    }
+    default:
+        break;
+    }
+}
+
+// Computes the row coefficients for separable resampling
+inline void compute_row_coefficients(RpptInterpolationType interpolationType, GenericFilter &filter , Rpp32f weight, Rpp32f *coeffs, Rpp32u srcStride = 1)
+{
+    Rpp32f sum = 0;
+    weight = weight - filter.radius;
+    for(int k = 0; k < filter.size; k++)
+    {
+        compute_coefficient(interpolationType, (weight + k) * filter.scale, coeffs[k]);
+        sum += coeffs[k];
+    }
+    if(sum)
+    {
+        sum = 1 / sum;
+        for(int k = 0; k < filter.size; k++)
+            coeffs[k] = coeffs[k] * sum;
+    }
+}
+
+// Computes the column coefficients for separable resampling
+inline void compute_col_coefficients(RpptInterpolationType interpolationType, GenericFilter &filter, Rpp32f weight, Rpp32f *coeffs, Rpp32u srcStride = 1)
+{
+    Rpp32f sum = 0;
+    weight = weight - filter.radius;
+
+    // The coefficients are computed for 4 dst locations and stored consecutively for ease of access
+    for(int k = 0, kPos = 0; k < filter.size; k++, kPos += 4)
+    {
+        compute_coefficient(interpolationType, (weight + k) * filter.scale, coeffs[kPos]);
+        sum += coeffs[kPos];
+    }
+    if(sum)
+    {
+        sum = 1 / sum;
+        for(int k = 0, kPos = 0; k < filter.size; k++, kPos += 4)
+            coeffs[kPos] = coeffs[kPos] * sum;
+    }
+}
+
+inline void set_zeros(__m128 *pVecs, Rpp32s numVecs)
+{
+    for(int i = 0; i < numVecs; i++)
+        pVecs[i] = xmm_p0;
+}
+
+inline void set_zeros_avx(__m256 *pVecs, Rpp32s numVecs)
+{
+    for(int i = 0; i < numVecs; i++)
+        pVecs[i] = avx_p0;
+}
+
+inline void compute_bilinear_coefficients(Rpp32f *weightParams, Rpp32f *bilinearCoeffs)
+{
+    bilinearCoeffs[0] = weightParams[1] * weightParams[3];    // (1 - weightedHeight) * (1 - weightedWidth)
+    bilinearCoeffs[1] = weightParams[1] * weightParams[2];    // (1 - weightedHeight) * weightedWidth
+    bilinearCoeffs[2] = weightParams[0] * weightParams[3];    // weightedHeight * (1 - weightedWidth)
+    bilinearCoeffs[3] = weightParams[0] * weightParams[2];    // weightedHeight * weightedWidth
+}
+
+inline void compute_bilinear_coefficients_avx(__m256 *pWeightParams, __m256 *pBilinearCoeffs)
+{
+    pBilinearCoeffs[0] = __lasx_xvfmul_s(pWeightParams[1], pWeightParams[3]);    // (1 - weightedHeight) * (1 - weightedWidth)
+    pBilinearCoeffs[1] = __lasx_xvfmul_s(pWeightParams[1], pWeightParams[2]);    // (1 - weightedHeight) * weightedWidth
+    pBilinearCoeffs[2] = __lasx_xvfmul_s(pWeightParams[0], pWeightParams[3]);    // weightedHeight * (1 - weightedWidth)
+    pBilinearCoeffs[3] = __lasx_xvfmul_s(pWeightParams[0], pWeightParams[2]);    // weightedHeight * weightedWidth
+}
+
+template <typename T, typename U>
+inline void compute_bilinear_interpolation_1c(T **srcRowPtrsForInterp, Rpp32s loc, Rpp32s limit, Rpp32f *bilinearCoeffs, U *dstPtr)
+{
+    Rpp32s loc1 = std::min(std::max(loc, 0), limit);
+    Rpp32s loc2 = std::min(std::max(loc + 1, 0), limit);
+    *dstPtr = (U)(((*(srcRowPtrsForInterp[0] + loc1)) * bilinearCoeffs[0]) +     // TopRow 1st Pixel * coeff0
+                  ((*(srcRowPtrsForInterp[0] + loc2)) * bilinearCoeffs[1]) +     // TopRow 2nd Pixel * coeff1
+                  ((*(srcRowPtrsForInterp[1] + loc1)) * bilinearCoeffs[2]) +     // BottomRow 1st Pixel * coeff2
+                  ((*(srcRowPtrsForInterp[1] + loc2)) * bilinearCoeffs[3]));    // BottomRow 2nd Pixel * coeff3
+}
+
+template <typename T, typename U>
+inline void compute_bilinear_interpolation_3c_pkd(T **srcRowPtrsForInterp, Rpp32s loc, Rpp32s limit, Rpp32f *bilinearCoeffs, U *dstPtrR, U *dstPtrG, U *dstPtrB)
+{
+    Rpp32s loc1 = std::min(std::max(loc, 0), limit);
+    Rpp32s loc2 = std::min(std::max(loc + 3, 0), limit);
+    *dstPtrR = (U)(((*(srcRowPtrsForInterp[0] + loc1)) * bilinearCoeffs[0]) +        // TopRow R01 Pixel * coeff0
+                   ((*(srcRowPtrsForInterp[0] + loc2)) * bilinearCoeffs[1]) +        // TopRow R02 Pixel * coeff1
+                   ((*(srcRowPtrsForInterp[1] + loc1)) * bilinearCoeffs[2]) +        // BottomRow R01 Pixel * coeff2
+                   ((*(srcRowPtrsForInterp[1] + loc2)) * bilinearCoeffs[3]));       // BottomRow R02 Pixel * coeff3
+    *dstPtrG = (U)(((*(srcRowPtrsForInterp[0] + loc1 + 1)) * bilinearCoeffs[0]) +    // TopRow G01 Pixel * coeff0
+                   ((*(srcRowPtrsForInterp[0] + loc2 + 1)) * bilinearCoeffs[1]) +    // TopRow G02 Pixel * coeff1
+                   ((*(srcRowPtrsForInterp[1] + loc1 + 1)) * bilinearCoeffs[2]) +    // BottomRow G01 Pixel * coeff2
+                   ((*(srcRowPtrsForInterp[1] + loc2 + 1)) * bilinearCoeffs[3]));   // BottomRow G02 Pixel * coeff3
+    *dstPtrB = (U)(((*(srcRowPtrsForInterp[0] + loc1 + 2)) * bilinearCoeffs[0]) +    // TopRow B01 Pixel * coeff0
+                   ((*(srcRowPtrsForInterp[0] + loc2 + 2)) * bilinearCoeffs[1]) +    // TopRow B02 Pixel * coeff1
+                   ((*(srcRowPtrsForInterp[1] + loc1 + 2)) * bilinearCoeffs[2]) +    // BottomRow B01 Pixel * coeff2
+                   ((*(srcRowPtrsForInterp[1] + loc2 + 2)) * bilinearCoeffs[3]));   // BottomRow B02 Pixel * coeff3
+}
+
+template <typename T, typename U>
+inline void compute_bilinear_interpolation_3c_pln(T **srcRowPtrsForInterp, Rpp32s loc, Rpp32s limit, Rpp32f *bilinearCoeffs, U *dstPtrR, U *dstPtrG, U *dstPtrB)
+{
+    compute_bilinear_interpolation_1c(srcRowPtrsForInterp, loc, limit, bilinearCoeffs, dstPtrR);
+    compute_bilinear_interpolation_1c(srcRowPtrsForInterp + 2, loc, limit, bilinearCoeffs, dstPtrG);
+    compute_bilinear_interpolation_1c(srcRowPtrsForInterp + 4, loc, limit, bilinearCoeffs, dstPtrB);
+}
+
+inline void compute_bilinear_interpolation_1c_avx(__m256 *pSrcPixels, __m256 *pBilinearCoeffs, __m256 &pDstPixels)
+{
+    pDstPixels = __lasx_xvfmadd_s(pSrcPixels[3], pBilinearCoeffs[3], __lasx_xvfmadd_s(pSrcPixels[2], pBilinearCoeffs[2],
+                 __lasx_xvfmadd_s(pSrcPixels[1], pBilinearCoeffs[1], __lasx_xvfmul_s(pSrcPixels[0], pBilinearCoeffs[0]))));
+}
+
+inline void compute_bilinear_interpolation_3c_avx(__m256 *pSrcPixels, __m256 *pBilinearCoeffs, __m256 *pDstPixels)
+{
+    compute_bilinear_interpolation_1c_avx(pSrcPixels, pBilinearCoeffs, pDstPixels[0]);
+    compute_bilinear_interpolation_1c_avx(pSrcPixels + 4, pBilinearCoeffs, pDstPixels[1]);
+    compute_bilinear_interpolation_1c_avx(pSrcPixels + 8, pBilinearCoeffs, pDstPixels[2]);
+}
+
+template <typename T>
+inline void compute_src_row_ptrs_for_bilinear_interpolation(T **rowPtrsForInterp, T *srcPtr, Rpp32s loc, Rpp32s limit, RpptDescPtr descPtr)
+{
+    rowPtrsForInterp[0] = srcPtr + std::min(std::max(loc, 0), limit) * descPtr->strides.hStride;          // TopRow for bilinear interpolation
+    rowPtrsForInterp[1]  = srcPtr + std::min(std::max(loc + 1, 0), limit) * descPtr->strides.hStride;     // BottomRow for bilinear interpolation
+}
+
+template <typename T>
+inline void compute_src_row_ptrs_for_bilinear_interpolation_pln(T **rowPtrsForInterp, T *srcPtr, Rpp32s loc, Rpp32s limit, RpptDescPtr descPtr)
+{
+    rowPtrsForInterp[0] = srcPtr + std::min(std::max(loc, 0), limit) * descPtr->strides.hStride;          // TopRow for bilinear interpolation (R channel)
+    rowPtrsForInterp[1] = srcPtr + std::min(std::max(loc + 1, 0), limit) * descPtr->strides.hStride;      // BottomRow for bilinear interpolation (R channel)
+    rowPtrsForInterp[2] = rowPtrsForInterp[0] + descPtr->strides.cStride;   // TopRow for bilinear interpolation (G channel)
+    rowPtrsForInterp[3] = rowPtrsForInterp[1] + descPtr->strides.cStride;   // BottomRow for bilinear interpolation (G channel)
+    rowPtrsForInterp[4] = rowPtrsForInterp[2] + descPtr->strides.cStride;   // TopRow for bilinear interpolation (B channel)
+    rowPtrsForInterp[5] = rowPtrsForInterp[3] + descPtr->strides.cStride;   // BottomRow for bilinear interpolation (B channel)
+}
+
+// Perform resampling along the rows
+template <typename T>
+inline void compute_separable_vertical_resample(T *inputPtr, Rpp32f *outputPtr, RpptDescPtr inputDescPtr, RpptDescPtr outputDescPtr,
+                                                RpptImagePatch inputImgSize, RpptImagePatch outputImgSize, Rpp32s *index, Rpp32f *coeffs, GenericFilter &filter)
+{
+
+    static constexpr Rpp32s maxNumLanes = 16;                                  // Maximum number of pixels that can be present in a vector for U8 type
+    static constexpr Rpp32s loadLanes = maxNumLanes / sizeof(T);
+    static constexpr Rpp32s storeLanes = maxNumLanes / sizeof(Rpp32f);
+    static constexpr Rpp32s numLanes = std::max(loadLanes, storeLanes);        // No of pixels that can be present in a vector wrt data type
+    static constexpr Rpp32s numVecs = numLanes * sizeof(Rpp32f) / maxNumLanes; // No of float vectors required to process numLanes pixels
+
+    Rpp32s inputHeightLimit = inputImgSize.height - 1;
+    Rpp32s outPixelsPerIter = 4;
+
+    // For PLN3 inputs/outputs
+    if (inputDescPtr->c == 3 && inputDescPtr->layout == RpptLayout::NCHW)
+    {
+        T *inRowPtrR[filter.size];
+        T *inRowPtrG[filter.size];
+        T *inRowPtrB[filter.size];
+        for (int outLocRow = 0; outLocRow < outputImgSize.height; outLocRow++)
+        {
+            Rpp32f *outRowPtrR = outputPtr + outLocRow * outputDescPtr->strides.hStride;
+            Rpp32f *outRowPtrG = outRowPtrR + outputDescPtr->strides.cStride;
+            Rpp32f *outRowPtrB = outRowPtrG + outputDescPtr->strides.cStride;
+            Rpp32s k0 = outLocRow * filter.size;
+            __m128 pCoeff[filter.size];
+
+            // Determine the input row pointers and coefficients to be used for interpolation
+            for (int k = 0; k < filter.size; k++)
+            {
+                Rpp32s inLocRow = index[outLocRow] + k;
+                inLocRow = std::min(std::max(inLocRow, 0), inputHeightLimit);
+                inRowPtrR[k] = inputPtr + inLocRow * inputDescPtr->strides.hStride;
+                inRowPtrG[k] = inRowPtrR[k] + inputDescPtr->strides.cStride;
+                inRowPtrB[k] = inRowPtrG[k] + inputDescPtr->strides.cStride;
+                pCoeff[k] = lsx_set1_f32(coeffs[k0 + k]);    // Each row is associated with a single coeff
+            }
+            Rpp32s bufferLength = inputImgSize.width;
+            Rpp32s alignedLength = bufferLength &~ (numLanes-1);
+            Rpp32s outLocCol = 0;
+
+            // Load the input pixels from filter.size rows
+            // Multiply input vec from each row with it's correspondig coefficient
+            // Add the results from filter.size rows to obtain the pixels of an output row
+            for (; outLocCol + numLanes <= alignedLength; outLocCol += numLanes)
+            {
+                __m128 pTempR[numVecs], pTempG[numVecs], pTempB[numVecs];
+                set_zeros(pTempR, numVecs);
+                set_zeros(pTempG, numVecs);
+                set_zeros(pTempB, numVecs);
+                for (int k = 0; k < filter.size; k++)
+                {
+                    __m128 pInputR[numVecs], pInputG[numVecs], pInputB[numVecs];
+
+                    // Load numLanes input pixels from each row
+                    rpp_resize_load(inRowPtrR[k] + outLocCol, pInputR);
+                    rpp_resize_load(inRowPtrG[k] + outLocCol, pInputG);
+                    rpp_resize_load(inRowPtrB[k] + outLocCol, pInputB);
+                    for (int v = 0; v < numVecs; v++)
+                    {
+                        pTempR[v] = __lsx_vfmadd_s(pCoeff[k], pInputR[v], pTempR[v]);
+                        pTempG[v] = __lsx_vfmadd_s(pCoeff[k], pInputG[v], pTempG[v]);
+                        pTempB[v] = __lsx_vfmadd_s(pCoeff[k], pInputB[v], pTempB[v]);
+                    }
+                }
+                for(int vec = 0, outStoreStride = 0; vec < numVecs; vec++, outStoreStride += outPixelsPerIter)    // Since 4 output pixels are stored per iteration
+                {
+                    rpp_simd_store(rpp_store4_f32_to_f32, outRowPtrR + outLocCol + outStoreStride, pTempR + vec);
+                    rpp_simd_store(rpp_store4_f32_to_f32, outRowPtrG + outLocCol + outStoreStride, pTempG + vec);
+                    rpp_simd_store(rpp_store4_f32_to_f32, outRowPtrB + outLocCol + outStoreStride, pTempB + vec);
+                }
+            }
+
+            for (; outLocCol < bufferLength; outLocCol++)
+            {
+                Rpp32f tempR, tempG, tempB;
+                tempR = tempG = tempB = 0;
+                for (int k = 0; k < filter.size; k++)
+                {
+                    Rpp32f coefficient = coeffs[k0 + k];
+                    tempR += (inRowPtrR[k][outLocCol] * coefficient);
+                    tempG += (inRowPtrG[k][outLocCol] * coefficient);
+                    tempB += (inRowPtrB[k][outLocCol] * coefficient);
+                }
+                outRowPtrR[outLocCol] = tempR;
+                outRowPtrG[outLocCol] = tempG;
+                outRowPtrB[outLocCol] = tempB;
+            }
+        }
+    }
+    // For PKD3 and PLN1 inputs/outputs
+    else
+    {
+        T *inRowPtr[filter.size];
+        for (int outLocRow = 0; outLocRow < outputImgSize.height; outLocRow++)
+        {
+            __m128 pCoeff[filter.size];
+            Rpp32s k0 = outLocRow * filter.size;
+            Rpp32f *outRowPtr = outputPtr + outLocRow * outputDescPtr->strides.hStride;
+
+            // Determine the input row pointers and coefficients to be used for interpolation
+            for (int k = 0; k < filter.size; k++)
+            {
+                Rpp32s inLocRow = index[outLocRow] + k;
+                inLocRow = std::min(std::max(inLocRow, 0), inputHeightLimit);
+                inRowPtr[k] = inputPtr + inLocRow * inputDescPtr->strides.hStride;
+                pCoeff[k] = lsx_set1_f32(coeffs[k0 + k]);    // Each row is associated with a single coeff
+            }
+            Rpp32s bufferLength = inputImgSize.width * inputDescPtr->strides.wStride;
+            Rpp32s alignedLength = bufferLength &~ (numLanes-1);
+            Rpp32s outLocCol = 0;
+
+            // Load the input pixels from filter.size rows
+            // Multiply input vec from each row with it's correspondig coefficient
+            // Add the results from filter.size rows to obtain the pixels of an output row
+            for (; outLocCol + numLanes <= alignedLength; outLocCol += numLanes)
+            {
+                __m128 pTemp[numVecs];
+                set_zeros(pTemp, numVecs);
+                for (int k = 0; k < filter.size; k++)
+                {
+                    __m128 pInput[numVecs];
+                    rpp_resize_load(inRowPtr[k] + outLocCol, pInput);   // Load numLanes input pixels from each row
+                    for (int v = 0; v < numVecs; v++)
+                        pTemp[v] = __lsx_vfmadd_s(pInput[v], pCoeff[k], pTemp[v]);
+                }
+                for(int vec = 0, outStoreStride = 0; vec < numVecs; vec++, outStoreStride += outPixelsPerIter)     // Since 4 output pixels are stored per iteration
+                    rpp_simd_store(rpp_store4_f32_to_f32, outRowPtr + outLocCol + outStoreStride, &pTemp[vec]);
+            }
+
+            for (; outLocCol < bufferLength; outLocCol++)
+            {
+                Rpp32f temp = 0;
+                for (int k = 0; k < filter.size; k++)
+                    temp += (inRowPtr[k][outLocCol] * coeffs[k0 + k]);
+                outRowPtr[outLocCol] = temp;
+            }
+        }
+    }
+}
+
+// Perform resampling along the columns
+template <typename T>
+inline void compute_separable_horizontal_resample(Rpp32f *inputPtr, T *outputPtr, RpptDescPtr inputDescPtr, RpptDescPtr outputDescPtr,
+                        RpptImagePatch inputImgSize, RpptImagePatch outputImgSize, Rpp32s *index, Rpp32f *coeffs, GenericFilter &filter)
+{
+    static constexpr Rpp32s maxNumLanes = 16;                                   // Maximum number of pixels that can be present in a vector
+    static constexpr Rpp32s numLanes = maxNumLanes / sizeof(T);                 // No of pixels that can be present in a vector wrt data type
+    static constexpr Rpp32s numVecs = numLanes * sizeof(Rpp32f) / maxNumLanes;  // No of float vectors required to process numLanes pixels
+    Rpp32s numOutPixels, filterKernelStride;
+    numOutPixels = filterKernelStride = 4;
+    Rpp32s filterKernelSizeOverStride = filter.size % filterKernelStride;
+    Rpp32s filterKernelRadiusWStrided = (Rpp32s)(filter.radius) * inputDescPtr->strides.wStride;
+
+    Rpp32s inputWidthLimit = (inputImgSize.width - 1) * inputDescPtr->strides.wStride;
+    __m128i pxInputWidthLimit = __lsx_vreplgr2vr_w(inputWidthLimit);
+
+    // For PLN3 inputs
+    if(inputDescPtr->c == 3 && inputDescPtr->layout == RpptLayout::NCHW)
+    {
+        for (int outLocRow = 0; outLocRow < outputImgSize.height; outLocRow++)
+        {
+            T *outRowPtrR = outputPtr + outLocRow * outputDescPtr->strides.hStride;
+            T *outRowPtrG = outRowPtrR + outputDescPtr->strides.cStride;
+            T *outRowPtrB = outRowPtrG + outputDescPtr->strides.cStride;
+            Rpp32f *inRowPtrR = inputPtr + outLocRow * inputDescPtr->strides.hStride;
+            Rpp32f *inRowPtrG = inRowPtrR + inputDescPtr->strides.cStride;
+            Rpp32f *inRowPtrB = inRowPtrG + inputDescPtr->strides.cStride;
+            Rpp32s bufferLength = outputImgSize.width;
+            Rpp32s alignedLength = bufferLength &~ (numLanes-1);
+            __m128 pFirstValR = lsx_set1_f32(inRowPtrR[0]);
+            __m128 pFirstValG = lsx_set1_f32(inRowPtrG[0]);
+            __m128 pFirstValB = lsx_set1_f32(inRowPtrB[0]);
+            bool breakLoop = false;
+            Rpp32s outLocCol = 0;
+
+            // Load filter.size consecutive pixels from a location in the row
+            // Multiply with corresponding coeffs and add together to obtain the output pixel
+            for (; outLocCol + numLanes <= alignedLength; outLocCol += numLanes)
+            {
+                __m128 pOutputChannel[numVecs * 3];
+                set_zeros(pOutputChannel, numVecs * 3);
+                __m128 *pOutputR = pOutputChannel;
+                __m128 *pOutputG = pOutputChannel + numVecs;
+                __m128 *pOutputB = pOutputChannel + (numVecs * 2);
+                for(int vec = 0, x = outLocCol; vec < numVecs; vec++, x += numOutPixels)
+                {
+                    Rpp32s coeffIdx = (x * filter.size);
+                    if(index[x] < 0)
+                    {
+                        __m128i pxIdx[numOutPixels];
+                        pxIdx[0] = __lsx_vreplgr2vr_w(index[x]);
+                        pxIdx[1] = __lsx_vreplgr2vr_w(index[x + 1]);
+                        pxIdx[2] = __lsx_vreplgr2vr_w(index[x + 2]);
+                        pxIdx[3] = __lsx_vreplgr2vr_w(index[x + 3]);
+                        for(int k = 0; k < filter.size; k += filterKernelStride)
+                        {
+                            // Generate mask to determine the negative indices in the iteration
+                            __m128 pNegativeIndexMask[numOutPixels];
+                            __m128i pxKernelIdx = __lsx_vreplgr2vr_w(k);
+                            __m128 pInputR[numOutPixels], pInputG[numOutPixels], pInputB[numOutPixels], pCoeffs[numOutPixels];
+                            Rpp32s kernelAdd = (k + filterKernelStride) > filter.size ? filterKernelSizeOverStride : filterKernelStride;
+                            set_zeros(pInputR, numOutPixels);
+                            set_zeros(pInputG, numOutPixels);
+                            set_zeros(pInputB, numOutPixels);
+                            set_zeros(pCoeffs, numOutPixels);
+
+                            for(int l = 0; l < numOutPixels; l++)
+                            {
+                                pNegativeIndexMask[l] = (__m128)(__lsx_vslt_w(__lsx_vadd_w(__lsx_vadd_w(pxIdx[l], pxKernelIdx), xmm_pxDstLocInit), xmm_px0));    // Generate mask to determine the negative indices in the iteration
+                                Rpp32s srcx = index[x + l] + k;
+
+                                // Load filterKernelStride(4) consecutive pixels
+                                rpp_simd_load(rpp_load4_f32_to_f32, inRowPtrR + srcx, pInputR + l);
+                                rpp_simd_load(rpp_load4_f32_to_f32, inRowPtrG + srcx, pInputG + l);
+                                rpp_simd_load(rpp_load4_f32_to_f32, inRowPtrB + srcx, pInputB + l);
+                                pCoeffs[l] = (__m128)__lsx_vld(&(coeffs[coeffIdx + ((l + k) * 4)]), 0);        // Load coefficients
+
+                                // If negative index is present replace the input pixel value with first value in the row
+                                pInputR[l] = lsx_blendv_f32(pInputR[l], pFirstValR, pNegativeIndexMask[l]);
+                                pInputG[l] = lsx_blendv_f32(pInputG[l], pFirstValG, pNegativeIndexMask[l]);
+                                pInputB[l] = lsx_blendv_f32(pInputB[l], pFirstValB, pNegativeIndexMask[l]);
+                            }
+
+                            // Perform transpose operation to arrange input pixels from different output locations in each vector
+                            _MM_TRANSPOSE4_PS(pInputR[0], pInputR[1], pInputR[2], pInputR[3]);
+                            _MM_TRANSPOSE4_PS(pInputG[0], pInputG[1], pInputG[2], pInputG[3]);
+                            _MM_TRANSPOSE4_PS(pInputB[0], pInputB[1], pInputB[2], pInputB[3]);
+                            for (int l = 0; l < kernelAdd; l++)
+                            {
+                                pOutputR[vec] = __lsx_vfmadd_s(pCoeffs[l], pInputR[l], pOutputR[vec]);
+                                pOutputG[vec] = __lsx_vfmadd_s(pCoeffs[l], pInputG[l], pOutputG[vec]);
+                                pOutputB[vec] = __lsx_vfmadd_s(pCoeffs[l], pInputB[l], pOutputB[vec]);
+                            }
+                        }
+                    }
+                    else if(index[x + 3] >= (inputWidthLimit - filterKernelRadiusWStrided))    // If the index value exceeds the limit, break the loop
+                    {
+                        breakLoop = true;
+                        break;
+                    }
+                    else
+                    {
+                        // Considers a 4x1 window for computation each time
+                        for(int k = 0; k < filter.size; k += filterKernelStride)
+                        {
+                            __m128 pInputR[numOutPixels], pInputG[numOutPixels], pInputB[numOutPixels];
+                            __m128 pCoeffs[numOutPixels];
+                            Rpp32s kernelAdd = (k + filterKernelStride) > filter.size ? filterKernelSizeOverStride : filterKernelStride;
+                            for (int l = 0; l < numOutPixels; l++)
+                            {
+                                pInputR[l] = pInputG[l] = pInputB[l] = pCoeffs[l] = xmm_p0;
+                                pCoeffs[l] = (__m128)__lsx_vld(&(coeffs[coeffIdx + ((l + k) * 4)]), 0); // Load coefficients
+                                Rpp32s srcx = index[x + l] + k;
+                                srcx = std::min(std::max(srcx, 0), inputWidthLimit);
+                                // Load filterKernelStride(4) consecutive pixels
+                                rpp_simd_load(rpp_load4_f32_to_f32, inRowPtrR + srcx, pInputR + l);
+                                rpp_simd_load(rpp_load4_f32_to_f32, inRowPtrG + srcx, pInputG + l);
+                                rpp_simd_load(rpp_load4_f32_to_f32, inRowPtrB + srcx, pInputB + l);
+                            }
+
+                            // Perform transpose operation to arrange input pixels from different output locations in each vector
+                            _MM_TRANSPOSE4_PS(pInputR[0], pInputR[1], pInputR[2], pInputR[3]);
+                            _MM_TRANSPOSE4_PS(pInputG[0], pInputG[1], pInputG[2], pInputG[3]);
+                            _MM_TRANSPOSE4_PS(pInputB[0], pInputB[1], pInputB[2], pInputB[3]);
+                            for (int l = 0; l < kernelAdd; l++)
+                            {
+                                pOutputR[vec] = __lsx_vfmadd_s(pCoeffs[l], pInputR[l], pOutputR[vec]);
+                                pOutputG[vec] = __lsx_vfmadd_s(pCoeffs[l], pInputG[l], pOutputG[vec]);
+                                pOutputB[vec] = __lsx_vfmadd_s(pCoeffs[l], pInputB[l], pOutputB[vec]);
+                            }
+                        }
+                    }
+                }
+                if(breakLoop) break;
+                Rpp32s xStride = outLocCol * outputDescPtr->strides.wStride;
+                if(outputDescPtr->layout == RpptLayout::NCHW)       // For PLN3 outputs
+                    rpp_resize_store_pln3(outRowPtrR + xStride, outRowPtrG + xStride, outRowPtrB + xStride, pOutputChannel);
+                else if(outputDescPtr->layout == RpptLayout::NHWC)  // For PKD3 outputs
+                    rpp_resize_store_pkd3(outRowPtrR + xStride, pOutputChannel);
+            }
+            Rpp32s k0 = 0;
+            for (; outLocCol < outputImgSize.width; outLocCol++)
+            {
+                Rpp32s x0 = index[outLocCol];
+                k0 = outLocCol % 4 == 0 ? outLocCol * filter.size : k0 + 1; // Since coeffs are stored in continuously for 4 dst locations
+                Rpp32f sumR, sumG, sumB;
+                sumR = sumG = sumB = 0;
+                for (int k = 0; k < filter.size; k++)
+                {
+                    Rpp32s srcx = x0 + k;
+                    srcx = std::min(std::max(srcx, 0), inputWidthLimit);
+                    Rpp32s kPos = (k * 4);      // Since coeffs are stored in continuously for 4 dst locations
+                    sumR += (coeffs[k0 + kPos] * inRowPtrR[srcx]);
+                    sumG += (coeffs[k0 + kPos] * inRowPtrG[srcx]);
+                    sumB += (coeffs[k0 + kPos] * inRowPtrB[srcx]);
+                }
+                Rpp32s xStride = outLocCol * outputDescPtr->strides.wStride;
+                saturate_pixel(sumR, outRowPtrR + xStride);
+                saturate_pixel(sumG, outRowPtrG + xStride);
+                saturate_pixel(sumB, outRowPtrB + xStride);
+            }
+        }
+    }
+    // For PKD3 inputs
+    else if(inputDescPtr->c == 3 && inputDescPtr->layout == RpptLayout::NHWC)
+    {
+        for (int outLocRow = 0; outLocRow < outputImgSize.height; outLocRow++)
+        {
+            T *outRowPtrR = outputPtr + outLocRow * outputDescPtr->strides.hStride;
+            T *outRowPtrG = outRowPtrR + outputDescPtr->strides.cStride;
+            T *outRowPtrB = outRowPtrG + outputDescPtr->strides.cStride;
+            Rpp32f *inRowPtr = inputPtr + outLocRow * inputDescPtr->strides.hStride;
+            Rpp32s bufferLength = outputImgSize.width;
+            Rpp32s alignedLength = bufferLength &~ (numLanes-1);
+            Rpp32s outLocCol = 0;
+
+            // Load filter.size consecutive pixels from a location in the row
+            // Multiply with corresponding coeffs and add together to obtain the output pixel
+            for (; outLocCol + numLanes <= alignedLength; outLocCol += numLanes)
+            {
+                __m128 pOutputChannel[numVecs * 3];
+                set_zeros(pOutputChannel, numVecs * 3);
+                __m128 *pOutputR = pOutputChannel;
+                __m128 *pOutputG = pOutputChannel + numVecs;
+                __m128 *pOutputB = pOutputChannel + (numVecs * 2);
+                for(int vec = 0, x = outLocCol; vec < numVecs; vec++, x += numOutPixels)   // 4 dst pixels processed per iteration
+                {
+                    Rpp32s coeffIdx = (x * filter.size);
+                    for(int k = 0, kStrided = 0; k < filter.size; k ++, kStrided = k * 3)
+                    {
+                        __m128 pInput[numOutPixels];
+                        __m128 pCoeffs = (__m128)__lsx_vld(&(coeffs[coeffIdx + (k * numOutPixels)]), 0);
+                        for (int l = 0; l < numOutPixels; l++)
+                        {
+                            Rpp32s srcx = index[x + l] + kStrided;
+                            srcx = std::min(std::max(srcx, 0), inputWidthLimit);
+                            rpp_simd_load(rpp_load4_f32_to_f32, &inRowPtr[srcx], &pInput[l]);   // Load RGB pixel from a src location
+                        }
+
+                        // Perform transpose operation to arrange input pixels by R,G and B separately in each vector
+                        _MM_TRANSPOSE4_PS(pInput[0], pInput[1], pInput[2], pInput[3]);
+                        pOutputR[vec] = __lsx_vfmadd_s(pCoeffs, pInput[0], pOutputR[vec]);
+                        pOutputG[vec] = __lsx_vfmadd_s(pCoeffs, pInput[1], pOutputG[vec]);
+                        pOutputB[vec] = __lsx_vfmadd_s(pCoeffs, pInput[2], pOutputB[vec]);
+                    }
+                }
+
+                Rpp32s xStride = outLocCol * outputDescPtr->strides.wStride;
+                if(outputDescPtr->layout == RpptLayout::NCHW)       // For PLN3 outputs
+                    rpp_resize_store_pln3(outRowPtrR + xStride, outRowPtrG + xStride, outRowPtrB + xStride, pOutputChannel);
+                else if(outputDescPtr->layout == RpptLayout::NHWC)  // For PKD3 outputs
+                    rpp_resize_store_pkd3(outRowPtrR + xStride, pOutputChannel);
+            }
+            Rpp32s k0 = 0;
+            for (; outLocCol < outputImgSize.width; outLocCol++)
+            {
+                Rpp32s x0 = index[outLocCol];
+                k0 = outLocCol % 4 == 0 ? outLocCol * filter.size : k0 + 1;  // Since coeffs are stored in continuously for 4 dst locations
+                Rpp32f sumR, sumG, sumB;
+                sumR = sumG = sumB = 0;
+                for (int k = 0; k < filter.size; k++)
+                {
+                    Rpp32s srcx = x0 + (k * 3);
+                    srcx = std::min(std::max(srcx, 0), inputWidthLimit);
+                    Rpp32s kPos = (k * 4);      // Since coeffs are stored in continuously for 4 dst locations
+                    sumR += (coeffs[k0 + kPos] * inRowPtr[srcx]);
+                    sumG += (coeffs[k0 + kPos] * inRowPtr[srcx + 1]);
+                    sumB += (coeffs[k0 + kPos] * inRowPtr[srcx + 2]);
+                }
+                Rpp32s xStride = outLocCol * outputDescPtr->strides.wStride;
+                saturate_pixel(sumR, outRowPtrR + xStride);
+                saturate_pixel(sumG, outRowPtrG + xStride);
+                saturate_pixel(sumB, outRowPtrB + xStride);
+            }
+        }
+    }
+    else
+    {
+        for (int outLocRow = 0; outLocRow < outputImgSize.height; outLocRow++)
+        {
+            T *out_row = outputPtr + outLocRow * outputDescPtr->strides.hStride;
+            Rpp32f *inRowPtr = inputPtr + outLocRow * inputDescPtr->strides.hStride;
+            Rpp32s bufferLength = outputImgSize.width;
+            Rpp32s alignedLength = bufferLength &~ (numLanes-1);
+            __m128 pFirstVal = lsx_set1_f32(inRowPtr[0]);
+            bool breakLoop = false;
+            Rpp32s outLocCol = 0;
+
+            // Load filter.size consecutive pixels from a location in the row
+            // Multiply with corresponding coeffs and add together to obtain the output pixel
+            for (; outLocCol + numLanes <= alignedLength; outLocCol += numLanes)
+            {
+                __m128 pOutput[numVecs];
+                set_zeros(pOutput, numVecs);
+                for(int vec = 0, x = outLocCol; vec < numVecs; vec++, x += numOutPixels)
+                {
+                    Rpp32s coeffIdx = (x * filter.size);
+                    if(index[x] < 0)
+                    {
+                        __m128i pxIdx[numOutPixels];
+                        pxIdx[0] = __lsx_vreplgr2vr_w(index[x]);
+                        pxIdx[1] = __lsx_vreplgr2vr_w(index[x + 1]);
+                        pxIdx[2] = __lsx_vreplgr2vr_w(index[x + 2]);
+                        pxIdx[3] = __lsx_vreplgr2vr_w(index[x + 3]);
+                        for(int k = 0; k < filter.size; k += filterKernelStride)
+                        {
+                            __m128 pNegativeIndexMask[numOutPixels];
+                            __m128i pxKernelIdx = __lsx_vreplgr2vr_w(k);
+                            __m128 pInput[numOutPixels], pCoeffs[numOutPixels];
+                            Rpp32s kernelAdd = (k + filterKernelStride) > filter.size ? filterKernelSizeOverStride : filterKernelStride;
+                            set_zeros(pInput, numOutPixels);
+                            set_zeros(pCoeffs, numOutPixels);
+                            for(int l = 0; l < numOutPixels; l++)
+                            {
+                                pNegativeIndexMask[l] = (__m128)(__lsx_vslt_w(__lsx_vadd_w(__lsx_vadd_w(pxIdx[l], pxKernelIdx), xmm_pxDstLocInit), xmm_px0));    // Generate mask to determine the negative indices in the iteration
+                                rpp_simd_load(rpp_load4_f32_to_f32, &inRowPtr[index[x + l] + k], &pInput[l]);   // Load filterKernelStride(4) consecutive pixels
+                                pCoeffs[l] = (__m128)__lsx_vld(&(coeffs[coeffIdx + ((l + k) * 4)]), 0);                 // Load coefficients
+                                pInput[l] = lsx_blendv_f32(pInput[l], pFirstVal, pNegativeIndexMask[l]);         // If negative index is present replace the pixel value with first value in the row
+                            }
+                            _MM_TRANSPOSE4_PS(pInput[0], pInput[1], pInput[2], pInput[3]);  // Perform transpose operation to arrange input pixels from different output locations in each vector
+                            for (int l = 0; l < kernelAdd; l++)
+                                pOutput[vec] = __lsx_vfmadd_s(pCoeffs[l], pInput[l], pOutput[vec]);
+                        }
+                    }
+                    else if(index[x + 3] >= (inputWidthLimit - filterKernelRadiusWStrided))   // If the index value exceeds the limit, break the loop
+                    {
+                        breakLoop = true;
+                        break;
+                    }
+                    else
+                    {
+                        for(int k = 0; k < filter.size; k += filterKernelStride)
+                        {
+                            __m128 pInput[numOutPixels], pCoeffs[numOutPixels];
+                            Rpp32s kernelAdd = (k + filterKernelStride) > filter.size ? filterKernelSizeOverStride : filterKernelStride;
+                            for (int l = 0; l < numOutPixels; l++)
+                            {
+                                pInput[l] = pCoeffs[l] = xmm_p0;
+                                pCoeffs[l] = (__m128)__lsx_vld(&(coeffs[coeffIdx + ((l + k) * 4)]), 0);     // Load coefficients
+                                Rpp32s srcx = index[x + l] + k;
+                                srcx = std::min(std::max(srcx, 0), inputWidthLimit);
+                                rpp_simd_load(rpp_load4_f32_to_f32, inRowPtr + srcx, pInput + l);   // Load filterKernelStride(4) consecutive pixels
+                            }
+                            _MM_TRANSPOSE4_PS(pInput[0], pInput[1], pInput[2], pInput[3]);  // Perform transpose operation to arrange input pixels from different output locations in each vector
+                            for (int l = 0; l < kernelAdd; l++)
+                                pOutput[vec] = __lsx_vfmadd_s(pCoeffs[l], pInput[l], pOutput[vec]);
+                        }
+                    }
+                }
+                if(breakLoop) break;
+                rpp_resize_store(out_row + outLocCol, pOutput);
+            }
+            Rpp32s k0 = 0;
+            for (; outLocCol < bufferLength; outLocCol++)
+            {
+                Rpp32s x0 = index[outLocCol];
+                k0 = outLocCol % 4 == 0 ? outLocCol * filter.size : k0 + 1;  // Since coeffs are stored in continuously for 4 dst locations
+                Rpp32f sum = 0;
+                for (int k = 0; k < filter.size; k++)
+                {
+                    Rpp32s srcx = x0 + k;
+                    srcx = std::min(std::max(srcx, 0), inputWidthLimit);
+                    sum += (coeffs[k0 + (k * 4)] * inRowPtr[srcx]);
+                }
+                saturate_pixel(sum, out_row + outLocCol);
+            }
+        }
+    }
+}
+
+inline void compute_jitter_src_loc_avx(__m256i *pxXorwowStateX, __m256i *pxXorwowStateCounter, __m256 &pRow, __m256 &pCol, __m256 &pKernelSize, __m256 &pBound, __m256 &pHeightLimit, __m256 &pWidthLimit, __m256 &pStride, __m256 &pChannel, Rpp32s *srcLoc)
+{
+    __m256 pRngX = rpp_host_rng_xorwow_8_f32_avx(pxXorwowStateX, pxXorwowStateCounter);
+    __m256 pRngY = rpp_host_rng_xorwow_8_f32_avx(pxXorwowStateX, pxXorwowStateCounter);
+    __m256 pX = __lasx_xvfmul_s(pRngX, pKernelSize);
+    __m256 pY = __lasx_xvfmul_s(pRngY, pKernelSize);
+    pX = __lasx_xvfmax_s(__lasx_xvfmin_s(__lasx_xvfrintrm_s(__lasx_xvfadd_s(pRow, __lasx_xvfsub_s(pX, pBound))), pHeightLimit), avx_p0);
+    pY = __lasx_xvfmax_s(__lasx_xvfmin_s(__lasx_xvfrintrm_s(__lasx_xvfadd_s(pCol, __lasx_xvfsub_s(pY, pBound))), pWidthLimit), avx_p0);
+    __m256i pxSrcLoc = __lasx_xvftint_w_s(__lasx_xvfmadd_s(pX, pStride, __lasx_xvfmul_s(pY, pChannel)));
+    __lasx_xvst(pxSrcLoc, (__m256i*) srcLoc, 0);
+}
+
+inline void compute_jitter_src_loc(RpptXorwowStateBoxMuller *xorwowState, Rpp32s row, Rpp32s col, Rpp32s kSize, Rpp32s heightLimit, Rpp32s widthLimit, Rpp32s stride, Rpp32s bound, Rpp32s channels, Rpp32s &loc)
+{
+    Rpp32u heightIncrement = rpp_host_rng_xorwow_f32(xorwowState) * kSize;
+    Rpp32u widthIncrement = rpp_host_rng_xorwow_f32(xorwowState) * kSize;
+    loc = std::max(std::min(static_cast<int>(row + heightIncrement - bound), heightLimit), 0) * stride;
+    loc += std::max(std::min(static_cast<int>(col + widthIncrement  - bound), (widthLimit - 1)), 0) * channels;
+}
+inline void compute_sum_16_host(__m256i *p, __m256i *pSum)
+{
+    pSum[0] = __lasx_xvadd_w(__lasx_xvadd_w(p[0], p[1]), pSum[0]); //add 16 values to 8
+}
+
+inline void compute_sum_48_host(__m256i *p, __m256i *pSumR, __m256i *pSumG, __m256i *pSumB)
+{
+    pSumR[0] = __lasx_xvadd_w(__lasx_xvadd_w(p[0], p[1]), pSumR[0]); //add 16R values and bring it down to 8
+    pSumG[0] = __lasx_xvadd_w(__lasx_xvadd_w(p[2], p[3]), pSumG[0]); //add 16G values and bring it down to 8
+    pSumB[0] = __lasx_xvadd_w(__lasx_xvadd_w(p[4], p[5]), pSumB[0]); //add 16B values and bring it down to 8
+}
+
+inline void compute_sum_8_host(__m256d *p, __m256d *pSum)
+{
+    pSum[0] = __lasx_xvfadd_d(__lasx_xvfadd_d(p[0], p[1]), pSum[0]); //add 8 values and bring it down to 4
+}
+
+inline void compute_sum_24_host(__m256d *p, __m256d *pSumR, __m256d *pSumG, __m256d *pSumB)
+{
+    pSumR[0] = __lasx_xvfadd_d(__lasx_xvfadd_d(p[0], p[1]), pSumR[0]); //add 8R values and bring it down to 4
+    pSumG[0] = __lasx_xvfadd_d(__lasx_xvfadd_d(p[2], p[3]), pSumG[0]); //add 8G values and bring it down to 4
+    pSumB[0] = __lasx_xvfadd_d(__lasx_xvfadd_d(p[4], p[5]), pSumB[0]); //add 8B values and bring it down to 4
+}
+
+inline void compute_variance_8_host(__m256d *p1, __m256d *pMean, __m256d *pVar)
+{
+    __m256d pSub = __lasx_xvfsub_d(p1[0], pMean[0]);
+    pVar[0] = __lasx_xvfadd_d(__lasx_xvfmul_d(pSub, pSub), pVar[0]);
+    pSub = __lasx_xvfsub_d(p1[1], pMean[0]);
+    pVar[0] = __lasx_xvfadd_d(__lasx_xvfmul_d(pSub, pSub), pVar[0]);
+}
+
+inline void compute_variance_channel_pln3_24_host(__m256d *p1, __m256d *pMeanR, __m256d *pMeanG, __m256d *pMeanB, __m256d *pVarR, __m256d *pVarG, __m256d *pVarB)
+{
+    __m256d pSub = __lasx_xvfsub_d(p1[0], pMeanR[0]);
+    pVarR[0] = __lasx_xvfadd_d(__lasx_xvfmul_d(pSub, pSub), pVarR[0]);
+    pSub = __lasx_xvfsub_d(p1[1], pMeanR[0]);
+    pVarR[0] = __lasx_xvfadd_d(__lasx_xvfmul_d(pSub, pSub), pVarR[0]);
+    pSub = __lasx_xvfsub_d(p1[2], pMeanG[0]);
+    pVarG[0] = __lasx_xvfadd_d(__lasx_xvfmul_d(pSub, pSub), pVarG[0]);
+    pSub = __lasx_xvfsub_d(p1[3], pMeanG[0]);
+    pVarG[0] = __lasx_xvfadd_d(__lasx_xvfmul_d(pSub, pSub), pVarG[0]);
+    pSub = __lasx_xvfsub_d(p1[4], pMeanB[0]);
+    pVarB[0] = __lasx_xvfadd_d(__lasx_xvfmul_d(pSub, pSub), pVarB[0]);
+    pSub = __lasx_xvfsub_d(p1[5], pMeanB[0]);
+    pVarB[0] = __lasx_xvfadd_d(__lasx_xvfmul_d(pSub, pSub), pVarB[0]);
+}
+
+inline void compute_variance_image_pln3_24_host(__m256d *p1, __m256d *pMean, __m256d *pVarR, __m256d *pVarG, __m256d *pVarB)
+{
+    __m256d pSub = __lasx_xvfsub_d(p1[0], pMean[0]);
+    pVarR[0] = __lasx_xvfadd_d(__lasx_xvfmul_d(pSub, pSub), pVarR[0]);
+    pSub = __lasx_xvfsub_d(p1[1], pMean[0]);
+    pVarR[0] = __lasx_xvfadd_d(__lasx_xvfmul_d(pSub, pSub), pVarR[0]);
+    pSub = __lasx_xvfsub_d(p1[2], pMean[0]);
+    pVarG[0] = __lasx_xvfadd_d(__lasx_xvfmul_d(pSub, pSub), pVarG[0]);
+    pSub = __lasx_xvfsub_d(pMean[0], p1[3]);
+    pVarG[0] = __lasx_xvfadd_d(__lasx_xvfmul_d(pSub, pSub), pVarG[0]);
+    pSub = __lasx_xvfsub_d(p1[4], pMean[0]);
+    pVarB[0] = __lasx_xvfadd_d(__lasx_xvfmul_d(pSub, pSub), pVarB[0]);
+    pSub = __lasx_xvfsub_d(p1[5], pMean[0]);
+    pVarB[0] = __lasx_xvfadd_d(__lasx_xvfmul_d(pSub, pSub), pVarB[0]);
+}
+
+inline void compute_vignette_48_host(__m256 *p, __m256 &pMultiplier, __m256 &pILocComponent, __m256 &pJLocComponent)
+{
+    __m256 pGaussianValue;
+    pGaussianValue = fast_exp_avx(__lasx_xvfmul_s(__lasx_xvfmadd_s(pJLocComponent, pJLocComponent, pILocComponent), pMultiplier));
+    p[0] = __lasx_xvfmul_s(p[0], pGaussianValue);    // vignette adjustment
+    p[2] = __lasx_xvfmul_s(p[2], pGaussianValue);    // vignette adjustment
+    p[4] = __lasx_xvfmul_s(p[4], pGaussianValue);    // vignette adjustment
+    pJLocComponent = __lasx_xvfadd_s(pJLocComponent, avx_p8);
+    pGaussianValue = fast_exp_avx(__lasx_xvfmul_s(__lasx_xvfmadd_s(pJLocComponent, pJLocComponent, pILocComponent), pMultiplier));
+    p[1] = __lasx_xvfmul_s(p[1], pGaussianValue);    // vignette adjustment
+    p[3] = __lasx_xvfmul_s(p[3], pGaussianValue);    // vignette adjustment
+    p[5] = __lasx_xvfmul_s(p[5], pGaussianValue);    // vignette adjustment
+    pJLocComponent = __lasx_xvfadd_s(pJLocComponent, avx_p8);
+}
+
+inline void compute_vignette_24_host(__m256 *p, __m256 &pMultiplier, __m256 &pILocComponent, __m256 &pJLocComponent)
+{
+    __m256 pGaussianValue;
+    pGaussianValue = fast_exp_avx(__lasx_xvfmul_s(__lasx_xvfmadd_s(pJLocComponent, pJLocComponent, pILocComponent), pMultiplier));
+    p[0] = __lasx_xvfmul_s(p[0], pGaussianValue);    // vignette adjustment
+    p[1] = __lasx_xvfmul_s(p[1], pGaussianValue);    // vignette adjustment
+    p[2] = __lasx_xvfmul_s(p[2], pGaussianValue);    // vignette adjustment
+    pJLocComponent = __lasx_xvfadd_s(pJLocComponent, avx_p8);
+}
+
+inline void compute_vignette_16_host(__m256 *p, __m256 &pMultiplier, __m256 &pILocComponent, __m256 &pJLocComponent)
+{
+    __m256 pGaussianValue;
+    pGaussianValue = fast_exp_avx(__lasx_xvfmul_s(__lasx_xvfmadd_s(pJLocComponent, pJLocComponent, pILocComponent), pMultiplier));
+    p[0] = __lasx_xvfmul_s(p[0], pGaussianValue);    // vignette adjustment
+    pJLocComponent = __lasx_xvfadd_s(pJLocComponent, avx_p8);
+    pGaussianValue = fast_exp_avx(__lasx_xvfmul_s(__lasx_xvfmadd_s(pJLocComponent, pJLocComponent, pILocComponent), pMultiplier));
+    p[1] = __lasx_xvfmul_s(p[1], pGaussianValue);    // vignette adjustment
+    pJLocComponent = __lasx_xvfadd_s(pJLocComponent, avx_p8);
+}
+
+inline void compute_vignette_8_host(__m256 *p, __m256 &pMultiplier, __m256 &pILocComponent, __m256 &pJLocComponent)
+{
+    __m256 pGaussianValue;
+    pGaussianValue = fast_exp_avx(__lasx_xvfmul_s(__lasx_xvfmadd_s(pJLocComponent, pJLocComponent, pILocComponent), pMultiplier));
+    p[0] = __lasx_xvfmul_s(p[0], pGaussianValue);    // vignette adjustment
+    pJLocComponent = __lasx_xvfadd_s(pJLocComponent, avx_p8);
+}
+
+inline void reduce_min_32_host(__m256i *pMin, __m128i *result)
+{
+    __m128i px[2];
+    __m128i zero = __lsx_vldi(0);
+    __m128i mask = lsx_set_i8(0,1,2,3,4,5,6,8,9,10,11,12,13,14,15,7);
+    px[0] = __lasx_cvt_256_128(pMin[0]);
+    px[1] = lasx_extracti128_m256i(pMin[0], 1);
+    px[0] = __lsx_vmin_bu(px[0], px[1]);
+    px[1] = __lsx_vilvl_b(px[0], zero);
+    px[0] = __lsx_vilvh_b(px[0], zero);
+    px[0] = __lsx_vmin_bu(px[0], px[1]);
+    px[1] = __lsx_vilvl_h(px[0], zero);
+    px[0] = __lsx_vilvh_h(px[0], zero);
+    px[0] = __lsx_vmin_hu(px[0], px[1]);
+    px[1] = __lsx_vilvl_w(px[0], zero);
+    px[0] = __lsx_vilvh_w(px[0], zero);
+    px[0] = __lsx_vmin_wu(px[0], px[1]);
+    result[0] = lsx_shuffle_i8(px[0], mask);
+}
+
+inline void compute_min_96_host(__m256i *p1, __m256i *pMinR, __m256i *pMinG, __m256i *pMinB)
+{
+    pMinR[0] = __lasx_xvmin_bu(p1[0], pMinR[0]); //compare and store min of 32 R values into global min
+    pMinG[0] = __lasx_xvmin_bu(p1[1], pMinG[0]); //compare and store min of 32 G values into global min
+    pMinB[0] = __lasx_xvmin_bu(p1[2], pMinB[0]); //compare and store min of 32 B values into global min
+}
+
+inline void reduce_min_96_host(__m256i *pMinR, __m256i *pMinG, __m256i *pMinB, __m128i *result)
+{
+    __m128i px[4];
+    __m128i zero = __lsx_vldi(0);
+    px[0] = __lsx_vmin_bu(__lasx_cvt_256_128(pMinR[0]), lasx_extracti128_m256i(pMinR[0], 1));
+    px[1] = __lsx_vmin_bu(__lasx_cvt_256_128(pMinG[0]), lasx_extracti128_m256i(pMinG[0], 1));
+    px[1] = __lsx_vmin_bu(__lsx_vilvl_b(px[1], px[0]), __lsx_vilvh_b(px[1], px[0]));
+    px[0] = __lsx_vmin_bu(__lasx_cvt_256_128(pMinB[0]), lasx_extracti128_m256i(pMinB[0], 1));
+    px[0] = __lsx_vmin_bu(__lsx_vilvl_b(zero, px[0]), __lsx_vilvh_b(zero, px[0]));
+    px[1] = __lsx_vmin_bu(__lsx_vilvl_h(px[0], px[1]), __lsx_vilvh_h(px[0], px[1]));
+    px[0] = __lsx_vmin_bu(__lsx_vilvl_w(zero, px[1]), __lsx_vilvh_w(zero, px[1]));
+    result[0] = __lsx_vmin_bu(__lsx_vilvl_d(zero, px[0]), __lsx_vilvh_d(zero, px[0]));
+}
+
+inline void compute_min_48_host(__m128i *p1, __m128i *pMinR, __m128i *pMinG, __m128i *pMinB)
+{
+    pMinR[0] = __lsx_vmin_bu(p1[0], pMinR[0]); //compare and store min of 16 R values into global min
+    pMinG[0] = __lsx_vmin_bu(p1[1], pMinG[0]); //compare and store min of 16 G values into global min
+    pMinB[0] = __lsx_vmin_bu(p1[2], pMinB[0]); //compare and store min of 16 B values into global min
+}
+
+inline void reduce_min_48_host(__m128i *pMinR, __m128i *pMinG, __m128i *pMinB, __m128i *result)
+{
+    __m128i px[2];
+    __m128i zero = __lsx_vldi(0);
+    px[1] = __lsx_vmin_bu(__lsx_vilvl_b(pMinG[0], pMinR[0]), __lsx_vilvh_b(pMinG[0], pMinR[0]));
+    px[0] = __lsx_vmin_bu(__lsx_vilvl_b(zero, pMinB[0]), __lsx_vilvh_b(zero, pMinB[0]));
+    px[1] = __lsx_vmin_bu(__lsx_vilvl_h(px[0], px[1]), __lsx_vilvh_h(px[0], px[1]));
+    px[0] = __lsx_vmin_bu(__lsx_vilvl_w(zero, px[1]), __lsx_vilvh_w(zero, px[1]));
+    result[0] = __lsx_vmin_bu(__lsx_vilvl_d(zero, px[0]), __lsx_vilvh_d(zero, px[0]));
+}
+
+inline void reduce_max_32_host(__m256i *pMax, __m128i *result)
+{
+    __m128i px;
+    __m128i zero = __lsx_vldi(0);
+    __m128i mask = lsx_set_i8(0,1,2,3,4,5,6,8,9,10,11,12,13,14,15,7);
+    px = __lsx_vmax_bu(__lasx_cvt_256_128(pMax[0]), lasx_extracti128_m256i(pMax[0], 1));
+    px = __lsx_vmax_bu(__lsx_vilvl_b(px, zero), __lsx_vilvh_b(px, zero));
+    px = __lsx_vmax_hu(__lsx_vilvl_h(px, zero), __lsx_vilvh_h(px, zero));
+    px = __lsx_vmax_wu(__lsx_vilvl_w(px, zero), __lsx_vilvh_w(px, zero));
+    result[0] = lsx_shuffle_i8(px, mask);
+}
+
+inline void compute_max_96_host(__m256i *p1, __m256i *pMaxR, __m256i *pMaxG, __m256i *pMaxB)
+{
+    pMaxR[0] = __lasx_xvmax_bu(p1[0], pMaxR[0]); //compare and store max of 32 R values into global max
+    pMaxG[0] = __lasx_xvmax_bu(p1[1], pMaxG[0]); //compare and store max of 32 G values into global max
+    pMaxB[0] = __lasx_xvmax_bu(p1[2], pMaxB[0]); //compare and store max of 32 B values into global max
+}
+
+inline void reduce_max_96_host(__m256i *pMaxR, __m256i *pMaxG, __m256i *pMaxB, __m128i *result)
+{
+    __m128i px[4];
+    __m128i zero = __lsx_vldi(0);
+    px[0] = __lsx_vmax_bu(__lasx_cvt_256_128(pMaxR[0]), lasx_extracti128_m256i(pMaxR[0], 1));
+    px[1] = __lsx_vmax_bu(__lasx_cvt_256_128(pMaxG[0]), lasx_extracti128_m256i(pMaxG[0], 1));
+    px[1] = __lsx_vmax_bu(__lsx_vilvl_b(px[1], px[0]), __lsx_vilvh_b(px[1], px[0]));
+    px[0] = __lsx_vmax_bu(__lasx_cvt_256_128(pMaxB[0]), lasx_extracti128_m256i(pMaxB[0], 1));
+    px[0] = __lsx_vmax_bu(__lsx_vilvl_b(zero, px[0]), __lsx_vilvh_b(zero, px[0]));
+    px[1] = __lsx_vmax_bu(__lsx_vilvl_h(px[0], px[1]), __lsx_vilvh_h(px[0], px[1]));
+    px[0] = __lsx_vmax_bu(__lsx_vilvl_w(zero, px[1]), __lsx_vilvh_w(zero, px[1]));
+    result[0] = __lsx_vmax_bu(__lsx_vilvl_d(zero, px[0]), __lsx_vilvh_d(zero, px[0]));
+}
+
+inline void compute_max_48_host(__m128i *p1, __m128i *pMaxR, __m128i *pMaxG, __m128i *pMaxB)
+{
+    pMaxR[0] = __lsx_vmax_bu(p1[0], pMaxR[0]); //compare and store max of 16 R values into global max
+    pMaxG[0] = __lsx_vmax_bu(p1[1], pMaxG[0]); //compare and store max of 16 G values into global max
+    pMaxB[0] = __lsx_vmax_bu(p1[2], pMaxB[0]); //compare and store max of 16 B values into global max
+}
+
+inline void reduce_max_48_host(__m128i *pMaxR, __m128i *pMaxG, __m128i *pMaxB, __m128i *result)
+{
+    __m128i px[2];
+    __m128i zero = __lsx_vldi(0);
+    px[1] = __lsx_vmax_b(__lsx_vilvl_b(pMaxG[0], pMaxR[0]), __lsx_vilvh_b(pMaxG[0], pMaxR[0]));
+    px[0] = __lsx_vmax_b(__lsx_vilvl_b(zero, pMaxB[0]), __lsx_vilvh_b(zero, pMaxB[0]));
+    px[1] = __lsx_vmax_b(__lsx_vilvl_h(px[0], px[1]), __lsx_vilvh_h(px[0], px[1]));
+    px[0] = __lsx_vmax_b(__lsx_vilvl_w(zero, px[1]), __lsx_vilvh_w(zero, px[1]));
+    result[0] = __lsx_vmax_b(__lsx_vilvl_d(zero, px[0]), __lsx_vilvh_d(zero, px[0]));
+}
+
+inline void compute_min_float8_host(__m256 *p1, __m256 *pMin)
+{
+    pMin[0] = __lasx_xvfmin_s(p1[0], pMin[0]); //compare and store min of 8 values into global min
+}
+
+inline void reduce_min_float8_host(__m256 *pMin, __m128 *result)
+{
+    __m128 px;
+    px = __lsx_vfmin_s((__m128)__lasx_cvt_256_128((__m256i)pMin[0]), lasx_extractf128_f32(pMin[0], 1));
+    px = __lsx_vfmin_s((__m128)__lsx_vilvl_w((__m128i)px, (__m128i)xmm_p0), (__m128)__lsx_vilvh_w((__m128i)px, (__m128i)xmm_p0));
+    result[0] = (__m128)__lsx_vpermi_w((__m128i)px, (__m128i)px, 39);
+}
+
+inline void compute_min_float24_host(__m256 *p1, __m256 *pMinR, __m256 *pMinG, __m256 *pMinB)
+{
+    pMinR[0] = __lasx_xvfmin_s(p1[0], pMinR[0]); //compare and store min of 8 R values into global min
+    pMinG[0] = __lasx_xvfmin_s(p1[1], pMinG[0]); //compare and store min of 8 G values into global min
+    pMinB[0] = __lasx_xvfmin_s(p1[2], pMinB[0]); //compare and store min of 8 B values into global min
+}
+
+inline void reduce_min_float24_host(__m256 *pMinR, __m256 *pMinG, __m256 *pMinB, __m256 *result)   // TO CHANGE
+{
+    __m128 px[2];
+    px[0] = __lsx_vfmin_s((__m128)__lasx_cvt_256_128((__m256i)pMinR[0]), lasx_extractf128_f32(pMinR[0], 1));
+    px[1] = __lsx_vfmin_s((__m128)__lasx_cvt_256_128((__m256i)pMinG[0]), lasx_extractf128_f32(pMinG[0], 1));
+    px[0] = __lsx_vfmin_s((__m128)__lsx_vilvl_w((__m128i)px[1], (__m128i)px[0]), (__m128)__lsx_vilvh_w((__m128i)px[1], (__m128i)px[0]));
+    px[0] = (__m128)__lsx_vshuf4i_w((__m128i)px[0], 0b11011000);
+    result[0] = lasx_castm128_m256(px[0]);
+    px[0] = __lsx_vfmin_s((__m128)__lasx_cvt_256_128((__m256i)pMinB[0]), lasx_extractf128_f32(pMinB[0], 1));
+    px[1] = __lsx_vfmin_s((__m128)__lsx_vilvl_w((__m128i)xmm_p0, (__m128i)px[0]), (__m128)__lsx_vilvh_w((__m128i)xmm_p0, (__m128i)px[0]));
+    px[0] = (__m128)__lsx_vpermi_w((__m128i)px[1], (__m128i)px[1], 34);
+    result[0] = lasx_insertf128_f32(result[0], px[0], 1);
+}
+
+inline void compute_max_float8_host(__m256 *p1, __m256 *pMax)
+{
+    pMax[0] = __lasx_xvfmax_s(p1[0], pMax[0]); //compare and store max of 8 values into global min
+}
+
+inline void reduce_max_float8_host(__m256 *pMax, __m128 *result)
+{
+    __m128 px;
+    px = __lsx_vfmax_s((__m128)__lasx_cvt_256_128((__m256i)pMax[0]), lasx_extractf128_f32(pMax[0], 1));
+    px = __lsx_vfmax_s((__m128)__lsx_vilvl_w((__m128i)px, (__m128i)xmm_p0), (__m128)__lsx_vilvh_w((__m128i)px, (__m128i)xmm_p0));
+    result[0] = (__m128)__lsx_vpermi_w((__m128i)px, (__m128i)px, 39);
+}
+
+inline void compute_max_float24_host(__m256 *p1, __m256 *pMaxR, __m256 *pMaxG, __m256 *pMaxB)
+{
+    pMaxR[0] = __lasx_xvfmax_s(p1[0], pMaxR[0]); //compare and store max of 8 R values into global min
+    pMaxG[0] = __lasx_xvfmax_s(p1[1], pMaxG[0]); //compare and store max of 8 G values into global min
+    pMaxB[0] = __lasx_xvfmax_s(p1[2], pMaxB[0]); //compare and store max of 8 B values into global min
+}
+
+inline void reduce_max_float24_host(__m256 *pMaxR, __m256 *pMaxG, __m256 *pMaxB, __m256 *result)
+{
+    __m128 px[2];
+    px[0] = __lsx_vfmax_s((__m128)__lasx_cvt_256_128((__m256i)pMaxR[0]), lasx_extractf128_f32(pMaxR[0], 1));
+    px[1] = __lsx_vfmax_s((__m128)__lasx_cvt_256_128((__m256i)pMaxG[0]), lasx_extractf128_f32(pMaxG[0], 1));
+    px[0] = __lsx_vfmax_s((__m128)__lsx_vilvl_w((__m128i)px[1], (__m128i)px[0]), (__m128)__lsx_vilvh_w((__m128i)px[1], (__m128i)px[0]));
+    px[0] = (__m128)__lsx_vshuf4i_w((__m128i)px[0], 0b11011000);
+    result[0] = lasx_castm128_m256(px[0]);
+    px[0] = __lsx_vfmax_s((__m128)__lasx_cvt_256_128((__m256i)pMaxB[0]), lasx_extractf128_f32(pMaxB[0], 1));
+    px[1] = __lsx_vfmax_s((__m128)__lsx_vilvl_w((__m128i)xmm_p0, (__m128i)px[0]), (__m128)__lsx_vilvh_w((__m128i)xmm_p0, (__m128i)px[0]));
+    px[0] = (__m128)__lsx_vpermi_w((__m128i)px[1], (__m128i)px[1], 34);
+    result[0] = lasx_insertf128_f32(result[0], px[0], 1);
+}
+
+inline void reduce_min_i32_host(__m256i *pMin, __m128i *result)
+{
+    __m128i px;
+    __m128i zero = __lsx_vldi(0);
+    __m128i mask = lsx_set_i8(0,1,2,3,4,5,6,8,9,10,11,12,13,14,15,7);
+    px = __lsx_vmin_b(__lasx_cvt_256_128(pMin[0]), lasx_extracti128_m256i(pMin[0], 1));
+    px = __lsx_vmin_b(__lsx_vilvl_b(px, zero), __lsx_vilvh_b(px, zero));
+    px = __lsx_vmin_h(__lsx_vilvl_h(px, zero), __lsx_vilvh_h(px, zero));
+    px = __lsx_vmin_w(__lsx_vilvl_w(px, zero), __lsx_vilvh_w(px, zero));
+    result[0] = lsx_shuffle_i8(px, mask);
+}
+
+inline void compute_min_i96_host(__m256i *p1, __m256i *pMinR, __m256i *pMinG, __m256i *pMinB)
+{
+    pMinR[0] = __lasx_xvmin_b(p1[0], pMinR[0]); //compare and store min of 32 R values into global min
+    pMinG[0] = __lasx_xvmin_b(p1[1], pMinG[0]); //compare and store min of 32 G values into global min
+    pMinB[0] = __lasx_xvmin_b(p1[2], pMinB[0]); //compare and store min of 32 B values into global min
+}
+
+inline void reduce_min_i96_host(__m256i *pMinR, __m256i *pMinG, __m256i *pMinB, __m128i *result)
+{
+    __m128i px[4];
+    __m128i zero = __lsx_vldi(0);
+    px[0] = __lsx_vmin_b(__lasx_cvt_256_128(pMinR[0]), lasx_extracti128_m256i(pMinR[0], 1));
+    px[1] = __lsx_vmin_b(__lasx_cvt_256_128(pMinG[0]), lasx_extracti128_m256i(pMinG[0], 1));
+    px[1] = __lsx_vmin_b(__lsx_vilvl_b(px[1], px[0]), __lsx_vilvh_b(px[1], px[0]));
+    px[0] = __lsx_vmin_b(__lasx_cvt_256_128(pMinB[0]), lasx_extracti128_m256i(pMinB[0], 1));
+    px[0] = __lsx_vmin_b(__lsx_vilvl_b(zero, px[0]), __lsx_vilvh_b(zero, px[0]));
+    px[1] = __lsx_vmin_b(__lsx_vilvl_h(px[0], px[1]), __lsx_vilvh_h(px[0], px[1]));
+    px[0] = __lsx_vmin_b(__lsx_vilvl_w(zero, px[1]), __lsx_vilvh_w(zero, px[1]));
+    result[0] = __lsx_vmin_b(__lsx_vilvl_d(zero, px[0]), __lsx_vilvh_d(zero, px[0]));
+}
+
+inline void compute_min_i48_host(__m128i *p1, __m128i *pMinR, __m128i *pMinG, __m128i *pMinB)
+{
+    pMinR[0] = __lsx_vmin_b(p1[0], pMinR[0]); //compare and store min of 16 R values into global min
+    pMinG[0] = __lsx_vmin_b(p1[1], pMinG[0]); //compare and store min of 16 G values into global min
+    pMinB[0] = __lsx_vmin_b(p1[2], pMinB[0]); //compare and store min of 16 B values into global min
+}
+
+inline void reduce_min_i48_host(__m128i *pMinR, __m128i *pMinG, __m128i *pMinB, __m128i *result)
+{
+    __m128i px[2];
+    __m128i zero = __lsx_vldi(0);
+    px[1] = __lsx_vmin_b(__lsx_vilvl_b(pMinG[0], pMinR[0]), __lsx_vilvh_b(pMinG[0], pMinR[0]));
+    px[0] = __lsx_vmin_b(__lsx_vilvl_b(zero, pMinB[0]), __lsx_vilvh_b(zero, pMinB[0]));
+    px[1] = __lsx_vmin_b(__lsx_vilvl_h(px[0], px[1]), __lsx_vilvh_h(px[0], px[1]));
+    px[0] = __lsx_vmin_b(__lsx_vilvl_w(zero, px[1]), __lsx_vilvh_w(zero, px[1]));
+    result[0] = __lsx_vmin_b(__lsx_vilvl_d(zero, px[0]), __lsx_vilvh_d(zero, px[0]));
+}
+
+inline void reduce_max_i32_host(__m256i *pMax, __m128i *result)
+{
+    __m128i px[2];
+    __m128i zero = __lsx_vldi(0);
+    __m128i mask = lsx_set_i8(0,1,2,3,4,5,6,8,9,10,11,12,13,14,15,7);
+    px[0] = __lsx_vmax_b(__lasx_cvt_256_128(pMax[0]), lasx_extracti128_m256i(pMax[0], 1));
+    px[0] = __lsx_vmax_b(__lsx_vilvl_b(px[0], zero), __lsx_vilvh_b(px[0], zero));
+    px[0] = __lsx_vmax_h(__lsx_vilvl_h(px[0], zero), __lsx_vilvh_h(px[0], zero));
+    px[0] = __lsx_vmax_w(__lsx_vilvl_w(px[0], zero), __lsx_vilvh_w(px[0], zero));
+    result[0] = lsx_shuffle_i8(px[0], mask);
+}
+
+inline void compute_max_i96_host(__m256i *p1, __m256i *pMaxR, __m256i *pMaxG, __m256i *pMaxB)
+{
+    pMaxR[0] = __lasx_xvmax_b(p1[0], pMaxR[0]); //compare and store max of 32 R values into global max
+    pMaxG[0] = __lasx_xvmax_b(p1[1], pMaxG[0]); //compare and store max of 32 G values into global max
+    pMaxB[0] = __lasx_xvmax_b(p1[2], pMaxB[0]); //compare and store max of 32 B values into global max
+}
+
+inline void reduce_max_i96_host(__m256i *pMaxR, __m256i *pMaxG, __m256i *pMaxB, __m128i *result)
+{
+    __m128i px[4];
+    __m128i zero = __lsx_vldi(0);
+    px[0] = __lsx_vmax_b(__lasx_cvt_256_128(pMaxR[0]), lasx_extracti128_m256i(pMaxR[0], 1));
+    px[1] = __lsx_vmax_b(__lasx_cvt_256_128(pMaxG[0]), lasx_extracti128_m256i(pMaxG[0], 1));
+    px[1] = __lsx_vmax_b(__lsx_vilvl_b(px[1], px[0]), __lsx_vilvh_b(px[1], px[0]));
+    px[0] = __lsx_vmax_b(__lasx_cvt_256_128(pMaxB[0]), lasx_extracti128_m256i(pMaxB[0], 1));
+    px[0] = __lsx_vmax_b(__lsx_vilvl_b(zero, px[0]), __lsx_vilvh_b(zero, px[0]));
+    px[1] = __lsx_vmax_b(__lsx_vilvl_h(px[0], px[1]), __lsx_vilvh_h(px[0], px[1]));
+    px[0] = __lsx_vmax_b(__lsx_vilvl_w(zero, px[1]), __lsx_vilvh_w(zero, px[1]));
+    result[0] = __lsx_vmax_b(__lsx_vilvl_d(zero, px[0]), __lsx_vilvh_d(zero, px[0]));
+}
+
+inline void compute_max_i48_host(__m128i *p1, __m128i *pMaxR, __m128i *pMaxG, __m128i *pMaxB)
+{
+    pMaxR[0] = __lsx_vmax_b(p1[0], pMaxR[0]); //compare and store max of 16 R values into global max
+    pMaxG[0] = __lsx_vmax_b(p1[1], pMaxG[0]); //compare and store max of 16 G values into global max
+    pMaxB[0] = __lsx_vmax_b(p1[2], pMaxB[0]); //compare and store max of 16 B values into global max
+}
+
+inline void reduce_max_i48_host(__m128i *pMaxR, __m128i *pMaxG, __m128i *pMaxB, __m128i *result)
+{
+    __m128i px[2];
+    __m128i zero = __lsx_vldi(0);
+    px[1] = __lsx_vmax_b(__lsx_vilvl_b(pMaxG[0], pMaxR[0]), __lsx_vilvh_b(pMaxG[0], pMaxR[0]));
+    px[0] = __lsx_vmax_b(__lsx_vilvl_b(zero, pMaxB[0]), __lsx_vilvh_b(zero, pMaxB[0]));
+    px[1] = __lsx_vmax_b(__lsx_vilvl_h(px[0], px[1]), __lsx_vilvh_h(px[0], px[1]));
+    px[0] = __lsx_vmax_b(__lsx_vilvl_w(zero, px[1]), __lsx_vilvh_w(zero, px[1]));
+    result[0] = __lsx_vmax_b(__lsx_vilvl_d(zero, px[0]), __lsx_vilvh_d(zero, px[0]));
+}
+
+inline void compute_remap_src_loc_sse(Rpp32f *rowRemapTablePtr, Rpp32f *colRemapTablePtr, Rpp32s *locArray, __m128 &pStride, __m128 &pWidthLimit, __m128 &pHeightLimit, const __m128 &pChannel = xmm_p1)
+{
+    __m128 pRowRemapVal = (__m128)__lsx_vld(rowRemapTablePtr, 0);
+    pRowRemapVal = __lsx_vfmax_s(__lsx_vfmin_s(pRowRemapVal, pHeightLimit), xmm_p0);
+    __m128 pColRemapVal = (__m128)__lsx_vld(colRemapTablePtr, 0);
+    pColRemapVal = __lsx_vfmax_s(__lsx_vfmin_s(pColRemapVal, pWidthLimit), xmm_p0);
+    __m128i pxRemappedSrcLoc = __lsx_vftint_w_s(__lsx_vfmadd_s(pRowRemapVal, pStride, __lsx_vfmul_s(pColRemapVal, pChannel)));
+    __lsx_vst(pxRemappedSrcLoc, (__m128i*) locArray, 0);
+}
+
+inline void compute_remap_src_loc(Rpp32f rowLoc, Rpp32f colLoc, Rpp32s &srcLoc, Rpp32s stride, Rpp32f widthLimit, Rpp32f heightLimit, Rpp32s channels = 1)
+{
+    rowLoc = std::max(0.0f, std::min(rowLoc, heightLimit));
+    colLoc = std::max(0.0f, std::min(colLoc, widthLimit));
+    srcLoc = (rowLoc * stride) + colLoc * channels;
+}
+
+inline void compute_log_16_host(__m256 *p)
+{
+    p[0] = log_ps(p[0]);    // log compute
+    p[1] = log_ps(p[1]);    // log compute
+}
+
+inline void compute_transpose4x8_avx(__m256 *pSrc, __m128 *pDst)
+{
+    __m256 tmp0, tmp1, tmp2, tmp3;
+    tmp0 = (__m256)__lasx_xvpermi_w(pSrc[0], pSrc[1], 0x44);   /* shuffle to get [P01|P02|P09|P10|P05|P06|P13|P14] */
+    tmp2 = (__m256)__lasx_xvpermi_w(pSrc[0], pSrc[1], 0xEE);   /* shuffle to get [P03|P04|P11|P12|P07|P08|P15|P16] */
+    tmp1 = (__m256)__lasx_xvpermi_w(pSrc[2], pSrc[3], 0x44);   /* shuffle to get [P17|P18|P25|P26|P21|P22|P29|P30] */
+    tmp3 = (__m256)__lasx_xvpermi_w(pSrc[2], pSrc[3], 0xEE);   /* shuffle to get [P19|P20|P27|P28|P23|P24|P31|P32] */
+    pSrc[0] = (__m256)__lasx_xvpermi_w(tmp0, tmp1, 0x88);  /* shuffle to get [P01|P09|P17|P25|P05|P13|P21|P29] */
+    pSrc[1] = (__m256)__lasx_xvpermi_w(tmp0, tmp1, 0xDD);  /* shuffle to get [P02|P10|P18|P26|P06|P14|P22|P30] */
+    pSrc[2] = (__m256)__lasx_xvpermi_w(tmp2, tmp3, 0x88);  /* shuffle to get [P03|P11|P19|P27|P07|P15|P23|P31] */
+    pSrc[3] = (__m256)__lasx_xvpermi_w(tmp2, tmp3, 0xDD);  /* shuffle to get [P04|P12|P20|P28|P08|P16|P24|P32] */
+
+    pDst[0] = (__m128)__lasx_cvt_256_128((__m256i)pSrc[0]);  /* extract [P01|P09|P17|P25] */
+    pDst[1] = (__m128)__lasx_cvt_256_128((__m256i)pSrc[1]);  /* extract [P02|P10|P18|P26] */
+    pDst[2] = (__m128)__lasx_cvt_256_128((__m256i)pSrc[2]);  /* extract [P03|P11|P19|P27] */
+    pDst[3] = (__m128)__lasx_cvt_256_128((__m256i)pSrc[3]);  /* extract [P04|P12|P20|P28] */
+    pDst[4] = lasx_extractf128_f32(pSrc[0], 1);    /* extract [P05|P13|P21|P29] */
+    pDst[5] = lasx_extractf128_f32(pSrc[1], 1);    /* extract [P06|P14|P22|P30] */
+    pDst[6] = lasx_extractf128_f32(pSrc[2], 1);    /* extract [P07|P15|P23|P31] */
+    pDst[7] = lasx_extractf128_f32(pSrc[3], 1);    /* extract [P08|P16|P24|P32] */
+}
+
+inline void compute_rain_48_host(__m256 *p1, __m256 *p2, __m256 &pMul)
+{
+    p1[0] = __lasx_xvfmadd_s(__lasx_xvfsub_s(p2[0], p1[0]), pMul, p1[0]);    // alpha-blending adjustment
+    p1[1] = __lasx_xvfmadd_s(__lasx_xvfsub_s(p2[1], p1[1]), pMul, p1[1]);    // alpha-blending adjustment
+    p1[2] = __lasx_xvfmadd_s(__lasx_xvfsub_s(p2[0], p1[2]), pMul, p1[2]);    // alpha-blending adjustment
+    p1[3] = __lasx_xvfmadd_s(__lasx_xvfsub_s(p2[1], p1[3]), pMul, p1[3]);    // alpha-blending adjustment
+    p1[4] = __lasx_xvfmadd_s(__lasx_xvfsub_s(p2[0], p1[4]), pMul, p1[4]);    // alpha-blending adjustment
+    p1[5] = __lasx_xvfmadd_s(__lasx_xvfsub_s(p2[1], p1[5]), pMul, p1[5]);    // alpha-blending adjustment
+}
+
+inline void compute_rain_32_host(__m256 *p1, __m256 *p2, __m256 &pMul)
+{
+    p1[0] = __lasx_xvfmadd_s(__lasx_xvfsub_s(p2[0], p1[0]), pMul, p1[0]);    // alpha-blending adjustment
+    p1[1] = __lasx_xvfmadd_s(__lasx_xvfsub_s(p2[1], p1[1]), pMul, p1[1]);    // alpha-blending adjustment
+    p1[2] = __lasx_xvfmadd_s(__lasx_xvfsub_s(p2[2], p1[2]), pMul, p1[2]);    // alpha-blending adjustment
+    p1[3] = __lasx_xvfmadd_s(__lasx_xvfsub_s(p2[3], p1[3]), pMul, p1[3]);    // alpha-blending adjustment
+}
+
+inline void compute_rain_24_host(__m256 *p1, __m256 p2, __m256 &pMul)
+{
+    p1[0] = __lasx_xvfmadd_s(__lasx_xvfsub_s(p2, p1[0]), pMul, p1[0]);    // alpha-blending adjustment
+    p1[1] = __lasx_xvfmadd_s(__lasx_xvfsub_s(p2, p1[1]), pMul, p1[1]);    // alpha-blending adjustment
+    p1[2] = __lasx_xvfmadd_s(__lasx_xvfsub_s(p2, p1[2]), pMul, p1[2]);    // alpha-blending adjustment
+}
+
+// Compute hanning window
+inline RPP_HOST_DEVICE void hann_window(Rpp32f *output, Rpp32s windowSize)
+{
+    Rpp64f a = (2.0 * M_PI) / windowSize;
+    for (Rpp32s t = 0; t < windowSize; t++)
+    {
+        Rpp64f phase = a * (t + 0.5);
+        output[t] = (0.5 * (1.0 - std::cos(phase)));
+    }
+}
+
+// Compute number of spectrogram windows
+inline RPP_HOST_DEVICE Rpp32s get_num_windows(Rpp32s length, Rpp32s windowLength, Rpp32s windowStep, bool centerWindows)
+{
+    if (!centerWindows)
+        length -= windowLength;
+    return ((length / windowStep) + 1);
+}
+
+// Compute reflect start idx to pad
+inline RPP_HOST_DEVICE Rpp32s get_idx_reflect(Rpp32s loc, Rpp32s minLoc, Rpp32s maxLoc)
+{
+    if (maxLoc - minLoc < 2)
+        return maxLoc - 1;
+    for (;;)
+    {
+        if (loc < minLoc)
+            loc = 2 * minLoc - loc;
+        else if (loc >= maxLoc)
+            loc = 2 * maxLoc - 2 - loc;
+        else
+            break;
+    }
+    return loc;
+}
+
+inline void compute_threshold_8_host(__m256 *p, __m256 *pThresholdParams)
+{
+    p[0] = lasx_blendv_f32(avx_p0, avx_p1, (__m256)__lasx_xvand_v((__m256i)(__m256)__lasx_xvfcmp_xxx_s(p[0], pThresholdParams[0], _CMP_GE_OQ), (__m256i)(__m256)__lasx_xvfcmp_xxx_s(p[0], pThresholdParams[1],_CMP_LE_OQ)));
+}
+
+inline void compute_threshold_16_host(__m256 *p, __m256 *pThresholdParams)
+{
+    p[0] = lasx_blendv_f32(avx_p0, avx_p255, (__m256)__lasx_xvand_v((__m256i)(__m256)__lasx_xvfcmp_xxx_s(p[0], pThresholdParams[0], _CMP_GE_OQ), (__m256i)(__m256)__lasx_xvfcmp_xxx_s(p[0], pThresholdParams[1],_CMP_LE_OQ)));
+    p[1] = lasx_blendv_f32(avx_p0, avx_p255, (__m256)__lasx_xvand_v((__m256i)(__m256)__lasx_xvfcmp_xxx_s(p[1], pThresholdParams[0], _CMP_GE_OQ), (__m256i)(__m256)__lasx_xvfcmp_xxx_s(p[1], pThresholdParams[1],_CMP_LE_OQ)));
+}
+
+inline void compute_threshold_24_host(__m256 *p, __m256 *pThresholdParams)
+{
+    __m256 pChannelCheck[3];
+    pChannelCheck[0] = (__m256)__lasx_xvand_v((__m256i)(__m256)__lasx_xvfcmp_xxx_s(p[0], pThresholdParams[0], _CMP_GE_OQ), (__m256i)(__m256)__lasx_xvfcmp_xxx_s(p[0], pThresholdParams[1],_CMP_LE_OQ));
+    pChannelCheck[1] = (__m256)__lasx_xvand_v((__m256i)(__m256)__lasx_xvfcmp_xxx_s(p[1], pThresholdParams[2], _CMP_GE_OQ), (__m256i)(__m256)__lasx_xvfcmp_xxx_s(p[1], pThresholdParams[3],_CMP_LE_OQ));
+    pChannelCheck[2] = (__m256)__lasx_xvand_v((__m256i)(__m256)__lasx_xvfcmp_xxx_s(p[2], pThresholdParams[4], _CMP_GE_OQ), (__m256i)(__m256)__lasx_xvfcmp_xxx_s(p[2], pThresholdParams[5],_CMP_LE_OQ));
+    p[0] = lasx_blendv_f32(avx_p0, avx_p1, (__m256)__lasx_xvand_v((__m256i)__lasx_xvand_v((__m256i)pChannelCheck[0], (__m256i)pChannelCheck[1]), (__m256i)pChannelCheck[2]));
+    p[1] = p[0];
+    p[2] = p[0];
+}
+
+inline void compute_threshold_48_host(__m256 *p, __m256 *pThresholdParams)
+{
+    __m256 pChannelCheck[3];
+    pChannelCheck[0] = (__m256)__lasx_xvand_v((__m256i)(__m256)__lasx_xvfcmp_xxx_s(p[0], pThresholdParams[0], _CMP_GE_OQ), (__m256i)(__m256)__lasx_xvfcmp_xxx_s(p[0], pThresholdParams[1],_CMP_LE_OQ));
+    pChannelCheck[1] = (__m256)__lasx_xvand_v((__m256i)(__m256)__lasx_xvfcmp_xxx_s(p[2], pThresholdParams[2], _CMP_GE_OQ), (__m256i)(__m256)__lasx_xvfcmp_xxx_s(p[2], pThresholdParams[3],_CMP_LE_OQ));
+    pChannelCheck[2] = (__m256)__lasx_xvand_v((__m256i)(__m256)__lasx_xvfcmp_xxx_s(p[4], pThresholdParams[4], _CMP_GE_OQ), (__m256i)(__m256)__lasx_xvfcmp_xxx_s(p[4], pThresholdParams[5],_CMP_LE_OQ));
+    p[0] = lasx_blendv_f32(avx_p0, avx_p255, (__m256)__lasx_xvand_v((__m256i)__lasx_xvand_v((__m256i)pChannelCheck[0], (__m256i)pChannelCheck[1]), (__m256i)pChannelCheck[2]));
+    p[2] = p[0];
+    p[4] = p[0];
+
+    pChannelCheck[0] = (__m256)__lasx_xvand_v((__m256i)(__m256)__lasx_xvfcmp_xxx_s(p[1], pThresholdParams[0], _CMP_GE_OQ), (__m256i)(__m256)__lasx_xvfcmp_xxx_s(p[1], pThresholdParams[1],_CMP_LE_OQ));
+    pChannelCheck[1] = (__m256)__lasx_xvand_v((__m256i)(__m256)__lasx_xvfcmp_xxx_s(p[3], pThresholdParams[2], _CMP_GE_OQ), (__m256i)(__m256)__lasx_xvfcmp_xxx_s(p[3], pThresholdParams[3],_CMP_LE_OQ));
+    pChannelCheck[2] = (__m256)__lasx_xvand_v((__m256i)(__m256)__lasx_xvfcmp_xxx_s(p[5], pThresholdParams[4], _CMP_GE_OQ), (__m256i)(__m256)__lasx_xvfcmp_xxx_s(p[5], pThresholdParams[5],_CMP_LE_OQ));
+    p[1] = lasx_blendv_f32(avx_p0, avx_p255, (__m256)__lasx_xvand_v((__m256)__lasx_xvand_v((__m256i)pChannelCheck[0], (__m256i)pChannelCheck[1]), (__m256i)pChannelCheck[2]));
+    p[3] = p[1];
+    p[5] = p[1];
+}
+
+#endif //RPP_CPU_COMMON_H
diff --git a/src/include/cpu/rpp_loongarch_filter.hpp b/src/include/cpu/rpp_loongarch_filter.hpp
new file mode 100644
index 00000000..ea8a58b1
--- /dev/null
+++ b/src/include/cpu/rpp_loongarch_filter.hpp
@@ -0,0 +1,1242 @@
+/*
+MIT License
+
+Copyright (c) 2019 - 2024 Advanced Micro Devices, Inc.
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
+*/
+
+#ifndef AMD_RPP_RPP_CPU_FILTER_HPP
+#define AMD_RPP_RPP_CPU_FILTER_HPP
+
+#include "rpp_loongarch_simd.hpp"
+
+// declare masks used for shuffle and permute operations used in filter functions
+const __m128i xmm_pxMaskRotate0To1 = lsx_setr_i8(2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 0, 1);
+const __m128i xmm_pxMaskRotate0To3 = lsx_setr_i8(4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 0, 1, 2, 3);
+const __m128i xmm_pxMaskRotate0To5 = lsx_setr_i8(6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 0, 1, 2, 3, 4, 5);
+const __m128i xmm_pxMaskRotate0To7 = lsx_setr_i8(8, 9, 10, 11, 12, 13, 14, 15, 0, 1, 2, 3, 4, 5, 6, 7);
+const __m128i xmm_pxMaskRotate0To9 = lsx_setr_i8(10, 11, 12, 13, 14, 15, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9);
+const __m128i xmm_pxMaskRotate0To11 = lsx_setr_i8(12, 13, 14, 15, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11);
+const __m128i xmm_pxMaskRotate0To13 = lsx_setr_i8(14, 15, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13);
+const __m256i avx_pxMaskRotate0To1 = lasx_setr_i32(1, 2, 3, 4, 5, 6, 7, 0);
+const __m256i avx_pxMaskRotate0To2 = lasx_setr_i32(2, 3, 4, 5, 6, 7, 0, 1);
+const __m256i avx_pxMaskRotate0To3 = lasx_setr_i32(3, 4, 5, 6, 7, 0, 1, 2);
+const __m256i avx_pxMaskRotate0To4 = lasx_setr_i32(4, 5, 6, 7, 0, 1, 2, 3);
+const __m256i avx_pxMaskRotate0To5 = lasx_setr_i32(5, 6, 7, 0, 1, 2, 3, 4);
+const __m256i avx_pxMaskRotate0To6 = lasx_setr_i32(6, 7, 0, 1, 2, 3, 4, 5);
+const __m256i avx_pxMaskRotate0To7 = lasx_setr_i32(7, 0, 1, 2, 3, 4, 5, 6);
+
+// increment the kernel size number of row pointers with increment value
+template<typename T>
+inline void increment_row_ptrs(T **srcPtrTemp, Rpp32u kernelSize, Rpp32s increment)
+{
+    for (int i = 0; i < kernelSize; i++)
+        srcPtrTemp[i] += increment;
+}
+
+// get the kernel loop limit based on index
+inline void get_kernel_loop_limit(Rpp32s &index, Rpp32s &loopLimit, Rpp32u &padLength, Rpp32u &unpaddedLength)
+{
+    if ((index < padLength) || (index >= unpaddedLength))
+    {
+        Rpp32u factor = (index < padLength) ? (index - padLength) : (unpaddedLength - 1 - index);
+        loopLimit += factor;
+    }
+}
+
+template<typename T>
+inline void convolution_filter_generic_tensor(T **srcPtrTemp, T *dstPtrTemp, Rpp32s columnIndex,
+                                              Rpp32u kernelSize, Rpp32u padLength, Rpp32u unpaddedWidth, Rpp32s rowKernelLoopLimit,
+                                              Rpp32f *filterTensor, Rpp32u channels = 1)
+{
+    Rpp32f accum = 0.0f;
+    Rpp32s columnKernelLoopLimit = kernelSize;
+
+    // find the colKernelLoopLimit based on columnIndex
+    get_kernel_loop_limit(columnIndex, columnKernelLoopLimit, padLength, unpaddedWidth);
+    if constexpr (std::is_same<T, Rpp8s>::value)
+    {
+        for (int i = 0; i < rowKernelLoopLimit; i++)
+            for (int j = 0, k = 0 ; j < columnKernelLoopLimit; j++, k += channels)
+                accum += static_cast<Rpp32f>(srcPtrTemp[i][k] + 128) * filterTensor[i * kernelSize + j];
+    }
+    else
+    {
+        for (int i = 0; i < rowKernelLoopLimit; i++)
+            for (int j = 0, k = 0 ; j < columnKernelLoopLimit; j++, k += channels)
+                accum += static_cast<Rpp32f>(srcPtrTemp[i][k]) * filterTensor[i * kernelSize + j];
+    }
+
+    if constexpr (std::is_same<T, Rpp8u>::value || std::is_same<T, Rpp8s>::value)
+        accum = nearbyintf(accum);
+    saturate_pixel(accum, dstPtrTemp);
+}
+
+// process padLength number of columns in each row
+// left border pixels in image which does not have required pixels in 3x3 kernel, process them separately
+template<typename T>
+inline void process_left_border_columns_pln_pln(T **srcPtrTemp, T *dstPtrTemp, Rpp32u kernelSize, Rpp32u padLength,
+                                                Rpp32u unpaddedWidth, Rpp32s rowKernelLoopLimit, Rpp32f *filterTensor)
+{
+    for (int k = 0; k < padLength; k++)
+    {
+        convolution_filter_generic_tensor(srcPtrTemp, dstPtrTemp, k, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, filterTensor);
+        dstPtrTemp++;
+    }
+}
+
+template<typename T>
+inline void process_left_border_columns_pkd_pkd(T **srcPtrTemp, T **srcPtrRow, T *dstPtrTemp, Rpp32u kernelSize, Rpp32u padLength,
+                                                Rpp32u unpaddedWidth, Rpp32s rowKernelLoopLimit, Rpp32f *filterTensor)
+{
+    for (int c = 0; c < 3; c++)
+    {
+        T *dstPtrTempChannel = dstPtrTemp + c;
+        for (int k = 0; k < padLength; k++)
+        {
+            convolution_filter_generic_tensor(srcPtrTemp, dstPtrTempChannel, k, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, filterTensor, 3);
+            dstPtrTempChannel += 3;
+        }
+        increment_row_ptrs(srcPtrTemp, kernelSize, 1);
+    }
+    // reset source to initial position
+    for (int k = 0; k < kernelSize; k++)
+        srcPtrTemp[k] = srcPtrRow[k];
+}
+
+template<typename T>
+inline void process_left_border_columns_pkd_pln(T **srcPtrTemp, T **srcPtrRow, T **dstPtrTempChannels, Rpp32u kernelSize, Rpp32u padLength,
+                                                Rpp32u unpaddedWidth, Rpp32s rowKernelLoopLimit, Rpp32f *filterTensor)
+{
+    for (int c = 0; c < 3; c++)
+    {
+        for (int k = 0; k < padLength; k++)
+        {
+            convolution_filter_generic_tensor(srcPtrTemp, dstPtrTempChannels[c], k, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, filterTensor, 3);
+            dstPtrTempChannels[c] += 1;
+        }
+        increment_row_ptrs(srcPtrTemp, kernelSize, 1);
+    }
+
+    // reset source to initial position
+    for (int k = 0; k < kernelSize; k++)
+        srcPtrTemp[k] = srcPtrRow[k];
+}
+
+// extract 4 SSE registers from 2 AVX registers
+inline void extract_4sse_registers(__m256i *pxRowHalf, __m128i *px128)
+{
+    px128[0] =  __lasx_cvt_256_128(pxRowHalf[0]);
+    px128[1] =  __lasx_cvt_256_128(pxRowHalf[1]);
+    px128[2] =  lasx_extracti128_m256i(pxRowHalf[0], 1);
+    px128[3] =  lasx_extracti128_m256i(pxRowHalf[1], 1);
+}
+
+// extract 3 SSE registers from 2 AVX registers
+inline void extract_3sse_registers(__m256i *pxRowHalf, __m128i *px128)
+{
+    px128[0] =  __lasx_cvt_256_128(pxRowHalf[0]);
+    px128[1] =  __lasx_cvt_256_128(pxRowHalf[1]);
+    px128[2] =  lasx_extracti128_m256i(pxRowHalf[0], 1);
+}
+
+// -------------------- U8/I8 bitdepth compute functions for kernel size (3/5/7/9) --------------------
+
+// perform required blend shuffle add operations for 3x3 kernel size
+template <int blendMask1, int blendMask2> 
+inline void blend_shuffle_add_3x3_host(__m128i *px128, __m128i *pxMask, Rpp32u *index)
+{
+    /*  For PLN inputs                                                                             | For PKD inputs
+        px128[0] -  [X01|X02|X03|X04|X05|X06|X07|X08], px128[1] - [X09|X10|X11|X12|X13|X14|X15|X16]| px128[0]  - [R01|G01|B01|R02|G02|B02|R03|G03], px128[1] - [B03|R04|G04|B04|R05|G05|B05|R06]
+        pxTemp[0] - [X02|X03|X04|X05|X06|X07|X08|X09] (blend with mask [0000 0001] and shuffle)    | pxTemp[0] - [R02|G02|B02|R03|G03|B03|R04|G04] (blend with mask [0000 0111] and shuffle)
+        pxTemp[1] - [X03|X04|X05|X06|X07|X08|X09|X10] (blend with mask [0000 0011] and shuffle)    | pxTemp[1] - [R03|G03|B03|R04|G04|B04|R05|G05] (blend with mask [0011 1111] and shuffle) */ 
+    __m128i pxTemp[2];
+    pxTemp[0] = lsx_shuffle_i8(lsx_blend_i16(px128[index[0]], px128[index[0] + 1], blendMask1), pxMask[0]);    
+    pxTemp[1] = lsx_shuffle_i8(lsx_blend_i16(px128[index[1]], px128[index[1] + 1], blendMask2), pxMask[1]);
+    px128[0] = __lsx_vadd_h(__lsx_vadd_h(px128[0], pxTemp[0]), pxTemp[1]);
+}
+
+// perform required blend shuffle add operations for 5x5 kernel size
+template<int blendMask1, int blendMask2, int blendMask3, int blendMask4> 
+inline void blend_shuffle_add_5x5_host(__m128i *px128, __m128i *pxMask, Rpp32u *index)
+{
+    /*  For PLN inputs                                                                             | For PKD inputs
+        px128[0] -  [X01|X02|X03|X04|X05|X06|X07|X08], px128[1] - [X09|X10|X11|X12|X13|X14|X15|X16]| px128[0]  - [R01|G01|B01|R02|G02|B02|R03|G03], px128[1] - [B03|R04|G04|B04|R05|G05|B05|R06]
+        pxTemp[0] - [X02|X03|X04|X05|X06|X07|X08|X09] (blend with mask [0000 0001] and shuffle)    | px128[2] -  [G06|B06|R07|G07|B07|R08|G08|B08]
+        pxTemp[1] - [X03|X04|X05|X06|X07|X08|X09|X10] (blend with mask [0000 0011] and shuffle)    | pxTemp[0] - [R02|G02|B02|R03|G03|B03|R04|G04] (blend with mask [0000 0111] and shuffle)
+        pxTemp[2] - [X04|X05|X06|X07|X08|X09|X10|X11] (blend with mask [0000 0111] and shuffle)    | pxTemp[1] - [R03|G03|B03|R04|G04|B04|R05|G05] (blend with mask [0011 1111] and shuffle) 
+        pxTemp[3] - [X05|X06|X07|X08|X09|X10|X11|X12] (blend with mask [0000 1111] and shuffle)    | pxTemp[2] - [R04|G04|B04|R05|G05|B05|R06|G06] (blend with mask [0000 0001] and shuffle)
+                                                                                                   | pxTemp[3] - [R05|G05|B05|R06|G06|B06|R07|G07] (blend with mask [0000 1111] and shuffle) */ 
+    __m128i pxTemp[4];
+    pxTemp[0] = lsx_shuffle_i8(lsx_blend_i16(px128[index[0]], px128[index[0] + 1], blendMask1), pxMask[0]);
+    pxTemp[1] = lsx_shuffle_i8(lsx_blend_i16(px128[index[1]], px128[index[1] + 1], blendMask2), pxMask[1]);
+    pxTemp[2] = lsx_shuffle_i8(lsx_blend_i16(px128[index[2]], px128[index[2] + 1], blendMask3), pxMask[2]);
+    pxTemp[3] = lsx_shuffle_i8(lsx_blend_i16(px128[index[3]], px128[index[3] + 1], blendMask4), pxMask[3]);
+    px128[0] = __lsx_vadd_h(__lsx_vadd_h(__lsx_vadd_h(__lsx_vadd_h(px128[0], pxTemp[0]), pxTemp[1]), pxTemp[2]),  pxTemp[3]);
+}
+
+// perform required blend shuffle add operations for 7x7 kernel size
+template<int blendMask1, int blendMask2, int blendMask3, int blendMask4, int blendMask5, int blendMask6>
+inline void blend_shuffle_add_7x7_host(__m128i *px128, __m128i *pxMask, Rpp32u *index)
+{
+    /*  For PLN inputs                                                                             | For PKD inputs
+        px128[0] -  [X01|X02|X03|X04|X05|X06|X07|X08], px128[1] - [X09|X10|X11|X12|X13|X14|X15|X16]| px128[0]  - [R01|G01|B01|R02|G02|B02|R03|G03], px128[1] - [B03|R04|G04|B04|R05|G05|B05|R06],
+        pxTemp[0] - [X02|X03|X04|X05|X06|X07|X08|X09] (blend with mask [0000 0001] and shuffle)    | px128[2] -  [G06|B06|R07|G07|B07|R08|G08|B08], px128[3] - [R09|G09|B09|R10|G10|B10|R11|G11]
+        pxTemp[1] - [X03|X04|X05|X06|X07|X08|X09|X10] (blend with mask [0000 0011] and shuffle)    | pxTemp[0] - [R02|G02|B02|R03|G03|B03|R04|G04] (blend with mask [0000 0111] and shuffle)
+        pxTemp[2] - [X04|X05|X06|X07|X08|X09|X10|X11] (blend with mask [0000 0111] and shuffle)    | pxTemp[1] - [R03|G03|B03|R04|G04|B04|R05|G05] (blend with mask [0011 1111] and shuffle) 
+        pxTemp[3] - [X05|X06|X07|X08|X09|X10|X11|X12] (blend with mask [0000 1111] and shuffle)    | pxTemp[2] - [R04|G04|B04|R05|G05|B05|R06|G06] (blend with mask [0000 0001] and shuffle)
+        pxTemp[4] - [X06|X07|X08|X09|X10|X11|X12|X13] (blend with mask [0001 1111] and shuffle)    | pxTemp[3] - [R05|G05|B05|R06|G06|B06|R07|G07] (blend with mask [0000 1111] and shuffle)
+        pxTemp[5] - [X07|X08|X09|X10|X11|X12|X13|X14] (blend with mask [0011 1111] and shuffle)    | pxTemp[4] - [R06|G06|B06|R07|G07|B07|R08|G08] (blend with mask [0111 1111] and shuffle)
+                                                                                                   | pxTemp[5] - [R07|G07|B07|R08|G08|B08|R09|G09] (blend with mask [0000 0011] and shuffle) */ 
+    __m128i pxTemp[6];
+    pxTemp[0] = lsx_shuffle_i8(lsx_blend_i16(px128[index[0]], px128[index[0] + 1], blendMask1), pxMask[0]);
+    pxTemp[1] = lsx_shuffle_i8(lsx_blend_i16(px128[index[1]], px128[index[1] + 1], blendMask2), pxMask[1]);
+    pxTemp[2] = lsx_shuffle_i8(lsx_blend_i16(px128[index[2]], px128[index[2] + 1], blendMask3), pxMask[2]);
+    pxTemp[3] = lsx_shuffle_i8(lsx_blend_i16(px128[index[3]], px128[index[3] + 1], blendMask4), pxMask[3]);
+    pxTemp[4] = lsx_shuffle_i8(lsx_blend_i16(px128[index[4]], px128[index[4] + 1], blendMask5), pxMask[4]);
+    pxTemp[5] = lsx_shuffle_i8(lsx_blend_i16(px128[index[5]], px128[index[5] + 1], blendMask6), pxMask[5]);
+    px128[0] = __lsx_vadd_h(__lsx_vadd_h(__lsx_vadd_h(px128[0], pxTemp[0]), pxTemp[1]), pxTemp[2]);
+    px128[0] = __lsx_vadd_h(__lsx_vadd_h(__lsx_vadd_h(px128[0], pxTemp[3]), pxTemp[4]), pxTemp[5]);
+}
+
+// perform required blend shuffle add operations for 9x9 kernel size
+template<int blendMask1, int blendMask2, int blendMask3, int blendMask4, int blendMask5, int blendMask6, int blendMask7>
+inline void blend_shuffle_add_9x9_host(__m128i *px128, __m128i *pxMask, Rpp32u *index)
+{
+    /*  For PLN inputs                                                                             | For PKD inputs
+        px128[0] -  [X01|X02|X03|X04|X05|X06|X07|X08], px128[1] - [X09|X10|X11|X12|X13|X14|X15|X16]| px128[0]  - [R01|G01|B01|R02|G02|B02|R03|G03], px128[1] - [B03|R04|G04|B04|R05|G05|B05|R06],
+        pxTemp[0] - [X02|X03|X04|X05|X06|X07|X08|X09] (blend with mask [0000 0001] and shuffle)    | px128[2] -  [G06|B06|R07|G07|B07|R08|G08|B08], px128[3] - [R09|G09|B09|R10|G10|B10|R11|G11]
+        pxTemp[1] - [X03|X04|X05|X06|X07|X08|X09|X10] (blend with mask [0000 0011] and shuffle)    | pxTemp[0] - [R02|G02|B02|R03|G03|B03|R04|G04] (blend with mask [0000 0111] and shuffle)
+        pxTemp[2] - [X04|X05|X06|X07|X08|X09|X10|X11] (blend with mask [0000 0111] and shuffle)    | pxTemp[1] - [R03|G03|B03|R04|G04|B04|R05|G05] (blend with mask [0011 1111] and shuffle) 
+        pxTemp[3] - [X05|X06|X07|X08|X09|X10|X11|X12] (blend with mask [0000 1111] and shuffle)    | pxTemp[2] - [R04|G04|B04|R05|G05|B05|R06|G06] (blend with mask [0000 0001] and shuffle)
+        pxTemp[4] - [X06|X07|X08|X09|X10|X11|X12|X13] (blend with mask [0001 1111] and shuffle)    | pxTemp[3] - [R05|G05|B05|R06|G06|B06|R07|G07] (blend with mask [0000 1111] and shuffle)
+        pxTemp[5] - [X07|X08|X09|X10|X11|X12|X13|X14] (blend with mask [0011 1111] and shuffle)    | pxTemp[4] - [R06|G06|B06|R07|G07|B07|R08|G08] (blend with mask [0111 1111] and shuffle)
+        pxTemp[6] - [X08|X09|X10|X11|X12|X13|X14|X15] (blend with mask [0111 1111] and shuffle)    | pxTemp[5] - [R07|G07|B07|R08|G08|B08|R09|G09] (blend with mask [0000 0011] and shuffle)
+                                                                                                   | pxTemp[6] - [R08|G08|B08|R09|G09|B09|R10|G10] (blend with mask [0001 1111] and shuffle) */ 
+    __m128i pxTemp[7];
+    pxTemp[0] = lsx_shuffle_i8(lsx_blend_i16(px128[index[0]], px128[index[0] + 1], blendMask1), pxMask[0]);    
+    pxTemp[1] = lsx_shuffle_i8(lsx_blend_i16(px128[index[1]], px128[index[1] + 1], blendMask2), pxMask[1]);    
+    pxTemp[2] = lsx_shuffle_i8(lsx_blend_i16(px128[index[2]], px128[index[2] + 1], blendMask3), pxMask[2]);    
+    pxTemp[3] = lsx_shuffle_i8(lsx_blend_i16(px128[index[3]], px128[index[3] + 1], blendMask4), pxMask[3]);
+    pxTemp[4] = lsx_shuffle_i8(lsx_blend_i16(px128[index[4]], px128[index[4] + 1], blendMask5), pxMask[4]);
+    pxTemp[5] = lsx_shuffle_i8(lsx_blend_i16(px128[index[5]], px128[index[5] + 1], blendMask6), pxMask[5]);
+    pxTemp[6] = lsx_shuffle_i8(lsx_blend_i16(px128[index[6]], px128[index[6] + 1], blendMask7), pxMask[6]);
+    px128[0] = __lsx_vadd_h(__lsx_vadd_h(__lsx_vadd_h(__lsx_vadd_h(px128[0], pxTemp[0]), pxTemp[1]), pxTemp[2]),  pxTemp[3]);
+    px128[0] = __lsx_vadd_h(__lsx_vadd_h(__lsx_vadd_h(__lsx_vadd_h(px128[0], pxTemp[4]), pxTemp[5]), pxTemp[6]),  px128[index[6] + 1]);
+}
+
+// -------------------- F32/F16 bitdepth compute functions for kernel size (3/5/7/9) --------------------
+
+// perform required blend permute add multiplication operations for 3x3 kernel size
+template <int blendMask1, int blendMask2> 
+inline void blend_permute_add_mul_3x3_host(__m256 *pSrc, __m256 *pDst, __m256 pConvolutionFactor, __m256i *pxMask, Rpp32u *index)
+{
+    /*  For PLN inputs                                                                          | For PKD inputs
+        pSrc[0] - [X01|X02|X03|X04|X05|X06|X07|X08], pSrc[1] - [X09|X10|X11|X12|X13|X14|X15|X16]| pSrc[0] - [R01|G01|B01|R02|G02|B02|R03|G03], pSrc[1] - [B03|R04|G04|B04|R05|G05|B05|R06]
+                  [X02|X03|X04|X05|X06|X07|X08|X09] (blend with mask [0000 0001] and permute)   |           [R02|G02|B02|R03|G03|B03|R04|G04] (blend with mask [0000 0111] and permute)
+                  [X03|X04|X05|X06|X07|X08|X09|X10] (blend with mask [0000 0011] and permute)   |           [R03|G03|B03|R04|G04|B04|R05|G05] (blend with mask [0011 1111] and permute) */ 
+    pDst[0] = __lasx_xvfadd_s(pSrc[0], lasx_permutevar8x32_f32(lasx_blend_f32(pSrc[index[0]], pSrc[index[0] + 1], blendMask1), pxMask[0]));   
+    pDst[0] = __lasx_xvfadd_s(pDst[0], lasx_permutevar8x32_f32(lasx_blend_f32(pSrc[index[1]], pSrc[index[1] + 1], blendMask2), pxMask[1]));
+    pDst[0] = __lasx_xvfmul_s(pDst[0], pConvolutionFactor);
+}
+
+// perform required blend permute add multiplication operations for 5x5 kernel size
+template <int blendMask1, int blendMask2, int blendMask3, int blendMask4> 
+inline void blend_permute_add_mul_5x5_host(__m256 *pSrc, __m256 *pDst, __m256 pConvolutionFactor, __m256i *pxMask, Rpp32u *index)
+{
+   /*   For PLN inputs                                                                          | For PKD inputs
+        pSrc[0] - [X01|X02|X03|X04|X05|X06|X07|X08], pSrc[1] - [X09|X10|X11|X12|X13|X14|X15|X16]| pSrc[0] - [R01|G01|B01|R02|G02|B02|R03|G03], pSrc[1] - [B03|R04|G04|B04|R05|G05|B05|R06]
+                  [X02|X03|X04|X05|X06|X07|X08|X09] (blend with mask [0000 0001] and permute)   | pSrc[2] - [G06|B06|R07|G07|B07|R08|G08|B08]
+                  [X03|X04|X05|X06|X07|X08|X09|X10] (blend with mask [0000 0011] and permute)   |           [R02|G02|B02|R03|G03|B03|R04|G04] (blend with mask [0000 0111] and permute)
+                  [X04|X05|X06|X07|X08|X09|X10|X11] (blend with mask [0000 0111] and permute)   |           [R03|G03|B03|R04|G04|B04|R05|G05] (blend with mask [0011 1111] and permute) 
+                  [X05|X06|X07|X08|X09|X10|X11|X12] (blend with mask [0000 1111] and permute)   |           [R04|G04|B04|R05|G05|B05|R06|G06] (blend with mask [0000 0001] and permute)
+                                                                                                |           [R05|G05|B05|R06|G06|B06|R07|G07] (blend with mask [0000 1111] and permute) */ 
+    pDst[0] = __lasx_xvfadd_s(pSrc[0], lasx_permutevar8x32_f32(lasx_blend_f32(pSrc[index[0]], pSrc[index[0] + 1], blendMask1), pxMask[0]));   // blend with mask [0000 0001] and permute - [X02|X03|X04|X05|X06|X07|X08|X09]
+    pDst[0] = __lasx_xvfadd_s(pDst[0], lasx_permutevar8x32_f32(lasx_blend_f32(pSrc[index[1]], pSrc[index[1] + 1], blendMask2), pxMask[1]));   // blend with mask [0000 0011] and permute - [X03|X04|X05|X06|X07|X08|X09|X10]
+    pDst[0] = __lasx_xvfadd_s(pDst[0], lasx_permutevar8x32_f32(lasx_blend_f32(pSrc[index[2]], pSrc[index[2] + 1], blendMask3), pxMask[2]));   // blend with mask [0000 0111] and permute - [X04|X05|X06|X07|X08|X09|X10|X11]
+    pDst[0] = __lasx_xvfadd_s(pDst[0], lasx_permutevar8x32_f32(lasx_blend_f32(pSrc[index[3]], pSrc[index[3] + 1], blendMask4), pxMask[3]));  // blend with mask [0000 1111] and permute - [X05|X06|X07|X08|X09|X10|X11|X12]
+    pDst[0] = __lasx_xvfmul_s(pDst[0], pConvolutionFactor);
+}
+
+// perform required blend permute add multiplication operations for 7x7 kernel size
+template <int blendMask1, int blendMask2, int blendMask3, int blendMask4, int blendMask5, int blendMask6>  
+inline void blend_permute_add_mul_7x7_host(__m256 *pSrc, __m256 *pDst, __m256 pConvolutionFactor, __m256i *pxMask, Rpp32u *index)
+{
+    /*  For PLN inputs                                                                          | For PKD inputs
+        pSrc[0] - [X01|X02|X03|X04|X05|X06|X07|X08], pSrc[1] - [X09|X10|X11|X12|X13|X14|X15|X16]| pSrc[0] - [R01|G01|B01|R02|G02|B02|R03|G03], pSrc[1] - [B03|R04|G04|B04|R05|G05|B05|R06],
+                  [X02|X03|X04|X05|X06|X07|X08|X09] (blend with mask [0000 0001] and permute)   | pSrc[2] - [G06|B06|R07|G07|B07|R08|G08|B08], pSrc[3] - [R09|G09|B09|R10|G10|B10|R11|G11]
+                  [X03|X04|X05|X06|X07|X08|X09|X10] (blend with mask [0000 0011] and permute)   |           [R02|G02|B02|R03|G03|B03|R04|G04] (blend with mask [0000 0111] and permute)
+                  [X04|X05|X06|X07|X08|X09|X10|X11] (blend with mask [0000 0111] and permute)   |           [R03|G03|B03|R04|G04|B04|R05|G05] (blend with mask [0011 1111] and permute) 
+                  [X05|X06|X07|X08|X09|X10|X11|X12] (blend with mask [0000 1111] and permute)   |           [R04|G04|B04|R05|G05|B05|R06|G06] (blend with mask [0000 0001] and permute)
+                  [X06|X07|X08|X09|X10|X11|X12|X13] (blend with mask [0001 1111] and permute)   |           [R05|G05|B05|R06|G06|B06|R07|G07] (blend with mask [0000 1111] and permute)
+                  [X07|X08|X09|X10|X11|X12|X13|X14] (blend with mask [0011 1111] and permute)   |           [R06|G06|B06|R07|G07|B07|R08|G08] (blend with mask [0111 1111] and permute)
+                                                                                                |           [R07|G07|B07|R08|G08|B08|R09|G09] (blend with mask [0000 0011] and permute) */ 
+    pDst[0] = __lasx_xvfadd_s(pSrc[0], lasx_permutevar8x32_f32(lasx_blend_f32(pSrc[index[0]], pSrc[index[0] + 1], blendMask1), pxMask[0]));   // blend with mask [0000 0001] and permute - [X02|X03|X04|X05|X06|X07|X08|X09]
+    pDst[0] = __lasx_xvfadd_s(pDst[0], lasx_permutevar8x32_f32(lasx_blend_f32(pSrc[index[1]], pSrc[index[1] + 1], blendMask2), pxMask[1]));   // blend with mask [0000 0011] and permute - [X03|X04|X05|X06|X07|X08|X09|X10]
+    pDst[0] = __lasx_xvfadd_s(pDst[0], lasx_permutevar8x32_f32(lasx_blend_f32(pSrc[index[2]], pSrc[index[2] + 1], blendMask3), pxMask[2]));   // blend with mask [0000 0111] and permute - [X04|X05|X06|X07|X08|X09|X10|X11]
+    pDst[0] = __lasx_xvfadd_s(pDst[0], lasx_permutevar8x32_f32(lasx_blend_f32(pSrc[index[3]], pSrc[index[3] + 1], blendMask4), pxMask[3]));  // blend with mask [0000 1111] and permute - [X05|X06|X07|X08|X09|X10|X11|X12]
+    pDst[0] = __lasx_xvfadd_s(pDst[0], lasx_permutevar8x32_f32(lasx_blend_f32(pSrc[index[4]], pSrc[index[4] + 1], blendMask5), pxMask[4]));  // blend with mask [0001 1111] and permute - [X06|X07|X08|X09|X10|X11|X12|X13]
+    pDst[0] = __lasx_xvfadd_s(pDst[0], lasx_permutevar8x32_f32(lasx_blend_f32(pSrc[index[5]], pSrc[index[5] + 1], blendMask6), pxMask[5]));  // blend with mask [0011 1111] and permute - [X07|X08|X09|X10|X11|X12|X13|X14]
+    pDst[0] = __lasx_xvfmul_s(pDst[0], pConvolutionFactor);
+}
+
+// perform required blend permute add multiplication operations for 9x9 kernel size
+template <int blendMask1, int blendMask2, int blendMask3, int blendMask4, int blendMask5, int blendMask6, int blendMask7>  
+inline void blend_permute_add_mul_9x9_host(__m256 *pSrc, __m256 *pDst, __m256 pConvolutionFactor, __m256i *pxMask, Rpp32u *index)
+{
+    /*  For PLN inputs                                                                          | For PKD inputs
+        pSrc[0] - [X01|X02|X03|X04|X05|X06|X07|X08], pSrc[1] - [X09|X10|X11|X12|X13|X14|X15|X16]| pSrc[0] - [R01|G01|B01|R02|G02|B02|R03|G03], pSrc[1] - [B03|R04|G04|B04|R05|G05|B05|R06],
+                  [X02|X03|X04|X05|X06|X07|X08|X09] (blend with mask [0000 0001] and permute)     pSrc[2] - [G06|B06|R07|G07|B07|R08|G08|B08], pSrc[3] - [R09|G09|B09|R10|G10|B10|R11|G11]
+                  [X03|X04|X05|X06|X07|X08|X09|X10] (blend with mask [0000 0011] and permute)   |           [R02|G02|B02|R03|G03|B03|R04|G04] (blend with mask [0000 0111] and permute)
+                  [X04|X05|X06|X07|X08|X09|X10|X11] (blend with mask [0000 0111] and permute)   |           [R03|G03|B03|R04|G04|B04|R05|G05] (blend with mask [0011 1111] and permute) 
+                  [X05|X06|X07|X08|X09|X10|X11|X12] (blend with mask [0000 1111] and permute)   |           [R04|G04|B04|R05|G05|B05|R06|G06] (blend with mask [0000 0001] and permute)
+                  [X06|X07|X08|X09|X10|X11|X12|X13] (blend with mask [0001 1111] and permute)   |           [R05|G05|B05|R06|G06|B06|R07|G07] (blend with mask [0000 1111] and permute)
+                  [X07|X08|X09|X10|X11|X12|X13|X14] (blend with mask [0011 1111] and permute)   |           [R06|G06|B06|R07|G07|B07|R08|G08] (blend with mask [0111 1111] and permute)
+                  [X08|X09|X10|X11|X12|X13|X14|X15] (blend with mask [0111 1111] and permute)   |           [R07|G07|B07|R08|G08|B08|R09|G09] (blend with mask [0000 0011] and permute)
+                                                                                                |           [R08|G08|B08|R09|G09|B09|R10|G10] (blend with mask [0001 1111] and permute)
+    */
+    pDst[0] = __lasx_xvfadd_s(pSrc[0], lasx_permutevar8x32_f32(lasx_blend_f32(pSrc[index[0]], pSrc[index[0] + 1], blendMask1), pxMask[0]));   // blend with mask [0000 0001] and permute - [X02|X03|X04|X05|X06|X07|X08|X09]
+    pDst[0] = __lasx_xvfadd_s(pDst[0], lasx_permutevar8x32_f32(lasx_blend_f32(pSrc[index[1]], pSrc[index[1] + 1], blendMask2), pxMask[1]));   // blend with mask [0000 0011] and permute - [X03|X04|X05|X06|X07|X08|X09|X10]
+    pDst[0] = __lasx_xvfadd_s(pDst[0], lasx_permutevar8x32_f32(lasx_blend_f32(pSrc[index[2]], pSrc[index[2] + 1], blendMask3), pxMask[2]));   // blend with mask [0000 0111] and permute - [X04|X05|X06|X07|X08|X09|X10|X11]
+    pDst[0] = __lasx_xvfadd_s(pDst[0], lasx_permutevar8x32_f32(lasx_blend_f32(pSrc[index[3]], pSrc[index[3] + 1], blendMask4), pxMask[3]));   // blend with mask [0000 1111] and permute - [X05|X06|X07|X08|X09|X10|X11|X12]
+    pDst[0] = __lasx_xvfadd_s(pDst[0], lasx_permutevar8x32_f32(lasx_blend_f32(pSrc[index[4]], pSrc[index[4] + 1], blendMask5), pxMask[4]));   // blend with mask [0001 1111] and permute - [X06|X07|X08|X09|X10|X11|X12|X13]
+    pDst[0] = __lasx_xvfadd_s(pDst[0], lasx_permutevar8x32_f32(lasx_blend_f32(pSrc[index[5]], pSrc[index[5] + 1], blendMask6), pxMask[5]));   // blend with mask [0011 1111] and permute - [X07|X08|X09|X10|X11|X12|X13|X14]
+    pDst[0] = __lasx_xvfadd_s(pDst[0], lasx_permutevar8x32_f32(lasx_blend_f32(pSrc[index[6]], pSrc[index[6] + 1], blendMask7), pxMask[6]));   // blend with mask [0111 1111] and permute - [X08|X09|X10|X11|X12|X13|X14|X15]
+    pDst[0] = __lasx_xvfadd_s(pDst[0], pSrc[index[6] + 1]);
+    pDst[0] = __lasx_xvfmul_s(pDst[0], pConvolutionFactor);
+}
+
+template<int blendMask1, int blendMask2, int roatateMask1, int roatateMask2>
+inline void permute_blend_add_3x3(__m256 &pDst, __m256 pRow0, __m256 pRow1, __m256 *pFilter, __m256i *pxMask)
+{
+    __m256 pTemp[3];
+    pTemp[0] = __lasx_xvfmul_s(pRow0, pFilter[0]);
+    pTemp[1] = __lasx_xvfmul_s(lasx_permutevar8x32_f32(lasx_blend_f32(pRow0, pRow1, blendMask1), pxMask[roatateMask1]), pFilter[1]);
+    pTemp[2] = __lasx_xvfmul_s(lasx_permutevar8x32_f32(lasx_blend_f32(pRow0, pRow1, blendMask2), pxMask[roatateMask2]), pFilter[2]);
+    pDst = __lasx_xvfadd_s(pDst, __lasx_xvfadd_s(__lasx_xvfadd_s(pTemp[0], pTemp[1]), pTemp[2]));
+}
+
+inline void permute_blend_add_5x5_pln(__m256 &pDst, __m256 pRow0, __m256 pRow1, __m256 *pFilter)
+{
+    __m256 pTemp[5];
+    pTemp[0] = __lasx_xvfmul_s(pRow0, pFilter[0]);
+    pTemp[1] = __lasx_xvfmul_s(lasx_permutevar8x32_f32(lasx_blend_f32(pRow0, pRow1, 1), avx_pxMaskRotate0To1), pFilter[1]);
+    pTemp[2] = __lasx_xvfmul_s(lasx_permutevar8x32_f32(lasx_blend_f32(pRow0, pRow1, 3), avx_pxMaskRotate0To2), pFilter[2]);
+    pTemp[3] = __lasx_xvfmul_s(lasx_permutevar8x32_f32(lasx_blend_f32(pRow0, pRow1, 7), avx_pxMaskRotate0To3), pFilter[3]);
+    pTemp[4] = __lasx_xvfmul_s(lasx_permutevar8x32_f32(lasx_blend_f32(pRow0, pRow1, 15), avx_pxMaskRotate0To4), pFilter[4]);
+    pDst = __lasx_xvfadd_s(pDst, __lasx_xvfadd_s(__lasx_xvfadd_s(pTemp[0], __lasx_xvfadd_s(pTemp[1], pTemp[2])), __lasx_xvfadd_s(pTemp[3], pTemp[4])));
+}
+
+inline void permute_blend_add_5x5_pkd(__m256 &pDst, __m256 *pRow, __m256 *pFilter)
+{
+    __m256 pTemp[5];
+    pTemp[0] = __lasx_xvfmul_s(pRow[0], pFilter[0]);
+    pTemp[1] = __lasx_xvfmul_s(lasx_permutevar8x32_f32(lasx_blend_f32(pRow[0], pRow[1], 7), avx_pxMaskRotate0To3), pFilter[1]);
+    pTemp[2] = __lasx_xvfmul_s(lasx_permutevar8x32_f32(lasx_blend_f32(pRow[0], pRow[1], 63), avx_pxMaskRotate0To6), pFilter[2]);
+    pTemp[3] = __lasx_xvfmul_s(lasx_permutevar8x32_f32(lasx_blend_f32(pRow[1], pRow[2], 1), avx_pxMaskRotate0To1), pFilter[3]);
+    pTemp[4] = __lasx_xvfmul_s(lasx_permutevar8x32_f32(lasx_blend_f32(pRow[1], pRow[2], 15), avx_pxMaskRotate0To4), pFilter[4]);
+    pDst = __lasx_xvfadd_s(pDst, __lasx_xvfadd_s(__lasx_xvfadd_s(pTemp[0], __lasx_xvfadd_s(pTemp[1], pTemp[2])), __lasx_xvfadd_s(pTemp[3], pTemp[4])));
+}
+
+inline void permute_blend_add_7x7_pln(__m256 &pDst, __m256 *pRow, __m256 *pFilter)
+{
+    __m256 pTemp[7];
+    pTemp[0] = __lasx_xvfmul_s(pRow[0], pFilter[0]);
+    pTemp[1] = __lasx_xvfmul_s(lasx_permutevar8x32_f32(lasx_blend_f32(pRow[0], pRow[1], 1), avx_pxMaskRotate0To1), pFilter[1]);
+    pTemp[2] = __lasx_xvfmul_s(lasx_permutevar8x32_f32(lasx_blend_f32(pRow[0], pRow[1], 3), avx_pxMaskRotate0To2), pFilter[2]);
+    pTemp[3] = __lasx_xvfmul_s(lasx_permutevar8x32_f32(lasx_blend_f32(pRow[0], pRow[1], 7), avx_pxMaskRotate0To3), pFilter[3]);
+    pTemp[4] = __lasx_xvfmul_s(lasx_permutevar8x32_f32(lasx_blend_f32(pRow[0], pRow[1], 15), avx_pxMaskRotate0To4), pFilter[4]);
+    pTemp[5] = __lasx_xvfmul_s(lasx_permutevar8x32_f32(lasx_blend_f32(pRow[0], pRow[1], 31), avx_pxMaskRotate0To5), pFilter[5]);
+    pTemp[6] = __lasx_xvfmul_s(lasx_permutevar8x32_f32(lasx_blend_f32(pRow[0], pRow[1], 63), avx_pxMaskRotate0To6), pFilter[6]);
+    pDst =  __lasx_xvfadd_s(pDst, __lasx_xvfadd_s(__lasx_xvfadd_s(pTemp[0], __lasx_xvfadd_s(pTemp[1], pTemp[2])), __lasx_xvfadd_s(__lasx_xvfadd_s(pTemp[3], pTemp[4]), __lasx_xvfadd_s(pTemp[5], pTemp[6]))));
+}
+
+inline void permute_blend_add_7x7_pkd(__m256 &pDst, __m256 *pRow, __m256 pRow2, __m256 *pFilter)
+{
+    __m256 pTemp[7];
+    pTemp[0] = __lasx_xvfmul_s(pRow[0], pFilter[0]);
+    pTemp[1] = __lasx_xvfmul_s(lasx_permutevar8x32_f32(lasx_blend_f32(pRow[0], pRow[1], 7), avx_pxMaskRotate0To3), pFilter[1]);
+    pTemp[2] = __lasx_xvfmul_s(lasx_permutevar8x32_f32(lasx_blend_f32(pRow[0], pRow[1], 63), avx_pxMaskRotate0To6), pFilter[2]);
+    pTemp[3] = __lasx_xvfmul_s(lasx_permutevar8x32_f32(lasx_blend_f32(pRow[1], pRow[2], 1), avx_pxMaskRotate0To1), pFilter[3]);
+    pTemp[4] = __lasx_xvfmul_s(lasx_permutevar8x32_f32(lasx_blend_f32(pRow[1], pRow[2], 15), avx_pxMaskRotate0To4), pFilter[4]);
+    pTemp[5] = __lasx_xvfmul_s(lasx_permutevar8x32_f32(lasx_blend_f32(pRow[1], pRow[2], 127), avx_pxMaskRotate0To7), pFilter[5]);
+    pTemp[6] = __lasx_xvfmul_s(lasx_permutevar8x32_f32(lasx_blend_f32(pRow[2], pRow2, 3), avx_pxMaskRotate0To2), pFilter[6]);
+    pDst =  __lasx_xvfadd_s(pDst, __lasx_xvfadd_s(__lasx_xvfadd_s(pTemp[0], __lasx_xvfadd_s(pTemp[1], pTemp[2])), __lasx_xvfadd_s(__lasx_xvfadd_s(pTemp[3], pTemp[4]), __lasx_xvfadd_s(pTemp[5], pTemp[6]))));
+}
+
+inline void permute_blend_add_9x9_pln(__m256 &pDst, __m256 *pRow, __m256 *pFilter)
+{
+    __m256 pTemp[9];
+    pTemp[0] = __lasx_xvfmul_s(pRow[0], pFilter[0]);
+    pTemp[1] = __lasx_xvfmul_s(lasx_permutevar8x32_f32(lasx_blend_f32(pRow[0], pRow[1], 1), avx_pxMaskRotate0To1), pFilter[1]);
+    pTemp[2] = __lasx_xvfmul_s(lasx_permutevar8x32_f32(lasx_blend_f32(pRow[0], pRow[1], 3), avx_pxMaskRotate0To2), pFilter[2]);
+    pTemp[3] = __lasx_xvfmul_s(lasx_permutevar8x32_f32(lasx_blend_f32(pRow[0], pRow[1], 7), avx_pxMaskRotate0To3), pFilter[3]);
+    pTemp[4] = __lasx_xvfmul_s(lasx_permutevar8x32_f32(lasx_blend_f32(pRow[0], pRow[1], 15), avx_pxMaskRotate0To4), pFilter[4]);
+    pTemp[5] = __lasx_xvfmul_s(lasx_permutevar8x32_f32(lasx_blend_f32(pRow[0], pRow[1], 31), avx_pxMaskRotate0To5), pFilter[5]);
+    pTemp[6] = __lasx_xvfmul_s(lasx_permutevar8x32_f32(lasx_blend_f32(pRow[0], pRow[1], 63), avx_pxMaskRotate0To6), pFilter[6]);
+    pTemp[7] = __lasx_xvfmul_s(lasx_permutevar8x32_f32(lasx_blend_f32(pRow[0], pRow[1], 127), avx_pxMaskRotate0To7), pFilter[7]);
+    pTemp[8] = __lasx_xvfmul_s(pRow[1], pFilter[8]);
+    pDst = __lasx_xvfadd_s(pDst, __lasx_xvfadd_s(__lasx_xvfadd_s(__lasx_xvfadd_s(pTemp[0], pTemp[1]), __lasx_xvfadd_s(pTemp[2], pTemp[3])), __lasx_xvfadd_s(__lasx_xvfadd_s(pTemp[4], pTemp[5]), __lasx_xvfadd_s(pTemp[6], __lasx_xvfadd_s(pTemp[7], pTemp[8])))));
+}
+
+inline void permute_blend_add_9x9_pkd(__m256 &pDst, __m256 *pRow, __m256 *pFilter)
+{
+    __m256 pTemp[9];
+    pTemp[0] = __lasx_xvfmul_s(pRow[0], pFilter[0]);
+    pTemp[1] = __lasx_xvfmul_s(lasx_permutevar8x32_f32(lasx_blend_f32(pRow[0], pRow[1], 7), avx_pxMaskRotate0To3), pFilter[1]);
+    pTemp[2] = __lasx_xvfmul_s(lasx_permutevar8x32_f32(lasx_blend_f32(pRow[0], pRow[1], 63), avx_pxMaskRotate0To6), pFilter[2]);
+    pTemp[3] = __lasx_xvfmul_s(lasx_permutevar8x32_f32(lasx_blend_f32(pRow[1], pRow[2], 1), avx_pxMaskRotate0To1), pFilter[3]);
+    pTemp[4] = __lasx_xvfmul_s(lasx_permutevar8x32_f32(lasx_blend_f32(pRow[1], pRow[2], 15), avx_pxMaskRotate0To4), pFilter[4]);
+    pTemp[5] = __lasx_xvfmul_s(lasx_permutevar8x32_f32(lasx_blend_f32(pRow[1], pRow[2], 127), avx_pxMaskRotate0To7), pFilter[5]);
+    pTemp[6] = __lasx_xvfmul_s(lasx_permutevar8x32_f32(lasx_blend_f32(pRow[2], pRow[3], 3), avx_pxMaskRotate0To2), pFilter[6]);
+    pTemp[7] = __lasx_xvfmul_s(lasx_permutevar8x32_f32(lasx_blend_f32(pRow[2], pRow[3], 31), avx_pxMaskRotate0To5), pFilter[7]);
+    pTemp[8] = __lasx_xvfmul_s(pRow[3], pFilter[8]);
+    pDst = __lasx_xvfadd_s(pDst, __lasx_xvfadd_s(__lasx_xvfadd_s(__lasx_xvfadd_s(pTemp[0], pTemp[1]), __lasx_xvfadd_s(pTemp[2], pTemp[3])), __lasx_xvfadd_s(__lasx_xvfadd_s(pTemp[4], pTemp[5]), __lasx_xvfadd_s(pTemp[6], __lasx_xvfadd_s(pTemp[7], pTemp[8])))));
+}
+
+// -------------------- Filter load functions for U8 bitdepth --------------------
+
+// load function for 3x3 kernel size
+inline void rpp_load_filter_3x3_pln_host(__m256 *pRow, Rpp8u **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 2 rows for 3x3 kernel
+    rpp_load16_u8_to_f32_avx(srcPtrTemp[0], &pRow[0]);
+    rpp_load16_u8_to_f32_avx(srcPtrTemp[1], &pRow[2]);
+
+    // if rowKernelLoopLimit is 3 load values from 3rd row pointer else set it 0
+    if (rowKernelLoopLimit == 3)
+        rpp_load16_u8_to_f32_avx(srcPtrTemp[2], &pRow[4]);
+    else
+        pRow[4] = pRow[5] = avx_p0;
+}
+
+// load function for 5x5 kernel size
+inline void rpp_load_filter_5x5_pln_host(__m256 *pRow, Rpp8u **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 3 rows for 5x5 kernel
+    rpp_load16_u8_to_f32_avx(srcPtrTemp[0], &pRow[0]);
+    rpp_load16_u8_to_f32_avx(srcPtrTemp[1], &pRow[2]);
+    rpp_load16_u8_to_f32_avx(srcPtrTemp[2], &pRow[4]);
+
+    for (int k = 3; k < rowKernelLoopLimit; k++)
+        rpp_load16_u8_to_f32_avx(srcPtrTemp[k], &pRow[k * 2]);
+    for (int k = rowKernelLoopLimit * 2; k < 10; k += 2)
+        pRow[k] = pRow[k + 1] = avx_p0;
+}
+
+// load function for 7x7 kernel size
+inline void rpp_load_filter_7x7_pln_host(__m256 *pRow, Rpp8u **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 4 rows for 7x7 kernel
+    rpp_load16_u8_to_f32_avx(srcPtrTemp[0], &pRow[0]);
+    rpp_load16_u8_to_f32_avx(srcPtrTemp[1], &pRow[2]);
+    rpp_load16_u8_to_f32_avx(srcPtrTemp[2], &pRow[4]);
+    rpp_load16_u8_to_f32_avx(srcPtrTemp[3], &pRow[6]);
+    for (int k = 4; k < rowKernelLoopLimit; k++)
+        rpp_load16_u8_to_f32_avx(srcPtrTemp[k], &pRow[k * 2]);
+    for (int k = rowKernelLoopLimit * 2; k < 14; k += 2)
+        pRow[k] = pRow[k + 1] = avx_p0;
+}
+
+// load function for 9x9 kernel size
+inline void rpp_load_filter_9x9_pln_host(__m256 *pRow, Rpp8u **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 5 rows for 9x9 kernel
+    rpp_load16_u8_to_f32_avx(srcPtrTemp[0], &pRow[0]);
+    rpp_load16_u8_to_f32_avx(srcPtrTemp[1], &pRow[2]);
+    rpp_load16_u8_to_f32_avx(srcPtrTemp[2], &pRow[4]);
+    rpp_load16_u8_to_f32_avx(srcPtrTemp[3], &pRow[6]);
+    rpp_load16_u8_to_f32_avx(srcPtrTemp[4], &pRow[8]);
+    for (int k = 5; k < rowKernelLoopLimit; k++)
+        rpp_load16_u8_to_f32_avx(srcPtrTemp[k], &pRow[k * 2]);
+    for (int k = rowKernelLoopLimit * 2; k < 18; k += 2)
+        pRow[k] = pRow[k + 1] = avx_p0;
+}
+
+inline void rpp_load_filter_3x3_pkd_host(__m256 *pRow, Rpp8u **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 2 rows for 3x3 kernel
+    rpp_load24_u8_to_f32_avx(srcPtrTemp[0], &pRow[0]);
+    rpp_load24_u8_to_f32_avx(srcPtrTemp[1], &pRow[3]);
+
+    // if rowKernelLoopLimit is 3 load values from 3rd row pointer else set it 0
+    if (rowKernelLoopLimit == 3)
+        rpp_load24_u8_to_f32_avx(srcPtrTemp[2], &pRow[6]);
+    else
+    {
+        pRow[6] = avx_p0;
+        pRow[7] = avx_p0;
+        pRow[8] = avx_p0;
+    }
+}
+
+// load function for 5x5 kernel size
+inline void rpp_load_filter_5x5_pkd_host(__m256 *pRow, Rpp8u **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 3 rows for 5x5 kernel
+    rpp_load32_u8_to_f32_avx(srcPtrTemp[0], &pRow[0]);
+    rpp_load32_u8_to_f32_avx(srcPtrTemp[1], &pRow[4]);
+    rpp_load32_u8_to_f32_avx(srcPtrTemp[2], &pRow[8]);
+    for (int k = 3; k < rowKernelLoopLimit; k++)
+        rpp_load32_u8_to_f32_avx(srcPtrTemp[k], &pRow[k * 4]);
+    for (int k = rowKernelLoopLimit * 4; k < 20; k += 4)
+    {
+        pRow[k] = avx_p0;
+        pRow[k + 1] = avx_p0;
+        pRow[k + 2] = avx_p0;
+        pRow[k + 3] = avx_p0;
+    }
+}
+
+inline void rpp_load_filter_7x7_pkd_host(__m256 *pRow, Rpp8u **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 4 rows for 7x7 kernel
+    rpp_load32_u8_to_f32_avx(srcPtrTemp[0], &pRow[0]);
+    rpp_load32_u8_to_f32_avx(srcPtrTemp[1], &pRow[4]);
+    rpp_load32_u8_to_f32_avx(srcPtrTemp[2], &pRow[8]);
+    rpp_load32_u8_to_f32_avx(srcPtrTemp[3], &pRow[12]);
+    for (int k = 4; k < rowKernelLoopLimit; k++)
+        rpp_load32_u8_to_f32_avx(srcPtrTemp[k], &pRow[k * 4]);
+    for (int k = rowKernelLoopLimit * 4; k < 28; k += 4)
+    {
+        pRow[k] = avx_p0;
+        pRow[k + 1] = avx_p0;
+        pRow[k + 2] = avx_p0;
+        pRow[k + 3] = avx_p0;
+    }
+}
+
+inline void rpp_load_filter_9x9_pkd_host(__m256 *pRow, Rpp8u **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 5 rows for 9x9 kernel
+    rpp_load32_u8_to_f32_avx(srcPtrTemp[0], &pRow[0]);
+    rpp_load32_u8_to_f32_avx(srcPtrTemp[1], &pRow[4]);
+    rpp_load32_u8_to_f32_avx(srcPtrTemp[2], &pRow[8]);
+    rpp_load32_u8_to_f32_avx(srcPtrTemp[3], &pRow[12]);
+    rpp_load32_u8_to_f32_avx(srcPtrTemp[4], &pRow[16]);
+    for (int k = 5; k < rowKernelLoopLimit; k++)
+        rpp_load32_u8_to_f32_avx(srcPtrTemp[k], &pRow[k * 4]);
+    for (int k = rowKernelLoopLimit * 4; k < 36; k += 4)
+    {
+        pRow[k] = avx_p0;
+        pRow[k + 1] = avx_p0;
+        pRow[k + 2] = avx_p0;
+        pRow[k + 3] = avx_p0;
+    }
+}
+
+inline void rpp_load_gaussian_filter_9x9_pkd_pln_host(__m256 *pRow, Rpp8u **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 5 rows for 9x9 kernel
+    rpp_load40_u8_to_f32_avx(srcPtrTemp[0], &pRow[0]);
+    rpp_load40_u8_to_f32_avx(srcPtrTemp[1], &pRow[5]);
+    rpp_load40_u8_to_f32_avx(srcPtrTemp[2], &pRow[10]);
+    rpp_load40_u8_to_f32_avx(srcPtrTemp[3], &pRow[15]);
+    rpp_load40_u8_to_f32_avx(srcPtrTemp[4], &pRow[20]);
+    for (int k = 5; k < rowKernelLoopLimit; k++)
+        rpp_load40_u8_to_f32_avx(srcPtrTemp[k], &pRow[k * 5]);
+    for (int k = rowKernelLoopLimit * 5; k < 45; k += 5)
+    {
+        pRow[k] = avx_p0;
+        pRow[k + 1] = avx_p0;
+        pRow[k + 2] = avx_p0;
+        pRow[k + 3] = avx_p0;
+        pRow[k + 4] = avx_p0;
+    }
+}
+
+// -------------------- Filter load functions for I8 bitdepth --------------------
+
+inline void rpp_load_filter_3x3_pln_host(__m256 *pRow, Rpp8s **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 2 rows for 3x3 kernel
+    rpp_load16_i8_to_f32_avx(srcPtrTemp[0], &pRow[0]);
+    rpp_load16_i8_to_f32_avx(srcPtrTemp[1], &pRow[2]);
+
+    // if rowKernelLoopLimit is 3 load values from 3rd row pointer else set it 0
+    if (rowKernelLoopLimit == 3)
+        rpp_load16_i8_to_f32_avx(srcPtrTemp[2], &pRow[4]);
+    else
+    {
+        pRow[4] = avx_p0;
+        pRow[5] = avx_p0;
+    }
+}
+
+inline void rpp_load_filter_5x5_pln_host(__m256 *pRow, Rpp8s **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 3 rows for 5x5 kernel
+    rpp_load16_i8_to_f32_avx(srcPtrTemp[0], &pRow[0]);
+    rpp_load16_i8_to_f32_avx(srcPtrTemp[1], &pRow[2]);
+    rpp_load16_i8_to_f32_avx(srcPtrTemp[2], &pRow[4]);
+
+    for (int k = 3; k < rowKernelLoopLimit; k++)
+        rpp_load16_i8_to_f32_avx(srcPtrTemp[k], &pRow[k * 2]);
+    for (int k = rowKernelLoopLimit * 2; k < 10; k += 2)
+        pRow[k] = pRow[k + 1] = avx_p0;
+}
+
+inline void rpp_load_filter_7x7_pln_host(__m256 *pRow, Rpp8s **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 4 rows for 7x7 kernel
+    rpp_load16_i8_to_f32_avx(srcPtrTemp[0], &pRow[0]);
+    rpp_load16_i8_to_f32_avx(srcPtrTemp[1], &pRow[2]);
+    rpp_load16_i8_to_f32_avx(srcPtrTemp[2], &pRow[4]);
+    rpp_load16_i8_to_f32_avx(srcPtrTemp[3], &pRow[6]);
+    for (int k = 4; k < rowKernelLoopLimit; k++)
+        rpp_load16_i8_to_f32_avx(srcPtrTemp[k], &pRow[k * 2]);
+    for (int k = rowKernelLoopLimit * 2; k < 14; k += 2)
+        pRow[k] = pRow[k + 1] = avx_p0;
+}
+
+inline void rpp_load_filter_9x9_pln_host(__m256 *pRow, Rpp8s **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 5 rows for 9x9 kernel
+    rpp_load16_i8_to_f32_avx(srcPtrTemp[0], &pRow[0]);
+    rpp_load16_i8_to_f32_avx(srcPtrTemp[1], &pRow[2]);
+    rpp_load16_i8_to_f32_avx(srcPtrTemp[2], &pRow[4]);
+    rpp_load16_i8_to_f32_avx(srcPtrTemp[3], &pRow[6]);
+    rpp_load16_i8_to_f32_avx(srcPtrTemp[4], &pRow[8]);
+    for (int k = 5; k < rowKernelLoopLimit; k++)
+        rpp_load16_i8_to_f32_avx(srcPtrTemp[k], &pRow[k * 2]);
+    for (int k = rowKernelLoopLimit * 2; k < 18; k += 2)
+        pRow[k] = pRow[k + 1] = avx_p0;
+}
+
+inline void rpp_load_filter_3x3_pkd_host(__m256 *pRow, Rpp8s **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 2 rows for 3x3 kernel
+    rpp_load24_i8_to_f32_avx(srcPtrTemp[0], &pRow[0]);
+    rpp_load24_i8_to_f32_avx(srcPtrTemp[1], &pRow[3]);
+    // if rowKernelLoopLimit is 3 load values from 3rd row pointer else set it 0
+    if (rowKernelLoopLimit == 3)
+        rpp_load24_i8_to_f32_avx(srcPtrTemp[2], &pRow[6]);
+    else
+    {
+        pRow[6] = avx_p0;
+        pRow[7] = avx_p0;
+        pRow[8] = avx_p0;
+    }
+}
+
+// load function for 5x5 kernel size
+inline void rpp_load_filter_5x5_pkd_host(__m256 *pRow, Rpp8s **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 3 rows for 5x5 kernel
+    rpp_load32_i8_to_f32_avx(srcPtrTemp[0], &pRow[0]);
+    rpp_load32_i8_to_f32_avx(srcPtrTemp[1], &pRow[4]);
+    rpp_load32_i8_to_f32_avx(srcPtrTemp[2], &pRow[8]);
+    for (int k = 3; k < rowKernelLoopLimit; k++)
+        rpp_load32_i8_to_f32_avx(srcPtrTemp[k], &pRow[k * 4]);
+    for (int k = rowKernelLoopLimit * 4; k < 20; k += 4)
+    {
+        pRow[k] = avx_p0;
+        pRow[k + 1] = avx_p0;
+        pRow[k + 2] = avx_p0;
+        pRow[k + 3] = avx_p0;
+    }
+}
+
+inline void rpp_load_filter_7x7_pkd_host(__m256 *pRow, Rpp8s **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 4 rows for 7x7 kernel
+    rpp_load32_i8_to_f32_avx(srcPtrTemp[0], &pRow[0]);
+    rpp_load32_i8_to_f32_avx(srcPtrTemp[1], &pRow[4]);
+    rpp_load32_i8_to_f32_avx(srcPtrTemp[2], &pRow[8]);
+    rpp_load32_i8_to_f32_avx(srcPtrTemp[3], &pRow[12]);
+    for (int k = 4; k < rowKernelLoopLimit; k++)
+        rpp_load32_i8_to_f32_avx(srcPtrTemp[k], &pRow[k * 4]);
+    for (int k = rowKernelLoopLimit * 4; k < 28; k += 4)
+    {
+        pRow[k] = avx_p0;
+        pRow[k + 1] = avx_p0;
+        pRow[k + 2] = avx_p0;
+        pRow[k + 3] = avx_p0;
+    }
+}
+
+inline void rpp_load_filter_9x9_pkd_host(__m256 *pRow, Rpp8s **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 5 rows for 9x9 kernel
+    rpp_load32_i8_to_f32_avx(srcPtrTemp[0], &pRow[0]);
+    rpp_load32_i8_to_f32_avx(srcPtrTemp[1], &pRow[4]);
+    rpp_load32_i8_to_f32_avx(srcPtrTemp[2], &pRow[8]);
+    rpp_load32_i8_to_f32_avx(srcPtrTemp[3], &pRow[12]);
+    rpp_load32_i8_to_f32_avx(srcPtrTemp[4], &pRow[16]);
+    for (int k = 5; k < rowKernelLoopLimit; k++)
+        rpp_load32_i8_to_f32_avx(srcPtrTemp[k], &pRow[k * 4]);
+    for (int k = rowKernelLoopLimit * 4; k < 36; k += 4)
+    {
+        pRow[k] = avx_p0;
+        pRow[k + 1] = avx_p0;
+        pRow[k + 2] = avx_p0;
+        pRow[k + 3] = avx_p0;
+    }
+}
+
+inline void rpp_load_gaussian_filter_9x9_pkd_pln_host(__m256 *pRow, Rpp8s **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 5 rows for 9x9 kernel
+    rpp_load40_i8_to_f32_avx(srcPtrTemp[0], &pRow[0]);
+    rpp_load40_i8_to_f32_avx(srcPtrTemp[1], &pRow[5]);
+    rpp_load40_i8_to_f32_avx(srcPtrTemp[2], &pRow[10]);
+    rpp_load40_i8_to_f32_avx(srcPtrTemp[3], &pRow[15]);
+    rpp_load40_i8_to_f32_avx(srcPtrTemp[4], &pRow[20]);
+    for (int k = 5; k < rowKernelLoopLimit; k++)
+        rpp_load40_i8_to_f32_avx(srcPtrTemp[k], &pRow[k * 5]);
+    for (int k = rowKernelLoopLimit * 5; k < 45; k += 5)
+    {
+        pRow[k] = avx_p0;
+        pRow[k + 1] = avx_p0;
+        pRow[k + 2] = avx_p0;
+        pRow[k + 3] = avx_p0;
+        pRow[k + 4] = avx_p0;
+    }
+}
+
+// -------------------- Filter load functions for F32 bitdepth --------------------
+
+inline void rpp_load_filter_3x3_pln_host(__m256 *pRow, Rpp32f **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 2 rows for 3x3 kernel
+    rpp_load16_f32_to_f32_avx(srcPtrTemp[0], &pRow[0]);
+    rpp_load16_f32_to_f32_avx(srcPtrTemp[1], &pRow[2]);
+
+    // if rowKernelLoopLimit is 3 load values from 3rd row pointer else set it 0
+    if (rowKernelLoopLimit == 3)
+        rpp_load16_f32_to_f32_avx(srcPtrTemp[2], &pRow[4]);
+    else
+    {
+        pRow[4] = avx_p0;
+        pRow[5] = avx_p0;
+    }
+}
+
+inline void rpp_load_filter_5x5_pln_host(__m256 *pRow, Rpp32f **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 3 rows for 5x5 kernel
+    rpp_load16_f32_to_f32_avx(srcPtrTemp[0], &pRow[0]);
+    rpp_load16_f32_to_f32_avx(srcPtrTemp[1], &pRow[2]);
+    rpp_load16_f32_to_f32_avx(srcPtrTemp[2], &pRow[4]);
+
+    for (int k = 3; k < rowKernelLoopLimit; k++)
+        rpp_load16_f32_to_f32_avx(srcPtrTemp[k], &pRow[k * 2]);
+    for (int k = rowKernelLoopLimit * 2; k < 10; k += 2)
+        pRow[k] = pRow[k + 1] = avx_p0;
+}
+
+inline void rpp_load_filter_7x7_pln_host(__m256 *pRow, Rpp32f **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 4 rows for 7x7 kernel
+    rpp_load16_f32_to_f32_avx(srcPtrTemp[0], &pRow[0]);
+    rpp_load16_f32_to_f32_avx(srcPtrTemp[1], &pRow[2]);
+    rpp_load16_f32_to_f32_avx(srcPtrTemp[2], &pRow[4]);
+    rpp_load16_f32_to_f32_avx(srcPtrTemp[3], &pRow[6]);
+    for (int k = 4; k < rowKernelLoopLimit; k++)
+        rpp_load16_f32_to_f32_avx(srcPtrTemp[k], &pRow[k * 2]);
+    for (int k = rowKernelLoopLimit * 2; k < 14; k += 2)
+        pRow[k] = pRow[k + 1] = avx_p0;
+}
+
+inline void rpp_load_filter_9x9_pln_host(__m256 *pRow, Rpp32f **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 5 rows for 9x9 kernel
+    rpp_load16_f32_to_f32_avx(srcPtrTemp[0], &pRow[0]);
+    rpp_load16_f32_to_f32_avx(srcPtrTemp[1], &pRow[2]);
+    rpp_load16_f32_to_f32_avx(srcPtrTemp[2], &pRow[4]);
+    rpp_load16_f32_to_f32_avx(srcPtrTemp[3], &pRow[6]);
+    rpp_load16_f32_to_f32_avx(srcPtrTemp[4], &pRow[8]);
+    for (int k = 5; k < rowKernelLoopLimit; k++)
+        rpp_load16_f32_to_f32_avx(srcPtrTemp[k], &pRow[k * 2]);
+    for (int k = rowKernelLoopLimit * 2; k < 18; k += 2)
+        pRow[k] = pRow[k + 1] = avx_p0;
+}
+
+inline void rpp_load_filter_3x3_pkd_host(__m256 *pRow, Rpp32f **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 2 rows for 3x3 kernel
+    rpp_load24_f32_to_f32_avx(srcPtrTemp[0], &pRow[0]);
+    rpp_load24_f32_to_f32_avx(srcPtrTemp[1], &pRow[3]);
+    // if rowKernelLoopLimit is 3 load values from 3rd row pointer else set it 0
+    if (rowKernelLoopLimit == 3)
+        rpp_load24_f32_to_f32_avx(srcPtrTemp[2], &pRow[6]);
+    else
+    {
+        pRow[6] = avx_p0;
+        pRow[7] = avx_p0;
+        pRow[8] = avx_p0;
+    }
+}
+
+// load function for 5x5 kernel size
+inline void rpp_load_filter_5x5_pkd_host(__m256 *pRow, Rpp32f **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 3 rows for 5x5 kernel
+    rpp_load32_f32_to_f32_avx(srcPtrTemp[0], &pRow[0]);
+    rpp_load32_f32_to_f32_avx(srcPtrTemp[1], &pRow[4]);
+    rpp_load32_f32_to_f32_avx(srcPtrTemp[2], &pRow[8]);
+    for (int k = 3; k < rowKernelLoopLimit; k++)
+        rpp_load32_f32_to_f32_avx(srcPtrTemp[k], &pRow[k * 4]);
+    for (int k = rowKernelLoopLimit * 4 ; k < 20; k += 4)
+    {
+        pRow[k] = avx_p0;
+        pRow[k + 1] = avx_p0;
+        pRow[k + 2] = avx_p0;
+        pRow[k + 3] = avx_p0;
+    }
+}
+
+// load function for 7x7 kernel size
+inline void rpp_load_filter_7x7_pkd_host(__m256 *pRow, Rpp32f **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 4 rows for 7x7 kernel
+    rpp_load32_f32_to_f32_avx(srcPtrTemp[0], &pRow[0]);
+    rpp_load32_f32_to_f32_avx(srcPtrTemp[1], &pRow[4]);
+    rpp_load32_f32_to_f32_avx(srcPtrTemp[2], &pRow[8]);
+    rpp_load32_f32_to_f32_avx(srcPtrTemp[3], &pRow[12]);
+    for (int k = 4; k < rowKernelLoopLimit; k++)
+        rpp_load32_f32_to_f32_avx(srcPtrTemp[k], &pRow[k * 4]);
+    for (int k = rowKernelLoopLimit * 4; k < 28; k += 4)
+    {
+        pRow[k] = avx_p0;
+        pRow[k + 1] = avx_p0;
+        pRow[k + 2] = avx_p0;
+        pRow[k + 3] = avx_p0;
+    }
+}
+
+// load function for 9x9 kernel size
+inline void rpp_load_filter_9x9_pkd_host(__m256 *pRow, Rpp32f **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 5 rows for 9x9 kernel
+    rpp_load32_f32_to_f32_avx(srcPtrTemp[0], &pRow[0]);
+    rpp_load32_f32_to_f32_avx(srcPtrTemp[1], &pRow[4]);
+    rpp_load32_f32_to_f32_avx(srcPtrTemp[2], &pRow[8]);
+    rpp_load32_f32_to_f32_avx(srcPtrTemp[3], &pRow[12]);
+    rpp_load32_f32_to_f32_avx(srcPtrTemp[4], &pRow[16]);
+    for (int k = 5; k < rowKernelLoopLimit; k++)
+        rpp_load32_f32_to_f32_avx(srcPtrTemp[k], &pRow[k * 4]);
+    for (int k = rowKernelLoopLimit * 4; k < 36; k += 4)
+    {
+        pRow[k] = avx_p0;
+        pRow[k + 1] = avx_p0;
+        pRow[k + 2] = avx_p0;
+        pRow[k + 3] = avx_p0;
+    }
+}
+
+inline void rpp_load_gaussian_filter_9x9_pkd_pln_host(__m256 *pRow, Rpp32f **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 5 rows for 9x9 kernel
+    rpp_load40_f32_to_f32_avx(srcPtrTemp[0], &pRow[0]);
+    rpp_load40_f32_to_f32_avx(srcPtrTemp[1], &pRow[5]);
+    rpp_load40_f32_to_f32_avx(srcPtrTemp[2], &pRow[10]);
+    rpp_load40_f32_to_f32_avx(srcPtrTemp[3], &pRow[15]);
+    rpp_load40_f32_to_f32_avx(srcPtrTemp[4], &pRow[20]);
+    for (int k = 5; k < rowKernelLoopLimit; k++)
+        rpp_load40_f32_to_f32_avx(srcPtrTemp[k], &pRow[k * 5]);
+    for (int k = rowKernelLoopLimit * 5; k < 45; k += 5)
+    {
+        pRow[k] = avx_p0;
+        pRow[k + 1] = avx_p0;
+        pRow[k + 2] = avx_p0;
+        pRow[k + 3] = avx_p0;
+        pRow[k + 4] = avx_p0;
+    }
+}
+
+// -------------------- Filter load functions for F16 bitdepth --------------------
+
+// load function for 3x3 kernel size
+inline void rpp_load_filter_3x3_pln_host(__m256 *pRow, Rpp16f **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 2 rows for 3x3 kernel
+    rpp_load16_f16_to_f32_avx(srcPtrTemp[0], &pRow[0]);
+    rpp_load16_f16_to_f32_avx(srcPtrTemp[1], &pRow[2]);
+
+    // if rowKernelLoopLimit is 3 load values from 3rd row pointer else set it 0
+    if (rowKernelLoopLimit == 3)
+        rpp_load16_f16_to_f32_avx(srcPtrTemp[2], &pRow[4]);
+    else
+    {
+        pRow[4] = avx_p0;
+        pRow[5] = avx_p0;
+    }
+}
+
+inline void rpp_load_filter_5x5_pln_host(__m256 *pRow, Rpp16f **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 3 rows for 5x5 kernel
+    rpp_load16_f16_to_f32_avx(srcPtrTemp[0], &pRow[0]);
+    rpp_load16_f16_to_f32_avx(srcPtrTemp[1], &pRow[2]);
+    rpp_load16_f16_to_f32_avx(srcPtrTemp[2], &pRow[4]);
+
+    for (int k = 3; k < rowKernelLoopLimit; k++)
+        rpp_load16_f16_to_f32_avx(srcPtrTemp[k], &pRow[k * 2]);
+    for (int k = rowKernelLoopLimit * 2; k < 10; k += 2)
+        pRow[k] = pRow[k + 1] = avx_p0;
+}
+
+inline void rpp_load_filter_7x7_pln_host(__m256 *pRow, Rpp16f **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 3 rows for 5x5 kernel
+    rpp_load16_f16_to_f32_avx(srcPtrTemp[0], &pRow[0]);
+    rpp_load16_f16_to_f32_avx(srcPtrTemp[1], &pRow[2]);
+    rpp_load16_f16_to_f32_avx(srcPtrTemp[2], &pRow[4]);
+    rpp_load16_f16_to_f32_avx(srcPtrTemp[3], &pRow[6]);
+    for (int k = 4; k < rowKernelLoopLimit; k++)
+        rpp_load16_f16_to_f32_avx(srcPtrTemp[k], &pRow[k * 2]);
+    for (int k = rowKernelLoopLimit * 2; k < 14; k += 2)
+        pRow[k] = pRow[k + 1] = avx_p0;
+}
+
+inline void rpp_load_filter_9x9_pln_host(__m256 *pRow, Rpp16f **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 3 rows for 5x5 kernel
+    rpp_load16_f16_to_f32_avx(srcPtrTemp[0], &pRow[0]);
+    rpp_load16_f16_to_f32_avx(srcPtrTemp[1], &pRow[2]);
+    rpp_load16_f16_to_f32_avx(srcPtrTemp[2], &pRow[4]);
+    rpp_load16_f16_to_f32_avx(srcPtrTemp[3], &pRow[6]);
+    rpp_load16_f16_to_f32_avx(srcPtrTemp[4], &pRow[8]);
+    for (int k = 5; k < rowKernelLoopLimit; k++)
+        rpp_load16_f16_to_f32_avx(srcPtrTemp[k], &pRow[k * 2]);
+    for (int k = rowKernelLoopLimit * 2; k < 18; k += 2)
+        pRow[k] = pRow[k + 1] = avx_p0;
+}
+
+inline void rpp_load_filter_3x3_pkd_host(__m256 *pRow, Rpp16f **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 2 rows for 3x3 kernel
+    rpp_load24_f16_to_f32_avx(srcPtrTemp[0], &pRow[0]);
+    rpp_load24_f16_to_f32_avx(srcPtrTemp[1], &pRow[3]);
+    // if rowKernelLoopLimit is 3 load values from 3rd row pointer else set it 0
+    if (rowKernelLoopLimit == 3)
+        rpp_load24_f16_to_f32_avx(srcPtrTemp[2], &pRow[6]);
+    else
+    {
+        pRow[6] = avx_p0;
+        pRow[7] = avx_p0;
+        pRow[8] = avx_p0;
+    }
+}
+
+// load function for 5x5 kernel size
+inline void rpp_load_filter_5x5_pkd_host(__m256 *pRow, Rpp16f **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 3 rows for 5x5 kernel
+    rpp_load32_f16_to_f32_avx(srcPtrTemp[0], &pRow[0]);
+    rpp_load32_f16_to_f32_avx(srcPtrTemp[1], &pRow[4]);
+    rpp_load32_f16_to_f32_avx(srcPtrTemp[2], &pRow[8]);
+    for (int k = 3; k < rowKernelLoopLimit; k++)
+        rpp_load32_f16_to_f32_avx(srcPtrTemp[k], &pRow[k * 4]);
+    for (int k = rowKernelLoopLimit * 4 ; k < 20; k += 4)
+    {
+        pRow[k] = avx_p0;
+        pRow[k + 1] = avx_p0;
+        pRow[k + 2] = avx_p0;
+        pRow[k + 3] = avx_p0;
+    }
+}
+
+// load function for 7x7 kernel size
+inline void rpp_load_filter_7x7_pkd_host(__m256 *pRow, Rpp16f **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 4 rows for 7x7 kernel
+    rpp_load32_f16_to_f32_avx(srcPtrTemp[0], &pRow[0]);
+    rpp_load32_f16_to_f32_avx(srcPtrTemp[1], &pRow[4]);
+    rpp_load32_f16_to_f32_avx(srcPtrTemp[2], &pRow[8]);
+    rpp_load32_f16_to_f32_avx(srcPtrTemp[3], &pRow[12]);
+    for (int k = 4; k < rowKernelLoopLimit; k++)
+        rpp_load32_f16_to_f32_avx(srcPtrTemp[k], &pRow[k * 4]);
+    for (int k = rowKernelLoopLimit * 4; k < 28; k += 4)
+    {
+        pRow[k] = avx_p0;
+        pRow[k + 1] = avx_p0;
+        pRow[k + 2] = avx_p0;
+        pRow[k + 3] = avx_p0;
+    }
+}
+
+// load function for 9x9 kernel size
+inline void rpp_load_filter_9x9_pkd_host(__m256 *pRow, Rpp16f **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 5 rows for 9x9 kernel
+    rpp_load32_f16_to_f32_avx(srcPtrTemp[0], &pRow[0]);
+    rpp_load32_f16_to_f32_avx(srcPtrTemp[1], &pRow[4]);
+    rpp_load32_f16_to_f32_avx(srcPtrTemp[2], &pRow[8]);
+    rpp_load32_f16_to_f32_avx(srcPtrTemp[3], &pRow[12]);
+    rpp_load32_f16_to_f32_avx(srcPtrTemp[4], &pRow[16]);
+    for (int k = 5; k < rowKernelLoopLimit; k++)
+        rpp_load32_f16_to_f32_avx(srcPtrTemp[k], &pRow[k * 4]);
+    for (int k = rowKernelLoopLimit * 4; k < 36; k += 4)
+    {
+        pRow[k] = avx_p0;
+        pRow[k + 1] = avx_p0;
+        pRow[k + 2] = avx_p0;
+        pRow[k + 3] = avx_p0;
+    }
+}
+
+inline void rpp_load_gaussian_filter_9x9_pkd_pln_host(__m256 *pRow, Rpp16f **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 5 rows for 9x9 kernel
+    rpp_load40_f16_to_f32_avx(srcPtrTemp[0], &pRow[0]);
+    rpp_load40_f16_to_f32_avx(srcPtrTemp[1], &pRow[5]);
+    rpp_load40_f16_to_f32_avx(srcPtrTemp[2], &pRow[10]);
+    rpp_load40_f16_to_f32_avx(srcPtrTemp[3], &pRow[15]);
+    rpp_load40_f16_to_f32_avx(srcPtrTemp[4], &pRow[20]);
+    for (int k = 5; k < rowKernelLoopLimit; k++)
+        rpp_load40_f16_to_f32_avx(srcPtrTemp[k], &pRow[k * 5]);
+    for (int k = rowKernelLoopLimit * 5; k < 45; k += 5)
+    {
+        pRow[k] = avx_p0;
+        pRow[k + 1] = avx_p0;
+        pRow[k + 2] = avx_p0;
+        pRow[k + 3] = avx_p0;
+        pRow[k + 4] = avx_p0;
+    }
+}
+
+
+// -------------------- Filter load functions for U8 bitdepth --------------------
+
+// load function for 3x3 kernel size
+inline void rpp_load_box_filter_char_3x3_host(__m256i *pxRow, Rpp8u **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 2 rows for 3x3 kernel
+    pxRow[0] = __lasx_xvld((__m256i *)srcPtrTemp[0], 0);
+    pxRow[1] = __lasx_xvld((__m256i *)srcPtrTemp[1], 0);
+    if (rowKernelLoopLimit == 3)
+        pxRow[2] = __lasx_xvld((__m256i *)srcPtrTemp[2], 0);
+    else
+        pxRow[2] = avx_px0;
+}
+
+// load function for 5x5 kernel size
+inline void rpp_load_box_filter_char_5x5_host(__m256i *pxRow, Rpp8u **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 3 rows for 5x5 kernel
+    pxRow[0] = __lasx_xvld((__m256i *)srcPtrTemp[0], 0);
+    pxRow[1] = __lasx_xvld((__m256i *)srcPtrTemp[1], 0);
+    pxRow[2] = __lasx_xvld((__m256i *)srcPtrTemp[2], 0);
+    for (int k = 3; k < rowKernelLoopLimit; k++)
+        pxRow[k] = __lasx_xvld((__m256i *)srcPtrTemp[k], 0);
+    for (int k = rowKernelLoopLimit; k < 5; k++)
+        pxRow[k] = avx_px0;
+}
+
+// load function for 7x7 kernel size
+inline void rpp_load_box_filter_char_7x7_host(__m256i *pxRow, Rpp8u **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 4 rows for 7x7 kernel
+    pxRow[0] = __lasx_xvld((__m256i *)srcPtrTemp[0], 0);
+    pxRow[1] = __lasx_xvld((__m256i *)srcPtrTemp[1], 0);
+    pxRow[2] = __lasx_xvld((__m256i *)srcPtrTemp[2], 0);
+    pxRow[3] = __lasx_xvld((__m256i *)srcPtrTemp[3], 0);
+    for (int k = 4; k < rowKernelLoopLimit; k++)
+        pxRow[k] = __lasx_xvld((__m256i *)srcPtrTemp[k], 0);
+    for (int k = rowKernelLoopLimit; k < 7; k++)
+        pxRow[k] = avx_px0;
+}
+
+// load function for 9x9 kernel size
+inline void rpp_load_box_filter_char_9x9_host(__m256i *pxRow, Rpp8u **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 5 rows for 9x9 kernel
+    pxRow[0] = __lasx_xvld((__m256i *)srcPtrTemp[0], 0);
+    pxRow[1] = __lasx_xvld((__m256i *)srcPtrTemp[1], 0);
+    pxRow[2] = __lasx_xvld((__m256i *)srcPtrTemp[2], 0);
+    pxRow[3] = __lasx_xvld((__m256i *)srcPtrTemp[3], 0);
+    pxRow[4] = __lasx_xvld((__m256i *)srcPtrTemp[4], 0);
+    for (int k = 5; k < rowKernelLoopLimit; k++)
+        pxRow[k] = __lasx_xvld((__m256i *)srcPtrTemp[k], 0);
+    for (int k = rowKernelLoopLimit; k < 9; k++)
+        pxRow[k] = avx_px0;
+}
+
+// -------------------- Filter load functions for I8 bitdepth --------------------
+
+// load function for 3x3 kernel size
+inline void rpp_load_box_filter_char_3x3_host(__m256i *pxRow, Rpp8s **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 2 rows for 3x3 kernel
+    pxRow[0] = __lasx_xvadd_b(avx_pxConvertI8, __lasx_xvld((__m256i *)srcPtrTemp[0], 0));
+    pxRow[1] = __lasx_xvadd_b(avx_pxConvertI8, __lasx_xvld((__m256i *)srcPtrTemp[1], 0));
+    if (rowKernelLoopLimit == 3)
+        pxRow[2] =  __lasx_xvadd_b(avx_pxConvertI8, __lasx_xvld((__m256i *)srcPtrTemp[2], 0));
+    else
+        pxRow[2] = avx_px0;
+}
+
+// load function for 5x5 kernel size
+inline void rpp_load_box_filter_char_5x5_host(__m256i *pxRow, Rpp8s **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 3 rows for 5x5 kernel
+    pxRow[0] = __lasx_xvadd_b(avx_pxConvertI8, __lasx_xvld((__m256i *)srcPtrTemp[0], 0));
+    pxRow[1] = __lasx_xvadd_b(avx_pxConvertI8, __lasx_xvld((__m256i *)srcPtrTemp[1], 0));
+    pxRow[2] = __lasx_xvadd_b(avx_pxConvertI8, __lasx_xvld((__m256i *)srcPtrTemp[2], 0));
+    for (int k = 3; k < rowKernelLoopLimit; k++)
+        pxRow[k] = __lasx_xvadd_b(avx_pxConvertI8, __lasx_xvld((__m256i *)srcPtrTemp[k], 0));
+    for (int k = rowKernelLoopLimit; k < 5; k++)
+        pxRow[k] = avx_px0;
+}
+
+// load function for 7x7 kernel size
+inline void rpp_load_box_filter_char_7x7_host(__m256i *pxRow, Rpp8s **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 4 rows for 7x7 kernel
+    pxRow[0] = __lasx_xvadd_b(avx_pxConvertI8, __lasx_xvld((__m256i *)srcPtrTemp[0], 0));
+    pxRow[1] = __lasx_xvadd_b(avx_pxConvertI8, __lasx_xvld((__m256i *)srcPtrTemp[1], 0));
+    pxRow[2] = __lasx_xvadd_b(avx_pxConvertI8, __lasx_xvld((__m256i *)srcPtrTemp[2], 0));
+    pxRow[3] = __lasx_xvadd_b(avx_pxConvertI8, __lasx_xvld((__m256i *)srcPtrTemp[3], 0));
+    for (int k = 4; k < rowKernelLoopLimit; k++)
+        pxRow[k] = __lasx_xvadd_b(avx_pxConvertI8, __lasx_xvld((__m256i *)srcPtrTemp[k], 0));
+    for (int k = rowKernelLoopLimit; k < 7; k++)
+        pxRow[k] = avx_px0;
+}
+
+// load function for 9x9 kernel size
+inline void rpp_load_box_filter_char_9x9_host(__m256i *pxRow, Rpp8s **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 5 rows for 9x9 kernel
+    pxRow[0] = __lasx_xvadd_b(avx_pxConvertI8, __lasx_xvld((__m256i *)srcPtrTemp[0], 0));
+    pxRow[1] = __lasx_xvadd_b(avx_pxConvertI8, __lasx_xvld((__m256i *)srcPtrTemp[1], 0));
+    pxRow[2] = __lasx_xvadd_b(avx_pxConvertI8, __lasx_xvld((__m256i *)srcPtrTemp[2], 0));
+    pxRow[3] = __lasx_xvadd_b(avx_pxConvertI8, __lasx_xvld((__m256i *)srcPtrTemp[3], 0));
+    pxRow[4] = __lasx_xvadd_b(avx_pxConvertI8, __lasx_xvld((__m256i *)srcPtrTemp[4], 0));
+    for (int k = 5; k < rowKernelLoopLimit; k++)
+        pxRow[k] = __lasx_xvadd_b(avx_pxConvertI8, __lasx_xvld((__m256i *)srcPtrTemp[k], 0));
+    for (int k = rowKernelLoopLimit; k < 9; k++)
+        pxRow[k] = avx_px0;
+}
+
+// -------------------- Filter load functions for F32 bitdepth --------------------
+
+// load function for 3x3 kernel size
+inline void rpp_load_box_filter_float_3x3_host(__m256 *pRow, Rpp32f **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 2 rows for 3x3 kernel
+    pRow[0] = (__m256)__lasx_xvld(srcPtrTemp[0], 0);
+    pRow[1] = (__m256)__lasx_xvld(srcPtrTemp[1], 0);
+    if (rowKernelLoopLimit == 3)
+        pRow[2] = (__m256)__lasx_xvld(srcPtrTemp[2], 0);
+    else
+        pRow[2] = avx_p0;
+}
+
+// load function for 5x5 kernel size
+inline void rpp_load_box_filter_float_5x5_host(__m256 *pRow, Rpp32f **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 3 rows for 5x5 kernel
+    pRow[0] = (__m256)__lasx_xvld(srcPtrTemp[0], 0);
+    pRow[1] = (__m256)__lasx_xvld(srcPtrTemp[1], 0);
+    pRow[2] = (__m256)__lasx_xvld(srcPtrTemp[2], 0);
+    for (int k = 3; k < rowKernelLoopLimit; k++)
+        pRow[k] = (__m256)__lasx_xvld(srcPtrTemp[k], 0);
+    for (int k = rowKernelLoopLimit; k < 5; k++)
+        pRow[k] = avx_p0;
+}
+
+// load function for 7x7 kernel size
+inline void rpp_load_box_filter_float_7x7_host(__m256 *pRow, Rpp32f **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 4 rows for 7x7 kernel
+    pRow[0] = (__m256)__lasx_xvld(srcPtrTemp[0], 0);
+    pRow[1] = (__m256)__lasx_xvld(srcPtrTemp[1], 0);
+    pRow[2] = (__m256)__lasx_xvld(srcPtrTemp[2], 0);
+    pRow[3] = (__m256)__lasx_xvld(srcPtrTemp[3], 0);
+    for (int k = 4; k < rowKernelLoopLimit; k++)
+        pRow[k] = (__m256)__lasx_xvld(srcPtrTemp[k], 0);
+    for (int k = rowKernelLoopLimit; k < 7; k++)
+        pRow[k] = avx_p0;
+}
+
+// load function for 9x9 kernel size
+inline void rpp_load_box_filter_float_9x9_host(__m256 *pRow, Rpp32f **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 5 rows for 9x9 kernel
+    pRow[0] = (__m256)__lasx_xvld(srcPtrTemp[0], 0);
+    pRow[1] = (__m256)__lasx_xvld(srcPtrTemp[1], 0);
+    pRow[2] = (__m256)__lasx_xvld(srcPtrTemp[2], 0);
+    pRow[3] = (__m256)__lasx_xvld(srcPtrTemp[3], 0);
+    pRow[4] = (__m256)__lasx_xvld(srcPtrTemp[4], 0);
+    for (int k = 5; k < rowKernelLoopLimit; k++)
+        pRow[k] = (__m256)__lasx_xvld(srcPtrTemp[k], 0);
+    for (int k = rowKernelLoopLimit; k < 9; k++)
+        pRow[k] = avx_p0;
+}
+
+// -------------------- Filter load functions for F16 bitdepth --------------------
+
+// load function for 3x3 kernel size
+inline void rpp_load_box_filter_float_3x3_host(__m256 *pRow, Rpp16f **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 2 rows for 3x3 kernel
+    pRow[0] = lasx_cvtph_f32((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtrTemp[0]), 0)));
+    pRow[1] = lasx_cvtph_f32((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtrTemp[1]), 0)));
+    if (rowKernelLoopLimit == 3)
+        pRow[2] = lasx_cvtph_f32((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtrTemp[2]), 0)));
+    else
+        pRow[2] = avx_p0;
+}
+
+// load function for 5x5 kernel size
+inline void rpp_load_box_filter_float_5x5_host(__m256 *pRow, Rpp16f **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 3 rows for 5x5 kernel
+    pRow[0] = lasx_cvtph_f32((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtrTemp[0]), 0)));
+    pRow[1] = lasx_cvtph_f32((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtrTemp[1]), 0)));
+    pRow[2] = lasx_cvtph_f32((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtrTemp[2]), 0)));
+    for (int k = 3; k < rowKernelLoopLimit; k++)
+        pRow[k] = lasx_cvtph_f32((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtrTemp[k]), 0)));
+    for (int k = rowKernelLoopLimit; k < 5; k++)
+        pRow[k] = avx_p0;
+}
+
+// load function for 7x7 kernel size
+inline void rpp_load_box_filter_float_7x7_host(__m256 *pRow, Rpp16f **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 4 rows for 7x7 kernel
+    pRow[0] = lasx_cvtph_f32((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtrTemp[0]), 0)));
+    pRow[1] = lasx_cvtph_f32((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtrTemp[1]), 0)));
+    pRow[2] = lasx_cvtph_f32((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtrTemp[2]), 0)));
+    pRow[3] = lasx_cvtph_f32((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtrTemp[3]), 0)));
+    for (int k = 4; k < rowKernelLoopLimit; k++)
+        pRow[k] = lasx_cvtph_f32((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtrTemp[k]), 0)));
+    for (int k = rowKernelLoopLimit; k < 7; k++)
+        pRow[k] = avx_p0;
+}
+
+// load function for 9x9 kernel size
+inline void rpp_load_box_filter_float_9x9_host(__m256 *pRow, Rpp16f **srcPtrTemp, Rpp32s rowKernelLoopLimit)
+{
+    // irrespective of row location, we need to load 5 rows for 9x9 kernel
+    pRow[0] = lasx_cvtph_f32((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtrTemp[0]), 0)));
+    pRow[1] = lasx_cvtph_f32((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtrTemp[1]), 0)));
+    pRow[2] = lasx_cvtph_f32((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtrTemp[2]), 0)));
+    pRow[3] = lasx_cvtph_f32((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtrTemp[3]), 0)));
+    pRow[4] = lasx_cvtph_f32((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtrTemp[4]), 0)));
+    for (int k = 5; k < rowKernelLoopLimit; k++)
+        pRow[k] = lasx_cvtph_f32((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtrTemp[k]), 0)));
+    for (int k = rowKernelLoopLimit; k < 9; k++)
+        pRow[k] = avx_p0;
+}
+
+#endif //RPP_CPU_FILTER_HPP
diff --git a/src/include/cpu/rpp_loongarch_simd.hpp b/src/include/cpu/rpp_loongarch_simd.hpp
new file mode 100644
index 00000000..fee3c5d8
--- /dev/null
+++ b/src/include/cpu/rpp_loongarch_simd.hpp
@@ -0,0 +1,4672 @@
+/*
+MIT License
+
+Copyright (c) 2019 - 2024 Advanced Micro Devices, Inc.
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
+*/
+
+#ifndef AMD_RPP_RPP_CPU_SIMD_HPP
+#define AMD_RPP_RPP_CPU_SIMD_HPP
+
+#include "stdio.h"
+#include "rppdefs.h"
+
+#if _WIN32
+#include <intrin.h>
+#else
+#ifdef __loongarch64
+#include "xxl-clang.h"
+#else
+#include <x86intrin.h>
+#include <smmintrin.h>
+#include <immintrin.h>
+#endif
+#endif
+
+#define M256I(m256i_register) (*((_m256i_union*)&m256i_register))
+typedef union
+{
+    char m256i_i8[32];
+    short m256i_i16[16];
+    int m256i_i32[8];
+    long long m256i_i64[4];
+    __m128i m256i_i128[2];
+} _m256i_union;
+
+#if defined(_MSC_VER)
+#define SIMD_ALIGN_VAR(type, name, alignment) \
+    __declspec(align(alignment)) type name
+#else
+#define SIMD_ALIGN_VAR(type, name, alignment) \
+    type __attribute__((__aligned__(alignment))) name
+#endif // _MSC_VER
+
+#define SIMD_CONST_PI(name, val0, val1, val2, val3) \
+    SIMD_ALIGN_VAR(static const int, _xmm_const_##name[4], 16) = { \
+        static_cast<int>(val3), \
+        static_cast<int>(val2), \
+        static_cast<int>(val1), \
+        static_cast<int>(val0)  \
+    }
+
+#define SIMD_CONST_PS(name, val0, val1, val2, val3) \
+    SIMD_ALIGN_VAR(static const float, _xmm_const_##name[4], 16) = { \
+        static_cast<float>(val3), \
+        static_cast<float>(val2), \
+        static_cast<float>(val1), \
+        static_cast<float>(val0)  \
+    }
+
+#define SIMD_GET_PS(name) (*(const __m128  *)_xmm_const_##name)
+
+const __m128 xmm_p0 = (__m128)__lsx_vldi(0);
+const __m128 xmm_p1 = lsx_set1_f32(1.0f);
+const __m128 xmm_p2 = lsx_set1_f32(2.0f);
+const __m128 xmm_pm2 = lsx_set1_f32(-2.0f);
+const __m128 xmm_p3 = lsx_set1_f32(3.0f);
+const __m128 xmm_p4 = lsx_set1_f32(4.0f);
+const __m128 xmm_p6 = lsx_set1_f32(6.0f);
+const __m128 xmm_p16 = lsx_set1_f32(16.0f);
+const __m128 xmm_p255 = lsx_set1_f32(255.0f);
+const __m128 xmm_p1op255 = lsx_set1_f32(1.0f / 255.0f);
+const __m128 xmm_p1op3 = lsx_set1_f32(1.0f / 3.0f);
+const __m128 xmm_p2op3 = lsx_set1_f32(2.0f / 3.0f);
+const __m128 xmm_pDstLocInit = lsx_setr_f32(0, 1, 2, 3);
+
+const __m128 xmm_cephesSQRTHF = lsx_set1_f32(0.707106781186547524);
+const __m128 xmm_cephesLogP0 = lsx_set1_f32(7.0376836292E-2);
+const __m128 xmm_cephesLogP1 = lsx_set1_f32(-1.1514610310E-1);
+const __m128 xmm_cephesLogP2 = lsx_set1_f32(1.1676998740E-1);
+const __m128 xmm_cephesLogP3 = lsx_set1_f32(-1.2420140846E-1);
+const __m128 xmm_cephesLogP4 = lsx_set1_f32(1.4249322787E-1);
+const __m128 xmm_cephesLogP5 = lsx_set1_f32(-1.6668057665E-1);
+const __m128 xmm_cephesLogP6 = lsx_set1_f32(2.0000714765E-1);
+const __m128 xmm_cephesLogP7 = lsx_set1_f32(-2.4999993993E-1);
+const __m128 xmm_cephesLogP8 = lsx_set1_f32(3.3333331174E-1);
+const __m128 xmm_cephesLogQ1 = lsx_set1_f32(-2.12194440e-4);
+const __m128 xmm_cephesLogQ2 = lsx_set1_f32(0.693359375);
+
+const __m128i xmm_px0 = __lsx_vreplgr2vr_w(0);
+const __m128i xmm_px1 = __lsx_vreplgr2vr_w(1);
+const __m128i xmm_px2 = __lsx_vreplgr2vr_w(2);
+const __m128i xmm_px3 = __lsx_vreplgr2vr_w(3);
+const __m128i xmm_px4 = __lsx_vreplgr2vr_w(4);
+const __m128i xmm_px5 = __lsx_vreplgr2vr_w(5);
+const __m128i xmm_pxConvertI8 = __lsx_vreplgr2vr_b((char)128);
+const __m128i xmm_pxDstLocInit = lsx_setr_i32(0, 1, 2, 3);
+
+const __m256 avx_p0 = lasx_set1_f32(0.0f);
+const __m256 avx_p1 = lasx_set1_f32(1.0f);
+const __m256 avx_p2 = lasx_set1_f32(2.0f);
+const __m256 avx_pm2 = lasx_set1_f32(-2.0f);
+const __m256 avx_p3 = lasx_set1_f32(3.0f);
+const __m256 avx_p4 = lasx_set1_f32(4.0f);
+const __m256 avx_p6 = lasx_set1_f32(6.0f);
+const __m256 avx_p8 = lasx_set1_f32(8.0f);
+const __m256 avx_p128 = lasx_set1_f32(128.0f);
+const __m256 avx_p255 = lasx_set1_f32(255.0f);
+const __m256 avx_p1op255 = lasx_set1_f32(1.0f / 255.0f);
+const __m256 avx_p1op3 = lasx_set1_f32(1.0f / 3.0f);
+const __m256 avx_p2op3 = lasx_set1_f32(2.0f / 3.0f);
+
+const __m256 avx_cephesSQRTHF = lasx_set1_f32(0.707106781186547524);
+const __m256 avx_cephesLogP0 = lasx_set1_f32(7.0376836292E-2);
+const __m256 avx_cephesLogP1 = lasx_set1_f32(-1.1514610310E-1);
+const __m256 avx_cephesLogP2 = lasx_set1_f32(1.1676998740E-1);
+const __m256 avx_cephesLogP3 = lasx_set1_f32(-1.2420140846E-1);
+const __m256 avx_cephesLogP4 = lasx_set1_f32(1.4249322787E-1);
+const __m256 avx_cephesLogP5 = lasx_set1_f32(-1.6668057665E-1);
+const __m256 avx_cephesLogP6 = lasx_set1_f32(2.0000714765E-1);
+const __m256 avx_cephesLogP7 = lasx_set1_f32(-2.4999993993E-1);
+const __m256 avx_cephesLogP8 = lasx_set1_f32(3.3333331174E-1);
+const __m256 avx_cephesLogQ1 = lasx_set1_f32(-2.12194440e-4);
+const __m256 avx_cephesLogQ2 = lasx_set1_f32(0.693359375);
+
+const __m256i avx_px0 = __lasx_xvreplgr2vr_w(0);
+const __m256i avx_px1 = __lasx_xvreplgr2vr_w(1);
+const __m256i avx_px2 = __lasx_xvreplgr2vr_w(2);
+const __m256i avx_px3 = __lasx_xvreplgr2vr_w(3);
+const __m256i avx_px4 = __lasx_xvreplgr2vr_w(4);
+const __m256i avx_px5 = __lasx_xvreplgr2vr_w(5);
+const __m256i avx_pxConvertI8 = __lasx_xvreplgr2vr_b((char)128);
+const __m256 avx_pDstLocInit = lasx_setr_f32(0, 1, 2, 3, 4, 5, 6, 7);
+
+const __m128i xmm_pxMask00To03 = lsx_setr_i8(0, 0x80, 0x80, 0x80, 1, 0x80, 0x80, 0x80, 2, 0x80, 0x80, 0x80, 3, 0x80, 0x80, 0x80);
+const __m128i xmm_pxMask04To07 = lsx_setr_i8(4, 0x80, 0x80, 0x80, 5, 0x80, 0x80, 0x80, 6, 0x80, 0x80, 0x80, 7, 0x80, 0x80, 0x80);
+const __m128i xmm_pxMask08To11 = lsx_setr_i8(8, 0x80, 0x80, 0x80, 9, 0x80, 0x80, 0x80, 10, 0x80, 0x80, 0x80, 11, 0x80, 0x80, 0x80);
+const __m128i xmm_pxMask12To15 = lsx_setr_i8(12, 0x80, 0x80, 0x80, 13, 0x80, 0x80, 0x80, 14, 0x80, 0x80, 0x80, 15, 0x80, 0x80, 0x80);
+
+const __m128i xmm_pxMask00To02 = lsx_setr_i8(0, 0x80, 0x80, 0x80, 1, 0x80, 0x80, 0x80, 2, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80);
+const __m128i xmm_pxMask03To05 = lsx_setr_i8(3, 0x80, 0x80, 0x80, 4, 0x80, 0x80, 0x80, 5, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80);
+const __m128i xmm_pxMask06To08 = lsx_setr_i8(6, 0x80, 0x80, 0x80, 7, 0x80, 0x80, 0x80, 8, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80);
+const __m128i xmm_pxMask09To11 = lsx_setr_i8(9, 0x80, 0x80, 0x80, 10, 0x80, 0x80, 0x80, 11, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80);
+const __m128i xmm_pxMask08To15 = lsx_setr_i8(8, 9, 10, 11, 12, 13, 14, 15, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80);
+
+const __m128i xmm_pxMask03To00 = lsx_setr_i8(3, 0x80, 0x80, 0x80, 2, 0x80, 0x80, 0x80, 1, 0x80, 0x80, 0x80, 0, 0x80, 0x80, 0x80);
+const __m128i xmm_pxMask07To04 = lsx_setr_i8(7, 0x80, 0x80, 0x80, 6, 0x80, 0x80, 0x80, 5, 0x80, 0x80, 0x80, 4, 0x80, 0x80, 0x80);
+const __m128i xmm_pxMask11To08 = lsx_setr_i8(11, 0x80, 0x80, 0x80, 10, 0x80, 0x80, 0x80, 9, 0x80, 0x80, 0x80, 8, 0x80, 0x80, 0x80);
+const __m128i xmm_pxMask15To12 = lsx_setr_i8(15, 0x80, 0x80, 0x80, 14, 0x80, 0x80, 0x80, 13, 0x80, 0x80, 0x80, 12, 0x80, 0x80, 0x80);
+
+const __m128i xmm_pxMaskR = lsx_setr_i8(0, 0x80, 0x80, 0x80, 3, 0x80, 0x80, 0x80, 6, 0x80, 0x80, 0x80, 9, 0x80, 0x80, 0x80);
+const __m128i xmm_pxMaskG = lsx_setr_i8(1, 0x80, 0x80, 0x80, 4, 0x80, 0x80, 0x80, 7, 0x80, 0x80, 0x80, 10, 0x80, 0x80, 0x80);
+const __m128i xmm_pxMaskB = lsx_setr_i8(2, 0x80, 0x80, 0x80, 5, 0x80, 0x80, 0x80, 8, 0x80, 0x80, 0x80, 11, 0x80, 0x80, 0x80);
+
+const __m128i xmm_pxMaskRMirror = lsx_setr_i8(9, 0x80, 0x80, 0x80, 6, 0x80, 0x80, 0x80, 3, 0x80, 0x80, 0x80, 0, 0x80, 0x80, 0x80);
+const __m128i xmm_pxMaskGMirror = lsx_setr_i8(10, 0x80, 0x80, 0x80, 7, 0x80, 0x80, 0x80, 4, 0x80, 0x80, 0x80, 1, 0x80, 0x80, 0x80);
+const __m128i xmm_pxMaskBMirror = lsx_setr_i8(11, 0x80, 0x80, 0x80, 8, 0x80, 0x80, 0x80, 5, 0x80, 0x80, 0x80, 2, 0x80, 0x80, 0x80);
+
+const __m128i xmm_char_maskR = lsx_setr_i8(0, 3, 6, 9, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80);
+const __m128i xmm_char_maskG = lsx_setr_i8(1, 4, 7, 10, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80);
+const __m128i xmm_char_maskB = lsx_setr_i8(2, 5, 8, 11, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80);
+const __m128i xmm_pkd_mask = lsx_setr_i8(0, 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 14, 0x80, 0x80, 0x80, 0x80);
+const __m128i xmm_store4_pkd_pixels = lsx_setr_i8(0, 1, 8, 2, 3, 9, 4, 5, 10, 6, 7, 11, 0x80, 0x80, 0x80, 0x80);
+const __m256i avx_store8_pkd_pixels = (__m256i)(v32i8){0, 1, 16, 2, 3, 17, 4, 5, 18, 6, 7, 19, 8, 9, 20, 10, 11, 21, 12, 13, 22, 14, 15, 23, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80};
+
+
+const __m128i xmm_pxStore4Pkd = lsx_setr_i8(0, 4, 8, 1, 5, 9, 2, 6, 10, 3, 7, 11, 0x80, 0x80, 0x80, 0x80);
+const __m256i avx_pxPermPkd = lasx_setr_i32(0, 1, 2, 4, 5, 6, 7, 3);
+const __m256i avx_pxShufflePkd = lasx_setr_m128i(xmm_pxStore4Pkd, xmm_pxStore4Pkd);
+
+const __m128i xmm_pxMask00 = lsx_setr_i8(0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0, 1, 2, 3);
+const __m128i xmm_pxMask04To11 = lsx_setr_i8(4, 5, 6, 7, 8, 9, 10, 11, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80);
+
+const __m256i avx_pxMaskR = lasx_setr_i8(0, 0x80, 0x80, 3, 0x80, 0x80, 6, 0x80, 0x80, 9, 0x80, 0x80, 12, 0x80, 0x80, 15, 0x80, 0x80, 18, 0x80, 0x80, 21, 0x80, 0x80, 24, 0x80, 0x80, 27, 0x80, 0x80, 0x80, 0x80);
+const __m256i avx_pxMaskG = lasx_setr_i8(0x80, 1, 0x80, 0x80, 4, 0x80, 0x80, 7, 0x80, 0x80, 10, 0x80, 0x80, 13, 0x80, 0x80, 16, 0x80, 0x80, 19, 0x80, 0x80, 22, 0x80, 0x80, 25, 0x80, 0x80, 28, 0x80, 0x80, 0x80);
+const __m256i avx_pxMaskB = lasx_setr_i8(0x80, 0x80, 2, 0x80, 0x80, 5, 0x80, 0x80, 8, 0x80, 0x80, 11, 0x80, 0x80, 14, 0x80, 0x80, 17, 0x80, 0x80, 20, 0x80, 0x80, 23, 0x80, 0x80, 26, 0x80, 0x80, 29, 0x80, 0x80);
+
+// Print helpers
+
+inline void rpp_mm_print_epi8(__m128i vPrintArray)
+{
+    char printArray[16];
+    __lsx_vst(vPrintArray, (__m128i *)printArray, 0);
+    printf("\n");
+    for (int ct = 0; ct < 16; ct++)
+    {
+        printf("%d ", printArray[ct]);
+    }
+}
+
+inline void rpp_storeu_si32(void *__p,
+                        	__m128i __b) {
+  struct __storeu_si32 {
+    int __v;
+  } __attribute__((__packed__, __may_alias__));
+  ((struct __storeu_si32 *)__p)->__v = ((__v4si)__b)[0];
+}
+
+inline void rpp_storeu_si64(void *__p,
+                            __m128i __b) {
+  struct __storeu_si64 {
+    long long __v;
+  } __attribute__((__packed__, __may_alias__));
+  ((struct __storeu_si64 *)__p)->__v = ((__v2di)__b)[0];
+}
+
+inline void rpp_mm_print_epi32(__m128i vPrintArray)
+{
+    int printArray[4];
+    __lsx_vst(vPrintArray, (__m128i *)printArray, 0);
+    printf("\n");
+    for (int ct = 0; ct < 4; ct++)
+    {
+        printf("%d ", printArray[ct]);
+    }
+}
+
+inline void rpp_mm_print_epi16(__m128i vPrintArray)
+{
+    unsigned short int printArray[8];
+    __lsx_vst(vPrintArray, (__m128i *)printArray, 0);
+    printf("\n");
+    for (int ct = 0; ct < 8; ct++)
+    {
+        printf("%hu ", printArray[ct]);
+    }
+}
+
+inline void rpp_mm_print_ps(__m128 vPrintArray)
+{
+    float printArray[4];
+    __lsx_vst(vPrintArray, printArray, 0);
+    printf("\n");
+    for (int ct = 0; ct < 4; ct++)
+    {
+        printf("%0.6f ", printArray[ct]);
+    }
+}
+
+inline void rpp_mm256_print_epi8(__m256i vPrintArray)
+{
+    unsigned char printArray[32];
+    __lasx_xvst(vPrintArray, (__m256i *)printArray, 0);
+    printf("\n");
+    for (int ct = 0; ct < 32; ct++)
+    {
+        printf("%d ", (unsigned char)printArray[ct]);
+    }
+}
+
+inline void rpp_mm256_print_epi32(__m256i vPrintArray)
+{
+    int printArray[8];
+    __lasx_xvst(vPrintArray, (__m256i *)printArray, 0);
+    printf("\n");
+    for (int ct = 0; ct < 8; ct++)
+    {
+        printf("%d ", printArray[ct]);
+    }
+}
+
+inline void rpp_mm256_print_epi16(__m256i vPrintArray)
+{
+    unsigned short int printArray[8];
+    __lasx_xvst(vPrintArray, (__m256i *)printArray, 0);
+    printf("\n");
+    for (int ct = 0; ct < 16; ct++)
+    {
+        printf("%hu ", printArray[ct]);
+    }
+}
+
+inline void rpp_mm256_print_ps(__m256 vPrintArray)
+{
+    float printArray[8];
+    __lasx_xvst((__m256i)vPrintArray, printArray, 0);
+    printf("\n");
+    for (int ct = 0; ct < 8; ct++)
+    {
+        printf("%0.6f ", printArray[ct]);
+    }
+}
+
+inline void rpp_saturate64_0to1_avx(__m256 *p)
+{
+    p[0] = __lasx_xvfmin_s(__lasx_xvfmax_s(p[0], avx_p0), avx_p1);
+    p[1] = __lasx_xvfmin_s(__lasx_xvfmax_s(p[1], avx_p0), avx_p1);
+    p[2] = __lasx_xvfmin_s(__lasx_xvfmax_s(p[2], avx_p0), avx_p1);
+    p[3] = __lasx_xvfmin_s(__lasx_xvfmax_s(p[3], avx_p0), avx_p1);
+    p[4] = __lasx_xvfmin_s(__lasx_xvfmax_s(p[4], avx_p0), avx_p1);
+    p[5] = __lasx_xvfmin_s(__lasx_xvfmax_s(p[5], avx_p0), avx_p1);
+    p[6] = __lasx_xvfmin_s(__lasx_xvfmax_s(p[6], avx_p0), avx_p1);
+    p[7] = __lasx_xvfmin_s(__lasx_xvfmax_s(p[7], avx_p0), avx_p1);
+}
+
+inline void rpp_saturate48_0to1_avx(__m256 *p)
+{
+    p[0] = __lasx_xvfmin_s(__lasx_xvfmax_s(p[0], avx_p0), avx_p1);
+    p[1] = __lasx_xvfmin_s(__lasx_xvfmax_s(p[1], avx_p0), avx_p1);
+    p[2] = __lasx_xvfmin_s(__lasx_xvfmax_s(p[2], avx_p0), avx_p1);
+    p[3] = __lasx_xvfmin_s(__lasx_xvfmax_s(p[3], avx_p0), avx_p1);
+    p[4] = __lasx_xvfmin_s(__lasx_xvfmax_s(p[4], avx_p0), avx_p1);
+    p[5] = __lasx_xvfmin_s(__lasx_xvfmax_s(p[5], avx_p0), avx_p1);
+}
+
+inline void rpp_saturate24_0to1_avx(__m256 *p)
+{
+    p[0] = __lasx_xvfmin_s(__lasx_xvfmax_s(p[0], avx_p0), avx_p1);
+    p[1] = __lasx_xvfmin_s(__lasx_xvfmax_s(p[1], avx_p0), avx_p1);
+    p[2] = __lasx_xvfmin_s(__lasx_xvfmax_s(p[2], avx_p0), avx_p1);
+}
+
+inline void rpp_saturate16_0to1_avx(__m256 *p)
+{
+    p[0] = __lasx_xvfmin_s(__lasx_xvfmax_s(p[0], avx_p0), avx_p1);
+    p[1] = __lasx_xvfmin_s(__lasx_xvfmax_s(p[1], avx_p0), avx_p1);
+}
+
+inline void rpp_saturate8_0to1_avx(__m256 *p)
+{
+    p[0] = __lasx_xvfmin_s(__lasx_xvfmax_s(p[0], avx_p0), avx_p1);
+}
+
+// SSE loads and stores
+
+inline void rpp_load48_u8pkd3_to_f32pln3(Rpp8u *srcPtr, __m128 *p)
+{
+    __m128i px[4];
+
+    px[0] = __lsx_vld((__m128i *)srcPtr, 0);           /* load [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|R05|G05|B05|R06] - Need RGB 01-04 */
+    px[1] = __lsx_vld((__m128i *)(srcPtr + 12), 0);    /* load [R05|G05|B05|R06|G06|B06|R07|G07|B07|R08|G08|B08|R09|G09|B09|R10] - Need RGB 05-08 */
+    px[2] = __lsx_vld((__m128i *)(srcPtr + 24), 0);    /* load [R09|G09|B09|R10|G10|B10|R11|G11|B11|R12|G12|B12|R13|G13|B13|R14] - Need RGB 09-12 */
+    px[3] = __lsx_vld((__m128i *)(srcPtr + 36), 0);    /* load [R13|G13|B13|R14|G14|B14|R15|G15|B15|R16|G16|B16|R17|G17|B17|R18] - Need RGB 13-16 */
+    p[0] = __lsx_vffint_s_w(lsx_shuffle_i8(px[0], xmm_pxMaskR));    /* Contains R01-04 */
+    p[1] = __lsx_vffint_s_w(lsx_shuffle_i8(px[1], xmm_pxMaskR));    /* Contains R05-08 */
+    p[2] = __lsx_vffint_s_w(lsx_shuffle_i8(px[2], xmm_pxMaskR));    /* Contains R09-12 */
+    p[3] = __lsx_vffint_s_w(lsx_shuffle_i8(px[3], xmm_pxMaskR));    /* Contains R13-16 */
+    p[4] = __lsx_vffint_s_w(lsx_shuffle_i8(px[0], xmm_pxMaskG));    /* Contains G01-04 */
+    p[5] = __lsx_vffint_s_w(lsx_shuffle_i8(px[1], xmm_pxMaskG));    /* Contains G05-08 */
+    p[6] = __lsx_vffint_s_w(lsx_shuffle_i8(px[2], xmm_pxMaskG));    /* Contains G09-12 */
+    p[7] = __lsx_vffint_s_w(lsx_shuffle_i8(px[3], xmm_pxMaskG));    /* Contains G13-16 */
+    p[8] = __lsx_vffint_s_w(lsx_shuffle_i8(px[0], xmm_pxMaskB));    /* Contains B01-04 */
+    p[9] = __lsx_vffint_s_w(lsx_shuffle_i8(px[1], xmm_pxMaskB));    /* Contains B05-08 */
+    p[10] = __lsx_vffint_s_w(lsx_shuffle_i8(px[2], xmm_pxMaskB));    /* Contains B09-12 */
+    p[11] = __lsx_vffint_s_w(lsx_shuffle_i8(px[3], xmm_pxMaskB));    /* Contains B13-16 */
+}
+
+inline void rpp_store48_f32pln3_to_u8pln3(Rpp8u *dstPtrR, Rpp8u *dstPtrG, Rpp8u *dstPtrB, __m128 *p)
+{
+    __m128i px[8];
+
+    px[4] = __lsx_vftint_w_s(p[0]);    /* convert to int32 for R */
+    px[5] = __lsx_vftint_w_s(p[1]);    /* convert to int32 for R */
+    px[6] = __lsx_vftint_w_s(p[2]);    /* convert to int32 for R */
+    px[7] = __lsx_vftint_w_s(p[3]);    /* convert to int32 for R */
+    px[4] = lsx_packus_i32(px[4], px[5]);    /* pack pixels 0-7 for R */
+    px[5] = lsx_packus_i32(px[6], px[7]);    /* pack pixels 8-15 for R */
+    px[0] = lsx_packus_i16(px[4], px[5]);    /* pack pixels 0-15 for R */
+    px[4] = __lsx_vftint_w_s(p[4]);    /* convert to int32 for G */
+    px[5] = __lsx_vftint_w_s(p[5]);    /* convert to int32 for G */
+    px[6] = __lsx_vftint_w_s(p[6]);    /* convert to int32 for G */
+    px[7] = __lsx_vftint_w_s(p[7]);    /* convert to int32 for G */
+    px[4] = lsx_packus_i32(px[4], px[5]);    /* pack pixels 0-7 for G */
+    px[5] = lsx_packus_i32(px[6], px[7]);    /* pack pixels 8-15 for G */
+    px[1] = lsx_packus_i16(px[4], px[5]);    /* pack pixels 0-15 for G */
+    px[4] = __lsx_vftint_w_s(p[8]);    /* convert to int32 for B */
+    px[5] = __lsx_vftint_w_s(p[9]);    /* convert to int32 for B */
+    px[6] = __lsx_vftint_w_s(p[10]);    /* convert to int32 for B */
+    px[7] = __lsx_vftint_w_s(p[11]);    /* convert to int32 for B */
+    px[4] = lsx_packus_i32(px[4], px[5]);    /* pack pixels 0-7 for B */
+    px[5] = lsx_packus_i32(px[6], px[7]);    /* pack pixels 8-15 for B */
+    px[2] = lsx_packus_i16(px[4], px[5]);    /* pack pixels 0-15 for B */
+    __lsx_vst(px[0], (__m128i *)dstPtrR, 0);    /* store [R01|R02|R03|R04|R05|R06|R07|R08|R09|R10|R11|R12|R13|R14|R15|R16] */
+    __lsx_vst(px[1], (__m128i *)dstPtrG, 0);    /* store [G01|G02|G03|G04|G05|G06|G07|G08|G09|G10|G11|G12|G13|G14|G15|G16] */
+    __lsx_vst(px[2], (__m128i *)dstPtrB, 0);    /* store [B01|B02|B03|B04|B05|B06|B07|B08|B09|B10|B11|B12|B13|B14|B15|B16] */
+}
+
+inline void rpp_load48_u8pln3_to_f32pln3(Rpp8u *srcPtrR, Rpp8u *srcPtrG, Rpp8u *srcPtrB, __m128 *p)
+{
+    __m128i px[3];
+
+    px[0] = __lsx_vld((__m128i *)srcPtrR, 0);    /* load [R01|R02|R03|R04|R05|R06|R07|R08|R09|R10|R11|R12|R13|R14|R15|R16] */
+    px[1] = __lsx_vld((__m128i *)srcPtrG, 0);    /* load [G01|G02|G03|G04|G05|G06|G07|G08|G09|G10|G11|G12|G13|G14|G15|G16] */
+    px[2] = __lsx_vld((__m128i *)srcPtrB, 0);    /* load [B01|B02|B03|B04|B05|B06|B07|B08|B09|B10|B11|B12|B13|B14|B15|B16] */
+    p[0] = __lsx_vffint_s_w(lsx_shuffle_i8(px[0], xmm_pxMask00To03));    /* Contains R01-04 */
+    p[1] = __lsx_vffint_s_w(lsx_shuffle_i8(px[0], xmm_pxMask04To07));    /* Contains R05-08 */
+    p[2] = __lsx_vffint_s_w(lsx_shuffle_i8(px[0], xmm_pxMask08To11));    /* Contains R09-12 */
+    p[3] = __lsx_vffint_s_w(lsx_shuffle_i8(px[0], xmm_pxMask12To15));    /* Contains R13-16 */
+    p[4] = __lsx_vffint_s_w(lsx_shuffle_i8(px[1], xmm_pxMask00To03));    /* Contains G01-04 */
+    p[5] = __lsx_vffint_s_w(lsx_shuffle_i8(px[1], xmm_pxMask04To07));    /* Contains G05-08 */
+    p[6] = __lsx_vffint_s_w(lsx_shuffle_i8(px[1], xmm_pxMask08To11));    /* Contains G09-12 */
+    p[7] = __lsx_vffint_s_w(lsx_shuffle_i8(px[1], xmm_pxMask12To15));    /* Contains G13-16 */
+    p[8] = __lsx_vffint_s_w(lsx_shuffle_i8(px[2], xmm_pxMask00To03));    /* Contains B01-04 */
+    p[9] = __lsx_vffint_s_w(lsx_shuffle_i8(px[2], xmm_pxMask04To07));    /* Contains B05-08 */
+    p[10] = __lsx_vffint_s_w(lsx_shuffle_i8(px[2], xmm_pxMask08To11));    /* Contains B09-12 */
+    p[11] = __lsx_vffint_s_w(lsx_shuffle_i8(px[2], xmm_pxMask12To15));    /* Contains B13-16 */
+}
+
+inline void rpp_store48_f32pln3_to_u8pkd3(Rpp8u *dstPtr, __m128 *p)
+{
+    __m128i px[7];
+    __m128i pxMask = lsx_setr_i8(0, 4, 8, 1, 5, 9, 2, 6, 10, 3, 7, 11, 12, 13, 14, 15);
+    __m128i pxZero = __lsx_vldi(0);
+
+    px[4] = __lsx_vftint_w_s(p[0]);    /* convert to int32 for R01-04 */
+    px[5] = __lsx_vftint_w_s(p[4]);    /* convert to int32 for G01-04 */
+    px[6] = __lsx_vftint_w_s(p[8]);    /* convert to int32 for B01-04 */
+    px[4] = lsx_packus_i32(px[4], px[5]);    /* pack pixels 0-7 as R01-04|G01-04 */
+    px[5] = lsx_packus_i32(px[6], pxZero);    /* pack pixels 8-15 as B01-04|X01-04 */
+    px[0] = lsx_packus_i16(px[4], px[5]);    /* pack pixels 0-15 as [R01|R02|R03|R04|G01|G02|G03|G04|B01|B02|B03|B04|00|00|00|00] */
+    px[4] = __lsx_vftint_w_s(p[1]);    /* convert to int32 for R05-08 */
+    px[5] = __lsx_vftint_w_s(p[5]);    /* convert to int32 for G05-08 */
+    px[6] = __lsx_vftint_w_s(p[9]);    /* convert to int32 for B05-08 */
+    px[4] = lsx_packus_i32(px[4], px[5]);    /* pack pixels 0-7 as R05-08|G05-08 */
+    px[5] = lsx_packus_i32(px[6], pxZero);    /* pack pixels 8-15 as B05-08|X01-04 */
+    px[1] = lsx_packus_i16(px[4], px[5]);    /* pack pixels 0-15 as [R05|R06|R07|R08|G05|G06|G07|G08|B05|B06|B07|B08|00|00|00|00] */
+    px[4] = __lsx_vftint_w_s(p[2]);    /* convert to int32 for R09-12 */
+    px[5] = __lsx_vftint_w_s(p[6]);    /* convert to int32 for G09-12 */
+    px[6] = __lsx_vftint_w_s(p[10]);    /* convert to int32 for B09-12 */
+    px[4] = lsx_packus_i32(px[4], px[5]);    /* pack pixels 0-7 as R09-12|G09-12 */
+    px[5] = lsx_packus_i32(px[6], pxZero);    /* pack pixels 8-15 as B09-12|X01-04 */
+    px[2] = lsx_packus_i16(px[4], px[5]);    /* pack pixels 0-15 as [R09|R10|R11|R12|G09|G10|G11|G12|B09|B10|B11|B12|00|00|00|00] */
+    px[4] = __lsx_vftint_w_s(p[3]);    /* convert to int32 for R13-16 */
+    px[5] = __lsx_vftint_w_s(p[7]);    /* convert to int32 for G13-16 */
+    px[6] = __lsx_vftint_w_s(p[11]);    /* convert to int32 for B13-16 */
+    px[4] = lsx_packus_i32(px[4], px[5]);    /* pack pixels 0-7 as R13-16|G13-16 */
+    px[5] = lsx_packus_i32(px[6], pxZero);    /* pack pixels 8-15 as B13-16|X01-04 */
+    px[3] = lsx_packus_i16(px[4], px[5]);    /* pack pixels 0-15 as [R13|R14|R15|R16|G13|G14|G15|G16|B13|B14|B15|B16|00|00|00|00] */
+    px[0] = lsx_shuffle_i8(px[0], pxMask);    /* shuffle to get [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|00|00|00|00] */
+    px[1] = lsx_shuffle_i8(px[1], pxMask);    /* shuffle to get [R05|G05|B05|R06|G06|B06|R07|G07|B07|R08|G08|B08|00|00|00|00] */
+    px[2] = lsx_shuffle_i8(px[2], pxMask);    /* shuffle to get [R09|G09|B09|R10|G10|B10|R11|G11|B11|R12|G12|B12|00|00|00|00] */
+    px[3] = lsx_shuffle_i8(px[3], pxMask);    /* shuffle to get [R13|G13|B13|R14|G14|B14|R15|G15|B15|R16|G16|B16|00|00|00|00] */
+    __lsx_vst(px[0], (__m128i *)dstPtr, 0);           /* store [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|00|00|00|00] */
+    __lsx_vst(px[1], (__m128i *)(dstPtr + 12), 0);    /* store [R05|G05|B05|R06|G06|B06|R07|G07|B07|R08|G08|B08|00|00|00|00] */
+    __lsx_vst(px[2], (__m128i *)(dstPtr + 24), 0);    /* store [R09|G09|B09|R10|G10|B10|R11|G11|B11|R12|G12|B12|00|00|00|00] */
+    __lsx_vst(px[3], (__m128i *)(dstPtr + 36), 0);    /* store [R13|G13|B13|R14|G14|B14|R15|G15|B15|R16|G16|B16|00|00|00|00] */
+}
+
+inline void rpp_load48_u8pkd3_to_u8pln3(Rpp8u *srcPtr, __m128i *px)
+{
+    __m128i pxSrc[8];
+    __m128i pxMask = lsx_setr_i8(0, 3, 6, 9, 1, 4, 7, 10, 2, 5, 8, 11, 12, 13, 14, 15);
+    __m128i pxMaskRGB = lsx_setr_i8(0, 4, 8, 12, 2, 6, 10, 14, 1, 5, 9, 13, 3, 7, 11, 15);
+
+    pxSrc[0] = __lsx_vld((__m128i *)srcPtr, 0);           /* load [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|R05|G05|B05|R06] - Need RGB 01-04 */
+    pxSrc[1] = __lsx_vld((__m128i *)(srcPtr + 12), 0);    /* load [R05|G05|B05|R06|G06|B06|R07|G07|B07|R08|G08|B08|R09|G09|B09|R10] - Need RGB 05-08 */
+    pxSrc[2] = __lsx_vld((__m128i *)(srcPtr + 24), 0);    /* load [R09|G09|B09|R10|G10|B10|R11|G11|B11|R12|G12|B12|R13|G13|B13|R14] - Need RGB 09-12 */
+    pxSrc[3] = __lsx_vld((__m128i *)(srcPtr + 36), 0);    /* load [R13|G13|B13|R14|G14|B14|R15|G15|B15|R16|G16|B16|R17|G17|B17|R18] - Need RGB 13-16 */
+    pxSrc[0] = lsx_shuffle_i8(pxSrc[0], pxMask);    /* shuffle to get [R01|R02|R03|R04|G01|G02|G03|G04 || B01|B02|B03|B04|R05|G05|B05|R06] - Need R01-04, G01-04, B01-04 */
+    pxSrc[1] = lsx_shuffle_i8(pxSrc[1], pxMask);    /* shuffle to get [R05|R06|R07|R08|G05|G06|G07|G08 || B05|B06|B07|B08|R09|G09|B09|R10] - Need R05-08, G05-08, B05-08 */
+    pxSrc[2] = lsx_shuffle_i8(pxSrc[2], pxMask);    /* shuffle to get [R09|R10|R11|R12|G09|G10|G11|G12 || B09|B10|B11|B12|R13|G13|B13|R14] - Need R09-12, G09-12, B09-12 */
+    pxSrc[3] = lsx_shuffle_i8(pxSrc[3], pxMask);    /* shuffle to get [R13|R14|R15|R16|G13|G14|G15|G16 || B13|B14|B15|B16|R17|G17|B17|R18] - Need R13-16, G13-16, B13-16 */
+    pxSrc[4] = __lsx_vilvl_b(pxSrc[1], pxSrc[0]);    /* unpack 8 lo-pixels of pxSrc[0] and pxSrc[1] */
+    pxSrc[5] = __lsx_vilvl_b(pxSrc[3], pxSrc[2]);    /* unpack 8 lo-pixels of pxSrc[2] and pxSrc[3] */
+    pxSrc[6] = __lsx_vilvh_b(pxSrc[1], pxSrc[0]);    /* unpack 8 hi-pixels of pxSrc[0] and pxSrc[1] */
+    pxSrc[7] = __lsx_vilvh_b(pxSrc[3], pxSrc[2]);    /* unpack 8 hi-pixels of pxSrc[2] and pxSrc[3] */
+    px[0] = lsx_shuffle_i8(__lsx_vilvl_b(pxSrc[5], pxSrc[4]), pxMaskRGB);    /* unpack 8 lo-pixels of pxSrc[4] and pxSrc[5] to get R01-16 */
+    px[1] = lsx_shuffle_i8(__lsx_vilvh_b(pxSrc[5], pxSrc[4]), pxMaskRGB);    /* unpack 8 hi-pixels of pxSrc[4] and pxSrc[5] to get G01-16 */
+    px[2] = lsx_shuffle_i8(__lsx_vilvl_b(pxSrc[7], pxSrc[6]), pxMaskRGB);    /* unpack 8 lo-pixels of pxSrc[6] and pxSrc[7] to get B01-16 */
+}
+
+inline void rpp_load96_u8pkd3_to_u8pln3(Rpp8u *srcPtr, __m256i *px)
+{
+    __m128i tmp = lsx_setr_i8(0, 3, 6, 9, 1, 4, 7, 10, 2, 5, 8, 11, 12, 13, 14, 15);
+    __m128i tmp2 = lsx_setr_i8(0, 4, 8, 12, 2, 6, 10, 14, 1, 5, 9, 13, 3, 7, 11, 15);
+    __m256i pxSrc[8];
+    __m256i pxMask = lasx_castm128i_m256i(tmp);
+    pxMask = lasx_permute2f128_m256i(pxMask, pxMask, 0);
+    __m256i pxMaskRGB = lasx_castm128i_m256i(tmp2);
+    pxMaskRGB = lasx_permute2f128_m256i(pxMaskRGB, pxMaskRGB, 0);
+    pxSrc[0] = lasx_insertf128_m256i(lasx_castm128i_m256i(__lsx_vld((__m128i *)srcPtr, 0)), __lsx_vld((__m128i *)(srcPtr + 48), 0), 1);           /* load [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|R05|G05|B05|R06|R17|G17|B17|R18|G18|B18|R19|G19|B19|R20|G20|B20|R21|G21|B21|R22] - Need RGB 01-04, 17-20 */
+    pxSrc[1] = lasx_insertf128_m256i(lasx_castm128i_m256i(__lsx_vld((__m128i *)(srcPtr + 12), 0)), __lsx_vld((__m128i *)(srcPtr + 60), 0), 1);    /* load [R05|G05|B05|R06|G06|B06|R07|G07|B07|R08|G08|B08|R09|G09|B09|R10|R21|G21|B21|R22|G22|B22|R23|G23|B23|R24|G24|B24|R25|G25|B25|R26] - Need RGB 05-08, 21-24 */
+    pxSrc[2] = lasx_insertf128_m256i(lasx_castm128i_m256i(__lsx_vld((__m128i *)(srcPtr + 24), 0)), __lsx_vld((__m128i *)(srcPtr + 72), 0), 1);    /* load [R09|G09|B09|R10|G10|B10|R11|G11|B11|R12|G12|B12|R13|G13|B13|R14|R25|G25|B25|R26|G26|B26|R27|G27|B27|R28|G28|B28|R29|G29|B29|R30] - Need RGB 09-12, 25-28 */
+    pxSrc[3] = lasx_insertf128_m256i(lasx_castm128i_m256i(__lsx_vld((__m128i *)(srcPtr + 36), 0)), __lsx_vld((__m128i *)(srcPtr + 84), 0), 1);    /* load [R13|G13|B13|R14|G14|B14|R15|G15|B15|R16|G16|B16|R17|G17|B17|R18|R29|G29|B29|R30|G30|B30|R31|G31|B31|R32|G32|B32|R33|G33|B33|R34] - Need RGB 13-16, 29-32 */
+    pxSrc[4] = lasx_shuffle_i8(pxSrc[0], pxMask);    /* shuffle to get [R01|R02|R03|R04|G01|G02|G03|G04 || B01|B02|B03|B04|R05|G05|B05|R06 || R17|R18|R19|R20|G17|G18|G19|G20 || B17|B18|B19|B20|R21|G21|B21|R22] - Need R01-04, G01-04, B01-04, R17-20, G17-20, B17-20 */
+    pxSrc[5] = lasx_shuffle_i8(pxSrc[1], pxMask);    /* shuffle to get [R05|R06|R07|R08|G05|G06|G07|G08 || B05|B06|B07|B08|R09|G09|B09|R10 || R21|R22|R23|R24|G21|G22|G23|G24 || B21|B22|B23|B24|R25|G25|B25|R26] - Need R05-08, G05-08, B05-08, R21-24, G21-24, B21-24 */
+    pxSrc[6] = lasx_shuffle_i8(pxSrc[2], pxMask);    /* shuffle to get [R09|R10|R11|R12|G09|G10|G11|G12 || B09|B10|B11|B12|R13|G13|B13|R14 || R25|R26|R27|R28|G25|G26|G27|G28 || B25|B26|B27|B28|R29|G29|B29|R30] - Need R09-12, G09-12, B09-12, R25-28, G25-28, B25-28 */
+    pxSrc[7] = lasx_shuffle_i8(pxSrc[3], pxMask);    /* shuffle to get [R13|R14|R15|R16|G13|G14|G15|G16 || B13|B14|B15|B16|R17|G17|B17|R18 || R29|R30|R31|R32|G29|G30|G31|G32 || B29|B30|B31|B32|R33|G33|B33|R34] - Need R13-16, G13-16, B13-16, R29-32, G29-32, B29-32 */
+    pxSrc[0] = __lasx_xvilvl_b(pxSrc[5], pxSrc[4]);    /* unpack 8 lo-pixels of pxSrc[4] and pxSrc[5] */
+    pxSrc[1] = __lasx_xvilvl_b(pxSrc[7], pxSrc[6]);    /* unpack 8 lo-pixels of pxSrc[6] and pxSrc[7] */
+    pxSrc[2] = __lasx_xvilvh_b(pxSrc[5], pxSrc[4]);    /* unpack 8 hi-pixels of pxSrc[4] and pxSrc[5] */
+    pxSrc[3] = __lasx_xvilvh_b(pxSrc[7], pxSrc[6]);    /* unpack 8 hi-pixels of pxSrc[6] and pxSrc[7] */
+    px[0] = lasx_shuffle_i8(__lasx_xvilvl_b(pxSrc[1], pxSrc[0]), pxMaskRGB);    /* unpack 8 lo-pixels of pxSrc[0] and pxSrc[1] to get R01-16 */
+    px[1] = lasx_shuffle_i8(__lasx_xvilvh_b(pxSrc[1], pxSrc[0]), pxMaskRGB);    /* unpack 8 hi-pixels of pxSrc[0] and pxSrc[1] to get G01-16 */
+    px[2] = lasx_shuffle_i8(__lasx_xvilvl_b(pxSrc[3], pxSrc[2]), pxMaskRGB);    /* unpack 8 lo-pixels of pxSrc[2] and pxSrc[3] to get B01-16 */
+}
+
+inline void rpp_store48_u8pln3_to_u8pln3(Rpp8u *dstPtrR, Rpp8u *dstPtrG, Rpp8u *dstPtrB, __m128i *px)
+{
+    __lsx_vst(px[0], (__m128i *)dstPtrR, 0);    /* store [R01|R02|R03|R04|R05|R06|R07|R08|R09|R10|R11|R12|R13|R14|R15|R16] */
+    __lsx_vst(px[1], (__m128i *)dstPtrG, 0);    /* store [G01|G02|G03|G04|G05|G06|G07|G08|G09|G10|G11|G12|G13|G14|G15|G16] */
+    __lsx_vst(px[2], (__m128i *)dstPtrB, 0);    /* store [B01|B02|B03|B04|B05|B06|B07|B08|B09|B10|B11|B12|B13|B14|B15|B16] */
+}
+
+inline void rpp_store96_u8pln3_to_u8pln3(Rpp8u *dstPtrR, Rpp8u *dstPtrG, Rpp8u *dstPtrB, __m256i *px)
+{
+    __lasx_xvst(px[0], (__m256i *)dstPtrR, 0);    /* store [R01|R02|R03|R04|R05|R06|R07|R08|R09|R10|R11|R12|R13|R14|R15|R16|R17|R18|R19|R20|R21|R22|R23|R24|R25|R26|R27|R28|R29|R30|R31|R32] */
+    __lasx_xvst(px[1], (__m256i *)dstPtrG, 0);    /* store [G01|G02|G03|G04|G05|G06|G07|G08|G09|G10|G11|G12|G13|G14|G15|G16|G17|G18|G19|G20|G21|G22|G23|G24|G25|G26|G27|G28|G29|G30|G31|G32] */
+    __lasx_xvst(px[2], (__m256i *)dstPtrB, 0);    /* store [B01|B02|B03|B04|B05|B06|B07|B08|B09|B10|B11|B12|B13|B14|B15|B16|B17|B18|B19|B20|B21|B22|B23|B24|B25|B26|B27|B28|B29|B30|B31|B32] */
+}
+
+inline void rpp_load48_u8pln3_to_u8pln3(Rpp8u *srcPtrR, Rpp8u *srcPtrG, Rpp8u *srcPtrB, __m128i *px)
+{
+    px[0] = __lsx_vld((__m128i *)srcPtrR, 0);    /* load [R01|R02|R03|R04|R05|R06|R07|R08|R09|R10|R11|R12|R13|R14|R15|R16] */
+    px[1] = __lsx_vld((__m128i *)srcPtrG, 0);    /* load [G01|G02|G03|G04|G05|G06|G07|G08|G09|G10|G11|G12|G13|G14|G15|G16] */
+    px[2] = __lsx_vld((__m128i *)srcPtrB, 0);    /* load [B01|B02|B03|B04|B05|B06|B07|B08|B09|B10|B11|B12|B13|B14|B15|B16] */
+}
+
+inline void rpp_store48_u8pln3_to_u8pkd3(Rpp8u *dstPtr, __m128i *px)
+{
+    __m128i pxDst[4];
+    __m128i pxZero = __lsx_vldi(0);
+    __m128i pxMaskRGBAtoRGB = lsx_setr_i8(0, 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 14, 3, 7, 11, 15);
+    pxDst[0] = __lsx_vilvl_b(pxZero, px[1]);
+    pxDst[1] = __lsx_vilvh_b(pxZero, px[1]);
+    pxDst[2] = __lsx_vilvl_b(px[2], px[0]);
+    pxDst[3] = __lsx_vilvh_b(px[2], px[0]);
+    __lsx_vst(lsx_shuffle_i8(__lsx_vilvl_b(pxDst[0], pxDst[2]), pxMaskRGBAtoRGB), (__m128i *)dstPtr, 0);  /* store [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|00|00|00|00] */
+    __lsx_vst(lsx_shuffle_i8(__lsx_vilvh_b(pxDst[0], pxDst[2]), pxMaskRGBAtoRGB), (__m128i *)(dstPtr + 12), 0); /* store [R05|G05|B05|R06|G06|B06|R07|G07|B07|R08|G08|B08|00|00|00|00] */
+    __lsx_vst(lsx_shuffle_i8(__lsx_vilvl_b(pxDst[1], pxDst[3]), pxMaskRGBAtoRGB), (__m128i *)(dstPtr + 24), 0); /* store [R09|G09|B09|R10|G10|B10|R11|G11|B11|R12|G12|B12|00|00|00|00] */
+    __lsx_vst(lsx_shuffle_i8(__lsx_vilvh_b(pxDst[1], pxDst[3]), pxMaskRGBAtoRGB), (__m128i *)(dstPtr + 36), 0); /* store [R13|G13|B13|R14|G14|B14|R15|G15|B15|R16|G16|B16|00|00|00|00] */
+}
+
+inline void rpp_store96_u8pln3_to_u8pkd3(Rpp8u *dstPtr, __m256i *px)
+{
+    __m256i pxDst[8];
+    __m128i tmp = lsx_setr_i8(0, 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 14, 3, 7, 11, 15);
+    //__m256i pxMaskRGBAtoRGB = lasx_castm128i_m256i(lsx_setr_i8(0, 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 14, 3, 7, 11, 15));
+    __m256i pxMaskRGBAtoRGB = lasx_castm128i_m256i(tmp);
+    pxMaskRGBAtoRGB = lasx_permute2f128_m256i(pxMaskRGBAtoRGB, pxMaskRGBAtoRGB, 0);
+    pxDst[0] = __lasx_xvilvl_b(avx_px0, px[1]);
+    pxDst[1] = __lasx_xvilvh_b(avx_px0, px[1]);
+    pxDst[2] = __lasx_xvilvl_b(px[2], px[0]);
+    pxDst[3] = __lasx_xvilvh_b(px[2], px[0]);
+    pxDst[4] = lasx_shuffle_i8(__lasx_xvilvl_b(pxDst[0], pxDst[2]), pxMaskRGBAtoRGB);
+    pxDst[5] = lasx_shuffle_i8(__lasx_xvilvh_b(pxDst[0], pxDst[2]), pxMaskRGBAtoRGB);
+    pxDst[6] = lasx_shuffle_i8(__lasx_xvilvl_b(pxDst[1], pxDst[3]), pxMaskRGBAtoRGB);
+    pxDst[7] = lasx_shuffle_i8(__lasx_xvilvh_b(pxDst[1], pxDst[3]), pxMaskRGBAtoRGB);
+    __lsx_vst(__lasx_cvt_256_128(pxDst[4]), (__m128i *)dstPtr, 0);                /* store [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|00|00|00|00] */
+    __lsx_vst(__lasx_cvt_256_128(pxDst[5]), (__m128i *)(dstPtr + 12), 0);         /* store [R05|G05|B05|R06|G06|B06|R07|G07|B07|R08|G08|B08|00|00|00|00] */
+    __lsx_vst(__lasx_cvt_256_128(pxDst[6]), (__m128i *)(dstPtr + 24), 0);         /* store [R09|G09|B09|R10|G10|B10|R11|G11|B11|R12|G12|B12|00|00|00|00] */
+    __lsx_vst(__lasx_cvt_256_128(pxDst[7]), (__m128i *)(dstPtr + 36), 0);         /* store [R13|G13|B13|R14|G14|B14|R15|G15|B15|R16|G16|B16|00|00|00|00] */
+    __lsx_vst(lasx_extractf128_m256i(pxDst[4], 1), (__m128i *)(dstPtr + 48), 0);    /* store [R17|G17|B17|R18|G18|B18|R19|G19|B19|R20|G20|B20|00|00|00|00] */
+    __lsx_vst(lasx_extractf128_m256i(pxDst[5], 1), (__m128i *)(dstPtr + 60), 0);    /* store [R21|G21|B21|R22|G22|B22|R23|G23|B23|R24|G24|B24|00|00|00|00] */
+    __lsx_vst(lasx_extractf128_m256i(pxDst[6], 1), (__m128i *)(dstPtr + 72), 0);    /* store [R25|G25|B25|R26|G26|B26|R27|G27|B27|R28|G28|B28|00|00|00|00] */
+    __lsx_vst(lasx_extractf128_m256i(pxDst[7], 1), (__m128i *)(dstPtr + 84), 0);    /* store [R29|G29|B29|R30|G30|B30|R31|G31|B31|R32|G32|B32|00|00|00|00] */
+}
+
+inline void rpp_load16_u8_to_f32(Rpp8u *srcPtr, __m128 *p)
+{
+    __m128i px = __lsx_vld((__m128i *)srcPtr, 0);    /* load pixels 0-15 */
+    p[0] = __lsx_vffint_s_w(lsx_shuffle_i8(px, xmm_pxMask00To03));    /* pixels 0-3 */
+    p[1] = __lsx_vffint_s_w(lsx_shuffle_i8(px, xmm_pxMask04To07));    /* pixels 4-7 */
+    p[2] = __lsx_vffint_s_w(lsx_shuffle_i8(px, xmm_pxMask08To11));    /* pixels 8-11 */
+    p[3] = __lsx_vffint_s_w(lsx_shuffle_i8(px, xmm_pxMask12To15));    /* pixels 12-15 */
+}
+
+inline void rpp_store16_f32_to_u8(Rpp8u *dstPtr, __m128 *p)
+{
+    __m128i px[4];
+
+    px[0] = __lsx_vftint_w_s(p[0]);    /* pixels 0-3 */
+    px[1] = __lsx_vftint_w_s(p[1]);    /* pixels 4-7 */
+    px[2] = __lsx_vftint_w_s(p[2]);    /* pixels 8-11 */
+    px[3] = __lsx_vftint_w_s(p[3]);    /* pixels 12-15 */
+    px[0] = lsx_packus_i32(px[0], px[1]);    /* pixels 0-7 */
+    px[1] = lsx_packus_i32(px[2], px[3]);    /* pixels 8-15 */
+    px[0] = lsx_packus_i16(px[0], px[1]);    /* pixels 0-15 */
+    __lsx_vst(px[0], (__m128i *)dstPtr, 0);    /* store pixels 0-15 */
+}
+
+inline void rpp_load16_f32_to_f32(Rpp32f *srcPtr, __m128 *p)
+{
+    p[0] = (__m128)__lsx_vld(srcPtr, 0);
+    p[1] = (__m128)__lsx_vld(srcPtr + 4, 0);
+    p[2] = (__m128)__lsx_vld(srcPtr + 8, 0);
+    p[3] = (__m128)__lsx_vld(srcPtr + 12, 0);
+}
+
+inline void rpp_load12_f32pkd3_to_f32pln3(Rpp32f *srcPtr, __m128 *p)
+{
+    p[0] = (__m128)__lsx_vld(srcPtr, 0);
+    p[1] = (__m128)__lsx_vld(srcPtr + 3, 0);
+    p[2] = (__m128)__lsx_vld(srcPtr + 6, 0);
+    p[3] = (__m128)__lsx_vld(srcPtr + 9, 0);
+    _MM_TRANSPOSE4_PS(p[0], p[1], p[2], p[3]);
+}
+
+inline void rpp_store12_f32pln3_to_f32pln3(Rpp32f *dstPtrR, Rpp32f *dstPtrG, Rpp32f *dstPtrB, __m128 *p)
+{
+    __lsx_vst(p[0], dstPtrR, 0);
+    __lsx_vst(p[1], dstPtrG, 0);
+    __lsx_vst(p[2], dstPtrB, 0);
+}
+
+inline void rpp_load12_f32pln3_to_f32pln3(Rpp32f *srcPtrR, Rpp32f *srcPtrG, Rpp32f *srcPtrB, __m128 *p)
+{
+    p[0] = (__m128)__lsx_vld(srcPtrR, 0);
+    p[1] = (__m128)__lsx_vld(srcPtrG, 0);
+    p[2] = (__m128)__lsx_vld(srcPtrB, 0);
+}
+
+inline void rpp_store12_f32pln3_to_f32pkd3(Rpp32f *dstPtr, __m128 *p)
+{
+    _MM_TRANSPOSE4_PS(p[0], p[1], p[2], p[3]);
+    __lsx_vst(p[0], dstPtr, 0);
+    __lsx_vst(p[1], dstPtr + 3, 0);
+    __lsx_vst(p[2], dstPtr + 6, 0);
+    __lsx_vst(p[3], dstPtr + 9, 0);
+}
+
+inline void rpp_load8_f32_to_f32(Rpp32f *srcPtr, __m128 *p)
+{
+    p[0] = (__m128)__lsx_vld(srcPtr, 0);
+    p[1] = (__m128)__lsx_vld(srcPtr + 4, 0);
+}
+
+inline void rpp_store8_f32_to_f32(Rpp32f *dstPtr, __m128 *p)
+{
+    __lsx_vst(p[0], dstPtr, 0);
+    __lsx_vst(p[1], dstPtr + 4, 0);
+}
+
+inline void rpp_load4_f32_to_f32(Rpp32f *srcPtr, __m128 *p)
+{
+    p[0] = (__m128)__lsx_vld(srcPtr, 0);
+}
+
+inline void rpp_store4_f32_to_f32(Rpp32f *dstPtr, __m128 *p)
+{
+    __lsx_vst(p[0], dstPtr, 0);
+}
+
+inline void rpp_load48_i8pkd3_to_f32pln3(Rpp8s *srcPtr, __m128 *p)
+{
+    __m128i px[4];
+
+    px[0] = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)srcPtr, 0));           /* add I8 conversion param to load [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|R05|G05|B05|R06] - Need RGB 01-04 */
+    px[1] = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)(srcPtr + 12), 0));    /* add I8 conversion param to load [R05|G05|B05|R06|G06|B06|R07|G07|B07|R08|G08|B08|R09|G09|B09|R10] - Need RGB 05-08 */
+    px[2] = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)(srcPtr + 24), 0));    /* add I8 conversion param to load [R09|G09|B09|R10|G10|B10|R11|G11|B11|R12|G12|B12|R13|G13|B13|R14] - Need RGB 09-12 */
+    px[3] = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)(srcPtr + 36), 0));    /* add I8 conversion param to load [R13|G13|B13|R14|G14|B14|R15|G15|B15|R16|G16|B16|R17|G17|B17|R18] - Need RGB 13-16 */
+    p[0] = __lsx_vffint_s_w(lsx_shuffle_i8(px[0], xmm_pxMaskR));    /* Contains R01-04 */
+    p[1] = __lsx_vffint_s_w(lsx_shuffle_i8(px[1], xmm_pxMaskR));    /* Contains R05-08 */
+    p[2] = __lsx_vffint_s_w(lsx_shuffle_i8(px[2], xmm_pxMaskR));    /* Contains R09-12 */
+    p[3] = __lsx_vffint_s_w(lsx_shuffle_i8(px[3], xmm_pxMaskR));    /* Contains R13-16 */
+    p[4] = __lsx_vffint_s_w(lsx_shuffle_i8(px[0], xmm_pxMaskG));    /* Contains G01-04 */
+    p[5] = __lsx_vffint_s_w(lsx_shuffle_i8(px[1], xmm_pxMaskG));    /* Contains G05-08 */
+    p[6] = __lsx_vffint_s_w(lsx_shuffle_i8(px[2], xmm_pxMaskG));    /* Contains G09-12 */
+    p[7] = __lsx_vffint_s_w(lsx_shuffle_i8(px[3], xmm_pxMaskG));    /* Contains G13-16 */
+    p[8] = __lsx_vffint_s_w(lsx_shuffle_i8(px[0], xmm_pxMaskB));    /* Contains B01-04 */
+    p[9] = __lsx_vffint_s_w(lsx_shuffle_i8(px[1], xmm_pxMaskB));    /* Contains B05-08 */
+    p[10] = __lsx_vffint_s_w(lsx_shuffle_i8(px[2], xmm_pxMaskB));    /* Contains B09-12 */
+    p[11] = __lsx_vffint_s_w(lsx_shuffle_i8(px[3], xmm_pxMaskB));    /* Contains B13-16 */
+}
+
+inline void rpp_store48_f32pln3_to_i8pln3(Rpp8s *dstPtrR, Rpp8s *dstPtrG, Rpp8s *dstPtrB, __m128 *p)
+{
+    __m128i px[8];
+
+    px[4] = __lsx_vftint_w_s(p[0]);    /* convert to int32 for R */
+    px[5] = __lsx_vftint_w_s(p[1]);    /* convert to int32 for R */
+    px[6] = __lsx_vftint_w_s(p[2]);    /* convert to int32 for R */
+    px[7] = __lsx_vftint_w_s(p[3]);    /* convert to int32 for R */
+    px[4] = lsx_packus_i32(px[4], px[5]);    /* pack pixels 0-7 for R */
+    px[5] = lsx_packus_i32(px[6], px[7]);    /* pack pixels 8-15 for R */
+    px[0] = lsx_packus_i16(px[4], px[5]);    /* pack pixels 0-15 for R */
+    px[4] = __lsx_vftint_w_s(p[4]);    /* convert to int32 for G */
+    px[5] = __lsx_vftint_w_s(p[5]);    /* convert to int32 for G */
+    px[6] = __lsx_vftint_w_s(p[6]);    /* convert to int32 for G */
+    px[7] = __lsx_vftint_w_s(p[7]);    /* convert to int32 for G */
+    px[4] = lsx_packus_i32(px[4], px[5]);    /* pack pixels 0-7 for G */
+    px[5] = lsx_packus_i32(px[6], px[7]);    /* pack pixels 8-15 for G */
+    px[1] = lsx_packus_i16(px[4], px[5]);    /* pack pixels 0-15 for G */
+    px[4] = __lsx_vftint_w_s(p[8]);    /* convert to int32 for B */
+    px[5] = __lsx_vftint_w_s(p[9]);    /* convert to int32 for B */
+    px[6] = __lsx_vftint_w_s(p[10]);    /* convert to int32 for B */
+    px[7] = __lsx_vftint_w_s(p[11]);    /* convert to int32 for B */
+    px[4] = lsx_packus_i32(px[4], px[5]);    /* pack pixels 0-7 for B */
+    px[5] = lsx_packus_i32(px[6], px[7]);    /* pack pixels 8-15 for B */
+    px[2] = lsx_packus_i16(px[4], px[5]);    /* pack pixels 0-15 for B */
+    px[0] = __lsx_vsub_b(px[0], xmm_pxConvertI8);    /* convert back to i8 for px0 store */
+    px[1] = __lsx_vsub_b(px[1], xmm_pxConvertI8);    /* convert back to i8 for px1 store */
+    px[2] = __lsx_vsub_b(px[2], xmm_pxConvertI8);    /* convert back to i8 for px2 store */
+    __lsx_vst(px[0], (__m128i *)dstPtrR, 0);    /* store [R01|R02|R03|R04|R05|R06|R07|R08|R09|R10|R11|R12|R13|R14|R15|R16] */
+    __lsx_vst(px[1], (__m128i *)dstPtrG, 0);    /* store [G01|G02|G03|G04|G05|G06|G07|G08|G09|G10|G11|G12|G13|G14|G15|G16] */
+    __lsx_vst(px[2], (__m128i *)dstPtrB, 0);    /* store [B01|B02|B03|B04|B05|B06|B07|B08|B09|B10|B11|B12|B13|B14|B15|B16] */
+}
+
+inline void rpp_load48_i8pln3_to_f32pln3(Rpp8s *srcPtrR, Rpp8s *srcPtrG, Rpp8s *srcPtrB, __m128 *p)
+{
+    __m128i px[3];
+
+    px[0] = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)srcPtrR, 0));    /* add I8 conversion param to load [R01|R02|R03|R04|R05|R06|R07|R08|R09|R10|R11|R12|R13|R14|R15|R16] */
+    px[1] = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)srcPtrG, 0));    /* add I8 conversion param to load [G01|G02|G03|G04|G05|G06|G07|G08|G09|G10|G11|G12|G13|G14|G15|G16] */
+    px[2] = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)srcPtrB, 0));    /* add I8 conversion param to load [B01|B02|B03|B04|B05|B06|B07|B08|B09|B10|B11|B12|B13|B14|B15|B16] */
+    p[0] = __lsx_vffint_s_w(lsx_shuffle_i8(px[0], xmm_pxMask00To03));    /* Contains R01-04 */
+    p[1] = __lsx_vffint_s_w(lsx_shuffle_i8(px[0], xmm_pxMask04To07));    /* Contains R05-08 */
+    p[2] = __lsx_vffint_s_w(lsx_shuffle_i8(px[0], xmm_pxMask08To11));    /* Contains R09-12 */
+    p[3] = __lsx_vffint_s_w(lsx_shuffle_i8(px[0], xmm_pxMask12To15));    /* Contains R13-16 */
+    p[4] = __lsx_vffint_s_w(lsx_shuffle_i8(px[1], xmm_pxMask00To03));    /* Contains G01-04 */
+    p[5] = __lsx_vffint_s_w(lsx_shuffle_i8(px[1], xmm_pxMask04To07));    /* Contains G05-08 */
+    p[6] = __lsx_vffint_s_w(lsx_shuffle_i8(px[1], xmm_pxMask08To11));    /* Contains G09-12 */
+    p[7] = __lsx_vffint_s_w(lsx_shuffle_i8(px[1], xmm_pxMask12To15));    /* Contains G13-16 */
+    p[8] = __lsx_vffint_s_w(lsx_shuffle_i8(px[2], xmm_pxMask00To03));    /* Contains B01-04 */
+    p[9] = __lsx_vffint_s_w(lsx_shuffle_i8(px[2], xmm_pxMask04To07));    /* Contains B05-08 */
+    p[10] = __lsx_vffint_s_w(lsx_shuffle_i8(px[2], xmm_pxMask08To11));    /* Contains B09-12 */
+    p[11] = __lsx_vffint_s_w(lsx_shuffle_i8(px[2], xmm_pxMask12To15));    /* Contains B13-16 */
+}
+
+inline void rpp_store48_f32pln3_to_i8pkd3(Rpp8s *dstPtr, __m128 *p)
+{
+    __m128i px[7];
+    __m128i pxMask = lsx_setr_i8(0, 4, 8, 1, 5, 9, 2, 6, 10, 3, 7, 11, 12, 13, 14, 15);
+    __m128i pxZero = __lsx_vldi(0);
+
+    px[4] = __lsx_vftint_w_s(p[0]);    /* convert to int32 for R01-04 */
+    px[5] = __lsx_vftint_w_s(p[4]);    /* convert to int32 for G01-04 */
+    px[6] = __lsx_vftint_w_s(p[8]);    /* convert to int32 for B01-04 */
+    px[4] = lsx_packus_i32(px[4], px[5]);    /* pack pixels 0-7 as R01-04|G01-04 */
+    px[5] = lsx_packus_i32(px[6], pxZero);    /* pack pixels 8-15 as B01-04|X01-04 */
+    px[0] = lsx_packus_i16(px[4], px[5]);    /* pack pixels 0-15 as [R01|R02|R03|R04|G01|G02|G03|G04|B01|B02|B03|B04|00|00|00|00] */
+    px[4] = __lsx_vftint_w_s(p[1]);    /* convert to int32 for R05-08 */
+    px[5] = __lsx_vftint_w_s(p[5]);    /* convert to int32 for G05-08 */
+    px[6] = __lsx_vftint_w_s(p[9]);    /* convert to int32 for B05-08 */
+    px[4] = lsx_packus_i32(px[4], px[5]);    /* pack pixels 0-7 as R05-08|G05-08 */
+    px[5] = lsx_packus_i32(px[6], pxZero);    /* pack pixels 8-15 as B05-08|X01-04 */
+    px[1] = lsx_packus_i16(px[4], px[5]);    /* pack pixels 0-15 as [R05|R06|R07|R08|G05|G06|G07|G08|B05|B06|B07|B08|00|00|00|00] */
+    px[4] = __lsx_vftint_w_s(p[2]);    /* convert to int32 for R09-12 */
+    px[5] = __lsx_vftint_w_s(p[6]);    /* convert to int32 for G09-12 */
+    px[6] = __lsx_vftint_w_s(p[10]);    /* convert to int32 for B09-12 */
+    px[4] = lsx_packus_i32(px[4], px[5]);    /* pack pixels 0-7 as R09-12|G09-12 */
+    px[5] = lsx_packus_i32(px[6], pxZero);    /* pack pixels 8-15 as B09-12|X01-04 */
+    px[2] = lsx_packus_i16(px[4], px[5]);    /* pack pixels 0-15 as [R09|R10|R11|R12|G09|G10|G11|G12|B09|B10|B11|B12|00|00|00|00] */
+    px[4] = __lsx_vftint_w_s(p[3]);    /* convert to int32 for R13-16 */
+    px[5] = __lsx_vftint_w_s(p[7]);    /* convert to int32 for G13-16 */
+    px[6] = __lsx_vftint_w_s(p[11]);    /* convert to int32 for B13-16 */
+    px[4] = lsx_packus_i32(px[4], px[5]);    /* pack pixels 0-7 as R13-16|G13-16 */
+    px[5] = lsx_packus_i32(px[6], pxZero);    /* pack pixels 8-15 as B13-16|X01-04 */
+    px[3] = lsx_packus_i16(px[4], px[5]);    /* pack pixels 0-15 as [R13|R14|R15|R16|G13|G14|G15|G16|B13|B14|B15|B16|00|00|00|00] */
+    px[0] = __lsx_vsub_b(px[0], xmm_pxConvertI8);    /* convert back to i8 for px0 store */
+    px[1] = __lsx_vsub_b(px[1], xmm_pxConvertI8);    /* convert back to i8 for px1 store */
+    px[2] = __lsx_vsub_b(px[2], xmm_pxConvertI8);    /* convert back to i8 for px2 store */
+    px[3] = __lsx_vsub_b(px[3], xmm_pxConvertI8);    /* convert back to i8 for px3 store */
+    px[0] = lsx_shuffle_i8(px[0], pxMask);    /* shuffle to get [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|00|00|00|00] */
+    px[1] = lsx_shuffle_i8(px[1], pxMask);    /* shuffle to get [R05|G05|B05|R06|G06|B06|R07|G07|B07|R08|G08|B08|00|00|00|00] */
+    px[2] = lsx_shuffle_i8(px[2], pxMask);    /* shuffle to get [R09|G09|B09|R10|G10|B10|R11|G11|B11|R12|G12|B12|00|00|00|00] */
+    px[3] = lsx_shuffle_i8(px[3], pxMask);    /* shuffle to get [R13|G13|B13|R14|G14|B14|R15|G15|B15|R16|G16|B16|00|00|00|00] */
+    __lsx_vst(px[0], (__m128i *)dstPtr, 0);           /* store [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|00|00|00|00] */
+    __lsx_vst(px[1], (__m128i *)(dstPtr + 12), 0);    /* store [R05|G05|B05|R06|G06|B06|R07|G07|B07|R08|G08|B08|00|00|00|00] */
+    __lsx_vst(px[2], (__m128i *)(dstPtr + 24), 0);    /* store [R09|G09|B09|R10|G10|B10|R11|G11|B11|R12|G12|B12|00|00|00|00] */
+    __lsx_vst(px[3], (__m128i *)(dstPtr + 36), 0);    /* store [R13|G13|B13|R14|G14|B14|R15|G15|B15|R16|G16|B16|00|00|00|00] */
+}
+
+inline void rpp_load48_i8pln3_to_i32pln3_avx(Rpp8s *srcPtrR, Rpp8s *srcPtrG, Rpp8s *srcPtrB, __m256i *p)
+{
+    __m128i px[3];
+
+    px[0] = __lsx_vld((__m128i *)srcPtrR, 0);    /* load [R01|R02|R03|R04|R05|R06|R07|R08|R09|R10|R11|R12|R13|R14|R15|R16] */
+    px[1] = __lsx_vld((__m128i *)srcPtrG, 0);    /* load [G01|G02|G03|G04|G05|G06|G07|G08|G09|G10|G11|G12|G13|G14|G15|G16] */
+    px[2] = __lsx_vld((__m128i *)srcPtrB, 0);    /* load [B01|B02|B03|B04|B05|B06|B07|B08|B09|B10|B11|B12|B13|B14|B15|B16] */
+    p[0] = lasx_cvti8_i32(px[0]);                                        /* Contains R01-08 */
+    p[1] = lasx_cvti8_i32(lsx_shuffle_i8(px[0], xmm_pxMask08To15));    /* Contains R09-16 */
+    p[2] = lasx_cvti8_i32(px[1]);                                        /* Contains G01-08 */
+    p[3] = lasx_cvti8_i32(lsx_shuffle_i8(px[1], xmm_pxMask08To15));    /* Contains G09-16 */
+    p[4] = lasx_cvti8_i32(px[2]);                                        /* Contains B01-08 */
+    p[5] = lasx_cvti8_i32(lsx_shuffle_i8(px[2], xmm_pxMask08To15));    /* Contains B09-16 */
+}
+
+inline void rpp_load48_i8pkd3_to_i8pln3(Rpp8s *srcPtr, __m128i *px)
+{
+    __m128i pxSrc[8];
+    __m128i pxMask = lsx_setr_i8(0, 3, 6, 9, 1, 4, 7, 10, 2, 5, 8, 11, 12, 13, 14, 15);
+    __m128i pxMaskRGB = lsx_setr_i8(0, 4, 8, 12, 2, 6, 10, 14, 1, 5, 9, 13, 3, 7, 11, 15);
+
+    pxSrc[0] = __lsx_vld((__m128i *)srcPtr, 0);           /* load [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|R05|G05|B05|R06] - Need RGB 01-04 */
+    pxSrc[1] = __lsx_vld((__m128i *)(srcPtr + 12), 0);    /* load [R05|G05|B05|R06|G06|B06|R07|G07|B07|R08|G08|B08|R09|G09|B09|R10] - Need RGB 05-08 */
+    pxSrc[2] = __lsx_vld((__m128i *)(srcPtr + 24), 0);    /* load [R09|G09|B09|R10|G10|B10|R11|G11|B11|R12|G12|B12|R13|G13|B13|R14] - Need RGB 09-12 */
+    pxSrc[3] = __lsx_vld((__m128i *)(srcPtr + 36), 0);    /* load [R13|G13|B13|R14|G14|B14|R15|G15|B15|R16|G16|B16|R17|G17|B17|R18] - Need RGB 13-16 */
+    pxSrc[0] = lsx_shuffle_i8(pxSrc[0], pxMask);    /* shuffle to get [R01|R02|R03|R04|G01|G02|G03|G04 || B01|B02|B03|B04|R05|G05|B05|R06] - Need R01-04, G01-04, B01-04 */
+    pxSrc[1] = lsx_shuffle_i8(pxSrc[1], pxMask);    /* shuffle to get [R05|R06|R07|R08|G05|G06|G07|G08 || B05|B06|B07|B08|R09|G09|B09|R10] - Need R05-08, G05-08, B05-08 */
+    pxSrc[2] = lsx_shuffle_i8(pxSrc[2], pxMask);    /* shuffle to get [R09|R10|R11|R12|G09|G10|G11|G12 || B09|B10|B11|B12|R13|G13|B13|R14] - Need R09-12, G09-12, B09-12 */
+    pxSrc[3] = lsx_shuffle_i8(pxSrc[3], pxMask);    /* shuffle to get [R13|R14|R15|R16|G13|G14|G15|G16 || B13|B14|B15|B16|R17|G17|B17|R18] - Need R13-16, G13-16, B13-16 */
+    pxSrc[4] = __lsx_vilvl_b(pxSrc[1], pxSrc[0]);    /* unpack 8 lo-pixels of pxSrc[0] and pxSrc[1] */
+    pxSrc[5] = __lsx_vilvl_b(pxSrc[3], pxSrc[2]);    /* unpack 8 lo-pixels of pxSrc[2] and pxSrc[3] */
+    pxSrc[6] = __lsx_vilvh_b(pxSrc[1], pxSrc[0]);    /* unpack 8 hi-pixels of pxSrc[0] and pxSrc[1] */
+    pxSrc[7] = __lsx_vilvh_b(pxSrc[3], pxSrc[2]);    /* unpack 8 hi-pixels of pxSrc[2] and pxSrc[3] */
+    px[0] = lsx_shuffle_i8(__lsx_vilvl_b(pxSrc[5], pxSrc[4]), pxMaskRGB);    /* unpack 8 lo-pixels of pxSrc[4] and pxSrc[5] to get R01-16 */
+    px[1] = lsx_shuffle_i8(__lsx_vilvh_b(pxSrc[5], pxSrc[4]), pxMaskRGB);    /* unpack 8 hi-pixels of pxSrc[4] and pxSrc[5] to get G01-16 */
+    px[2] = lsx_shuffle_i8(__lsx_vilvl_b(pxSrc[7], pxSrc[6]), pxMaskRGB);    /* unpack 8 lo-pixels of pxSrc[6] and pxSrc[7] to get B01-16 */
+}
+
+inline void rpp_load48_i8pkd3_to_u8pln3(Rpp8s *srcPtr, __m128i *px)
+{
+    __m128i pxSrc[8];
+    __m128i pxMask = lsx_setr_i8(0, 3, 6, 9, 1, 4, 7, 10, 2, 5, 8, 11, 12, 13, 14, 15);
+    __m128i pxMaskRGB = lsx_setr_i8(0, 4, 8, 12, 2, 6, 10, 14, 1, 5, 9, 13, 3, 7, 11, 15);
+
+    pxSrc[0] = __lsx_vld((__m128i *)srcPtr, 0);           /* load [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|R05|G05|B05|R06] - Need RGB 01-04 */
+    pxSrc[1] = __lsx_vld((__m128i *)(srcPtr + 12), 0);    /* load [R05|G05|B05|R06|G06|B06|R07|G07|B07|R08|G08|B08|R09|G09|B09|R10] - Need RGB 05-08 */
+    pxSrc[2] = __lsx_vld((__m128i *)(srcPtr + 24), 0);    /* load [R09|G09|B09|R10|G10|B10|R11|G11|B11|R12|G12|B12|R13|G13|B13|R14] - Need RGB 09-12 */
+    pxSrc[3] = __lsx_vld((__m128i *)(srcPtr + 36), 0);    /* load [R13|G13|B13|R14|G14|B14|R15|G15|B15|R16|G16|B16|R17|G17|B17|R18] - Need RGB 13-16 */
+    pxSrc[0] = lsx_shuffle_i8(pxSrc[0], pxMask);    /* shuffle to get [R01|R02|R03|R04|G01|G02|G03|G04 || B01|B02|B03|B04|R05|G05|B05|R06] - Need R01-04, G01-04, B01-04 */
+    pxSrc[1] = lsx_shuffle_i8(pxSrc[1], pxMask);    /* shuffle to get [R05|R06|R07|R08|G05|G06|G07|G08 || B05|B06|B07|B08|R09|G09|B09|R10] - Need R05-08, G05-08, B05-08 */
+    pxSrc[2] = lsx_shuffle_i8(pxSrc[2], pxMask);    /* shuffle to get [R09|R10|R11|R12|G09|G10|G11|G12 || B09|B10|B11|B12|R13|G13|B13|R14] - Need R09-12, G09-12, B09-12 */
+    pxSrc[3] = lsx_shuffle_i8(pxSrc[3], pxMask);    /* shuffle to get [R13|R14|R15|R16|G13|G14|G15|G16 || B13|B14|B15|B16|R17|G17|B17|R18] - Need R13-16, G13-16, B13-16 */
+    pxSrc[4] = __lsx_vilvl_b(pxSrc[1], pxSrc[0]);    /* unpack 8 lo-pixels of pxSrc[0] and pxSrc[1] */
+    pxSrc[5] = __lsx_vilvl_b(pxSrc[3], pxSrc[2]);    /* unpack 8 lo-pixels of pxSrc[2] and pxSrc[3] */
+    pxSrc[6] = __lsx_vilvh_b(pxSrc[1], pxSrc[0]);    /* unpack 8 hi-pixels of pxSrc[0] and pxSrc[1] */
+    pxSrc[7] = __lsx_vilvh_b(pxSrc[3], pxSrc[2]);    /* unpack 8 hi-pixels of pxSrc[2] and pxSrc[3] */
+    px[0] = __lsx_vadd_b(xmm_pxConvertI8, lsx_shuffle_i8(__lsx_vilvl_b(pxSrc[5], pxSrc[4]), pxMaskRGB));    /* unpack 8 lo-pixels of pxSrc[4] and pxSrc[5] to get R01-16 and add 128 to get u8 from i8 */
+    px[1] = __lsx_vadd_b(xmm_pxConvertI8, lsx_shuffle_i8(__lsx_vilvh_b(pxSrc[5], pxSrc[4]), pxMaskRGB));    /* unpack 8 hi-pixels of pxSrc[4] and pxSrc[5] to get G01-16 and add 128 to get u8 from i8 */
+    px[2] = __lsx_vadd_b(xmm_pxConvertI8, lsx_shuffle_i8(__lsx_vilvl_b(pxSrc[7], pxSrc[6]), pxMaskRGB));    /* unpack 8 lo-pixels of pxSrc[6] and pxSrc[7] to get B01-16 and add 128 to get u8 from i8 */
+}
+
+inline void rpp_store48_i8pln3_to_i8pln3(Rpp8s *dstPtrR, Rpp8s *dstPtrG, Rpp8s *dstPtrB, __m128i *px)
+{
+    __lsx_vst(px[0], (__m128i *)dstPtrR, 0);    /* store [R01|R02|R03|R04|R05|R06|R07|R08|R09|R10|R11|R12|R13|R14|R15|R16] */
+    __lsx_vst(px[1], (__m128i *)dstPtrG, 0);    /* store [G01|G02|G03|G04|G05|G06|G07|G08|G09|G10|G11|G12|G13|G14|G15|G16] */
+    __lsx_vst(px[2], (__m128i *)dstPtrB, 0);    /* store [B01|B02|B03|B04|B05|B06|B07|B08|B09|B10|B11|B12|B13|B14|B15|B16] */
+}
+
+inline void rpp_store48_u8pln3_to_i8pln3(Rpp8s *dstPtrR, Rpp8s *dstPtrG, Rpp8s *dstPtrB, __m128i *px)
+{
+    __lsx_vst(__lsx_vsub_b(px[0], xmm_pxConvertI8), (__m128i *)dstPtrR, 0);    /* store [R01|R02|R03|R04|R05|R06|R07|R08|R09|R10|R11|R12|R13|R14|R15|R16] */
+    __lsx_vst(__lsx_vsub_b(px[1], xmm_pxConvertI8), (__m128i *)dstPtrG, 0);    /* store [G01|G02|G03|G04|G05|G06|G07|G08|G09|G10|G11|G12|G13|G14|G15|G16] */
+    __lsx_vst(__lsx_vsub_b(px[2], xmm_pxConvertI8), (__m128i *)dstPtrB, 0);    /* store [B01|B02|B03|B04|B05|B06|B07|B08|B09|B10|B11|B12|B13|B14|B15|B16] */
+}
+
+inline void rpp_load48_i8pkd3_to_i32pln3_avx(Rpp8s *srcPtr, __m256i *p)
+{
+    __m128i pxSrc[8];
+    __m128i pxMask = lsx_setr_i8(0, 3, 6, 9, 1, 4, 7, 10, 2, 5, 8, 11, 12, 13, 14, 15);
+    __m128i pxMaskRGB = lsx_setr_i8(0, 4, 8, 12, 2, 6, 10, 14, 1, 5, 9, 13, 3, 7, 11, 15);
+
+    pxSrc[0] = __lsx_vld((__m128i *)srcPtr, 0);           /* load [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|R05|G05|B05|R06] - Need RGB 01-04 */
+    pxSrc[1] = __lsx_vld((__m128i *)(srcPtr + 12), 0);    /* load [R05|G05|B05|R06|G06|B06|R07|G07|B07|R08|G08|B08|R09|G09|B09|R10] - Need RGB 05-08 */
+    pxSrc[2] = __lsx_vld((__m128i *)(srcPtr + 24), 0);    /* load [R09|G09|B09|R10|G10|B10|R11|G11|B11|R12|G12|B12|R13|G13|B13|R14] - Need RGB 09-12 */
+    pxSrc[3] = __lsx_vld((__m128i *)(srcPtr + 36), 0);    /* load [R13|G13|B13|R14|G14|B14|R15|G15|B15|R16|G16|B16|R17|G17|B17|R18] - Need RGB 13-16 */
+    pxSrc[0] = lsx_shuffle_i8(pxSrc[0], pxMask);    /* shuffle to get [R01|R02|R03|R04|G01|G02|G03|G04 || B01|B02|B03|B04|R05|G05|B05|R06] - Need R01-04, G01-04, B01-04 */
+    pxSrc[1] = lsx_shuffle_i8(pxSrc[1], pxMask);    /* shuffle to get [R05|R06|R07|R08|G05|G06|G07|G08 || B05|B06|B07|B08|R09|G09|B09|R10] - Need R05-08, G05-08, B05-08 */
+    pxSrc[2] = lsx_shuffle_i8(pxSrc[2], pxMask);    /* shuffle to get [R09|R10|R11|R12|G09|G10|G11|G12 || B09|B10|B11|B12|R13|G13|B13|R14] - Need R09-12, G09-12, B09-12 */
+    pxSrc[3] = lsx_shuffle_i8(pxSrc[3], pxMask);    /* shuffle to get [R13|R14|R15|R16|G13|G14|G15|G16 || B13|B14|B15|B16|R17|G17|B17|R18] - Need R13-16, G13-16, B13-16 */
+    pxSrc[4] = __lsx_vilvl_w(pxSrc[1], pxSrc[0]); /* unpack 32 lo-pixels of pxSrc[0] and pxSrc[1] */
+    pxSrc[5] = __lsx_vilvh_w(pxSrc[1], pxSrc[0]); /* unpack 32 hi-pixels of pxSrc[0] and pxSrc[1] */
+    pxSrc[6] = __lsx_vilvl_w(pxSrc[3], pxSrc[2]); /* unpack 32 lo-pixels of pxSrc[2] and pxSrc[3] */
+    pxSrc[7] = __lsx_vilvh_w(pxSrc[3], pxSrc[2]); /* unpack 32 hi-pixels of pxSrc[2] and pxSrc[3] */
+    p[0] = lasx_cvti8_i32(pxSrc[4]);                                        /* Contains R01-08 */
+    p[1] = lasx_cvti8_i32(pxSrc[6]);                                        /* Contains R09-16 */
+    p[2] = lasx_cvti8_i32(lsx_shuffle_i8(pxSrc[4], xmm_pxMask08To15));    /* Contains G01-08 */
+    p[3] = lasx_cvti8_i32(lsx_shuffle_i8(pxSrc[6], xmm_pxMask08To15));    /* Contains G09-16 */
+    p[4] = lasx_cvti8_i32(pxSrc[5]);                                        /* Contains B01-08 */
+    p[5] = lasx_cvti8_i32(pxSrc[7]);                                        /* Contains B09-16 */
+}
+
+inline void rpp_load48_i8pln3_to_i8pln3(Rpp8s *srcPtrR, Rpp8s *srcPtrG, Rpp8s *srcPtrB, __m128i *px)
+{
+    px[0] = __lsx_vld((__m128i *)srcPtrR, 0);    /* load [R01|R02|R03|R04|R05|R06|R07|R08|R09|R10|R11|R12|R13|R14|R15|R16] */
+    px[1] = __lsx_vld((__m128i *)srcPtrG, 0);    /* load [G01|G02|G03|G04|G05|G06|G07|G08|G09|G10|G11|G12|G13|G14|G15|G16] */
+    px[2] = __lsx_vld((__m128i *)srcPtrB, 0);    /* load [B01|B02|B03|B04|B05|B06|B07|B08|B09|B10|B11|B12|B13|B14|B15|B16] */
+}
+
+inline void rpp_load48_i8pln3_to_u8pln3(Rpp8s *srcPtrR, Rpp8s *srcPtrG, Rpp8s *srcPtrB, __m128i *px)
+{
+    px[0] = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)srcPtrR, 0));    /* load and convert to u8 [R01|R02|R03|R04|R05|R06|R07|R08|R09|R10|R11|R12|R13|R14|R15|R16] */
+    px[1] = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)srcPtrG, 0));    /* load and convert to u8 [G01|G02|G03|G04|G05|G06|G07|G08|G09|G10|G11|G12|G13|G14|G15|G16] */
+    px[2] = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)srcPtrB, 0));    /* load and convert to u8 [B01|B02|B03|B04|B05|B06|B07|B08|B09|B10|B11|B12|B13|B14|B15|B16] */
+}
+
+inline void rpp_store48_i8pln3_to_i8pkd3(Rpp8s *dstPtr, __m128i *px)
+{
+    __m128i pxDst[4];
+    __m128i pxZero = __lsx_vldi(0);
+    __m128i pxMaskRGBAtoRGB = lsx_setr_i8(0, 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 14, 3, 7, 11, 15);
+    pxDst[0] = __lsx_vilvl_b(pxZero, px[1]);    /* unpack 8 lo-pixels of px[1] and pxZero */
+    pxDst[1] = __lsx_vilvh_b(pxZero, px[1]);    /* unpack 8 hi-pixels of px[1] and pxZero */
+    pxDst[2] = __lsx_vilvl_b(px[2], px[0]);    /* unpack 8 lo-pixels of px[0] and px[2] */
+    pxDst[3] = __lsx_vilvh_b(px[2], px[0]);    /* unpack 8 hi-pixels of px[0] and px[2] */
+    __lsx_vst(lsx_shuffle_i8(__lsx_vilvl_b(pxDst[0], pxDst[2]), pxMaskRGBAtoRGB), (__m128i *)dstPtr, 0);     /* store [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|00|00|00|00] */
+    __lsx_vst(lsx_shuffle_i8(__lsx_vilvh_b(pxDst[0], pxDst[2]), pxMaskRGBAtoRGB), (__m128i *)(dstPtr + 12), 0);    /* store [R05|G05|B05|R06|G06|B06|R07|G07|B07|R08|G08|B08|00|00|00|00] */
+    __lsx_vst(lsx_shuffle_i8(__lsx_vilvl_b(pxDst[1], pxDst[3]), pxMaskRGBAtoRGB), (__m128i *)(dstPtr + 24), 0);    /* store [R09|G09|B09|R10|G10|B10|R11|G11|B11|R12|G12|B12|00|00|00|00] */
+    __lsx_vst(lsx_shuffle_i8(__lsx_vilvh_b(pxDst[1], pxDst[3]), pxMaskRGBAtoRGB), (__m128i *)(dstPtr + 36), 0);    /* store [R13|G13|B13|R14|G14|B14|R15|G15|B15|R16|G16|B16|00|00|00|00] */
+}
+
+inline void rpp_store48_u8pln3_to_i8pkd3(Rpp8s *dstPtr, __m128i *px)
+{
+    __m128i pxDst[4];
+    __m128i pxZero = __lsx_vldi(0);
+    __m128i pxMaskRGBAtoRGB = lsx_setr_i8(0, 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 14, 3, 7, 11, 15);
+    pxDst[0] = __lsx_vilvl_b(pxZero, px[1]);    /* unpack 8 lo-pixels of px[1] and pxZero */
+    pxDst[1] = __lsx_vilvh_b(pxZero, px[1]);    /* unpack 8 hi-pixels of px[1] and pxZero */
+    pxDst[2] = __lsx_vilvl_b(px[2], px[0]);    /* unpack 8 lo-pixels of px[0] and px[2] */
+    pxDst[3] = __lsx_vilvh_b(px[2], px[0]);    /* unpack 8 hi-pixels of px[0] and px[2] */
+    __lsx_vst(__lsx_vsub_b(lsx_shuffle_i8(__lsx_vilvl_b(pxDst[0], pxDst[2]), pxMaskRGBAtoRGB), xmm_pxConvertI8), (__m128i *)dstPtr, 0);           /* store [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|00|00|00|00] */
+    __lsx_vst(__lsx_vsub_b(lsx_shuffle_i8(__lsx_vilvh_b(pxDst[0], pxDst[2]), pxMaskRGBAtoRGB), xmm_pxConvertI8), (__m128i *)(dstPtr + 12), 0);    /* store [R05|G05|B05|R06|G06|B06|R07|G07|B07|R08|G08|B08|00|00|00|00] */
+    __lsx_vst(__lsx_vsub_b(lsx_shuffle_i8(__lsx_vilvl_b(pxDst[1], pxDst[3]), pxMaskRGBAtoRGB), xmm_pxConvertI8), (__m128i *)(dstPtr + 24), 0);    /* store [R09|G09|B09|R10|G10|B10|R11|G11|B11|R12|G12|B12|00|00|00|00] */
+    __lsx_vst(__lsx_vsub_b(lsx_shuffle_i8(__lsx_vilvh_b(pxDst[1], pxDst[3]), pxMaskRGBAtoRGB), xmm_pxConvertI8), (__m128i *)(dstPtr + 36), 0);    /* store [R13|G13|B13|R14|G14|B14|R15|G15|B15|R16|G16|B16|00|00|00|00] */
+}
+
+inline void rpp_load16_i8_to_f32(Rpp8s *srcPtr, __m128 *p)
+{
+    __m128i px = __lsx_vld((__m128i *)srcPtr, 0);    /* load pixels 0-15 */
+    px = __lsx_vadd_b(px, xmm_pxConvertI8);    /* convert to u8 for px compute */
+    p[0] = __lsx_vffint_s_w(lsx_shuffle_i8(px, xmm_pxMask00To03));    /* pixels 0-3 */
+    p[1] = __lsx_vffint_s_w(lsx_shuffle_i8(px, xmm_pxMask04To07));    /* pixels 4-7 */
+    p[2] = __lsx_vffint_s_w(lsx_shuffle_i8(px, xmm_pxMask08To11));    /* pixels 8-11 */
+    p[3] = __lsx_vffint_s_w(lsx_shuffle_i8(px, xmm_pxMask12To15));    /* pixels 12-15 */
+}
+
+inline void rpp_store16_f32_to_i8(Rpp8s *dstPtr, __m128 *p)
+{
+    __m128i px[4];
+
+    px[0] = __lsx_vftint_w_s(p[0]);    /* pixels 0-3 */
+    px[1] = __lsx_vftint_w_s(p[1]);    /* pixels 4-7 */
+    px[2] = __lsx_vftint_w_s(p[2]);    /* pixels 8-11 */
+    px[3] = __lsx_vftint_w_s(p[3]);    /* pixels 12-15 */
+    px[0] = lsx_packus_i32(px[0], px[1]);    /* pixels 0-7 */
+    px[1] = lsx_packus_i32(px[2], px[3]);    /* pixels 8-15 */
+    px[0] = lsx_packus_i16(px[0], px[1]);    /* pixels 0-15 */
+    px[0] = __lsx_vsub_b(px[0], xmm_pxConvertI8);    /* convert back to i8 for px0 store */
+    __lsx_vst(px[0], (__m128i *)dstPtr, 0);    /* store pixels 0-15 */
+}
+
+inline void rpp_normalize48(__m128 *p)
+{
+    p[0] = __lsx_vfmul_s(p[0], xmm_p1op255);
+    p[1] = __lsx_vfmul_s(p[1], xmm_p1op255);
+    p[2] = __lsx_vfmul_s(p[2], xmm_p1op255);
+    p[3] = __lsx_vfmul_s(p[3], xmm_p1op255);
+    p[4] = __lsx_vfmul_s(p[4], xmm_p1op255);
+    p[5] = __lsx_vfmul_s(p[5], xmm_p1op255);
+    p[6] = __lsx_vfmul_s(p[6], xmm_p1op255);
+    p[7] = __lsx_vfmul_s(p[7], xmm_p1op255);
+    p[8] = __lsx_vfmul_s(p[8], xmm_p1op255);
+    p[9] = __lsx_vfmul_s(p[9], xmm_p1op255);
+    p[10] = __lsx_vfmul_s(p[10], xmm_p1op255);
+    p[11] = __lsx_vfmul_s(p[11], xmm_p1op255);
+}
+
+// AVX loads and stores
+
+inline void rpp_load48_u8pkd3_to_f32pkd3_avx(Rpp8u *srcPtr, __m256 *p)
+{
+    __m128i px[4];
+
+    px[0] = __lsx_vld((__m128i *)srcPtr, 0);           /* load [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|R05|G05|B05|R06] - Need RGB 01-04 */
+    px[1] = __lsx_vld((__m128i *)(srcPtr + 12), 0);    /* load [R05|G05|B05|R06|G06|B06|R07|G07|B07|R08|G08|B08|R09|G09|B09|R10] - Need RGB 05-08 */
+    px[2] = __lsx_vld((__m128i *)(srcPtr + 24), 0);    /* load [R09|G09|B09|R10|G10|B10|R11|G11|B11|R12|G12|B12|R13|G13|B13|R14] - Need RGB 09-12 */
+    px[3] = __lsx_vld((__m128i *)(srcPtr + 36), 0);    /* load [R13|G13|B13|R14|G14|B14|R15|G15|B15|R16|G16|B16|R17|G17|B17|R18] - Need RGB 13-16 */
+
+    p[0] =  __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[0], xmm_pxMask00To02), lsx_shuffle_i8(px[0], xmm_pxMask03To05)));    /* Contains RGB 01-02 */
+    p[1] =  __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[0], xmm_pxMask06To08), lsx_shuffle_i8(px[0], xmm_pxMask09To11)));    /* Contains RGB 03-04 */
+    p[2] =  __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[1], xmm_pxMask00To02), lsx_shuffle_i8(px[1], xmm_pxMask03To05)));    /* Contains RGB 05-06 */
+    p[3] =  __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[1], xmm_pxMask06To08), lsx_shuffle_i8(px[1], xmm_pxMask09To11)));    /* Contains RGB 07-08 */
+    p[4] =  __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMask00To02), lsx_shuffle_i8(px[2], xmm_pxMask03To05)));    /* Contains RGB 09-10 */
+    p[5] =  __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMask06To08), lsx_shuffle_i8(px[2], xmm_pxMask09To11)));    /* Contains RGB 11-12 */
+    p[6] =  __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[3], xmm_pxMask00To02), lsx_shuffle_i8(px[3], xmm_pxMask03To05)));    /* Contains RGB 13-14 */
+    p[7] =  __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[3], xmm_pxMask06To08), lsx_shuffle_i8(px[3], xmm_pxMask09To11)));    /* Contains RGB 15-16 */
+}
+
+inline void rpp_load48_u8pkd3_to_f32pkd3_mirror_avx(Rpp8u *srcPtr, __m256 *p)
+{
+    __m128i px[4];
+
+    px[0] = __lsx_vld((__m128i *)srcPtr, 0);           /* load [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|R05|G05|B05|R06] - Need RGB 01-04 */
+    px[1] = __lsx_vld((__m128i *)(srcPtr + 12), 0);    /* load [R05|G05|B05|R06|G06|B06|R07|G07|B07|R08|G08|B08|R09|G09|B09|R10] - Need RGB 05-08 */
+    px[2] = __lsx_vld((__m128i *)(srcPtr + 24), 0);    /* load [R09|G09|B09|R10|G10|B10|R11|G11|B11|R12|G12|B12|R13|G13|B13|R14] - Need RGB 09-12 */
+    px[3] = __lsx_vld((__m128i *)(srcPtr + 36), 0);    /* load [R13|G13|B13|R14|G14|B14|R15|G15|B15|R16|G16|B16|R17|G17|B17|R18] - Need RGB 13-16 */
+
+    p[0] =  __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[3], xmm_pxMask09To11), lsx_shuffle_i8(px[3], xmm_pxMask06To08)));    /* Contains RGB 16-15 */
+    p[1] =  __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[3], xmm_pxMask03To05), lsx_shuffle_i8(px[3], xmm_pxMask00To02)));    /* Contains RGB 14-13 */
+    p[2] =  __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMask09To11), lsx_shuffle_i8(px[2], xmm_pxMask06To08)));    /* Contains RGB 12-11 */
+    p[3] =  __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMask03To05), lsx_shuffle_i8(px[2], xmm_pxMask00To02)));    /* Contains RGB 10-09 */
+    p[4] =  __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[1], xmm_pxMask09To11), lsx_shuffle_i8(px[1], xmm_pxMask06To08)));    /* Contains RGB 08-07 */
+    p[5] =  __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[1], xmm_pxMask03To05), lsx_shuffle_i8(px[1], xmm_pxMask00To02)));    /* Contains RGB 06-05 */
+    p[6] =  __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[0], xmm_pxMask09To11), lsx_shuffle_i8(px[0], xmm_pxMask06To08)));    /* Contains RGB 04-03 */
+    p[7] =  __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[0], xmm_pxMask03To05), lsx_shuffle_i8(px[0], xmm_pxMask00To02)));    /* Contains RGB 02-01 */
+}
+
+inline void rpp_store48_f32pkd3_to_f32pkd3_avx(Rpp32f *dstPtr, __m256 *p)
+{
+    __m128 p128[4];
+    p128[0] = lasx_extractf128_f32(p[0], 0);
+    p128[1] = lasx_extractf128_f32(p[0], 1);
+    p128[2] = lasx_extractf128_f32(p[1], 0);
+    p128[3] = lasx_extractf128_f32(p[1], 1);
+    __lsx_vst(p128[0], dstPtr, 0);
+    __lsx_vst(p128[1], dstPtr + 3, 0);
+    __lsx_vst(p128[2], dstPtr + 6, 0);
+    __lsx_vst(p128[3], dstPtr + 9, 0);
+
+    p128[0] = lasx_extractf128_f32(p[2], 0);
+    p128[1] = lasx_extractf128_f32(p[2], 1);
+    p128[2] = lasx_extractf128_f32(p[3], 0);
+    p128[3] = lasx_extractf128_f32(p[3], 1);
+    __lsx_vst(p128[0], dstPtr + 12, 0);
+    __lsx_vst(p128[1], dstPtr + 15, 0);
+    __lsx_vst(p128[2], dstPtr + 18, 0);
+    __lsx_vst(p128[3], dstPtr + 21, 0);
+
+    p128[0] = lasx_extractf128_f32(p[4], 0);
+    p128[1] = lasx_extractf128_f32(p[4], 1);
+    p128[2] = lasx_extractf128_f32(p[5], 0);
+    p128[3] = lasx_extractf128_f32(p[5], 1);
+    __lsx_vst(p128[0], dstPtr + 24, 0);
+    __lsx_vst(p128[1], dstPtr + 27, 0);
+    __lsx_vst(p128[2], dstPtr + 30, 0);
+    __lsx_vst(p128[3], dstPtr + 33, 0);
+
+    p128[0] = lasx_extractf128_f32(p[6], 0);
+    p128[1] = lasx_extractf128_f32(p[6], 1);
+    p128[2] = lasx_extractf128_f32(p[7], 0);
+    p128[3] = lasx_extractf128_f32(p[7], 1);
+    __lsx_vst(p128[0], dstPtr + 36, 0);
+    __lsx_vst(p128[1], dstPtr + 39, 0);
+    __lsx_vst(p128[2], dstPtr + 42, 0);
+    __lsx_vst(p128[3], dstPtr + 45, 0);
+}
+
+inline void rpp_store48_f32pkd3_to_f16pkd3_avx(Rpp16f *dstPtr, __m256 *p)
+{
+    __m128 p128[4];
+    p128[0] = lasx_extractf128_f32(p[0], 0);
+    p128[1] = lasx_extractf128_f32(p[0], 1);
+    p128[2] = lasx_extractf128_f32(p[1], 0);
+    p128[3] = lasx_extractf128_f32(p[1], 1);
+
+    __m128i px128[4];
+    px128[0] = __lsx_vfcvtrxxx_h_s(p128[0], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    px128[1] = __lsx_vfcvtrxxx_h_s(p128[1], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    px128[2] = __lsx_vfcvtrxxx_h_s(p128[2], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    px128[3] = __lsx_vfcvtrxxx_h_s(p128[3], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    __lsx_vst(px128[0], (__m128i *)dstPtr, 0);
+    __lsx_vst(px128[1], (__m128i *)(dstPtr + 3), 0);
+    __lsx_vst(px128[2], (__m128i *)(dstPtr + 6), 0);
+    __lsx_vst(px128[3], (__m128i *)(dstPtr + 9), 0);
+
+    p128[0] = lasx_extractf128_f32(p[2], 0);
+    p128[1] = lasx_extractf128_f32(p[2], 1);
+    p128[2] = lasx_extractf128_f32(p[3], 0);
+    p128[3] = lasx_extractf128_f32(p[3], 1);
+    px128[0] = __lsx_vfcvtrxxx_h_s(p128[0], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    px128[1] = __lsx_vfcvtrxxx_h_s(p128[1], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    px128[2] = __lsx_vfcvtrxxx_h_s(p128[2], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    px128[3] = __lsx_vfcvtrxxx_h_s(p128[3], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    __lsx_vst(px128[0], (__m128i *)(dstPtr + 12), 0);
+    __lsx_vst(px128[1], (__m128i *)(dstPtr + 15), 0);
+    __lsx_vst(px128[2], (__m128i *)(dstPtr + 18), 0);
+    __lsx_vst(px128[3], (__m128i *)(dstPtr + 21), 0);
+
+    p128[0] = lasx_extractf128_f32(p[4], 0);
+    p128[1] = lasx_extractf128_f32(p[4], 1);
+    p128[2] = lasx_extractf128_f32(p[5], 0);
+    p128[3] = lasx_extractf128_f32(p[5], 1);
+    px128[0] = __lsx_vfcvtrxxx_h_s(p128[0], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    px128[1] = __lsx_vfcvtrxxx_h_s(p128[1], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    px128[2] = __lsx_vfcvtrxxx_h_s(p128[2], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    px128[3] = __lsx_vfcvtrxxx_h_s(p128[3], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    __lsx_vst(px128[0], (__m128i *)(dstPtr + 24), 0);
+    __lsx_vst(px128[1], (__m128i *)(dstPtr + 27), 0);
+    __lsx_vst(px128[2], (__m128i *)(dstPtr + 30), 0);
+    __lsx_vst(px128[3], (__m128i *)(dstPtr + 33), 0);
+
+    p128[0] = lasx_extractf128_f32(p[6], 0);
+    p128[1] = lasx_extractf128_f32(p[6], 1);
+    p128[2] = lasx_extractf128_f32(p[7], 0);
+    p128[3] = lasx_extractf128_f32(p[7], 1);
+    px128[0] = __lsx_vfcvtrxxx_h_s(p128[0], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    px128[1] = __lsx_vfcvtrxxx_h_s(p128[1], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    px128[2] = __lsx_vfcvtrxxx_h_s(p128[2], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    px128[3] = __lsx_vfcvtrxxx_h_s(p128[3], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    __lsx_vst(px128[0], (__m128i *)(dstPtr + 36), 0);
+    __lsx_vst(px128[1], (__m128i *)(dstPtr + 39), 0);
+    __lsx_vst(px128[2], (__m128i *)(dstPtr + 42), 0);
+    __lsx_vst(px128[3], (__m128i *)(dstPtr + 45), 0);
+}
+
+inline void rpp_load48_u8pkd3_to_f32pln3_avx(Rpp8u *srcPtr, __m256 *p)
+{
+    __m128i px[4];
+
+    px[0] = __lsx_vld((__m128i *)srcPtr, 0);           /* load [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|R05|G05|B05|R06] - Need RGB 01-04 */
+    px[1] = __lsx_vld((__m128i *)(srcPtr + 12), 0);    /* load [R05|G05|B05|R06|G06|B06|R07|G07|B07|R08|G08|B08|R09|G09|B09|R10] - Need RGB 05-08 */
+    px[2] = __lsx_vld((__m128i *)(srcPtr + 24), 0);    /* load [R09|G09|B09|R10|G10|B10|R11|G11|B11|R12|G12|B12|R13|G13|B13|R14] - Need RGB 09-12 */
+    px[3] = __lsx_vld((__m128i *)(srcPtr + 36), 0);    /* load [R13|G13|B13|R14|G14|B14|R15|G15|B15|R16|G16|B16|R17|G17|B17|R18] - Need RGB 13-16 */
+    p[0] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[0], xmm_pxMaskR), lsx_shuffle_i8(px[1], xmm_pxMaskR)));    /* Contains R01-08 */
+    p[1] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMaskR), lsx_shuffle_i8(px[3], xmm_pxMaskR)));    /* Contains R09-16 */
+    p[2] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[0], xmm_pxMaskG), lsx_shuffle_i8(px[1], xmm_pxMaskG)));    /* Contains G01-08 */
+    p[3] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMaskG), lsx_shuffle_i8(px[3], xmm_pxMaskG)));    /* Contains G09-16 */
+    p[4] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[0], xmm_pxMaskB), lsx_shuffle_i8(px[1], xmm_pxMaskB)));    /* Contains B01-08 */
+    p[5] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMaskB), lsx_shuffle_i8(px[3], xmm_pxMaskB)));    /* Contains B09-16 */
+}
+
+inline void rpp_glitch_load24_u8pkd3_to_f32pln3_avx(Rpp8u *srcPtr, __m256 *p, int *srcLocs)
+{
+    __m128i px[2];
+    px[0] = __lsx_vld((__m128i *)(srcPtr + srcLocs[0]), 0);      /* load [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|R05|G05|B05|R06] - Need R01-04 */
+    px[1] = __lsx_vld((__m128i *)(srcPtr + srcLocs[0] + 12), 0); /* load [R05|G05|B05|R06|G06|B06|R07|G07|B07|R08|G08|B08|R09|G09|B09|R10] - Need R05-08 */
+    p[0] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[0], xmm_pxMaskR), lsx_shuffle_i8(px[1], xmm_pxMaskR)));   /* Contains R01-08 */
+
+    px[0] = __lsx_vld((__m128i *)(srcPtr + srcLocs[1]), 0);      /* load [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|R05|G05|B05|R06] - Need G01-04 */
+    px[1] = __lsx_vld((__m128i *)(srcPtr + srcLocs[1] + 12), 0); /* load [R05|G05|B05|R06|G06|B06|R07|G07|B07|R08|G08|B08|R09|G09|B09|R10] - Need G05-08 */
+    p[1] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[0], xmm_pxMaskG), lsx_shuffle_i8(px[1], xmm_pxMaskG)));   /* Contains G01-08 */
+
+    px[0] = __lsx_vld((__m128i *)(srcPtr + srcLocs[2]), 0);      /* load [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|R05|G05|B05|R06] - Need B01-04 */
+    px[1] = __lsx_vld((__m128i *)(srcPtr + srcLocs[2] + 12), 0); /* load [R05|G05|B05|R06|G06|B06|R07|G07|B07|R08|G08|B08|R09|G09|B09|R10] - Need B05-08 */
+    p[2] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[0], xmm_pxMaskB), lsx_shuffle_i8(px[1], xmm_pxMaskB)));   /* Contains B01-08 */
+}
+
+inline void rpp_glitch_load24_f32pkd3_to_f32pln3_avx(Rpp32f *srcPtr, __m256 *p, int *srcLocs)
+{
+    __m128 p128[8];
+    Rpp32f *srcPtrTemp = srcPtr + srcLocs[0];
+    p[0] = lasx_setr_f32(*srcPtrTemp, *(srcPtrTemp + 3), *(srcPtrTemp + 6), *(srcPtrTemp + 9), 
+                         *(srcPtrTemp + 12), *(srcPtrTemp + 15), *(srcPtrTemp + 18), *(srcPtrTemp + 21));
+    srcPtrTemp = srcPtr + srcLocs[1];
+    p[1] = lasx_setr_f32(*(srcPtrTemp + 1), *(srcPtrTemp + 4), *(srcPtrTemp + 7), *(srcPtrTemp + 10), 
+                         *(srcPtrTemp + 13), *(srcPtrTemp + 16), *(srcPtrTemp + 19), *(srcPtrTemp + 22));
+    srcPtrTemp = srcPtr + srcLocs[2];
+    p[2] = lasx_setr_f32(*(srcPtrTemp + 2), *(srcPtrTemp + 5), *(srcPtrTemp + 8), *(srcPtrTemp + 11), 
+                         *(srcPtrTemp + 14), *(srcPtrTemp + 17), *(srcPtrTemp + 20), *(srcPtrTemp + 23));
+}
+
+inline void rpp_glitch_load24_i8pkd3_to_f32pln3_avx(Rpp8s *srcPtr, __m256 *p, int *srcLocs)
+{
+    __m128i px[2];
+    px[0] = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)(srcPtr + srcLocs[0]), 0));      /* load [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|R05|G05|B05|R06] - Need R01-04 */
+    px[1] = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)(srcPtr + srcLocs[0] + 12), 0)); /* load [R05|G05|B05|R06|G06|B06|R07|G07|B07|R08|G08|B08|R09|G09|B09|R10] - Need R05-08 */
+    p[0] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[0], xmm_pxMaskR), lsx_shuffle_i8(px[1], xmm_pxMaskR)));   /* Contains R01-08 */
+
+    px[0] = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)(srcPtr + srcLocs[1]), 0));      /* load [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|R05|G05|B05|R06] - Need G01-04 */
+    px[1] = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)(srcPtr + srcLocs[1] + 12), 0)); /* load [R05|G05|B05|R06|G06|B06|R07|G07|B07|R08|G08|B08|R09|G09|B09|R10] - Need G05-08 */
+    p[1] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[0], xmm_pxMaskG), lsx_shuffle_i8(px[1], xmm_pxMaskG)));   /* Contains G01-08 */
+
+    px[0] = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)(srcPtr + srcLocs[2]), 0));      /* load [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|R05|G05|B05|R06] - Need B01-04 */
+    px[1] = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)(srcPtr + srcLocs[2] + 12), 0)); /* load [R05|G05|B05|R06|G06|B06|R07|G07|B07|R08|G08|B08|R09|G09|B09|R10] - Need B05-08 */
+    p[2] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[0], xmm_pxMaskB), lsx_shuffle_i8(px[1], xmm_pxMaskB)));   /* Contains B01-08 */
+}
+
+inline void rpp_glitch_load30_u8pkd3_to_u8pkd3_avx(Rpp8u *srcPtr, int *srcLocs, __m256i &p)
+{
+    __m256i px[3];
+    px[0] = __lasx_xvld((__m256i *)(srcPtr + srcLocs[0]), 0);   // Load the source location1 values passed
+    px[1] = __lasx_xvld((__m256i *)(srcPtr + srcLocs[1]), 0);   // Load the source location2 values passed
+    px[2] = __lasx_xvld((__m256i *)(srcPtr + srcLocs[2]), 0);   // Load the source location3 values passed
+    px[0] = lasx_shuffle_i8(px[0], avx_pxMaskR);    /* Shuffle to obtain R channel values  */
+    px[1] = lasx_shuffle_i8(px[1], avx_pxMaskG);    /* Shuffle to obtain G channel values  */
+    px[2] = lasx_shuffle_i8(px[2], avx_pxMaskB);    /* Shuffle to obtain B channel values  */
+    px[0] = __lasx_xvor_v(px[0], px[1]);  /* Pack R and G channels to obtain RG format */
+    p = __lasx_xvor_v(px[0], px[2]);      /* Pack RG values and B channel to obtain RGB format */
+}
+
+inline void rpp_glitch_load30_i8pkd3_to_i8pkd3_avx(Rpp8s *srcPtr, int * srcLocs, __m256i &p)
+{
+    __m256i px[3];
+    px[0] = __lasx_xvld((__m256i *)(srcPtr + srcLocs[0]), 0);   // Load the source location1 values passed
+    px[1] = __lasx_xvld((__m256i *)(srcPtr + srcLocs[1]), 0);   // Load the source location2 values passed
+    px[2] = __lasx_xvld((__m256i *)(srcPtr + srcLocs[2]), 0);   // Load the source location3 values passed
+    px[0] = lasx_shuffle_i8(px[0], avx_pxMaskR);    /* Shuffle to obtain R channel values  */
+    px[1] = lasx_shuffle_i8(px[1], avx_pxMaskG);    /* Shuffle to obtain G channel values  */
+    px[2] = lasx_shuffle_i8(px[2], avx_pxMaskB);    /* Shuffle to obtain B channel values  */
+    px[0] = __lasx_xvor_v(px[0], px[1]);  /* Pack R and G channels to obtain RG format */
+    p = __lasx_xvor_v(px[0], px[2]);      /* Pack RG values and B channel to obtain RGB format */
+}
+
+inline void rpp_glitch_load6_f32pkd3_to_f32pkd3_avx(Rpp32f *srcPtr, int * srcLocs, __m256 &p)
+{
+    p =lasx_setr_f32(*(srcPtr + srcLocs[0]), *(srcPtr + srcLocs[1] + 1), *(srcPtr + srcLocs[2] + 2), *(srcPtr + srcLocs[0] + 3), 
+                      *(srcPtr + srcLocs[1] + 4), *(srcPtr + srcLocs[2] + 5), 0.0f, 0.0f);
+}
+
+inline void rpp_glitch_load48_u8pln3_to_f32pln3_avx(Rpp8u *srcPtrR, Rpp8u *srcPtrG, Rpp8u *srcPtrB, __m256 *p, int *srcLocs)
+{
+    __m128i px[3];
+
+    px[0] = __lsx_vld((__m128i *)srcPtrR + srcLocs[0], 0);       /* load [R01|R02|R03|R04|R05|R06|R07|R08|R09|R10|R11|R12|R13|R14|R15|R16] */
+    px[1] = __lsx_vld((__m128i *)srcPtrG + srcLocs[1], 0);       /* load [G01|G02|G03|G04|G05|G06|G07|G08|G09|G10|G11|G12|G13|G14|G15|G16] */
+    px[2] = __lsx_vld((__m128i *)srcPtrB + srcLocs[2], 0);       /* load [B01|B02|B03|B04|B05|B06|B07|B08|B09|B10|B11|B12|B13|B14|B15|B16] */
+    p[0] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[0], xmm_pxMask00To03), lsx_shuffle_i8(px[0], xmm_pxMask04To07)));    /* Contains R01-08 */
+    p[1] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[0], xmm_pxMask08To11), lsx_shuffle_i8(px[0], xmm_pxMask12To15)));    /* Contains R09-16 */
+    p[2] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[1], xmm_pxMask00To03), lsx_shuffle_i8(px[1], xmm_pxMask04To07)));    /* Contains G01-08 */
+    p[3] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[1], xmm_pxMask08To11), lsx_shuffle_i8(px[1], xmm_pxMask12To15)));    /* Contains G09-16 */
+    p[4] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMask00To03), lsx_shuffle_i8(px[2], xmm_pxMask04To07)));    /* Contains B01-08 */
+    p[5] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMask08To11), lsx_shuffle_i8(px[2], xmm_pxMask12To15)));    /* Contains B09-16 */
+}
+
+inline void rpp_load48_u8pkd3_to_f32pln3_mirror_avx(Rpp8u *srcPtr, __m256 *p)
+{
+    __m128i px[4];
+
+    px[0] = __lsx_vld((__m128i *)srcPtr, 0);           /* load [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|R05|G05|B05|R06] - Need RGB 01-04 */
+    px[1] = __lsx_vld((__m128i *)(srcPtr + 12), 0);    /* load [R05|G05|B05|R06|G06|B06|R07|G07|B07|R08|G08|B08|R09|G09|B09|R10] - Need RGB 05-08 */
+    px[2] = __lsx_vld((__m128i *)(srcPtr + 24), 0);    /* load [R09|G09|B09|R10|G10|B10|R11|G11|B11|R12|G12|B12|R13|G13|B13|R14] - Need RGB 09-12 */
+    px[3] = __lsx_vld((__m128i *)(srcPtr + 36), 0);    /* load [R13|G13|B13|R14|G14|B14|R15|G15|B15|R16|G16|B16|R17|G17|B17|R18] - Need RGB 13-16 */
+    p[0] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[3], xmm_pxMaskRMirror), lsx_shuffle_i8(px[2], xmm_pxMaskRMirror)));    /* Contains R16-09 */
+    p[1] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[1], xmm_pxMaskRMirror), lsx_shuffle_i8(px[0], xmm_pxMaskRMirror)));    /* Contains R01-08 */
+    p[2] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[3], xmm_pxMaskGMirror), lsx_shuffle_i8(px[2], xmm_pxMaskGMirror)));    /* Contains G16-09 */
+    p[3] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[1], xmm_pxMaskGMirror), lsx_shuffle_i8(px[0], xmm_pxMaskGMirror)));    /* Contains G01-08 */
+    p[4] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[3], xmm_pxMaskBMirror), lsx_shuffle_i8(px[2], xmm_pxMaskBMirror)));    /* Contains B16-09 */
+    p[5] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[1], xmm_pxMaskBMirror), lsx_shuffle_i8(px[0], xmm_pxMaskBMirror)));    /* Contains B01-08 */
+}
+
+inline void rpp_load48_u8pkd3_to_u32pln3_avx(Rpp8u *srcPtr, __m256i *p)
+{
+    __m128i px[4];
+
+    px[0] = __lsx_vld((__m128i *)srcPtr, 0);           /* load [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|R05|G05|B05|R06] - Need RGB 01-04 */
+    px[1] = __lsx_vld((__m128i *)(srcPtr + 12), 0);    /* load [R05|G05|B05|R06|G06|B06|R07|G07|B07|R08|G08|B08|R09|G09|B09|R10] - Need RGB 05-08 */
+    px[2] = __lsx_vld((__m128i *)(srcPtr + 24), 0);    /* load [R09|G09|B09|R10|G10|B10|R11|G11|B11|R12|G12|B12|R13|G13|B13|R14] - Need RGB 09-12 */
+    px[3] = __lsx_vld((__m128i *)(srcPtr + 36), 0);    /* load [R13|G13|B13|R14|G14|B14|R15|G15|B15|R16|G16|B16|R17|G17|B17|R18] - Need RGB 13-16 */
+    p[0] = lasx_setr_m128i(lsx_shuffle_i8(px[0], xmm_pxMaskR), lsx_shuffle_i8(px[1], xmm_pxMaskR));    /* Contains R01-08 */
+    p[1] = lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMaskR), lsx_shuffle_i8(px[3], xmm_pxMaskR));    /* Contains R09-16 */
+    p[2] = lasx_setr_m128i(lsx_shuffle_i8(px[0], xmm_pxMaskG), lsx_shuffle_i8(px[1], xmm_pxMaskG));    /* Contains G01-08 */
+    p[3] = lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMaskG), lsx_shuffle_i8(px[3], xmm_pxMaskG));    /* Contains G09-16 */
+    p[4] = lasx_setr_m128i(lsx_shuffle_i8(px[0], xmm_pxMaskB), lsx_shuffle_i8(px[1], xmm_pxMaskB));    /* Contains B01-08 */
+    p[5] = lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMaskB), lsx_shuffle_i8(px[3], xmm_pxMaskB));    /* Contains B09-16 */
+}
+
+inline void rpp_store48_f32pln3_to_u8pln3_avx(Rpp8u *dstPtrR, Rpp8u *dstPtrG, Rpp8u *dstPtrB, __m256 *p)
+{
+    __m256i pxCvt;
+    __m128i px[4];
+
+    pxCvt = __lasx_xvftint_w_s(p[0]);
+    px[2] = lsx_packus_i32(lasx_extracti128_m256i(pxCvt, 0), lasx_extracti128_m256i(pxCvt, 1));    /* pack pixels 0-7 for R */
+    pxCvt = __lasx_xvftint_w_s(p[1]);
+    px[3] = lsx_packus_i32(lasx_extracti128_m256i(pxCvt, 0), lasx_extracti128_m256i(pxCvt, 1));    /* pack pixels 8-15 for R */
+    px[0] = lsx_packus_i16(px[2], px[3]);    /* pack pixels 0-15 for R */
+    pxCvt = __lasx_xvftint_w_s(p[2]);
+    px[2] = lsx_packus_i32(lasx_extracti128_m256i(pxCvt, 0), lasx_extracti128_m256i(pxCvt, 1));    /* pack pixels 0-7 for G */
+    pxCvt = __lasx_xvftint_w_s(p[3]);
+    px[3] = lsx_packus_i32(lasx_extracti128_m256i(pxCvt, 0), lasx_extracti128_m256i(pxCvt, 1));    /* pack pixels 8-15 for G */
+    px[1] = lsx_packus_i16(px[2], px[3]);    /* pack pixels 0-15 for G */
+    pxCvt = __lasx_xvftint_w_s(p[4]);
+    px[2] = lsx_packus_i32(lasx_extracti128_m256i(pxCvt, 0), lasx_extracti128_m256i(pxCvt, 1));    /* pack pixels 0-7 for B */
+    pxCvt = __lasx_xvftint_w_s(p[5]);
+    px[3] = lsx_packus_i32(lasx_extracti128_m256i(pxCvt, 0), lasx_extracti128_m256i(pxCvt, 1));    /* pack pixels 8-15 for B */
+    px[2] = lsx_packus_i16(px[2], px[3]);    /* pack pixels 0-15 for B */
+    __lsx_vst(px[0], (__m128i *)dstPtrR, 0);    /* store [R01|R02|R03|R04|R05|R06|R07|R08|R09|R10|R11|R12|R13|R14|R15|R16] */
+    __lsx_vst(px[1], (__m128i *)dstPtrG, 0);    /* store [G01|G02|G03|G04|G05|G06|G07|G08|G09|G10|G11|G12|G13|G14|G15|G16] */
+    __lsx_vst(px[2], (__m128i *)dstPtrB, 0);    /* store [B01|B02|B03|B04|B05|B06|B07|B08|B09|B10|B11|B12|B13|B14|B15|B16] */
+}
+
+inline void rpp_store48_f32pln3_to_f32pln3_avx(Rpp32f *dstPtrR, Rpp32f *dstPtrG, Rpp32f *dstPtrB, __m256 *p)
+{
+    __lasx_xvst((__m256i)p[0], dstPtrR, 0);
+    __lasx_xvst((__m256i)p[2], dstPtrG, 0);
+    __lasx_xvst((__m256i)p[4], dstPtrB, 0);
+    __lasx_xvst((__m256i)p[1], dstPtrR + 8, 0);
+    __lasx_xvst((__m256i)p[3], dstPtrG + 8, 0);
+    __lasx_xvst((__m256i)p[5], dstPtrB + 8, 0);
+}
+
+inline void rpp_store48_f32pln3_to_f16pln3_avx(Rpp16f *dstPtrR, Rpp16f *dstPtrG, Rpp16f *dstPtrB, __m256 *p)
+{
+    __m128i px128[6];
+    px128[0] = lasx_cvtf32_ph(p[0], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    px128[1] = lasx_cvtf32_ph(p[1], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    px128[2] = lasx_cvtf32_ph(p[2], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    px128[3] = lasx_cvtf32_ph(p[3], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    px128[4] = lasx_cvtf32_ph(p[4], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    px128[5] = lasx_cvtf32_ph(p[5], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    __lsx_vst(px128[0], (__m128i *)dstPtrR, 0);
+    __lsx_vst(px128[2], (__m128i *)dstPtrG, 0);
+    __lsx_vst(px128[4], (__m128i *)dstPtrB, 0);
+    __lsx_vst(px128[1], (__m128i *)(dstPtrR + 8), 0);
+    __lsx_vst(px128[3], (__m128i *)(dstPtrG + 8), 0);
+    __lsx_vst(px128[5], (__m128i *)(dstPtrB + 8), 0);
+}
+
+inline void rpp_store48_f32pln3_to_f32pkd3_avx(Rpp32f *dstPtr, __m256 *p)
+{
+    __m128 p128[4];
+    p128[0] = (__m128)__lasx_cvt_256_128((__m256i)p[0]);
+    p128[1] = (__m128)__lasx_cvt_256_128((__m256i)p[2]);
+    p128[2] = (__m128)__lasx_cvt_256_128((__m256i)p[4]);
+    _MM_TRANSPOSE4_PS(p128[0], p128[1], p128[2], p128[3]);
+    __lsx_vst(p128[0], dstPtr, 0);
+    __lsx_vst(p128[1], dstPtr + 3, 0);
+    __lsx_vst(p128[2], dstPtr + 6, 0);
+    __lsx_vst(p128[3], dstPtr + 9, 0);
+    p128[0] = lasx_extractf128_f32(p[0], 1);
+    p128[1] = lasx_extractf128_f32(p[2], 1);
+    p128[2] = lasx_extractf128_f32(p[4], 1);
+    _MM_TRANSPOSE4_PS(p128[0], p128[1], p128[2], p128[3]);
+    __lsx_vst(p128[0], dstPtr + 12, 0);
+    __lsx_vst(p128[1], dstPtr + 15, 0);
+    __lsx_vst(p128[2], dstPtr + 18, 0);
+    __lsx_vst(p128[3], dstPtr + 21, 0);
+
+    p128[0] = (__m128)__lasx_cvt_256_128((__m256i)p[1]);
+    p128[1] = (__m128)__lasx_cvt_256_128((__m256i)p[3]);
+    p128[2] = (__m128)__lasx_cvt_256_128((__m256i)p[5]);
+    _MM_TRANSPOSE4_PS(p128[0], p128[1], p128[2], p128[3]);
+    __lsx_vst(p128[0], dstPtr + 24, 0);
+    __lsx_vst(p128[1], dstPtr + 27, 0);
+    __lsx_vst(p128[2], dstPtr + 30, 0);
+    __lsx_vst(p128[3], dstPtr + 33, 0);
+    p128[0] = lasx_extractf128_f32(p[1], 1);
+    p128[1] = lasx_extractf128_f32(p[3], 1);
+    p128[2] = lasx_extractf128_f32(p[5], 1);
+    _MM_TRANSPOSE4_PS(p128[0], p128[1], p128[2], p128[3]);
+    __lsx_vst(p128[0], dstPtr + 36, 0);
+    __lsx_vst(p128[1], dstPtr + 39, 0);
+    __lsx_vst(p128[2], dstPtr + 42, 0);
+    __lsx_vst(p128[3], dstPtr + 45, 0);
+}
+
+inline void rpp_store48_f32pln3_to_f16pkd3_avx(Rpp16f *dstPtr, __m256 *p)
+{
+    __m128 p128[4];
+    p128[0] = lasx_extractf128_f32(p[0], 0);
+    p128[1] = lasx_extractf128_f32(p[2], 0);
+    p128[2] = lasx_extractf128_f32(p[4], 0);
+    _MM_TRANSPOSE4_PS(p128[0], p128[1], p128[2], p128[3]);
+
+    __m128i px128[4];
+    px128[0] = __lsx_vfcvtrxxx_h_s(p128[0], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    px128[1] = __lsx_vfcvtrxxx_h_s(p128[1], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    px128[2] = __lsx_vfcvtrxxx_h_s(p128[2], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    px128[3] = __lsx_vfcvtrxxx_h_s(p128[3], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    __lsx_vst(px128[0], (__m128i *)dstPtr, 0);
+    __lsx_vst(px128[1], (__m128i *)(dstPtr + 3), 0);
+    __lsx_vst(px128[2], (__m128i *)(dstPtr + 6), 0);
+    __lsx_vst(px128[3], (__m128i *)(dstPtr + 9), 0);
+
+    p128[0] = lasx_extractf128_f32(p[0], 1);
+    p128[1] = lasx_extractf128_f32(p[2], 1);
+    p128[2] = lasx_extractf128_f32(p[4], 1);
+    _MM_TRANSPOSE4_PS(p128[0], p128[1], p128[2], p128[3]);
+    px128[0] = __lsx_vfcvtrxxx_h_s(p128[0], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    px128[1] = __lsx_vfcvtrxxx_h_s(p128[1], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    px128[2] = __lsx_vfcvtrxxx_h_s(p128[2], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    px128[3] = __lsx_vfcvtrxxx_h_s(p128[3], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    __lsx_vst(px128[0], (__m128i *)(dstPtr + 12), 0);
+    __lsx_vst(px128[1], (__m128i *)(dstPtr + 15), 0);
+    __lsx_vst(px128[2], (__m128i *)(dstPtr + 18), 0);
+    __lsx_vst(px128[3], (__m128i *)(dstPtr + 21), 0);
+
+    p128[0] = lasx_extractf128_f32(p[1], 0);
+    p128[1] = lasx_extractf128_f32(p[3], 0);
+    p128[2] = lasx_extractf128_f32(p[5], 0);
+    _MM_TRANSPOSE4_PS(p128[0], p128[1], p128[2], p128[3]);
+    px128[0] = __lsx_vfcvtrxxx_h_s(p128[0], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    px128[1] = __lsx_vfcvtrxxx_h_s(p128[1], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    px128[2] = __lsx_vfcvtrxxx_h_s(p128[2], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    px128[3] = __lsx_vfcvtrxxx_h_s(p128[3], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    __lsx_vst(px128[0], (__m128i *)(dstPtr + 24), 0);
+    __lsx_vst(px128[1], (__m128i *)(dstPtr + 27), 0);
+    __lsx_vst(px128[2], (__m128i *)(dstPtr + 30), 0);
+    __lsx_vst(px128[3], (__m128i *)(dstPtr + 33), 0);
+
+    p128[0] = lasx_extractf128_f32(p[1], 1);
+    p128[1] = lasx_extractf128_f32(p[3], 1);
+    p128[2] = lasx_extractf128_f32(p[5], 1);
+    _MM_TRANSPOSE4_PS(p128[0], p128[1], p128[2], p128[3]);
+    px128[0] = __lsx_vfcvtrxxx_h_s(p128[0], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    px128[1] = __lsx_vfcvtrxxx_h_s(p128[1], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    px128[2] = __lsx_vfcvtrxxx_h_s(p128[2], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    px128[3] = __lsx_vfcvtrxxx_h_s(p128[3], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    __lsx_vst(px128[0], (__m128i *)(dstPtr + 36), 0);
+    __lsx_vst(px128[1], (__m128i *)(dstPtr + 39), 0);
+    __lsx_vst(px128[2], (__m128i *)(dstPtr + 42), 0);
+    __lsx_vst(px128[3], (__m128i *)(dstPtr + 45), 0);
+}
+
+inline void rpp_load48_u8pln3_to_f32pln3_avx(Rpp8u *srcPtrR, Rpp8u *srcPtrG, Rpp8u *srcPtrB, __m256 *p)
+{
+    __m128i px[3];
+
+    px[0] = __lsx_vld((__m128i *)srcPtrR, 0);    /* load [R01|R02|R03|R04|R05|R06|R07|R08|R09|R10|R11|R12|R13|R14|R15|R16] */
+    px[1] = __lsx_vld((__m128i *)srcPtrG, 0);    /* load [G01|G02|G03|G04|G05|G06|G07|G08|G09|G10|G11|G12|G13|G14|G15|G16] */
+    px[2] = __lsx_vld((__m128i *)srcPtrB, 0);    /* load [B01|B02|B03|B04|B05|B06|B07|B08|B09|B10|B11|B12|B13|B14|B15|B16] */
+    p[0] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[0], xmm_pxMask00To03), lsx_shuffle_i8(px[0], xmm_pxMask04To07)));    /* Contains R01-08 */
+    p[1] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[0], xmm_pxMask08To11), lsx_shuffle_i8(px[0], xmm_pxMask12To15)));    /* Contains R09-16 */
+    p[2] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[1], xmm_pxMask00To03), lsx_shuffle_i8(px[1], xmm_pxMask04To07)));    /* Contains G01-08 */
+    p[3] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[1], xmm_pxMask08To11), lsx_shuffle_i8(px[1], xmm_pxMask12To15)));    /* Contains G09-16 */
+    p[4] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMask00To03), lsx_shuffle_i8(px[2], xmm_pxMask04To07)));    /* Contains B01-08 */
+    p[5] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMask08To11), lsx_shuffle_i8(px[2], xmm_pxMask12To15)));    /* Contains B09-16 */
+}
+
+inline void rpp_load48_u8pln3_to_f32pln3_mirror_avx(Rpp8u *srcPtrR, Rpp8u *srcPtrG, Rpp8u *srcPtrB, __m256 *p)
+{
+    __m128i px[3];
+
+    px[0] = __lsx_vld((__m128i *)srcPtrR, 0);    /* load [R01|R02|R03|R04|R05|R06|R07|R08|R09|R10|R11|R12|R13|R14|R15|R16] */
+    px[1] = __lsx_vld((__m128i *)srcPtrG, 0);    /* load [G01|G02|G03|G04|G05|G06|G07|G08|G09|G10|G11|G12|G13|G14|G15|G16] */
+    px[2] = __lsx_vld((__m128i *)srcPtrB, 0);    /* load [B01|B02|B03|B04|B05|B06|B07|B08|B09|B10|B11|B12|B13|B14|B15|B16] */
+    p[0] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[0], xmm_pxMask15To12), lsx_shuffle_i8(px[0], xmm_pxMask11To08)));    /* Contains R16-09 */
+    p[1] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[0], xmm_pxMask07To04), lsx_shuffle_i8(px[0], xmm_pxMask03To00)));    /* Contains R01-08 */
+    p[2] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[1], xmm_pxMask15To12), lsx_shuffle_i8(px[1], xmm_pxMask11To08)));    /* Contains G16-09 */
+    p[3] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[1], xmm_pxMask07To04), lsx_shuffle_i8(px[1], xmm_pxMask03To00)));    /* Contains G01-08 */
+    p[4] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMask15To12), lsx_shuffle_i8(px[2], xmm_pxMask11To08)));    /* Contains B16-09 */
+    p[5] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMask07To04), lsx_shuffle_i8(px[2], xmm_pxMask03To00)));    /* Contains B01-08 */
+}
+
+inline void rpp_load48_u8pln3_to_u32pln3_avx(Rpp8u *srcPtrR, Rpp8u *srcPtrG, Rpp8u *srcPtrB, __m256i *p)
+{
+    __m128i px[3];
+
+    px[0] = __lsx_vld((__m128i *)srcPtrR, 0);    /* load [R01|R02|R03|R04|R05|R06|R07|R08|R09|R10|R11|R12|R13|R14|R15|R16] */
+    px[1] = __lsx_vld((__m128i *)srcPtrG, 0);    /* load [G01|G02|G03|G04|G05|G06|G07|G08|G09|G10|G11|G12|G13|G14|G15|G16] */
+    px[2] = __lsx_vld((__m128i *)srcPtrB, 0);    /* load [B01|B02|B03|B04|B05|B06|B07|B08|B09|B10|B11|B12|B13|B14|B15|B16] */
+    p[0] = lasx_setr_m128i(lsx_shuffle_i8(px[0], xmm_pxMask00To03), lsx_shuffle_i8(px[0], xmm_pxMask04To07));    /* Contains R01-08 */
+    p[1] = lasx_setr_m128i(lsx_shuffle_i8(px[0], xmm_pxMask08To11), lsx_shuffle_i8(px[0], xmm_pxMask12To15));    /* Contains R09-16 */
+    p[2] = lasx_setr_m128i(lsx_shuffle_i8(px[1], xmm_pxMask00To03), lsx_shuffle_i8(px[1], xmm_pxMask04To07));    /* Contains G01-08 */
+    p[3] = lasx_setr_m128i(lsx_shuffle_i8(px[1], xmm_pxMask08To11), lsx_shuffle_i8(px[1], xmm_pxMask12To15));    /* Contains G09-16 */
+    p[4] = lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMask00To03), lsx_shuffle_i8(px[2], xmm_pxMask04To07));    /* Contains B01-08 */
+    p[5] = lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMask08To11), lsx_shuffle_i8(px[2], xmm_pxMask12To15));    /* Contains B09-16 */
+}
+
+inline void rpp_store48_f32pln3_to_u8pkd3_avx(Rpp8u *dstPtr, __m256 *p)
+{
+    __m256i pxCvt[3];
+    __m128i px[5];
+    __m128i pxMask = lsx_setr_i8(0, 4, 8, 1, 5, 9, 2, 6, 10, 3, 7, 11, 12, 13, 14, 15);
+
+    pxCvt[0] = __lasx_xvftint_w_s(p[0]);    /* convert to int32 for R01-08 */
+    pxCvt[1] = __lasx_xvftint_w_s(p[2]);    /* convert to int32 for G01-08 */
+    pxCvt[2] = __lasx_xvftint_w_s(p[4]);    /* convert to int32 for B01-08 */
+    px[3] = lsx_packus_i32(lasx_extracti128_m256i(pxCvt[0], 0), lasx_extracti128_m256i(pxCvt[1], 0));    /* pack pixels 0-7 as R01-04|G01-04 */
+    px[4] = lsx_packus_i32(lasx_extracti128_m256i(pxCvt[2], 0), xmm_px0);    /* pack pixels 8-15 as B01-04|X01-04 */
+    px[0] = lsx_packus_i16(px[3], px[4]);    /* pack pixels 0-15 as [R01|R02|R03|R04|G01|G02|G03|G04|B01|B02|B03|B04|00|00|00|00] */
+    px[3] = lsx_packus_i32(lasx_extracti128_m256i(pxCvt[0], 1), lasx_extracti128_m256i(pxCvt[1], 1));    /* pack pixels 0-7 as R05-08|G05-08 */
+    px[4] = lsx_packus_i32(lasx_extracti128_m256i(pxCvt[2], 1), xmm_px0);    /* pack pixels 8-15 as B05-08|X05-08 */
+    px[1] = lsx_packus_i16(px[3], px[4]);    /* pack pixels 0-15 as [R05|R06|R07|R08|G05|G06|G07|G08|B05|B06|B07|B08|00|00|00|00] */
+    pxCvt[0] = __lasx_xvftint_w_s(p[1]);    /* convert to int32 for R09-16 */
+    pxCvt[1] = __lasx_xvftint_w_s(p[3]);    /* convert to int32 for G09-16 */
+    pxCvt[2] = __lasx_xvftint_w_s(p[5]);    /* convert to int32 for B09-16 */
+    px[3] = lsx_packus_i32(lasx_extracti128_m256i(pxCvt[0], 0), lasx_extracti128_m256i(pxCvt[1], 0));    /* pack pixels 0-7 as R09-12|G09-12 */
+    px[4] = lsx_packus_i32(lasx_extracti128_m256i(pxCvt[2], 0), xmm_px0);    /* pack pixels 8-15 as B09-12|X09-12 */
+    px[2] = lsx_packus_i16(px[3], px[4]);    /* pack pixels 0-15 as [R09|R10|R11|R12|G09|G10|G11|G12|B09|B10|B11|B12|00|00|00|00] */
+    px[3] = lsx_packus_i32(lasx_extracti128_m256i(pxCvt[0], 1), lasx_extracti128_m256i(pxCvt[1], 1));    /* pack pixels 0-7 as R13-16|G13-16 */
+    px[4] = lsx_packus_i32(lasx_extracti128_m256i(pxCvt[2], 1), xmm_px0);    /* pack pixels 8-15 as B13-16|X13-16 */
+    px[3] = lsx_packus_i16(px[3], px[4]);    /* pack pixels 0-15 as [R13|R14|R15|R16|G13|G14|G15|G16|B13|B14|B15|B16|00|00|00|00] */
+    px[0] = lsx_shuffle_i8(px[0], pxMask);    /* shuffle to get [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|00|00|00|00] */
+    px[1] = lsx_shuffle_i8(px[1], pxMask);    /* shuffle to get [R05|G05|B05|R06|G06|B06|R07|G07|B07|R08|G08|B08|00|00|00|00] */
+    px[2] = lsx_shuffle_i8(px[2], pxMask);    /* shuffle to get [R09|G09|B09|R10|G10|B10|R11|G11|B11|R12|G12|B12|00|00|00|00] */
+    px[3] = lsx_shuffle_i8(px[3], pxMask);    /* shuffle to get [R13|G13|B13|R14|G14|B14|R15|G15|B15|R16|G16|B16|00|00|00|00] */
+    __lsx_vst(px[0], (__m128i *)dstPtr, 0);           /* store [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|00|00|00|00] */
+    __lsx_vst(px[1], (__m128i *)(dstPtr + 12), 0);    /* store [R05|G05|B05|R06|G06|B06|R07|G07|B07|R08|G08|B08|00|00|00|00] */
+    __lsx_vst(px[2], (__m128i *)(dstPtr + 24), 0);    /* store [R09|G09|B09|R10|G10|B10|R11|G11|B11|R12|G12|B12|00|00|00|00] */
+    __lsx_vst(px[3], (__m128i *)(dstPtr + 36), 0);    /* store [R13|G13|B13|R14|G14|B14|R15|G15|B15|R16|G16|B16|00|00|00|00] */
+}
+
+inline void rpp_load24_u8pln3_to_f64pln3_avx(Rpp8u *srcPtrR, Rpp8u *srcPtrG, Rpp8u *srcPtrB, __m256d *p)
+{
+    __m128i px[3];
+
+    px[0] = __lsx_vld((__m128i *)srcPtrR, 0);    /* load [R00|R01|R02|R03|R04|R05|R06|R07|R08|R09|R10|R11|R12|R13|R14|R15|R16] */
+    px[1] = __lsx_vld((__m128i *)srcPtrG, 0);    /* load [G00|G01|G02|G03|G04|G05|G06|G07|G08|G09|G10|G11|G12|G13|G14|G15|G16] */
+    px[2] = __lsx_vld((__m128i *)srcPtrB, 0);    /* load [B00|B01|B02|B03|B04|B05|B06|B07|B08|B09|B10|B11|B12|B13|B14|B15|B16] */
+    p[0] = lasx_cvti32_f64(lsx_shuffle_i8(px[0], xmm_pxMask00To03));    /* Contains R00-03 */
+    p[1] = lasx_cvti32_f64(lsx_shuffle_i8(px[0], xmm_pxMask04To07));    /* Contains R04-07 */
+    p[2] = lasx_cvti32_f64(lsx_shuffle_i8(px[1], xmm_pxMask00To03));    /* Contains G00-03 */
+    p[3] = lasx_cvti32_f64(lsx_shuffle_i8(px[1], xmm_pxMask04To07));    /* Contains G04-07 */
+    p[4] = lasx_cvti32_f64(lsx_shuffle_i8(px[2], xmm_pxMask00To03));    /* Contains B00-03 */
+    p[5] = lasx_cvti32_f64(lsx_shuffle_i8(px[2], xmm_pxMask04To07));    /* Contains B04-07 */
+}
+
+inline void rpp_load24_u8pkd3_to_f64pln3_avx(Rpp8u *srcPtr, __m256d *p)
+{
+    __m128i px[2];
+
+    px[0] = __lsx_vld((__m128i *)srcPtr, 0);           /* load [R00|G00|B00|R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|R05] - Need RGB 00-03 */
+    px[1] = __lsx_vld((__m128i *)(srcPtr + 12), 0);    /* load [R04|G04|B04|R05|G05|B05|R06|G06|B06|R07|G07|B07|R08|G08|B08|R09] - Need RGB 04-07 */
+    p[0] = lasx_cvti32_f64(lsx_shuffle_i8(px[0], xmm_pxMaskR));    /* Contains R00-03 */
+    p[1] = lasx_cvti32_f64(lsx_shuffle_i8(px[1], xmm_pxMaskR));    /* Contains R04-07 */
+    p[2] = lasx_cvti32_f64(lsx_shuffle_i8(px[0], xmm_pxMaskG));    /* Contains G00-03 */
+    p[3] = lasx_cvti32_f64(lsx_shuffle_i8(px[1], xmm_pxMaskG));    /* Contains G04-07 */
+    p[4] = lasx_cvti32_f64(lsx_shuffle_i8(px[0], xmm_pxMaskB));    /* Contains B00-03 */
+    p[5] = lasx_cvti32_f64(lsx_shuffle_i8(px[1], xmm_pxMaskB));    /* Contains B04-07 */
+}
+
+inline void rpp_load16_u8_to_f32_avx(Rpp8u *srcPtr, __m256 *p)
+{
+    __m128i px;
+    px = __lsx_vld((__m128i *)srcPtr, 0);
+    p[0] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px, xmm_pxMask00To03), lsx_shuffle_i8(px, xmm_pxMask04To07)));    /* Contains pixels 01-08 */
+    p[1] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px, xmm_pxMask08To11), lsx_shuffle_i8(px, xmm_pxMask12To15)));    /* Contains pixels 09-16 */
+}
+
+inline void rpp_load24_u8_to_f32_avx(Rpp8u *srcPtr, __m256 *p)
+{
+    __m128i px1, px2;
+    px1 = __lsx_vld((__m128i *)(srcPtr), 0);
+    px2 = lsx_loadl_d((__m128i *)(srcPtr + 16));
+
+    p[0] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px1, xmm_pxMask00To03), lsx_shuffle_i8(px1, xmm_pxMask04To07)));  /* Contains pixels 01-08 */
+    p[1] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px1, xmm_pxMask08To11), lsx_shuffle_i8(px1, xmm_pxMask12To15)));  /* Contains pixels 09-16 */
+    p[2] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px2, xmm_pxMask00To03), lsx_shuffle_i8(px2, xmm_pxMask04To07)));  /* Contains pixels 17-24 */
+}
+
+inline void rpp_load32_u8_to_f32_avx(Rpp8u *srcPtr, __m256 *p)
+{
+    __m256i px = __lasx_xvld((__m256i *)srcPtr, 0);
+    __m128i px1 = __lasx_cvt_256_128(px);
+    __m128i px2 = lasx_extractf128_m256i(px, 1);
+
+    p[0] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px1, xmm_pxMask00To03), lsx_shuffle_i8(px1, xmm_pxMask04To07))); // Contains pixels 01-08
+    p[1] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px1, xmm_pxMask08To11), lsx_shuffle_i8(px1, xmm_pxMask12To15))); // Contains pixels 09-16
+    p[2] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px2, xmm_pxMask00To03), lsx_shuffle_i8(px2, xmm_pxMask04To07))); // Contains pixels 17-24
+    p[3] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px2, xmm_pxMask08To11), lsx_shuffle_i8(px2, xmm_pxMask12To15))); // Contains pixels 25-32
+}
+
+inline void rpp_load40_u8_to_f32_avx(Rpp8u *srcPtr, __m256 *p)
+{
+    __m256i px1 = __lasx_xvld((__m256i *)srcPtr, 0);     // Load the first 32 bytes
+    __m128i px2 = __lsx_vld((__m128i *)(srcPtr + 32), 0); // Load the remaining 8 bytes
+    __m128i px1Low  = __lasx_cvt_256_128(px1);
+    __m128i px1High = lasx_extractf128_m256i(px1, 1);
+
+    p[0] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px1Low, xmm_pxMask00To03), lsx_shuffle_i8(px1Low, xmm_pxMask04To07))); // Pixels 01-08
+    p[1] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px1Low, xmm_pxMask08To11), lsx_shuffle_i8(px1Low, xmm_pxMask12To15))); // Pixels 09-16
+    p[2] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px1High, xmm_pxMask00To03), lsx_shuffle_i8(px1High, xmm_pxMask04To07))); // Pixels 17-24
+    p[3] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px1High, xmm_pxMask08To11), lsx_shuffle_i8(px1High, xmm_pxMask12To15))); // Pixels 25-32
+    p[4] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px2, xmm_pxMask00To03), lsx_shuffle_i8(px2, xmm_pxMask04To07)));        // Pixels 33-40
+}
+
+inline void rpp_load8_u8_to_f32_avx(Rpp8u *srcPtr, __m256 *p)
+{
+    __m128i px;
+    px = __lsx_vld((__m128i *)srcPtr, 0);
+    p[0] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px, xmm_pxMask00To03), lsx_shuffle_i8(px, xmm_pxMask04To07)));    /* Contains pixels 01-08 */
+}
+
+inline void rpp_load16_u8_to_f32_mirror_avx(Rpp8u *srcPtr, __m256 *p)
+{
+    __m128i px;
+    px = __lsx_vld((__m128i *)srcPtr, 0);    /* load pixels 0-15 */
+    p[0] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px, xmm_pxMask15To12), lsx_shuffle_i8(px, xmm_pxMask11To08)));    /* Contains pixels 15-08 */
+    p[1] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px, xmm_pxMask07To04), lsx_shuffle_i8(px, xmm_pxMask03To00)));    /* Contains pixels 7-0 */
+}
+
+inline void rpp_store16_f32_to_u8_avx(Rpp8u *dstPtr, __m256 *p)
+{
+    __m256i pxCvt;
+    __m128i px[3];
+    pxCvt = __lasx_xvftint_w_s(p[0]);
+    px[1] = lsx_packus_i32(lasx_extracti128_m256i(pxCvt, 0), lasx_extracti128_m256i(pxCvt, 1));    /* pack pixels 0-7 for R */
+    pxCvt = __lasx_xvftint_w_s(p[1]);
+    px[2] = lsx_packus_i32(lasx_extracti128_m256i(pxCvt, 0), lasx_extracti128_m256i(pxCvt, 1));    /* pack pixels 8-15 for R */
+    px[0] = lsx_packus_i16(px[1], px[2]);    /* pack pixels 0-15 */
+    __lsx_vst(px[0], (__m128i *)dstPtr, 0);
+}
+
+inline void rpp_load8_u8_to_f64_avx(Rpp8u *srcPtr, __m256d *p)
+{
+    __m128i px;
+    px = __lsx_vld((__m128i *)srcPtr, 0);
+    p[0] = lasx_cvti32_f64(lsx_shuffle_i8(px, xmm_pxMask00To03));    /* Contains pixels 01-04 */
+    p[1] = lasx_cvti32_f64(lsx_shuffle_i8(px, xmm_pxMask04To07));    /* Contains pixels 05-08 */
+}
+
+inline void rpp_load8_i8_to_f64_avx(Rpp8s *srcPtr, __m256d *p)
+{
+    __m128i px;
+    px = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)srcPtr, 0));
+    p[0] = lasx_cvti32_f64(lsx_shuffle_i8(px, xmm_pxMask00To03));    /* Contains pixels 01-04 */
+    p[1] = lasx_cvti32_f64(lsx_shuffle_i8(px, xmm_pxMask04To07));    /* Contains pixels 05-08 */
+}
+
+inline void rpp_load8_i8_to_f32_avx(Rpp8s *srcPtr, __m256 *p)
+{
+    __m128i px;
+    px = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)srcPtr, 0));
+    p[0] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px, xmm_pxMask00To03), lsx_shuffle_i8(px, xmm_pxMask04To07)));    /* Contains pixels 01-08 */
+}
+
+inline void rpp_load16_u8_to_u32_avx(Rpp8u *srcPtr, __m256i *p)
+{
+    __m128i px;
+    px = __lsx_vld((__m128i *)srcPtr, 0);
+    p[0] = lasx_setr_m128i(lsx_shuffle_i8(px, xmm_pxMask00To03), lsx_shuffle_i8(px, xmm_pxMask04To07));    /* Contains pixels 01-08 */
+    p[1] = lasx_setr_m128i(lsx_shuffle_i8(px, xmm_pxMask08To11), lsx_shuffle_i8(px, xmm_pxMask12To15));    /* Contains pixels 09-16 */
+}
+
+inline void rpp_load96_u8_avx(Rpp8u *srcPtrR, Rpp8u *srcPtrG, Rpp8u *srcPtrB, __m256i *p)
+{
+    p[0] = __lasx_xvld((__m256i *)srcPtrR, 0);
+    p[1] = __lasx_xvld((__m256i *)srcPtrG, 0);
+    p[2] = __lasx_xvld((__m256i *)srcPtrB, 0);
+}
+
+inline void rpp_load96_i8_avx(Rpp8s *srcPtrR, Rpp8s *srcPtrG, Rpp8s *srcPtrB, __m256i *p)
+{
+    p[0] = __lasx_xvld((__m256i *)srcPtrR, 0);
+    p[1] = __lasx_xvld((__m256i *)srcPtrG, 0);
+    p[2] = __lasx_xvld((__m256i *)srcPtrB, 0);
+}
+
+inline void rpp_load24_f32pkd3_to_f32pln3_avx(Rpp32f *srcPtr, __m256 *p)
+{
+    __m128 p128[8];
+    p128[0] = (__m128)__lsx_vld(srcPtr, 0);
+    p128[1] = (__m128)__lsx_vld(srcPtr + 3, 0);
+    p128[2] = (__m128)__lsx_vld(srcPtr + 6, 0);
+    p128[3] = (__m128)__lsx_vld(srcPtr + 9, 0);
+    _MM_TRANSPOSE4_PS(p128[0], p128[1], p128[2], p128[3]);
+    p128[4] = (__m128)__lsx_vld(srcPtr + 12, 0);
+    p128[5] = (__m128)__lsx_vld(srcPtr + 15, 0);
+    p128[6] = (__m128)__lsx_vld(srcPtr + 18, 0);
+    p128[7] = (__m128)__lsx_vld(srcPtr + 21, 0);
+    _MM_TRANSPOSE4_PS(p128[4], p128[5], p128[6], p128[7]);
+    p[0] = lasx_setr_m128(p128[0], p128[4]);
+    p[1] = lasx_setr_m128(p128[1], p128[5]);
+    p[2] = lasx_setr_m128(p128[2], p128[6]);
+}
+
+inline void rpp_load24_f16pkd3_to_f32pln3_avx(Rpp16f *srcPtr, __m256 *p)
+{
+    __m128 p128[8];
+    p128[0] = __lsx_vfcvtl_s_h((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtr), 0)));
+    p128[1] = __lsx_vfcvtl_s_h((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtr + 3), 0)));
+    p128[2] = __lsx_vfcvtl_s_h((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtr + 6), 0)));
+    p128[3] = __lsx_vfcvtl_s_h((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtr + 9), 0)));
+    _MM_TRANSPOSE4_PS(p128[0], p128[1], p128[2], p128[3]);
+    p128[4] = __lsx_vfcvtl_s_h((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtr + 12), 0)));
+    p128[5] = __lsx_vfcvtl_s_h((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtr + 15), 0)));
+    p128[6] = __lsx_vfcvtl_s_h((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtr + 18), 0)));
+    p128[7] = __lsx_vfcvtl_s_h((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtr + 21), 0)));
+    _MM_TRANSPOSE4_PS(p128[4], p128[5], p128[6], p128[7]);
+    p[0] = lasx_setr_m128(p128[0], p128[4]);
+    p[1] = lasx_setr_m128(p128[1], p128[5]);
+    p[2] = lasx_setr_m128(p128[2], p128[6]);
+}
+
+inline void rpp_load24_f32pkd3_to_f32pln3_mirror_avx(Rpp32f *srcPtr, __m256 *p)
+{
+    __m128 p128[8];
+    __m256i pxMask = lasx_setr_i32(7, 6, 5, 4, 3, 2, 1, 0);
+    p128[0] = (__m128)__lsx_vld(srcPtr, 0); /* loads R01|G01|B01|R02 */
+    p128[1] = (__m128)__lsx_vld(srcPtr + 3, 0); /* loads R02|G02|B02|R03 */
+    p128[2] = (__m128)__lsx_vld(srcPtr + 6, 0); /* loads R03|G03|B03|R04 */
+    p128[3] = (__m128)__lsx_vld(srcPtr + 9, 0); /* loads R04|G04|B04|R05 */
+    _MM_TRANSPOSE4_PS(p128[0], p128[1], p128[2], p128[3]); /* Transpose the 4x4 matrix and forms [[R01 R02 R03 R04][B01 B02 B03 B04][G01 G02 G03 G03][R02 R03 R04 R05]] */
+    p128[4] = (__m128)__lsx_vld(srcPtr + 12, 0); /* loads R05|G05|B05|R06 */
+    p128[5] = (__m128)__lsx_vld(srcPtr + 15, 0); /* loads R06|G06|B06|R07 */
+    p128[6] = (__m128)__lsx_vld(srcPtr + 18, 0); /* loads R07|G07|B07|R08 */
+    p128[7] = (__m128)__lsx_vld(srcPtr + 21, 0); /* loads R08|G08|B08|R09 */
+    _MM_TRANSPOSE4_PS(p128[4], p128[5], p128[6], p128[7]); /* Transpose the 4x4 matrix and forms [[R05 R06 R07 R08][B05 B06 B07 B08][G05 G06 G07 G08][R06 R07 R08 R09]] */
+    p[0] = lasx_setr_m128(p128[0], p128[4]); /* packs as R01-R08 */
+    p[1] = lasx_setr_m128(p128[1], p128[5]); /* packs as G01-R08 */
+    p[2] = lasx_setr_m128(p128[2], p128[6]); /* packs as B01-R08 */
+
+    p[0] = lasx_permutevar8x32_f32(p[0], pxMask); /* shuffle as R08-R01 */
+    p[1] = lasx_permutevar8x32_f32(p[1], pxMask); /* shuffle as G08-G01 */
+    p[2] = lasx_permutevar8x32_f32(p[2], pxMask); /* shuffle as B08-B01 */
+}
+
+inline void rpp_store24_f32pln3_to_f32pln3_avx(Rpp32f *dstPtrR, Rpp32f *dstPtrG, Rpp32f *dstPtrB, __m256 *p)
+{
+    __lasx_xvst((__m256i)p[0], dstPtrR, 0);
+    __lasx_xvst((__m256i)p[1], dstPtrG, 0);
+    __lasx_xvst((__m256i)p[2], dstPtrB, 0);
+}
+
+inline void rpp_load24_f32pkd3_to_f64pln3_avx(Rpp32f *srcPtr, __m256d *p)
+{
+    __m128 p128[8];
+    p128[0] = (__m128)__lsx_vld(srcPtr, 0);
+    p128[1] = (__m128)__lsx_vld(srcPtr + 3, 0);
+    p128[2] = (__m128)__lsx_vld(srcPtr + 6, 0);
+    p128[3] = (__m128)__lsx_vld(srcPtr + 9, 0);
+    _MM_TRANSPOSE4_PS(p128[0], p128[1], p128[2], p128[3]);
+    p128[4] = (__m128)__lsx_vld(srcPtr + 12, 0);
+    p128[5] = (__m128)__lsx_vld(srcPtr + 15, 0);
+    p128[6] = (__m128)__lsx_vld(srcPtr + 18, 0);
+    p128[7] = (__m128)__lsx_vld(srcPtr + 21, 0);
+    _MM_TRANSPOSE4_PS(p128[4], p128[5], p128[6], p128[7]);
+    p[0] = lasx_cvtf32_f64(p128[0]);
+    p[1] = lasx_cvtf32_f64(p128[4]);
+    p[2] = lasx_cvtf32_f64(p128[1]);
+    p[3] = lasx_cvtf32_f64(p128[5]);
+    p[4] = lasx_cvtf32_f64(p128[2]);
+    p[5] = lasx_cvtf32_f64(p128[6]);
+}
+
+inline void rpp_load24_f32pln3_to_f32pln3_avx(Rpp32f *srcPtrR, Rpp32f *srcPtrG, Rpp32f *srcPtrB, __m256 *p)
+{
+    p[0] = (__m256)__lasx_xvld(srcPtrR, 0);
+    p[1] = (__m256)__lasx_xvld(srcPtrG, 0);
+    p[2] = (__m256)__lasx_xvld(srcPtrB, 0);
+}
+
+inline void rpp_load24_f16pln3_to_f32pln3_avx(Rpp16f *srcPtrR, Rpp16f *srcPtrG, Rpp16f *srcPtrB, __m256 *p)
+{
+    p[0] = lasx_cvtph_f32((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtrR), 0)));
+    p[1] = lasx_cvtph_f32((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtrG), 0)));
+    p[2] = lasx_cvtph_f32((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtrB), 0)));
+}
+
+inline void rpp_load24_f32pln3_to_f32pln3_mirror_avx(Rpp32f *srcPtrR, Rpp32f *srcPtrG, Rpp32f *srcPtrB, __m256 *p)
+{
+    __m256i pxMask = lasx_setr_i32(7, 6, 5, 4, 3, 2, 1, 0);
+
+    p[0] = (__m256)__lasx_xvld(srcPtrR, 0); /* loads pixels R01-R08 */
+    p[1] = (__m256)__lasx_xvld(srcPtrG, 0); /* loads pixels G01-G08 */
+    p[2] = (__m256)__lasx_xvld(srcPtrB, 0); /* loads pixels G01-B08 */
+
+    p[0] = lasx_permutevar8x32_f32(p[0], pxMask); /* shuffle as R08-R01 */
+    p[1] = lasx_permutevar8x32_f32(p[1], pxMask); /* shuffle as G08-G01 */
+    p[2] = lasx_permutevar8x32_f32(p[2], pxMask); /* shuffle as B08-B01 */
+}
+
+inline void rpp_store24_f32pln3_to_f32pkd3_avx(Rpp32f *dstPtr, __m256 *p)
+{
+    __m256 pTemp[4], pRow[4];
+    pTemp[0] = (__m256)__lasx_xvpermi_w(p[0], p[1], 0x44); /* shuffle to get R01|R02|G01|G02|R05|R06|G05|G06 */
+    pTemp[2] = (__m256)__lasx_xvpermi_w(p[0], p[1], 0xEE); /* shuffle to get R03|R04|G03|G04|R07|R08|G07|G08 */
+    pTemp[1] = (__m256)__lasx_xvpermi_w(p[2], avx_p0, 0x44); /* shuffle to get B01|B02|00|00|B05|B06|00|00 */
+    pTemp[3] = (__m256)__lasx_xvpermi_w(p[2], avx_p0, 0xEE); /* shuffle to get B03|B04|00|00|B07|B08|00|00 */
+    pRow[0] = (__m256)__lasx_xvpermi_w(pTemp[0], pTemp[1], 0x88); /* shuffle to get R01|G01|B01|00|R05|G05|B05|00 */
+    pRow[1] = (__m256)__lasx_xvpermi_w(pTemp[0], pTemp[1], 0xDD); /* shuffle to get R02|G02|B02|00|R06|G06|B05|00 */
+    pRow[2] = (__m256)__lasx_xvpermi_w(pTemp[2], pTemp[3], 0x88); /* shuffle to get R03|G03|B03|00|R07|G07|B05|00 */
+    pRow[3] = (__m256)__lasx_xvpermi_w(pTemp[2], pTemp[3], 0xDD); /* shuffle to get R04|G04|B04|00|R08|G08|B05|00 */
+
+    __m128 p128[4];
+    p128[1] = (__m128)__lasx_cvt_256_128((__m256i)pRow[1]); /* get R01|G01|B01|00 */
+    p128[2] = (__m128)__lasx_cvt_256_128((__m256i)pRow[2]); /* get R02|G02|B02|00 */
+    p128[3] = (__m128)__lasx_cvt_256_128((__m256i)pRow[3]); /* get R03|G03|B03|00 */
+    p128[0] = (__m128)__lasx_cvt_256_128((__m256i)pRow[0]); /* get R04|G04|B04|00 */
+    __lsx_vst(p128[0], dstPtr, 0);
+    __lsx_vst(p128[1], dstPtr + 3, 0);
+    __lsx_vst(p128[2], dstPtr + 6, 0);
+    __lsx_vst(p128[3], dstPtr + 9, 0);
+
+    p128[0] = lasx_extractf128_f32(pRow[0], 1); /* get R05|G05|B05|00 */
+    p128[1] = lasx_extractf128_f32(pRow[1], 1); /* get R06|G06|B06|00 */
+    p128[2] = lasx_extractf128_f32(pRow[2], 1); /* get R07|G07|B07|00 */
+    p128[3] = lasx_extractf128_f32(pRow[3], 1); /* get R08|G08|B08|00 */
+    __lsx_vst(p128[0], dstPtr + 12, 0);
+    __lsx_vst(p128[1], dstPtr + 15, 0);
+    __lsx_vst(p128[2], dstPtr + 18, 0);
+    __lsx_vst(p128[3], dstPtr + 21, 0);
+}
+
+inline void rpp_load24_f32pln3_to_f64pln3_avx(Rpp32f *srcPtrR, Rpp32f *srcPtrG, Rpp32f *srcPtrB, __m256d *p)
+{
+    __m128 px128[6];
+    __m256 px[3];
+    px[0] = (__m256)__lasx_xvld(srcPtrR, 0);
+    px[1] = (__m256)__lasx_xvld(srcPtrG, 0);
+    px[2] = (__m256)__lasx_xvld(srcPtrB, 0);
+    px128[0] = (__m128)__lasx_cvt_256_128((__m256i)px[0]);
+    px128[1] = lasx_extractf128_f32(px[0], 1);
+    px128[2] = (__m128)__lasx_cvt_256_128((__m256i)px[1]);
+    px128[3] = lasx_extractf128_f32(px[1], 1);
+    px128[4] = (__m128)__lasx_cvt_256_128((__m256i)px[2]);
+    px128[5] = lasx_extractf128_f32(px[2], 1);
+    p[0] = lasx_cvtf32_f64(px128[0]);
+    p[1] = lasx_cvtf32_f64(px128[1]);
+    p[2] = lasx_cvtf32_f64(px128[2]);
+    p[3] = lasx_cvtf32_f64(px128[3]);
+    p[4] = lasx_cvtf32_f64(px128[4]);
+    p[5] = lasx_cvtf32_f64(px128[5]);
+}
+
+inline void rpp_load16_f32_to_f32_avx(Rpp32f *srcPtr, __m256 *p)
+{
+    p[0] = (__m256)__lasx_xvld(srcPtr, 0);
+    p[1] = (__m256)__lasx_xvld(srcPtr + 8, 0);
+}
+
+inline void rpp_load24_f32_to_f32_avx(Rpp32f *srcPtr, __m256 *p)
+{
+    p[0] = (__m256)__lasx_xvld(srcPtr, 0);
+    p[1] = (__m256)__lasx_xvld(srcPtr + 8, 0);
+    p[2] = (__m256)__lasx_xvld(srcPtr + 16, 0);
+}
+
+inline void rpp_load32_f32_to_f32_avx(Rpp32f *srcPtr, __m256 *p)
+{
+    p[0] = (__m256)__lasx_xvld(srcPtr, 0);
+    p[1] = (__m256)__lasx_xvld(srcPtr + 8, 0);
+    p[2] = (__m256)__lasx_xvld(srcPtr + 16, 0);
+    p[3] = (__m256)__lasx_xvld(srcPtr + 24, 0);
+}
+
+inline void rpp_load40_f32_to_f32_avx(Rpp32f *srcPtr, __m256 *p)
+{
+    p[0] = (__m256)__lasx_xvld(srcPtr, 0);
+    p[1] = (__m256)__lasx_xvld(srcPtr + 8, 0);
+    p[2] = (__m256)__lasx_xvld(srcPtr + 16, 0);
+    p[3] = (__m256)__lasx_xvld(srcPtr + 24, 0);
+    p[4] = (__m256)__lasx_xvld(srcPtr + 32, 0);
+}
+
+inline void rpp_store16_f32_to_f32_avx(Rpp32f *dstPtr, __m256 *p)
+{
+    __lasx_xvst((__m256i)p[0], dstPtr, 0);
+    __lasx_xvst((__m256i)p[1], dstPtr + 8, 0);
+}
+
+inline void rpp_store16_f32_to_f16_avx(Rpp16f *dstPtr, __m256 *p)
+{
+    __m128i px128[2];
+    px128[0] = lasx_cvtf32_ph(p[0], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    px128[1] = lasx_cvtf32_ph(p[1], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    __lsx_vst(px128[0], (__m128i *)dstPtr, 0);
+    __lsx_vst(px128[1], (__m128i *)(dstPtr + 8), 0);
+}
+
+inline void rpp_load8_f32_to_f32_avx(Rpp32f *srcPtr, __m256 *p)
+{
+    p[0] = (__m256)__lasx_xvld(srcPtr, 0);
+}
+
+inline void rpp_load8_f16_to_f32_avx(Rpp16f *srcPtr, __m256 *p)
+{
+    p[0] =  lasx_cvtph_f32((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtr), 0)));
+}
+
+inline void rpp_load8_f32_to_f32_mirror_avx(Rpp32f *srcPtr, __m256 *p)
+{
+    __m256i pxMask = lasx_setr_i32(7, 6, 5, 4, 3, 2, 1, 0);
+
+    p[0] = (__m256)__lasx_xvld(srcPtr, 0);
+    p[0] = lasx_permutevar8x32_f32(p[0], pxMask); /* shuffle as R08-R01 */
+}
+
+inline void rpp_store8_f32_to_f32_avx(Rpp32f *dstPtr, __m256 *p)
+{
+    __lasx_xvst((__m256i)p[0], dstPtr, 0);
+}
+
+inline void rpp_load8_f32_to_f64_avx(Rpp32f *srcPtr, __m256d *p)
+{
+    __m128 px128[2];
+    __m256 px;
+    px = (__m256)__lasx_xvld(srcPtr, 0);
+    px128[0] = (__m128)__lasx_cvt_256_128((__m256i)px);
+    px128[1] = lasx_extractf128_f32(px, 1);
+    p[0] = lasx_cvtf32_f64(px128[0]);
+    p[1] = lasx_cvtf32_f64(px128[1]);
+}
+
+inline void rpp_store8_u32_to_u32_avx(Rpp32u *dstPtr, __m256i *p)
+{
+    __lasx_xvst(p[0], (__m256i *)dstPtr, 0);
+}
+
+inline void rpp_store8_i32_to_i32_avx(Rpp32s *dstPtr, __m256i *p)
+{
+    __lasx_xvst(p[0], (__m256i *)dstPtr, 0);
+}
+
+inline void rpp_store4_f64_to_f64_avx(Rpp64f *dstPtr, __m256d *p)
+{
+    __lasx_xvst((__m256i)p[0], dstPtr, 0);
+}
+
+inline void rpp_store16_u8_to_u8(Rpp8u *dstPtr, __m128i *p)
+{
+    __lsx_vst(p[0], (__m128i *)dstPtr, 0);
+}
+
+inline void rpp_store16_i8(Rpp8s *dstPtr, __m128i *p)
+{
+    __lsx_vst(p[0], (__m128i *)dstPtr, 0);
+}
+
+inline void rpp_store8_f32_to_f16_avx(Rpp16f *dstPtr, __m256 *p)
+{
+    __m128i px128 = lasx_cvtf32_ph(p[0], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    __lsx_vst(px128, (__m128i *)dstPtr, 0);
+}
+
+inline void rpp_load48_i8pkd3_to_f32pln3_avx(Rpp8s *srcPtr, __m256 *p)
+{
+    __m128i px[4];
+
+    px[0] = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)srcPtr, 0));           /* add I8 conversion param to load [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|R05|G05|B05|R06] - Need RGB 01-04 */
+    px[1] = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)(srcPtr + 12), 0));    /* add I8 conversion param to load [R05|G05|B05|R06|G06|B06|R07|G07|B07|R08|G08|B08|R09|G09|B09|R10] - Need RGB 05-08 */
+    px[2] = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)(srcPtr + 24), 0));    /* add I8 conversion param to load [R09|G09|B09|R10|G10|B10|R11|G11|B11|R12|G12|B12|R13|G13|B13|R14] - Need RGB 09-12 */
+    px[3] = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)(srcPtr + 36), 0));    /* add I8 conversion param to load [R13|G13|B13|R14|G14|B14|R15|G15|B15|R16|G16|B16|R17|G17|B17|R18] - Need RGB 13-16 */
+    p[0] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[0], xmm_pxMaskR), lsx_shuffle_i8(px[1], xmm_pxMaskR)));    /* Contains R01-08 */
+    p[1] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMaskR), lsx_shuffle_i8(px[3], xmm_pxMaskR)));    /* Contains R09-16 */
+    p[2] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[0], xmm_pxMaskG), lsx_shuffle_i8(px[1], xmm_pxMaskG)));    /* Contains G01-08 */
+    p[3] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMaskG), lsx_shuffle_i8(px[3], xmm_pxMaskG)));    /* Contains G09-16 */
+    p[4] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[0], xmm_pxMaskB), lsx_shuffle_i8(px[1], xmm_pxMaskB)));    /* Contains B01-08 */
+    p[5] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMaskB), lsx_shuffle_i8(px[3], xmm_pxMaskB)));    /* Contains B09-16 */
+}
+
+inline void rpp_load48_i8pkd3_to_f32pln3_mirror_avx(Rpp8s *srcPtr, __m256 *p)
+{
+    __m128i px[4];
+
+    px[0] = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)srcPtr, 0));           /* add I8 conversion param to load [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|R05|G05|B05|R06] - Need RGB 01-04 */
+    px[1] = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)(srcPtr + 12), 0));    /* add I8 conversion param to load [R05|G05|B05|R06|G06|B06|R07|G07|B07|R08|G08|B08|R09|G09|B09|R10] - Need RGB 05-08 */
+    px[2] = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)(srcPtr + 24), 0));    /* add I8 conversion param to load [R09|G09|B09|R10|G10|B10|R11|G11|B11|R12|G12|B12|R13|G13|B13|R14] - Need RGB 09-12 */
+    px[3] = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)(srcPtr + 36), 0));    /* add I8 conversion param to load [R13|G13|B13|R14|G14|B14|R15|G15|B15|R16|G16|B16|R17|G17|B17|R18] - Need RGB 13-16 */
+    p[0] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[3], xmm_pxMaskRMirror), lsx_shuffle_i8(px[2], xmm_pxMaskRMirror)));    /* Contains R01-08 */
+    p[1] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[1], xmm_pxMaskRMirror), lsx_shuffle_i8(px[0], xmm_pxMaskRMirror)));    /* Contains R09-16 */
+    p[2] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[3], xmm_pxMaskGMirror), lsx_shuffle_i8(px[2], xmm_pxMaskGMirror)));    /* Contains G01-08 */
+    p[3] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[1], xmm_pxMaskGMirror), lsx_shuffle_i8(px[0], xmm_pxMaskGMirror)));    /* Contains G09-16 */
+    p[4] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[3], xmm_pxMaskBMirror), lsx_shuffle_i8(px[2], xmm_pxMaskBMirror)));    /* Contains B01-08 */
+    p[5] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[1], xmm_pxMaskBMirror), lsx_shuffle_i8(px[0], xmm_pxMaskBMirror)));    /* Contains B09-16 */
+}
+
+inline void rpp_store48_f32pln3_to_i8pln3_avx(Rpp8s *dstPtrR, Rpp8s *dstPtrG, Rpp8s *dstPtrB, __m256 *p)
+{
+    __m256i pxCvt;
+    __m128i px[4];
+
+    pxCvt = __lasx_xvftint_w_s(p[0]);
+    px[2] = lsx_packus_i32(lasx_extracti128_m256i(pxCvt, 0), lasx_extracti128_m256i(pxCvt, 1));    /* pack pixels 0-7 for R */
+    pxCvt = __lasx_xvftint_w_s(p[1]);
+    px[3] = lsx_packus_i32(lasx_extracti128_m256i(pxCvt, 0), lasx_extracti128_m256i(pxCvt, 1));    /* pack pixels 8-15 for R */
+    px[0] = lsx_packus_i16(px[2], px[3]);    /* pack pixels 0-15 for R */
+    pxCvt = __lasx_xvftint_w_s(p[2]);
+    px[2] = lsx_packus_i32(lasx_extracti128_m256i(pxCvt, 0), lasx_extracti128_m256i(pxCvt, 1));    /* pack pixels 0-7 for G */
+    pxCvt = __lasx_xvftint_w_s(p[3]);
+    px[3] = lsx_packus_i32(lasx_extracti128_m256i(pxCvt, 0), lasx_extracti128_m256i(pxCvt, 1));    /* pack pixels 8-15 for G */
+    px[1] = lsx_packus_i16(px[2], px[3]);    /* pack pixels 0-15 for G */
+    pxCvt = __lasx_xvftint_w_s(p[4]);
+    px[2] = lsx_packus_i32(lasx_extracti128_m256i(pxCvt, 0), lasx_extracti128_m256i(pxCvt, 1));    /* pack pixels 0-7 for B */
+    pxCvt = __lasx_xvftint_w_s(p[5]);
+    px[3] = lsx_packus_i32(lasx_extracti128_m256i(pxCvt, 0), lasx_extracti128_m256i(pxCvt, 1));    /* pack pixels 8-15 for B */
+    px[2] = lsx_packus_i16(px[2], px[3]);    /* pack pixels 0-15 for B */
+    px[0] = __lsx_vsub_b(px[0], xmm_pxConvertI8);    /* convert back to i8 for px0 store */
+    px[1] = __lsx_vsub_b(px[1], xmm_pxConvertI8);    /* convert back to i8 for px1 store */
+    px[2] = __lsx_vsub_b(px[2], xmm_pxConvertI8);    /* convert back to i8 for px2 store */
+    __lsx_vst(px[0], (__m128i *)dstPtrR, 0);    /* store [R01|R02|R03|R04|R05|R06|R07|R08|R09|R10|R11|R12|R13|R14|R15|R16] */
+    __lsx_vst(px[1], (__m128i *)dstPtrG, 0);    /* store [G01|G02|G03|G04|G05|G06|G07|G08|G09|G10|G11|G12|G13|G14|G15|G16] */
+    __lsx_vst(px[2], (__m128i *)dstPtrB, 0);    /* store [B01|B02|B03|B04|B05|B06|B07|B08|B09|B10|B11|B12|B13|B14|B15|B16] */
+}
+
+inline void rpp_load48_i8pln3_to_f32pln3_avx(Rpp8s *srcPtrR, Rpp8s *srcPtrG, Rpp8s *srcPtrB, __m256 *p)
+{
+    __m128i px[3];
+
+    px[0] = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)srcPtrR, 0));    /* add I8 conversion param to load [R01|R02|R03|R04|R05|R06|R07|R08|R09|R10|R11|R12|R13|R14|R15|R16] */
+    px[1] = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)srcPtrG, 0));    /* add I8 conversion param to load [G01|G02|G03|G04|G05|G06|G07|G08|G09|G10|G11|G12|G13|G14|G15|G16] */
+    px[2] = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)srcPtrB, 0));    /* add I8 conversion param to load [B01|B02|B03|B04|B05|B06|B07|B08|B09|B10|B11|B12|B13|B14|B15|B16] */
+    p[0] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[0], xmm_pxMask00To03), lsx_shuffle_i8(px[0], xmm_pxMask04To07)));    /* Contains R01-08 */
+    p[1] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[0], xmm_pxMask08To11), lsx_shuffle_i8(px[0], xmm_pxMask12To15)));    /* Contains R09-16 */
+    p[2] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[1], xmm_pxMask00To03), lsx_shuffle_i8(px[1], xmm_pxMask04To07)));    /* Contains G01-08 */
+    p[3] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[1], xmm_pxMask08To11), lsx_shuffle_i8(px[1], xmm_pxMask12To15)));    /* Contains G09-16 */
+    p[4] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMask00To03), lsx_shuffle_i8(px[2], xmm_pxMask04To07)));    /* Contains B01-08 */
+    p[5] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMask08To11), lsx_shuffle_i8(px[2], xmm_pxMask12To15)));    /* Contains B09-16 */
+}
+
+inline void rpp_load48_i8pln3_to_f32pln3_mirror_avx(Rpp8s *srcPtrR, Rpp8s *srcPtrG, Rpp8s *srcPtrB, __m256 *p)
+{
+    __m128i px[3];
+
+    px[0] = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)srcPtrR, 0));    /* add I8 conversion param to load [R01|R02|R03|R04|R05|R06|R07|R08|R09|R10|R11|R12|R13|R14|R15|R16] */
+    px[1] = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)srcPtrG, 0));    /* add I8 conversion param to load [G01|G02|G03|G04|G05|G06|G07|G08|G09|G10|G11|G12|G13|G14|G15|G16] */
+    px[2] = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)srcPtrB, 0));    /* add I8 conversion param to load [B01|B02|B03|B04|B05|B06|B07|B08|B09|B10|B11|B12|B13|B14|B15|B16] */
+    p[0] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[0], xmm_pxMask15To12), lsx_shuffle_i8(px[0], xmm_pxMask11To08)));    /* Contains R01-08 */
+    p[1] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[0], xmm_pxMask07To04), lsx_shuffle_i8(px[0], xmm_pxMask03To00)));    /* Contains R09-16 */
+    p[2] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[1], xmm_pxMask15To12), lsx_shuffle_i8(px[1], xmm_pxMask11To08)));    /* Contains G01-08 */
+    p[3] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[1], xmm_pxMask07To04), lsx_shuffle_i8(px[1], xmm_pxMask03To00)));    /* Contains G09-16 */
+    p[4] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMask15To12), lsx_shuffle_i8(px[2], xmm_pxMask11To08)));    /* Contains B01-08 */
+    p[5] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMask07To04), lsx_shuffle_i8(px[2], xmm_pxMask03To00)));    /* Contains B09-16 */
+}
+
+inline void rpp_store48_f32pln3_to_i8pkd3_avx(Rpp8s *dstPtr, __m256 *p)
+{
+    __m256i pxCvt[3];
+    __m128i px[5];
+    __m128i pxMask = lsx_setr_i8(0, 4, 8, 1, 5, 9, 2, 6, 10, 3, 7, 11, 12, 13, 14, 15);
+
+    pxCvt[0] = __lasx_xvftint_w_s(p[0]);    /* convert to int32 for R01-08 */
+    pxCvt[1] = __lasx_xvftint_w_s(p[2]);    /* convert to int32 for G01-08 */
+    pxCvt[2] = __lasx_xvftint_w_s(p[4]);    /* convert to int32 for B01-08 */
+    px[3] = lsx_packus_i32(lasx_extracti128_m256i(pxCvt[0], 0), lasx_extracti128_m256i(pxCvt[1], 0));    /* pack pixels 0-7 as R01-04|G01-04 */
+    px[4] = lsx_packus_i32(lasx_extracti128_m256i(pxCvt[2], 0), xmm_px0);    /* pack pixels 8-15 as B01-04|X01-04 */
+    px[0] = lsx_packus_i16(px[3], px[4]);    /* pack pixels 0-15 as [R01|R02|R03|R04|G01|G02|G03|G04|B01|B02|B03|B04|00|00|00|00] */
+    px[3] = lsx_packus_i32(lasx_extracti128_m256i(pxCvt[0], 1), lasx_extracti128_m256i(pxCvt[1], 1));    /* pack pixels 0-7 as R05-08|G05-08 */
+    px[4] = lsx_packus_i32(lasx_extracti128_m256i(pxCvt[2], 1), xmm_px0);    /* pack pixels 8-15 as B05-08|X05-08 */
+    px[1] = lsx_packus_i16(px[3], px[4]);    /* pack pixels 0-15 as [R05|R06|R07|R08|G05|G06|G07|G08|B05|B06|B07|B08|00|00|00|00] */
+    pxCvt[0] = __lasx_xvftint_w_s(p[1]);    /* convert to int32 for R09-16 */
+    pxCvt[1] = __lasx_xvftint_w_s(p[3]);    /* convert to int32 for G09-16 */
+    pxCvt[2] = __lasx_xvftint_w_s(p[5]);    /* convert to int32 for B09-16 */
+    px[3] = lsx_packus_i32(lasx_extracti128_m256i(pxCvt[0], 0), lasx_extracti128_m256i(pxCvt[1], 0));    /* pack pixels 0-7 as R09-12|G09-12 */
+    px[4] = lsx_packus_i32(lasx_extracti128_m256i(pxCvt[2], 0), xmm_px0);    /* pack pixels 8-15 as B09-12|X09-12 */
+    px[2] = lsx_packus_i16(px[3], px[4]);    /* pack pixels 0-15 as [R09|R10|R11|R12|G09|G10|G11|G12|B09|B10|B11|B12|00|00|00|00] */
+    px[3] = lsx_packus_i32(lasx_extracti128_m256i(pxCvt[0], 1), lasx_extracti128_m256i(pxCvt[1], 1));    /* pack pixels 0-7 as R13-16|G13-16 */
+    px[4] = lsx_packus_i32(lasx_extracti128_m256i(pxCvt[2], 1), xmm_px0);    /* pack pixels 8-15 as B13-16|X13-16 */
+    px[3] = lsx_packus_i16(px[3], px[4]);    /* pack pixels 0-15 as [R13|R14|R15|R16|G13|G14|G15|G16|B13|B14|B15|B16|00|00|00|00] */
+    px[0] = __lsx_vsub_b(px[0], xmm_pxConvertI8);    /* convert back to i8 for px0 store */
+    px[1] = __lsx_vsub_b(px[1], xmm_pxConvertI8);    /* convert back to i8 for px1 store */
+    px[2] = __lsx_vsub_b(px[2], xmm_pxConvertI8);    /* convert back to i8 for px2 store */
+    px[3] = __lsx_vsub_b(px[3], xmm_pxConvertI8);    /* convert back to i8 for px3 store */
+    px[0] = lsx_shuffle_i8(px[0], pxMask);    /* shuffle to get [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|00|00|00|00] */
+    px[1] = lsx_shuffle_i8(px[1], pxMask);    /* shuffle to get [R05|G05|B05|R06|G06|B06|R07|G07|B07|R08|G08|B08|00|00|00|00] */
+    px[2] = lsx_shuffle_i8(px[2], pxMask);    /* shuffle to get [R09|G09|B09|R10|G10|B10|R11|G11|B11|R12|G12|B12|00|00|00|00] */
+    px[3] = lsx_shuffle_i8(px[3], pxMask);    /* shuffle to get [R13|G13|B13|R14|G14|B14|R15|G15|B15|R16|G16|B16|00|00|00|00] */
+    __lsx_vst(px[0], (__m128i *)dstPtr, 0);           /* store [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|00|00|00|00] */
+    __lsx_vst(px[1], (__m128i *)(dstPtr + 12), 0);    /* store [R05|G05|B05|R06|G06|B06|R07|G07|B07|R08|G08|B08|00|00|00|00] */
+    __lsx_vst(px[2], (__m128i *)(dstPtr + 24), 0);    /* store [R09|G09|B09|R10|G10|B10|R11|G11|B11|R12|G12|B12|00|00|00|00] */
+    __lsx_vst(px[3], (__m128i *)(dstPtr + 36), 0);    /* store [R13|G13|B13|R14|G14|B14|R15|G15|B15|R16|G16|B16|00|00|00|00] */
+}
+
+inline void rpp_load24_i8pln3_to_f64pln3_avx(Rpp8s *srcPtrR, Rpp8s *srcPtrG, Rpp8s *srcPtrB, __m256d *p)
+{
+    __m128i px[3];
+
+    px[0] = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)srcPtrR, 0));    /* add I8 conversion param to load [R01|R02|R03|R04|R05|R06|R07|R08|R09|R10|R11|R12|R13|R14|R15|R16] */
+    px[1] = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)srcPtrG, 0));    /* add I8 conversion param to load [G01|G02|G03|G04|G05|G06|G07|G08|G09|G10|G11|G12|G13|G14|G15|G16] */
+    px[2] = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)srcPtrB, 0));    /* add I8 conversion param to load [B01|B02|B03|B04|B05|B06|B07|B08|B09|B10|B11|B12|B13|B14|B15|B16] */
+    p[0] = lasx_cvti32_f64(lsx_shuffle_i8(px[0], xmm_pxMask00To03));    /* Contains R01-04 */
+    p[1] = lasx_cvti32_f64(lsx_shuffle_i8(px[0], xmm_pxMask04To07));    /* Contains R05-08 */
+    p[2] = lasx_cvti32_f64(lsx_shuffle_i8(px[1], xmm_pxMask00To03));    /* Contains G01-04 */
+    p[3] = lasx_cvti32_f64(lsx_shuffle_i8(px[1], xmm_pxMask04To07));    /* Contains G05-08 */
+    p[4] = lasx_cvti32_f64(lsx_shuffle_i8(px[2], xmm_pxMask00To03));    /* Contains B01-04 */
+    p[5] = lasx_cvti32_f64(lsx_shuffle_i8(px[2], xmm_pxMask04To07));    /* Contains B05-08 */
+}
+
+inline void rpp_load24_i8pkd3_to_f64pln3_avx(Rpp8s *srcPtr, __m256d *p)
+{
+    __m128i px[2];
+
+    px[0] = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)srcPtr, 0));           /* add I8 conversion param to load [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|R05|G05|B05|R06] - Need RGB 01-04 */
+    px[1] = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)(srcPtr + 12), 0));    /* add I8 conversion param to load [R05|G05|B05|R06|G06|B06|R07|G07|B07|R08|G08|B08|R09|G09|B09|R10] - Need RGB 05-08 */
+    p[0] = lasx_cvti32_f64(lsx_shuffle_i8(px[0], xmm_pxMaskR));    /* Contains R01-04 */
+    p[1] = lasx_cvti32_f64(lsx_shuffle_i8(px[1], xmm_pxMaskR));    /* Contains R05-08 */
+    p[2] = lasx_cvti32_f64(lsx_shuffle_i8(px[0], xmm_pxMaskG));    /* Contains G01-04 */
+    p[3] = lasx_cvti32_f64(lsx_shuffle_i8(px[1], xmm_pxMaskG));    /* Contains G05-08 */
+    p[4] = lasx_cvti32_f64(lsx_shuffle_i8(px[0], xmm_pxMaskB));    /* Contains B01-04 */
+    p[5] = lasx_cvti32_f64(lsx_shuffle_i8(px[1], xmm_pxMaskB));    /* Contains B05-08 */
+}
+
+inline void rpp_load16_i8_to_f32_avx(Rpp8s *srcPtr, __m256 *p)
+{
+    __m128i px;
+    px = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)srcPtr, 0));    /* add I8 conversion param to load */
+    p[0] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px, xmm_pxMask00To03), lsx_shuffle_i8(px, xmm_pxMask04To07)));    /* Contains pixels 01-08 */
+    p[1] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px, xmm_pxMask08To11), lsx_shuffle_i8(px, xmm_pxMask12To15)));    /* Contains pixels 09-16 */
+}
+
+inline void rpp_load24_i8_to_f32_avx(Rpp8s *srcPtr, __m256 *p)
+{
+    __m128i px1, px2;
+    px1 = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)(srcPtr), 0));
+    px2 = __lsx_vadd_b(xmm_pxConvertI8, lsx_loadl_d((__m128i *)(srcPtr + 16)));
+
+    p[0] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px1, xmm_pxMask00To03), lsx_shuffle_i8(px1, xmm_pxMask04To07)));  /* Contains pixels 01-08 */
+    p[1] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px1, xmm_pxMask08To11), lsx_shuffle_i8(px1, xmm_pxMask12To15)));  /* Contains pixels 09-16 */
+    p[2] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px2, xmm_pxMask00To03), lsx_shuffle_i8(px2, xmm_pxMask04To07)));  /* Contains pixels 17-24 */
+}
+
+inline void rpp_load32_i8_to_f32_avx(Rpp8s *srcPtr, __m256 *p)
+{
+    __m256i px = __lasx_xvadd_b(avx_pxConvertI8, __lasx_xvld((__m256i *)srcPtr, 0));
+    __m128i px1 = __lasx_cvt_256_128(px);
+    __m128i px2 = lasx_extractf128_m256i(px, 1);
+
+    p[0] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px1, xmm_pxMask00To03), lsx_shuffle_i8(px1, xmm_pxMask04To07))); // Contains pixels 01-08
+    p[1] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px1, xmm_pxMask08To11), lsx_shuffle_i8(px1, xmm_pxMask12To15))); // Contains pixels 09-16
+    p[2] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px2, xmm_pxMask00To03), lsx_shuffle_i8(px2, xmm_pxMask04To07))); // Contains pixels 17-24
+    p[3] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px2, xmm_pxMask08To11), lsx_shuffle_i8(px2, xmm_pxMask12To15))); // Contains pixels 25-32
+}
+
+inline void rpp_load40_i8_to_f32_avx(Rpp8s *srcPtr, __m256 *p)
+{
+    __m256i px1 = __lasx_xvadd_b(avx_pxConvertI8, __lasx_xvld((__m256i *)srcPtr, 0));     // Load the first 32 bytes
+    __m128i px2 = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)(srcPtr + 32), 0)); // Load the remaining 8 bytes
+    __m128i px1Low  = __lasx_cvt_256_128(px1);
+    __m128i px1High = lasx_extractf128_m256i(px1, 1);
+
+    p[0] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px1Low, xmm_pxMask00To03), lsx_shuffle_i8(px1Low, xmm_pxMask04To07))); // Pixels 01-08
+    p[1] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px1Low, xmm_pxMask08To11), lsx_shuffle_i8(px1Low, xmm_pxMask12To15))); // Pixels 09-16
+    p[2] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px1High, xmm_pxMask00To03), lsx_shuffle_i8(px1High, xmm_pxMask04To07))); // Pixels 17-24
+    p[3] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px1High, xmm_pxMask08To11), lsx_shuffle_i8(px1High, xmm_pxMask12To15))); // Pixels 25-32
+    p[4] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px2, xmm_pxMask00To03), lsx_shuffle_i8(px2, xmm_pxMask04To07)));        // Pixels 33-40
+}
+
+inline void rpp_load16_i8_to_f32_mirror_avx(Rpp8s *srcPtr, __m256 *p)
+{
+    __m128i px;
+    px = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)srcPtr, 0));    /* add I8 conversion param to load */
+    p[0] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px, xmm_pxMask15To12), lsx_shuffle_i8(px, xmm_pxMask11To08)));    /* Contains pixels 01-08 */
+    p[1] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px, xmm_pxMask07To04), lsx_shuffle_i8(px, xmm_pxMask03To00)));    /* Contains pixels 09-16 */
+}
+
+inline void rpp_store16_f32_to_i8_avx(Rpp8s *dstPtr, __m256 *p)
+{
+    __m256i pxCvt;
+    __m128i px[3];
+    pxCvt = __lasx_xvftint_w_s(p[0]);
+    px[1] = lsx_packus_i32(lasx_extracti128_m256i(pxCvt, 0), lasx_extracti128_m256i(pxCvt, 1));    /* pack pixels 0-7 for R */
+    pxCvt = __lasx_xvftint_w_s(p[1]);
+    px[2] = lsx_packus_i32(lasx_extracti128_m256i(pxCvt, 0), lasx_extracti128_m256i(pxCvt, 1));    /* pack pixels 8-15 for R */
+    px[0] = lsx_packus_i16(px[1], px[2]);    /* pack pixels 0-15 */
+    px[0] = __lsx_vsub_b(px[0], xmm_pxConvertI8);    /* convert back to i8 for px0 store */
+    __lsx_vst(px[0], (__m128i *)dstPtr, 0);
+}
+
+inline void rpp_load16_i8_to_i32_avx(Rpp8s *srcPtr, __m256i *p)
+{
+    __m128i px;
+    px = __lsx_vld((__m128i *)srcPtr, 0);
+    p[0] = lasx_cvti8_i32(px);    /* Contains pixels 01-08 */
+    p[1] = lasx_cvti8_i32(lsx_shuffle_i8(px, xmm_pxMask08To15));    /* Contains pixels 09-16 */
+}
+
+inline void rpp_normalize48_avx(__m256 *p)
+{
+    p[0] = __lasx_xvfmul_s(p[0], avx_p1op255);
+    p[1] = __lasx_xvfmul_s(p[1], avx_p1op255);
+    p[2] = __lasx_xvfmul_s(p[2], avx_p1op255);
+    p[3] = __lasx_xvfmul_s(p[3], avx_p1op255);
+    p[4] = __lasx_xvfmul_s(p[4], avx_p1op255);
+    p[5] = __lasx_xvfmul_s(p[5], avx_p1op255);
+}
+
+inline void rpp_normalize24_avx(__m256 *p)
+{
+    p[0] = __lasx_xvfmul_s(p[0], avx_p1op255);
+    p[1] = __lasx_xvfmul_s(p[1], avx_p1op255);
+    p[2] = __lasx_xvfmul_s(p[2], avx_p1op255);
+}
+
+inline void rpp_multiply48_constant(__m256 *p, __m256 pMultiplier)
+{
+    p[0] = __lasx_xvfmul_s(p[0], pMultiplier);
+    p[1] = __lasx_xvfmul_s(p[1], pMultiplier);
+    p[2] = __lasx_xvfmul_s(p[2], pMultiplier);
+    p[3] = __lasx_xvfmul_s(p[3], pMultiplier);
+    p[4] = __lasx_xvfmul_s(p[4], pMultiplier);
+    p[5] = __lasx_xvfmul_s(p[5], pMultiplier);
+}
+
+inline void rpp_multiply48_constant(__m128 *p, __m128 pMultiplier)
+{
+    p[0] = __lsx_vfmul_s(p[0], pMultiplier);
+    p[1] = __lsx_vfmul_s(p[1], pMultiplier);
+    p[2] = __lsx_vfmul_s(p[2], pMultiplier);
+    p[3] = __lsx_vfmul_s(p[3], pMultiplier);
+    p[4] = __lsx_vfmul_s(p[4], pMultiplier);
+    p[5] = __lsx_vfmul_s(p[5], pMultiplier);
+    p[6] = __lsx_vfmul_s(p[6], pMultiplier);
+    p[7] = __lsx_vfmul_s(p[7], pMultiplier);
+    p[8] = __lsx_vfmul_s(p[8], pMultiplier);
+    p[9] = __lsx_vfmul_s(p[9], pMultiplier);
+    p[10] = __lsx_vfmul_s(p[10], pMultiplier);
+    p[11] = __lsx_vfmul_s(p[11], pMultiplier);
+}
+
+inline void rpp_multiply24_constant(__m256 *p, __m256 pMultiplier)
+{
+    p[0] = __lasx_xvfmul_s(p[0], pMultiplier);
+    p[1] = __lasx_xvfmul_s(p[1], pMultiplier);
+    p[2] = __lasx_xvfmul_s(p[2], pMultiplier);
+}
+
+inline void rpp_multiply16_constant(__m256 *p, __m256 pMultiplier)
+{
+    p[0] = __lasx_xvfmul_s(p[0], pMultiplier);
+    p[1] = __lasx_xvfmul_s(p[1], pMultiplier);
+}
+
+inline void rpp_multiply16_constant(__m128 *p, __m128 pMultiplier)
+{
+    p[0] = __lsx_vfmul_s(p[0], pMultiplier);
+    p[1] = __lsx_vfmul_s(p[1], pMultiplier);
+    p[2] = __lsx_vfmul_s(p[2], pMultiplier);
+    p[3] = __lsx_vfmul_s(p[3], pMultiplier);
+}
+
+template <typename FuncType, typename... ArgTypes>
+inline void rpp_simd_load(FuncType &&rpp_simd_load_routine, ArgTypes&&... args)
+{
+    std::forward<FuncType>(rpp_simd_load_routine)(std::forward<ArgTypes>(args)...);
+}
+
+template <typename FuncType, typename... ArgTypes>
+inline void rpp_simd_store(FuncType &&rpp_simd_store_routine, ArgTypes&&... args)
+{
+    std::forward<FuncType>(rpp_simd_store_routine)(std::forward<ArgTypes>(args)...);
+}
+
+// Shuffle floats in `src` by using SSE2 `pshufd` instead of `shufps`, if possible.
+#define SIMD_SHUFFLE_PS(src, imm) \
+    (__m128)(__lsx_vshuf4i_w((__m128i)(src), imm))
+
+#define CHECK_SIMD  0
+#define FP_BITS     16
+#define FP_MUL      (1<<FP_BITS)
+
+const __m128 xmm_full = lsx_set1_f32((float)0xFFFFFFFF);
+const __m128 xmm_sn = (__m128)(__lsx_vreplgr2vr_w(0x80000000));
+const __m128 xmm_m6 = lsx_set1_f32((float)-6.0);
+const __m128 xmm_eps = lsx_set1_f32((float)1e-9f);
+const __m128 xmm_1o3 = lsx_set1_f32((float)1.0/3.0);
+const __m128 xmm_m4o6 = lsx_set1_f32((float) -4.0/6.0);
+const __m128 xmm_abs = lsx_set1_f32((float)0x80000000);
+
+const __m128 xmm_4o6_2o6_3o6_0  = lsx_set_f32(4.0f / 6.0f, 2.0f / 6.0f, 3.0f / 6.0f, 0.0f);
+const __m128 m6_m6_p6_p0        = lsx_set_f32(-6.0f ,-6.0f , 6.0f , 0.0f);
+const __m128 p1_p1_m2_p0        = lsx_set_f32(1.0f , 1.0f ,-2.0f , 0.0f);
+const __m128 m1_m1_m1_p1        = lsx_set_f32(-1.0f ,-1.0f ,-1.0f , 1.0f);
+
+
+SIMD_CONST_PI(full       , 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF);
+SIMD_CONST_PI(sn         , 0x80000000, 0x80000000, 0x80000000, 0x80000000);
+SIMD_CONST_PS(m6_m6_m6_m6,-6.0f ,-6.0f ,-6.0f ,-6.0f);
+SIMD_CONST_PS(m4o6_m4o6_m4o6_m4o6,-4.0f / 6.0f,-4.0f / 6.0f,-4.0f / 6.0f,-4.0f / 6.0f);
+SIMD_CONST_PS(eps        , 1e-9f, 1e-9f, 1e-9f, 1e-9f);
+SIMD_CONST_PS(p1         , 1.0f , 1.0f , 1.0f , 1.0f);
+
+SIMD_CONST_PS(p4o6_p2o6_p3o6_p0  , 4.0f / 6.0f, 2.0f / 6.0f, 3.0f / 6.0f, 0.0f);
+SIMD_CONST_PI(abs        , 0x7FFFFFFF, 0x7FFFFFFF, 0x7FFFFFFF, 0x7FFFFFFF);
+SIMD_CONST_PS(m6_m6_p6_p0,-6.0f ,-6.0f , 6.0f , 0.0f);
+SIMD_CONST_PS(p1_p1_m2_p0, 1.0f , 1.0f ,-2.0f , 0.0f);
+SIMD_CONST_PS(m1_m1_m1_p1,-1.0f ,-1.0f ,-1.0f , 1.0f);
+SIMD_CONST_PS(p0         , 0.0f , 0.0f , 0.0f , 0.0f);
+
+inline __m128i _mm_mullo_epi8(__m128i a, __m128i b)
+{
+    __m128i zero = __lsx_vldi(0);
+    __m128i Alo = __lsx_vsllwil_hu_bu(a, 0);
+    __m128i Ahi = __lsx_vilvh_b(zero, a);
+    __m128i Blo = __lsx_vsllwil_hu_bu(b, 0);
+    __m128i Bhi = __lsx_vilvh_b(zero, b);
+    __m128i Clo = __lsx_vmul_h(Alo, Blo);
+    __m128i Chi = __lsx_vmul_h(Ahi, Bhi);
+    __m128i maskLo = lsx_set_i8(0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 14, 12, 10, 8, 6, 4, 2, 0);
+    __m128i maskHi = lsx_set_i8(14, 12, 10, 8, 6, 4, 2, 0, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80);
+    __m128i C = __lsx_vor_v(lsx_shuffle_i8(Clo, maskLo), lsx_shuffle_i8(Chi, maskHi));
+
+     return C;
+}
+
+static inline Rpp32u HorMin(__m128i pmin)
+{
+    pmin = __lsx_vmin_bu(pmin, __lsx_vshuf4i_w(pmin, _MM_SHUFFLE(3, 2, 3, 2)));
+    pmin = __lsx_vmin_bu(pmin, __lsx_vshuf4i_w(pmin, _MM_SHUFFLE(1, 1, 1, 1)));
+    pmin = __lsx_vmin_bu(pmin, lsx_shufflelo_i16(pmin, _MM_SHUFFLE(1, 1, 1, 1)));
+    pmin = __lsx_vmin_bu(pmin, lsx_srli_i16(pmin, 8));
+    return (__lsx_vpickve2gr_w(pmin, 0) & 0x000000FF);
+}
+
+static inline Rpp32u HorMax(__m128i pmax)
+{
+    pmax = __lsx_vmin_bu(pmax, __lsx_vshuf4i_w(pmax, _MM_SHUFFLE(3, 2, 3, 2)));
+    pmax = __lsx_vmin_bu(pmax, __lsx_vshuf4i_w(pmax, _MM_SHUFFLE(1, 1, 1, 1)));
+    pmax = __lsx_vmin_bu(pmax, lsx_shufflelo_i16(pmax, _MM_SHUFFLE(1, 1, 1, 1)));
+    pmax = __lsx_vmin_bu(pmax, lsx_srli_i16(pmax, 8));
+    return (__lsx_vpickve2gr_w(pmax, 0) & 0x000000FF);
+}
+
+static inline Rpp32u HorMin256(__m256i pmin)
+{
+    __m128i pmin_128;
+    pmin = __lasx_xvmin_bu(pmin, __lasx_xvpermi_d(pmin, _MM_SHUFFLE(3, 2, 3, 2)));
+    pmin = __lasx_xvmin_bu(pmin, __lasx_xvpermi_d(pmin, _MM_SHUFFLE(1, 1, 1, 1)));
+    pmin_128 = M256I(pmin).m256i_i128[0];
+    pmin_128 = __lsx_vmin_bu(pmin_128, lsx_shufflelo_i16(pmin_128, _MM_SHUFFLE(1, 1, 1, 1)));
+    pmin_128 = __lsx_vmin_bu(pmin_128, lsx_srli_i16(pmin_128, 8));
+    return (__lsx_vpickve2gr_w(pmin_128, 0) & 0x000000FF);
+}
+
+static inline Rpp32u HorMax256(__m256i pmax)
+{
+    __m128i pmax_128;
+    pmax = __lasx_xvmax_bu(pmax, __lasx_xvpermi_d(pmax, _MM_SHUFFLE(3, 2, 3, 2)));
+    pmax = __lasx_xvmax_bu(pmax, __lasx_xvpermi_d(pmax, _MM_SHUFFLE(1, 1, 1, 1)));
+    pmax_128 = M256I(pmax).m256i_i128[0];
+    pmax_128 = __lsx_vmax_b(pmax_128, lsx_shufflelo_i16(pmax_128, _MM_SHUFFLE(1, 1, 1, 1)));
+    pmax_128 = __lsx_vmax_b(pmax_128, lsx_srli_i16(pmax_128, 8));
+    return (__lsx_vpickve2gr_w(pmax_128, 0) & 0x000000FF);
+}
+
+static  inline __m128 fast_exp_sse (__m128 x)
+{
+    __m128 t, f, e, p, r;
+    __m128i i, j;
+    __m128 l2e = lsx_set1_f32 (1.442695041f);  /* log2(e) */
+    __m128 c0  = lsx_set1_f32 (0.3371894346f);
+    __m128 c1  = lsx_set1_f32 (0.657636276f);
+    __m128 c2  = lsx_set1_f32 (1.00172476f);
+
+    /* exp(x) = 2^i * 2^f; i = floor (log2(e) * x), 0 <= f <= 1 */
+    t = __lsx_vfmul_s (x, l2e);             /* t = log2(e) * x */
+#ifdef __SSE4_1__
+    e = __lsx_vfrintrm_s (t);                /* floor(t) */
+    i = __lsx_vftint_w_s (e);             /* (int)floor(t) */
+#else /* __SSE4_1__*/
+    i = __lsx_vftintrz_w_s (t);            /* i = (int)t */
+    j = lsx_srli_i32 ((__m128i) (x), 31); /* signbit(t) */
+    i = __lsx_vsub_w (i, j);            /* (int)t - signbit(t) */
+    e = __lsx_vffint_s_w (i);             /* floor(t) ~= (int)t - signbit(t) */
+#endif /* __SSE4_1__*/
+    f = __lsx_vfsub_s (t, e);               /* f = t - floor(t) */
+    p = c0;                              /* c0 */
+    p = __lsx_vfmul_s (p, f);               /* c0 * f */
+    p = __lsx_vfadd_s (p, c1);              /* c0 * f + c1 */
+    p = __lsx_vfmul_s (p, f);               /* (c0 * f + c1) * f */
+    p = __lsx_vfadd_s (p, c2);              /* p = (c0 * f + c1) * f + c2 ~= 2^f */
+    j = lsx_slli_i32 (i, 23);          /* i << 23 */
+    r = (__m128) (__lsx_vadd_w (j, (__m128i) (p))); /* r = p * 2^i*/
+    return r;
+}
+
+#if __loongarch_asx
+static inline __m256 fast_exp_avx(__m256 x)
+{
+    __m256 t, f, e, p, r;
+    __m256i i, j;
+    __m256 l2e = lasx_set1_f32(1.442695041f);    /* log2(e) */
+    __m256 c0  = lasx_set1_f32(0.3371894346f);
+    __m256 c1  = lasx_set1_f32(0.657636276f);
+    __m256 c2  = lasx_set1_f32(1.00172476f);
+
+    /* exp(x) = 2^i * 2^f; i = floor (log2(e) * x), 0 <= f <= 1 */
+    t = __lasx_xvfmul_s(x, l2e);             /* t = log2(e) * x */
+    e = __lasx_xvfrintrm_s(t);                /* floor(t) */
+    i = __lasx_xvftint_w_s(e);             /* (int)floor(t) */
+    f = __lasx_xvfsub_s(t, e);               /* f = t - floor(t) */
+    p = c0;                                /* c0 */
+    p = __lasx_xvfmul_s(p, f);               /* c0 * f */
+    p = __lasx_xvfadd_s(p, c1);              /* c0 * f + c1 */
+    p = __lasx_xvfmul_s(p, f);               /* (c0 * f + c1) * f */
+    p = __lasx_xvfadd_s(p, c2);              /* p = (c0 * f + c1) * f + c2 ~= 2^f */
+    j = lasx_slli_i32(i, 23);          /* i << 23 */
+    r = (__m256)(__lasx_xvadd_w(j, (__m256i)(p)));    /* r = p * 2^i*/
+    return r;
+}
+#endif
+
+#define set1_ps_hex(x) (__m128)(__lsx_vreplgr2vr_w(x))
+#define set1_ps_hex_avx(x) (__m256)(__lasx_xvreplgr2vr_w(x))
+
+static const __m128 _ps_0 = lsx_set1_f32(0.f);
+static const __m128 _ps_1 = lsx_set1_f32(1.f);
+static const __m128 _ps_0p5 = lsx_set1_f32(0.5f);
+static const __m128 _ps_n0p5 = lsx_set1_f32(-0.5f);
+static const __m128 _ps_1p5 = lsx_set1_f32(1.5f);
+static const __m128 _ps_min_norm_pos = set1_ps_hex(0x00800000);
+static const __m128 _ps_mant_mask = set1_ps_hex(0x7f800000);
+static const __m128 _ps_inv_mant_mask = set1_ps_hex(~0x7f800000);
+static const __m128 _ps_sign_mask = set1_ps_hex(0x80000000);
+static const __m128 _ps_inv_sign_mask = set1_ps_hex(~0x80000000);
+
+static const __m128i _pi32_1 = __lsx_vreplgr2vr_w(1);
+static const __m128i _pi32_inv1 = __lsx_vreplgr2vr_w(~1);
+static const __m128i _pi32_2 = __lsx_vreplgr2vr_w(2);
+static const __m128i _pi32_4 = __lsx_vreplgr2vr_w(4);
+static const __m128i _pi32_0x7f = __lsx_vreplgr2vr_w(0x7f);
+
+static const __m128 _ps_minus_cephes_DP1 = lsx_set1_f32(-0.78515625f);
+static const __m128 _ps_minus_cephes_DP2 = lsx_set1_f32(-2.4187564849853515625e-4f);
+static const __m128 _ps_minus_cephes_DP3 = lsx_set1_f32(-3.77489497744594108e-8f);
+static const __m128 _ps_sincof_p0 = lsx_set1_f32(-1.9515295891E-4f);
+static const __m128 _ps_sincof_p1 = lsx_set1_f32( 8.3321608736E-3f);
+static const __m128 _ps_sincof_p2 = lsx_set1_f32(-1.6666654611E-1f);
+static const __m128 _ps_coscof_p0 = lsx_set1_f32( 2.443315711809948E-005f);
+static const __m128 _ps_coscof_p1 = lsx_set1_f32(-1.388731625493765E-003f);
+static const __m128 _ps_coscof_p2 = lsx_set1_f32( 4.166664568298827E-002f);
+static const __m128 _ps_cephes_FOPI = lsx_set1_f32(1.27323954473516f); // 4 / M_PI
+
+static const __m256 _ps_0p5_avx = lasx_set1_f32(0.5f);
+static const __m256 _ps_n0p5_avx = lasx_set1_f32(-0.5f);
+static const __m256 _ps_1p5_avx = lasx_set1_f32(1.5f);
+static const __m256 _ps_min_norm_pos_avx = set1_ps_hex_avx(0x00800000);
+static const __m256 _ps_inv_mant_mask_avx = set1_ps_hex_avx(~0x7f800000);
+static const __m256 _ps_sign_mask_avx = set1_ps_hex_avx(0x80000000);
+static const __m256 _ps_inv_sign_mask_avx = set1_ps_hex_avx(~0x80000000);
+
+static const __m256i _pi32_1_avx = __lasx_xvreplgr2vr_w(1);
+static const __m256i _pi32_inv1_avx = __lasx_xvreplgr2vr_w(~1);
+static const __m256i _pi32_2_avx = __lasx_xvreplgr2vr_w(2);
+static const __m256i _pi32_4_avx = __lasx_xvreplgr2vr_w(4);
+static const __m256i _pi32_0x7f_avx = __lasx_xvreplgr2vr_w(0x7f);
+
+static const __m256 _ps_minus_cephes_DP1_avx = lasx_set1_f32(-0.78515625f);
+static const __m256 _ps_minus_cephes_DP2_avx = lasx_set1_f32(-2.4187564849853515625e-4f);
+static const __m256 _ps_minus_cephes_DP3_avx = lasx_set1_f32(-3.77489497744594108e-8f);
+static const __m256 _ps_sincof_p0_avx = lasx_set1_f32(-1.9515295891E-4f);
+static const __m256 _ps_sincof_p1_avx = lasx_set1_f32( 8.3321608736E-3f);
+static const __m256 _ps_sincof_p2_avx = lasx_set1_f32(-1.6666654611E-1f);
+static const __m256 _ps_coscof_p0_avx = lasx_set1_f32( 2.443315711809948E-005f);
+static const __m256 _ps_coscof_p1_avx = lasx_set1_f32(-1.388731625493765E-003f);
+static const __m256 _ps_coscof_p2_avx = lasx_set1_f32( 4.166664568298827E-002f);
+static const __m256 _ps_cephes_FOPI_avx = lasx_set1_f32(1.27323954473516f); // 4 / M_PI
+
+static inline void sincos_ps(__m256 x, __m256 *s, __m256 *c)
+{
+    // Extract the sign bit (upper one)
+    __m256 sign_bit_sin = (__m256)__lasx_xvand_v((__m256i)x, (__m256i)_ps_sign_mask_avx);
+    // take the absolute value
+    x = (__m256)__lasx_xvxor_v((__m256i)x, (__m256i)sign_bit_sin);
+
+    // Scale by 4/Pi
+    __m256 y = __lasx_xvfmul_s(x, _ps_cephes_FOPI_avx);
+
+    // Store the integer part of y in emm2
+    __m256i emm2 = __lasx_xvftintrz_w_s(y);
+
+    // j=(j+1) & (~1) (see the cephes sources)
+    emm2 = __lasx_xvadd_w(emm2, _pi32_1_avx);
+    emm2 = __lasx_xvand_v(emm2, _pi32_inv1_avx);
+    y = __lasx_xvffint_s_w(emm2);
+
+    __m256i emm4 = emm2;
+
+    // Get the swap sign flag for the sine
+    __m256i emm0 = __lasx_xvand_v(emm2, _pi32_4_avx);
+    emm0 = lasx_slli_i32(emm0, 29);
+    __m256 swap_sign_bit_sin = (__m256)(emm0);
+
+    // Get the polynom selection mask for the sine
+    emm2 = __lasx_xvand_v(emm2, _pi32_2_avx);
+    emm2 = __lasx_xvseq_w(emm2, __lasx_xvldi(0));
+    __m256 poly_mask = (__m256)(emm2);
+    // The magic pass: "Extended precision modular arithmetic - x = ((x - y * DP1) - y * DP2) - y * DP3;
+    __m256 xmm1 = __lasx_xvfmul_s(y, _ps_minus_cephes_DP1_avx);
+    __m256 xmm2 = __lasx_xvfmul_s(y, _ps_minus_cephes_DP2_avx);
+    __m256 xmm3 = __lasx_xvfmul_s(y, _ps_minus_cephes_DP3_avx);
+    x = __lasx_xvfadd_s(__lasx_xvfadd_s(x, xmm1), __lasx_xvfadd_s(xmm2, xmm3));
+
+    emm4 = __lasx_xvsub_w(emm4, _pi32_2_avx);
+    emm4 = __lasx_xvandn_v(emm4, _pi32_4_avx);
+    emm4 = lasx_slli_i32(emm4, 29);
+    __m256 sign_bit_cos = (__m256)(emm4);
+
+    sign_bit_sin = (__m256)__lasx_xvxor_v((__m256i)sign_bit_sin, (__m256i)swap_sign_bit_sin);
+
+    // Evaluate the first polynom  (0 <= x <= Pi/4)
+    __m256 z = __lasx_xvfmul_s(x,x);
+    y = _ps_coscof_p0_avx;
+
+    y = __lasx_xvfmul_s(y, z);
+    y = __lasx_xvfadd_s(y, _ps_coscof_p1_avx);
+    y = __lasx_xvfmul_s(y, z);
+    y = __lasx_xvfadd_s(y, _ps_coscof_p2_avx);
+    y = __lasx_xvfmul_s(y, __lasx_xvfmul_s(z, z));
+    __m256 tmp = __lasx_xvfmul_s(z, _ps_0p5_avx);
+    y = __lasx_xvfsub_s(y, tmp);
+    y = __lasx_xvfadd_s(y, avx_p1);
+
+    // Evaluate the second polynom  (Pi/4 <= x <= 0)
+
+    __m256 y2 = _ps_sincof_p0_avx;
+    y2 = __lasx_xvfmul_s(y2, z);
+    y2 = __lasx_xvfadd_s(y2, _ps_sincof_p1_avx);
+    y2 = __lasx_xvfmul_s(y2, z);
+    y2 = __lasx_xvfadd_s(y2, _ps_sincof_p2_avx);
+    y2 = __lasx_xvfmul_s(y2, __lasx_xvfmul_s(z, x));
+    y2 = __lasx_xvfadd_s(y2, x);
+
+    // Select the correct result from the two polynoms
+    xmm3 = poly_mask;
+    __m256 ysin2 = (__m256)__lasx_xvand_v((__m256i)xmm3, (__m256i)y2);
+    __m256 ysin1 = (__m256)__lasx_xvandn_v((__m256i)xmm3, (__m256i)y);
+    y2 = __lasx_xvfsub_s(y2,ysin2);
+    y = __lasx_xvfsub_s(y, ysin1);
+
+    xmm1 = __lasx_xvfadd_s(ysin1,ysin2);
+    xmm2 = __lasx_xvfadd_s(y,y2);
+
+    // Update the sign
+    *s = (__m256)__lasx_xvxor_v((__m256i)xmm1, (__m256i)sign_bit_sin);
+    *c = (__m256)__lasx_xvxor_v((__m256i)xmm2, (__m256i)sign_bit_cos);
+}
+
+static inline void sincos_ps(__m128 x, __m128 *s, __m128 *c)
+{
+    // Extract the sign bit (upper one)
+    __m128 sign_bit_sin = (__m128)__lsx_vand_v((__m128i)x, (__m128i)_ps_sign_mask);
+    // take the absolute value
+    x = (__m128)__lsx_vxor_v((__m128i)x, (__m128i)sign_bit_sin);
+
+    // Scale by 4/Pi
+    __m128 y = __lsx_vfmul_s(x, _ps_cephes_FOPI);
+
+    // Store the integer part of y in emm2
+    __m128i emm2 = __lsx_vftintrz_w_s(y);
+
+    // j=(j+1) & (~1) (see the cephes sources)
+    emm2 = __lsx_vadd_w(emm2, _pi32_1);
+    emm2 = __lsx_vand_v(emm2, _pi32_inv1);
+    y = __lsx_vffint_s_w(emm2);
+
+    __m128i emm4 = emm2;
+
+    // Get the swap sign flag for the sine
+    __m128i emm0 = __lsx_vand_v(emm2, _pi32_4);
+    emm0 = lsx_slli_i32(emm0, 29);
+    __m128 swap_sign_bit_sin = (__m128)(emm0);
+
+    // Get the polynom selection mask for the sine
+    emm2 = __lsx_vand_v(emm2, _pi32_2);
+    emm2 = __lsx_vseq_w(emm2, __lsx_vldi(0));
+    __m128 poly_mask = (__m128)(emm2);
+    // The magic pass: "Extended precision modular arithmetic - x = ((x - y * DP1) - y * DP2) - y * DP3;
+    __m128 xmm1 = __lsx_vfmul_s(y, _ps_minus_cephes_DP1);
+    __m128 xmm2 = __lsx_vfmul_s(y, _ps_minus_cephes_DP2);
+    __m128 xmm3 = __lsx_vfmul_s(y, _ps_minus_cephes_DP3);
+    x = __lsx_vfadd_s(__lsx_vfadd_s(x, xmm1), __lsx_vfadd_s(xmm2, xmm3));
+
+    emm4 = __lsx_vsub_w(emm4, _pi32_2);
+    emm4 = __lsx_vandn_v(emm4, _pi32_4);
+    emm4 = lsx_slli_i32(emm4, 29);
+    __m128 sign_bit_cos = (__m128)(emm4);
+
+    sign_bit_sin = (__m128)__lsx_vxor_v((__m128i)sign_bit_sin, (__m128i)swap_sign_bit_sin);
+
+    // Evaluate the first polynom  (0 <= x <= Pi/4)
+    __m128 z = __lsx_vfmul_s(x,x);
+    y = _ps_coscof_p0;
+
+    y = __lsx_vfmul_s(y, z);
+    y = __lsx_vfadd_s(y, _ps_coscof_p1);
+    y = __lsx_vfmul_s(y, z);
+    y = __lsx_vfadd_s(y, _ps_coscof_p2);
+    y = __lsx_vfmul_s(y, __lsx_vfmul_s(z, z));
+    __m128 tmp = __lsx_vfmul_s(z, _ps_0p5);
+    y = __lsx_vfsub_s(y, tmp);
+    y = __lsx_vfadd_s(y, _ps_1);
+
+    // Evaluate the second polynom  (Pi/4 <= x <= 0)
+
+    __m128 y2 = _ps_sincof_p0;
+    y2 = __lsx_vfmul_s(y2, z);
+    y2 = __lsx_vfadd_s(y2, _ps_sincof_p1);
+    y2 = __lsx_vfmul_s(y2, z);
+    y2 = __lsx_vfadd_s(y2, _ps_sincof_p2);
+    y2 = __lsx_vfmul_s(y2, __lsx_vfmul_s(z, x));
+    y2 = __lsx_vfadd_s(y2, x);
+
+    // Select the correct result from the two polynoms
+    xmm3 = poly_mask;
+    __m128 ysin2 = (__m128)__lsx_vand_v((__m128i)xmm3, (__m128i)y2);
+    __m128 ysin1 = (__m128)__lsx_vandn_v((__m128i)xmm3, (__m128i)y);
+    y2 = __lsx_vfsub_s(y2,ysin2);
+    y = __lsx_vfsub_s(y, ysin1);
+
+    xmm1 = __lsx_vfadd_s(ysin1,ysin2);
+    xmm2 = __lsx_vfadd_s(y,y2);
+
+    // Update the sign
+    *s = (__m128)__lsx_vxor_v((__m128i)xmm1, (__m128i)sign_bit_sin);
+    *c = (__m128)__lsx_vxor_v((__m128i)xmm2, (__m128i)sign_bit_cos);
+}
+
+static const __m128 _ps_atanrange_hi = lsx_set1_f32(2.414213562373095);
+static const __m128 _ps_atanrange_lo = lsx_set1_f32(0.4142135623730950);
+static const __m128 _ps_cephes_PIF = lsx_set1_f32(3.141592653589793238);
+static const __m128 _ps_cephes_PIO2F = lsx_set1_f32(1.5707963267948966192);
+static const __m128 _ps_cephes_PIO4F = lsx_set1_f32(0.7853981633974483096);
+
+static const __m128 _ps_atancof_p0 = lsx_set1_f32(8.05374449538e-2);
+static const __m128 _ps_atancof_p1 = lsx_set1_f32(1.38776856032e-1);
+static const __m128 _ps_atancof_p2 = lsx_set1_f32(1.99777106478e-1);
+static const __m128 _ps_atancof_p3 = lsx_set1_f32(3.33329491539e-1);
+
+static inline __m128 atan_ps( __m128 x )
+{
+    __m128 sign_bit, y;
+
+    sign_bit = x;
+    // Take the absolute value
+    x = (__m128)__lsx_vand_v( (__m128i)x, (__m128i)_ps_inv_sign_mask );
+    // Extract the sign bit (upper one)
+    sign_bit = (__m128)__lsx_vand_v( (__m128i)sign_bit, (__m128i)_ps_sign_mask );
+
+    // Range reduction, init x and y depending on range
+
+    // x > 2.414213562373095
+    __m128 cmp0 = (__m128)__lsx_vfcmp_clt_s( x, _ps_atanrange_hi );
+    // x > 0.4142135623730950
+    __m128 cmp1 = (__m128)__lsx_vfcmp_clt_s( x, _ps_atanrange_lo );
+
+    // x > 0.4142135623730950 && !( x > 2.414213562373095 )
+    __m128 cmp2 = (__m128)__lsx_vandn_v( (__m128i)cmp0, (__m128i)cmp1 );
+
+    // -( 1.0/x )
+    __m128 y0 = (__m128)__lsx_vand_v( (__m128i)cmp0, (__m128i)_ps_cephes_PIO2F );
+    __m128 x0 = __lsx_vfdiv_s( _ps_1, x );
+    x0 = (__m128)__lsx_vxor_v( (__m128i)x0, (__m128i)_ps_sign_mask );
+
+    __m128 y1 = (__m128)__lsx_vand_v( (__m128i)cmp2, (__m128i)_ps_cephes_PIO4F );
+    // (x-1.0)/(x+1.0)
+    __m128 x1_o = __lsx_vfsub_s( x, _ps_1 );
+    __m128 x1_u = __lsx_vfadd_s( x, _ps_1 );
+    __m128 x1 = __lsx_vfdiv_s( x1_o, x1_u );
+
+    __m128 x2 = (__m128)__lsx_vand_v( (__m128i)cmp2, (__m128i)x1 );
+    x0 = (__m128)__lsx_vand_v( (__m128i)cmp0, (__m128i)x0 );
+    x2 = (__m128)__lsx_vor_v( (__m128i)x2, (__m128i)x0 );
+    cmp1 = (__m128)__lsx_vor_v( (__m128i)cmp0, (__m128i)cmp2 );
+    x2 = (__m128)__lsx_vand_v( (__m128i)cmp1, (__m128i)x2 );
+    x = (__m128)__lsx_vandn_v( (__m128i)cmp1, (__m128i)x );
+    x = (__m128)__lsx_vor_v( (__m128i)x2, (__m128i)x );
+
+    y = (__m128)__lsx_vor_v( (__m128i)y0, (__m128i)y1 );
+
+    __m128 zz = __lsx_vfmul_s( x, x );
+    __m128 acc = _ps_atancof_p0;
+    acc = __lsx_vfmul_s( acc, zz );
+    acc = __lsx_vfsub_s( acc, _ps_atancof_p1 );
+    acc = __lsx_vfmul_s( acc, zz );
+    acc = __lsx_vfadd_s( acc, _ps_atancof_p2 );
+    acc = __lsx_vfmul_s( acc, zz );
+    acc = __lsx_vfsub_s( acc, _ps_atancof_p3 );
+    acc = __lsx_vfmul_s( acc, zz );
+    acc = __lsx_vfmul_s( acc, x );
+    acc = __lsx_vfadd_s( acc, x );
+    y = __lsx_vfadd_s( y, acc );
+
+    // Update the sign
+    y = (__m128)__lsx_vxor_v( (__m128i)y, (__m128i)sign_bit );
+
+    return y;
+}
+
+static inline __m128 atan2_ps( __m128 y, __m128 x )
+{
+    __m128 x_eq_0 = (__m128)__lsx_vfcmp_ceq_s( x, _ps_0 );
+    __m128 x_gt_0 = (__m128)__lsx_vfcmp_clt_s( x, _ps_0 );
+    __m128 x_le_0 = (__m128)__lsx_vfcmp_cle_s( x, _ps_0 );
+    __m128 y_eq_0 = (__m128)__lsx_vfcmp_ceq_s( y, _ps_0 );
+    __m128 x_lt_0 = (__m128)__lsx_vfcmp_clt_s( x, _ps_0 );
+    __m128 y_lt_0 = (__m128)__lsx_vfcmp_clt_s( y, _ps_0 );
+
+    __m128 zero_mask = (__m128)__lsx_vand_v( (__m128i)x_eq_0, (__m128i)y_eq_0 );
+    __m128 zero_mask_other_case = (__m128)__lsx_vand_v( (__m128i)y_eq_0, (__m128i)x_gt_0 );
+    zero_mask = (__m128)__lsx_vor_v( (__m128i)zero_mask, (__m128i)zero_mask_other_case );
+
+    __m128 pio2_mask = (__m128)__lsx_vandn_v( (__m128i)y_eq_0, (__m128i)x_eq_0 );
+    __m128 pio2_mask_sign = (__m128)__lsx_vand_v( (__m128i)y_lt_0, (__m128i)_ps_sign_mask );
+    __m128 pio2_result = _ps_cephes_PIO2F;
+    pio2_result = (__m128)__lsx_vxor_v( (__m128i)pio2_result, (__m128i)pio2_mask_sign );
+    pio2_result = (__m128)__lsx_vand_v( (__m128i)pio2_mask, (__m128i)pio2_result );
+
+    __m128 pi_mask = (__m128)__lsx_vand_v( (__m128i)y_eq_0, (__m128i)x_le_0 );
+    __m128 pi = _ps_cephes_PIF;
+    __m128 pi_result = (__m128)__lsx_vand_v( (__m128i)pi_mask, (__m128i)pi );
+
+    __m128 swap_sign_mask_offset = (__m128)__lsx_vand_v( (__m128i)x_lt_0, (__m128i)y_lt_0 );
+    swap_sign_mask_offset = (__m128)__lsx_vand_v( (__m128i)swap_sign_mask_offset, (__m128i)_ps_sign_mask );
+
+    __m128 offset0 = (__m128)__lsx_vldi(0);
+    __m128 offset1 = _ps_cephes_PIF;
+    offset1 = (__m128)__lsx_vxor_v( (__m128i)offset1, (__m128i)swap_sign_mask_offset );
+
+    __m128 offset = (__m128)__lsx_vandn_v( (__m128i)x_lt_0, (__m128i)offset0 );
+    offset = (__m128)__lsx_vand_v( (__m128i)x_lt_0, (__m128i)offset1 );
+
+    __m128 arg = __lsx_vfdiv_s( y, x );
+    __m128 atan_result = atan_ps( arg );
+    atan_result = __lsx_vfadd_s( atan_result, offset );
+
+    // Select between zero_result, pio2_result and atan_result
+
+    __m128 result = (__m128)__lsx_vandn_v( (__m128i)zero_mask, (__m128i)pio2_result );
+    atan_result = (__m128)__lsx_vandn_v( (__m128i)pio2_mask, (__m128i)atan_result );
+    atan_result = (__m128)__lsx_vandn_v( (__m128i)pio2_mask, (__m128i)atan_result);
+    result = (__m128)__lsx_vor_v( (__m128i)result, (__m128i)atan_result );
+    result = (__m128)__lsx_vor_v( (__m128i)result, (__m128i)pi_result );
+
+    return result;
+}
+
+static const __m256 _ps_atanrange_hi_avx = lasx_set1_f32(2.414213562373095);
+static const __m256 _ps_atanrange_lo_avx = lasx_set1_f32(0.4142135623730950);
+static const __m256 _ps_cephes_PIF_avx = lasx_set1_f32(3.141592653589793238);
+static const __m256 _ps_cephes_PIO2F_avx = lasx_set1_f32(1.5707963267948966192);
+static const __m256 _ps_cephes_PIO4F_avx = lasx_set1_f32(0.7853981633974483096);
+
+static const __m256 _ps_atancof_p0_avx = lasx_set1_f32(8.05374449538e-2);
+static const __m256 _ps_atancof_p1_avx = lasx_set1_f32(1.38776856032e-1);
+static const __m256 _ps_atancof_p2_avx = lasx_set1_f32(1.99777106478e-1);
+static const __m256 _ps_atancof_p3_avx = lasx_set1_f32(3.33329491539e-1);
+
+// AVX2 version of the atan_ps() SSE version
+static inline __m256 atan_ps(__m256 x)
+{
+    __m256 sign_bit, y;
+
+    sign_bit = x;
+    // Take the absolute value
+    x = (__m256)__lasx_xvand_v((__m256i)x, (__m256i)_ps_inv_sign_mask_avx);
+    // Extract the sign bit (upper one)
+    sign_bit = (__m256)__lasx_xvand_v((__m256i)sign_bit, (__m256i)_ps_sign_mask_avx);
+
+    // Range reduction, init x and y depending on range
+    // x > 2.414213562373095
+    __m256 cmp0 = (__m256)__lasx_xvfcmp_xxx_s(x, _ps_atanrange_hi_avx, _CMP_GT_OS);
+    // x > 0.4142135623730950
+    __m256 cmp1 = (__m256)__lasx_xvfcmp_xxx_s(x, _ps_atanrange_lo_avx, _CMP_GT_OS);
+
+    // x > 0.4142135623730950 && !(x > 2.414213562373095)
+    __m256 cmp2 = (__m256)__lasx_xvandn_v((__m256i)cmp0, (__m256i)cmp1);
+
+    // -(1.0/x);
+    __m256 y0 = (__m256)__lasx_xvand_v((__m256i)cmp0, (__m256i)_ps_cephes_PIO2F_avx);
+    __m256 x0 = __lasx_xvfdiv_s(avx_p1, x);
+    x0 = (__m256)__lasx_xvxor_v((__m256i)x0, (__m256i)_ps_sign_mask_avx);
+
+    __m256 y1 = (__m256)__lasx_xvand_v((__m256i)cmp2, (__m256i)_ps_cephes_PIO4F_avx);
+    // (x-1.0)/(x+1.0)
+    __m256 x1_o = __lasx_xvfsub_s(x, avx_p1);
+    __m256 x1_u = __lasx_xvfadd_s(x, avx_p1);
+    __m256 x1 = __lasx_xvfdiv_s(x1_o, x1_u);
+
+    __m256 x2 = (__m256)__lasx_xvand_v((__m256i)cmp2, (__m256i)x1);
+    x0 = (__m256)__lasx_xvand_v((__m256i)cmp0, (__m256i)x0);
+    x2 = (__m256)__lasx_xvor_v((__m256i)x2, (__m256i)x0);
+    cmp1 =(__m256)__lasx_xvor_v((__m256i)cmp0, (__m256i)cmp2);
+    x2 = (__m256)__lasx_xvand_v((__m256i)cmp1, (__m256i)x2);
+    x = (__m256)__lasx_xvandn_v((__m256i)cmp1, (__m256i)x);
+    x = (__m256)__lasx_xvor_v((__m256i)x2, (__m256i)x);
+
+    y =(__m256)__lasx_xvor_v((__m256i)y0, (__m256i)y1);
+
+    __m256 zz = __lasx_xvfmul_s(x, x);
+    __m256 acc = _ps_atancof_p0_avx;
+    acc = __lasx_xvfmsub_s(acc, zz, _ps_atancof_p1_avx);
+    acc = __lasx_xvfmadd_s(acc, zz, _ps_atancof_p2_avx);
+    acc = __lasx_xvfmsub_s(acc, zz, _ps_atancof_p3_avx);
+    acc = __lasx_xvfmul_s(acc, zz);
+    acc = __lasx_xvfmadd_s(acc, x, x);
+    y = __lasx_xvfadd_s(y, acc);
+
+    // Update the sign
+    y = (__m256)__lasx_xvxor_v((__m256i)y, (__m256i)sign_bit);
+
+    return y;
+}
+
+// AVX2 version of the atan2_ps() SSE version
+static inline __m256 atan2_ps(__m256 y, __m256 x)
+{
+    __m256 x_eq_0 = (__m256)__lasx_xvfcmp_xxx_s(x, avx_p0, _CMP_EQ_OQ);
+    __m256 x_gt_0 = (__m256)__lasx_xvfcmp_xxx_s(x, avx_p0, _CMP_GT_OS);
+    __m256 x_le_0 = (__m256)__lasx_xvfcmp_xxx_s(x, avx_p0, _CMP_LE_OS);
+    __m256 y_eq_0 = (__m256)__lasx_xvfcmp_xxx_s(y, avx_p0, _CMP_EQ_OQ);
+    __m256 x_lt_0 = (__m256)__lasx_xvfcmp_xxx_s(x, avx_p0, _CMP_LT_OS);
+    __m256 y_lt_0 = (__m256)__lasx_xvfcmp_xxx_s(y, avx_p0, _CMP_LT_OS);
+
+    // Computes a zero mask, set if either both x=y=0 or y=0&x>0
+    __m256 zero_mask = (__m256)__lasx_xvand_v((__m256i)x_eq_0, (__m256i)y_eq_0);
+    __m256 zero_mask_other_case = (__m256)__lasx_xvand_v((__m256i)y_eq_0, (__m256i)x_gt_0);
+    zero_mask = (__m256)__lasx_xvor_v((__m256i)zero_mask, (__m256i)zero_mask_other_case);
+
+    // Computes pio2 intermediate result, set if (y!0 and x=0) & (pi/2 XOR (upper bit y<0))
+    __m256 pio2_mask = (__m256)__lasx_xvandn_v((__m256i)y_eq_0, (__m256i)x_eq_0);
+    __m256 pio2_mask_sign = (__m256)__lasx_xvand_v((__m256i)y_lt_0, (__m256i)_ps_sign_mask_avx);
+    __m256 pio2_result = _ps_cephes_PIO2F_avx;
+    pio2_result = (__m256)__lasx_xvxor_v((__m256i)pio2_result, (__m256i)pio2_mask_sign);
+    pio2_result = (__m256)__lasx_xvand_v((__m256i)pio2_mask, (__m256i)pio2_result);
+
+    // Computes pi intermediate result, set if y=0&x<0 and pi
+    __m256 pi_mask = (__m256)__lasx_xvand_v((__m256i)y_eq_0, (__m256i)x_le_0);
+    __m256 pi_result = (__m256)__lasx_xvand_v((__m256i)pi_mask, (__m256i)_ps_cephes_PIF_avx);
+
+    // Computes swap_sign_mask_offset, set if x<0 & y<0 of sign bit(uppermost bit)
+    __m256 swap_sign_mask_offset = (__m256)__lasx_xvand_v((__m256i)x_lt_0, (__m256i)y_lt_0);
+    swap_sign_mask_offset = (__m256)__lasx_xvand_v((__m256i)swap_sign_mask_offset, (__m256i)_ps_sign_mask_avx);
+
+     // Computes offset, set based on pi, swap_sign_mask_offset and x<0
+    __m256 offset0 = (__m256)__lasx_xvxor_v((__m256i)_ps_cephes_PIF_avx, (__m256i)swap_sign_mask_offset);
+    __m256 offset = (__m256)__lasx_xvandn_v((__m256i)x_lt_0, (__m256i)avx_p0);
+    offset = (__m256)__lasx_xvand_v((__m256i)x_lt_0, (__m256i)offset0);
+
+    // Computes division of x,y
+    __m256 arg = __lasx_xvfdiv_s(y, x);
+    __m256 atan_result = atan_ps(arg);
+    atan_result = __lasx_xvfadd_s(atan_result, offset);
+
+    // Select between zero_result, pio2_result and atan_result
+    __m256 result = (__m256)__lasx_xvandn_v((__m256i)zero_mask, (__m256i)pio2_result);
+    atan_result = (__m256)__lasx_xvandn_v((__m256i)pio2_mask, (__m256i)atan_result);
+    atan_result = (__m256)__lasx_xvandn_v((__m256i)pio2_mask, (__m256i)atan_result);
+    result = (__m256)__lasx_xvor_v((__m256i)result, (__m256i)atan_result);
+    result = (__m256)__lasx_xvor_v((__m256i)result, (__m256i)pi_result);
+
+    return result;
+}
+
+// Modified AVX2 version of the original SSE version at https://github.com/RJVB/sse_mathfun/blob/master/sse_mathfun.h
+static inline __m256 log_ps(__m256 x)
+{
+    __m256 e;
+    __m256i emm0;
+    __m256 one = *(__m256 *)&avx_p1;
+    __m256 invalid_mask = (__m256)__lasx_xvfcmp_xxx_s(x, avx_p0, _CMP_LE_OQ);
+
+    // cut off denormalized stuff
+    x = __lasx_xvfmax_s(x, *(__m256 *)&_ps_min_norm_pos_avx);
+
+    // part 1: x = frexpf(x, &e);
+    emm0 = lasx_srli_i32((__m256i)(x), 23);
+
+    // keep only the fractional part
+    x = (__m256)__lasx_xvand_v((__m256i)x, (__m256i)(*(__m256 *)&_ps_inv_mant_mask_avx));
+    x = (__m256)__lasx_xvor_v((__m256i)x, (__m256i)(*(__m256 *)&_ps_0p5_avx));
+
+    emm0 = __lasx_xvsub_w(emm0, *(__m256i *)&_pi32_0x7f_avx);
+    e = __lasx_xvffint_s_w(emm0);
+
+    e = __lasx_xvfadd_s(e, one);
+
+    // part 2: if( x < SQRTHF ) { e -= 1; x = x + x - 1.0; } else { x = x - 1.0; }
+    __m256 z, y;
+    __m256 mask = (__m256)__lasx_xvfcmp_xxx_s(x, avx_cephesSQRTHF, _CMP_LT_OQ);
+    __m256 tmp = (__m256)__lasx_xvand_v((__m256i)x, (__m256i)mask);
+    x = __lasx_xvfsub_s(x, one);
+    e = __lasx_xvfsub_s(e, (__m256)__lasx_xvand_v((__m256i)one, (__m256i)mask));
+    x = __lasx_xvfadd_s(x, tmp);
+    z = __lasx_xvfmul_s(x,x);
+    y = avx_cephesLogP0;
+    y = __lasx_xvfmul_s(y, x);
+    y = __lasx_xvfadd_s(y, avx_cephesLogP1);
+    y = __lasx_xvfmul_s(y, x);
+    y = __lasx_xvfadd_s(y, avx_cephesLogP2);
+    y = __lasx_xvfmul_s(y, x);
+    y = __lasx_xvfadd_s(y, avx_cephesLogP3);
+    y = __lasx_xvfmul_s(y, x);
+    y = __lasx_xvfadd_s(y, avx_cephesLogP4);
+    y = __lasx_xvfmul_s(y, x);
+    y = __lasx_xvfadd_s(y, avx_cephesLogP5);
+    y = __lasx_xvfmul_s(y, x);
+    y = __lasx_xvfadd_s(y, avx_cephesLogP6);
+    y = __lasx_xvfmul_s(y, x);
+    y = __lasx_xvfadd_s(y, avx_cephesLogP7);
+    y = __lasx_xvfmul_s(y, x);
+    y = __lasx_xvfadd_s(y, avx_cephesLogP8);
+    y = __lasx_xvfmul_s(y, x);
+    y = __lasx_xvfmul_s(y, z);
+    tmp = __lasx_xvfmul_s(e, avx_cephesLogQ1);
+    y = __lasx_xvfadd_s(y, tmp);
+    tmp = __lasx_xvfmul_s(z, *(__m256 *)&_ps_0p5_avx);
+    y = __lasx_xvfsub_s(y, tmp);
+    tmp = __lasx_xvfmul_s(e, avx_cephesLogQ2);
+    x = __lasx_xvfadd_s(x, y);
+    x = __lasx_xvfadd_s(x, tmp);
+    x = (__m256)__lasx_xvor_v((__m256i)x, (__m256i)invalid_mask); // negative arg will be NAN
+
+    return x;
+}
+
+// Modified version of the original SSE version at https://github.com/RJVB/sse_mathfun/blob/master/sse_mathfun.h
+static inline __m128 log_ps(__m128 x)
+{
+    __m128 e;
+    __m128i emm0;
+    __m128 one = *(__m128 *)&_ps_1;
+    __m128 invalid_mask = (__m128)__lsx_vfcmp_cle_s(x, xmm_p0);
+
+    // cut off denormalized stuff
+    x = __lsx_vfmax_s(x, *(__m128 *)&_ps_min_norm_pos);
+
+    // part 1: x = frexpf(x, &e);
+    emm0 = lsx_srli_i32((__m128i)(x), 23);
+
+    // keep only the fractional part
+    x = (__m128)__lsx_vand_v((__m128i)x, (__m128i)(*(__m128 *)&_ps_inv_mant_mask));
+    x = (__m128)__lsx_vor_v((__m128i)x, (__m128i)(*(__m128 *)&_ps_0p5));
+
+    emm0 = __lsx_vsub_w(emm0, *(__m128i *)&_pi32_0x7f);
+    e = __lsx_vffint_s_w(emm0);
+
+    e = __lsx_vfadd_s(e, one);
+
+    // part 2: if( x < SQRTHF ) { e -= 1; x = x + x - 1.0; } else { x = x - 1.0; }
+    __m128 z, y;
+    __m128 mask = (__m128)__lsx_vfcmp_clt_s(x, *(__m128 *)&xmm_cephesSQRTHF);
+    __m128 tmp = (__m128)__lsx_vand_v((__m128i)x, (__m128i)mask);
+    x = __lsx_vfsub_s(x, one);
+    e = __lsx_vfsub_s(e, (__m128)__lsx_vand_v((__m128i)one, (__m128i)mask));
+    x = __lsx_vfadd_s(x, tmp);
+    z = __lsx_vfmul_s(x,x);
+    y = *(__m128 *)&xmm_cephesLogP0;
+    y = __lsx_vfmul_s(y, x);
+    y = __lsx_vfadd_s(y, *(__m128 *)&xmm_cephesLogP1);
+    y = __lsx_vfmul_s(y, x);
+    y = __lsx_vfadd_s(y, *(__m128 *)&xmm_cephesLogP2);
+    y = __lsx_vfmul_s(y, x);
+    y = __lsx_vfadd_s(y, *(__m128 *)&xmm_cephesLogP3);
+    y = __lsx_vfmul_s(y, x);
+    y = __lsx_vfadd_s(y, *(__m128 *)&xmm_cephesLogP4);
+    y = __lsx_vfmul_s(y, x);
+    y = __lsx_vfadd_s(y, *(__m128 *)&xmm_cephesLogP5);
+    y = __lsx_vfmul_s(y, x);
+    y = __lsx_vfadd_s(y, *(__m128 *)&xmm_cephesLogP6);
+    y = __lsx_vfmul_s(y, x);
+    y = __lsx_vfadd_s(y, *(__m128 *)&xmm_cephesLogP7);
+    y = __lsx_vfmul_s(y, x);
+    y = __lsx_vfadd_s(y, *(__m128 *)&xmm_cephesLogP8);
+    y = __lsx_vfmul_s(y, x);
+    y = __lsx_vfmul_s(y, z);
+    tmp = __lsx_vfmul_s(e, *(__m128 *)&xmm_cephesLogQ1);
+    y = __lsx_vfadd_s(y, tmp);
+    tmp = __lsx_vfmul_s(z, *(__m128 *)&_ps_0p5);
+    y = __lsx_vfsub_s(y, tmp);
+    tmp = __lsx_vfmul_s(e, *(__m128 *)&xmm_cephesLogQ2);
+    x = __lsx_vfadd_s(x, y);
+    x = __lsx_vfadd_s(x, tmp);
+    x = (__m128)__lsx_vor_v((__m128i)x, (__m128i)invalid_mask); // negative arg will be NAN
+
+    return x;
+}
+
+inline Rpp32f rpp_hsum_ps(__m128 x)
+{
+    __m128 shuf = (__m128)__lsx_vpackod_w((__m128i)x, (__m128i)x);        // broadcast elements 3,1 to 2,0
+    __m128 sums = __lsx_vfadd_s(x, shuf);
+    shuf = (__m128)__lsx_vilvh_d((__m128i)sums, (__m128i)shuf);        // high half -> low half
+    sums = (__m128)__lsx_vextrins_w(sums, __lsx_vfadd_s(sums, shuf), 0);
+    return (((v4f32)(sums))[0]);
+}
+
+inline Rpp32f rpp_hsum_ps(__m256 x)
+{
+    __m128 p0 = lasx_extractf128_f32(x, 1); // Contains x7, x6, x5, x4
+    __m128 p1 = (__m128)__lasx_cvt_256_128((__m256i)x);   // Contains x3, x2, x1, x0
+    __m128 sum = __lsx_vfadd_s(p0, p1);         // Contains x3 + x7, x2 + x6, x1 + x5, x0 + x4
+    p0 = sum;                                // Contains -, -, x1 + x5, x0 + x4
+    p1 = (__m128)__lsx_vilvh_d((__m128i)sum, (__m128i)sum);            // Contains -, -, x3 + x7, x2 + x6
+    sum = __lsx_vfadd_s(p0, p1);                // Contains -, -, x1 + x3 + x5 + x7, x0 + x2 + x4 + x6
+    p0 = sum;                                // Contains -, -, -, x0 + x2 + x4 + x6
+    p1 = (__m128)__lsx_vpermi_w((__m128i)sum, (__m128i)sum, 0x1);      // Contains -, -, -, x1 + x3 + x5 + x7
+    sum = (__m128)__lsx_vextrins_w(p0, __lsx_vfadd_s(p0, p1), 0);                // Contains -, -, -, x0 + x1 + x2 + x3 + x4 + x5 + x6 + x7
+    return (((v4f32)(sum))[0]);
+}
+
+/* Computes inverse square root */
+inline Rpp32f rpp_rsqrt_ps(Rpp32f x)
+{
+    __m128 X = (__m128)(v4f32){x,0,0,0};
+    __m128 tmp = (__m128)__lsx_vextrins_w((__m128i)X, (__m128i)__lsx_vfrsqrt_s(X), 0);
+    Rpp32f y = (((v4f32)(tmp))[0]);
+    return y * (1.5f - x * 0.5f * y * y);
+}
+
+/* Compute inverse square root */
+/* SSE matches to 6 decimal places with raw C version due to newton rhapson approximation*/
+inline void rpp_rsqrt_sse(Rpp32f *input, Rpp64s numElements, Rpp32f eps, Rpp32f rdiv, Rpp32f mul)
+{
+    Rpp64s i = 0;
+    __m128 rdivx4 = lsx_set1_f32(rdiv);
+    __m128 mulx4 = lsx_set1_f32(mul * 0.5f);
+    if (eps) // epsilon is present - no need for masking, but we need to add it
+    {
+        __m128 epsx4 = lsx_set1_f32(eps);
+        for (; i + 4 <= numElements; i += 4)
+        {
+            __m128 x = (__m128)__lsx_vld(&input[i], 0);
+            x = __lsx_vfmul_s(x, rdivx4);
+            x = __lsx_vfadd_s(x, epsx4);
+            __m128 y = __lsx_vfrsqrt_s(x);
+            __m128 y2 = __lsx_vfmul_s(y, y);
+            __m128 xy2 = __lsx_vfmul_s(x, y2);
+            __m128 three_minus_xy2 = __lsx_vfsub_s(xmm_p3, xy2);
+            y = __lsx_vfmul_s(y, three_minus_xy2);
+            y = __lsx_vfmul_s(y, mulx4);
+            __lsx_vst(y, &input[i], 0);
+        }
+    }
+    else
+    {
+        for (; i + 4 <= numElements; i += 4)
+        {
+            __m128 x = (__m128)__lsx_vld(&input[i], 0);
+            x = __lsx_vfmul_s(x, rdivx4);
+            __m128 mask = (__m128)__lsx_vfcmp_cune_s(x, xmm_p0);
+            __m128 y = __lsx_vfrsqrt_s(x);
+            y = (__m128)__lsx_vand_v((__m128i)y, (__m128i)mask);
+            __m128 y2 = __lsx_vfmul_s(y, y);
+            __m128 xy2 = __lsx_vfmul_s(x, y2);
+            __m128 three_minus_xy2 = __lsx_vfsub_s(xmm_p3, xy2);
+            y = __lsx_vfmul_s(y, three_minus_xy2);
+            y = __lsx_vfmul_s(y, mulx4);
+            __lsx_vst(y, &input[i], 0);
+        }
+    }
+    if (eps)
+    {
+        for (; i < numElements; i++)
+            input[i] = rpp_rsqrt_ps(input[i] * rdiv + eps) * mul;
+    }
+    else
+    {
+        for (; i < numElements; i++)
+        {
+            Rpp32f x = input[i] * rdiv;
+            input[i] = x ? rpp_rsqrt_ps(x) * mul : 0;
+        }
+    }
+}
+
+/* Compute inverse square root */
+/* AVX2 matches to 6 decimal places with raw C version due to newton rhapson approximation*/
+inline void rpp_rsqrt_avx(Rpp32f *input, Rpp32s numElements, Rpp32f eps, Rpp32f rdiv, Rpp32f scale)
+{
+    Rpp32s i = 0;
+    __m256 rdivx8 = lasx_set1_f32(rdiv);
+    __m256 mulx8 = lasx_set1_f32(scale * 0.5f);
+    if (eps) // epsilon is present - no need for masking, but we need to add it
+    {
+        __m256 epsx8 = lasx_set1_f32(eps);
+        for (; i + 8 <= numElements; i += 8)
+        {
+            __m256 x = (__m256)__lasx_xvld(&input[i], 0);
+            x = __lasx_xvfmul_s(x, rdivx8);
+            x = __lasx_xvfadd_s(x, epsx8);
+            __m256 y = __lasx_xvfrsqrt_s(x);
+            __m256 y2 = __lasx_xvfmul_s(y, y);
+            __m256 xy2 = __lasx_xvfmul_s(x, y2);
+            __m256 three_minus_xy2 = __lasx_xvfsub_s(avx_p3, xy2);
+            y = __lasx_xvfmul_s(y, three_minus_xy2);
+            y = __lasx_xvfmul_s(y, mulx8);
+            __lasx_xvst((__m256i)y, &input[i], 0);
+        }
+    }
+    else
+    {
+        for (; i + 8 <= numElements; i += 8)
+        {
+            __m256 x = (__m256)__lasx_xvld(&input[i], 0);
+            x = __lasx_xvfmul_s(x, rdivx8);
+            __m256 mask = (__m256)__lasx_xvfcmp_xxx_s(x, avx_p0, _CMP_NEQ_OQ);
+            __m256 y = __lasx_xvfrsqrt_s(x);
+            y = (__m256)__lasx_xvand_v((__m256i)y, (__m256i)mask);
+            __m256 y2 = __lasx_xvfmul_s(y, y);
+            __m256 xy2 = __lasx_xvfmul_s(x, y2);
+            __m256 three_minus_xy2 = __lasx_xvfsub_s(avx_p3, xy2);
+            y = __lasx_xvfmul_s(y, three_minus_xy2);
+            y = __lasx_xvfmul_s(y, mulx8);
+            __lasx_xvst((__m256i)y, &input[i], 0);
+        }
+    }
+    if (eps)
+    {
+        for (; i < numElements; i++)
+            input[i] = rpp_rsqrt_ps(input[i] * rdiv + eps) * scale;
+    }
+    else
+    {
+        for (; i < numElements; i++)
+        {
+            Rpp32f x = input[i] * rdiv;
+            input[i] = x ? rpp_rsqrt_ps(x) * scale : 0;
+        }
+    }
+}
+
+static inline void fast_matmul4x4_sse(float *A, float *B, float *C)
+{
+    __m128 row1 = (__m128)__lsx_vld(&B[0], 0);                   // Row 0 of B
+    __m128 row2 = (__m128)__lsx_vld(&B[4], 0);                   // Row 1 of B
+    __m128 row3 = (__m128)__lsx_vld(&B[8], 0);                   // Row 2 of B
+    __m128 row4 = (__m128)__lsx_vld(&B[12], 0);                  // Row 3 of B
+
+    for(int i = 0; i < 4; i++)
+    {
+        __m128 brod1 = lsx_set1_f32(A[4 * i + 0]);       // Example for row 0 computation -> A[0][0] is broadcasted
+        __m128 brod2 = lsx_set1_f32(A[4 * i + 1]);       // Example for row 0 computation -> A[0][1] is broadcasted
+        __m128 brod3 = lsx_set1_f32(A[4 * i + 2]);       // Example for row 0 computation -> A[0][2] is broadcasted
+        __m128 brod4 = lsx_set1_f32(A[4 * i + 3]);       // Example for row 0 computation -> A[0][3] is broadcasted
+
+        __m128 row = __lsx_vfadd_s(                        // Example for row 0 computation -> P + Q
+                        __lsx_vfadd_s(                     // Example for row 0 computation -> P = A[0][0] * B[0][0] + A[0][1] * B[1][0]
+                            __lsx_vfmul_s(brod1, row1),    // Example for row 0 computation -> (A[0][0] * B[0][0], A[0][0] * B[0][1], A[0][0] * B[0][2], A[0][0] * B[0][3])
+                            __lsx_vfmul_s(brod2, row2)),   // Example for row 0 computation -> (A[0][1] * B[1][0], A[0][1] * B[1][1], A[0][1] * B[1][2], A[0][1] * B[1][3])
+                        __lsx_vfadd_s(                     // Example for row 0 computation -> Q = A[0][2] * B[2][0] + A[0][3] * B[3][0]
+                            __lsx_vfmul_s(brod3, row3),    // Example for row 0 computation -> (A[0][2] * B[2][0], A[0][2] * B[2][1], A[0][2] * B[2][2], A[0][2] * B[2][3])
+                            __lsx_vfmul_s(brod4, row4)));  // Example for row 0 computation -> (A[0][3] * B[3][0], A[0][3] * B[3][1], A[0][3] * B[3][2], A[0][3] * B[3][3])
+
+        __lsx_vst(row, &C[4*i], 0);                     // Example for row 0 computation -> Storing whole computed row 0
+    }
+}
+
+/* Generic interpolation loads  */
+
+inline void rpp_generic_nn_load_u8pkd3(Rpp8u *srcPtrChannel, Rpp32s *srcLoc, Rpp32s *invalidLoad, __m128i &p)
+{
+    __m128i px[4];
+    px[0] = invalidLoad[0] ? xmm_px0 : __lsx_vld((__m128i *)(srcPtrChannel + srcLoc[0]), 0); // LOC0 load [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|R05|G05|B05|R06] - Need RGB 01
+    px[1] = invalidLoad[1] ? xmm_px0 : __lsx_vld((__m128i *)(srcPtrChannel + srcLoc[1]), 0); // LOC1 load [R11|G11|B11|R12|G12|B12|R13|G13|B13|R14|G14|B14|R15|G15|B15|R16] - Need RGB 11
+    px[2] = invalidLoad[2] ? xmm_px0 : __lsx_vld((__m128i *)(srcPtrChannel + srcLoc[2]), 0); // LOC2 load [R21|G21|B21|R22|G22|B22|R23|G23|B23|R24|G24|B24|R25|G25|B25|R26] - Need RGB 21
+    px[3] = invalidLoad[3] ? xmm_px0 : __lsx_vld((__m128i *)(srcPtrChannel + srcLoc[3]), 0); // LOC3 load [R31|G31|B31|R32|G32|B32|R33|G33|B33|R34|G34|B34|R35|G35|B35|R36] - Need RGB 31
+    px[0] = __lsx_vilvl_d(__lsx_vilvl_w(px[3], px[2]), __lsx_vilvl_w(px[1], px[0])); // Unpack to obtain [R01|G01|B01|R02|R11|G11|B11|R12|R21|G21|B21|R22|R31|G31|B31|R32]
+    p = lsx_shuffle_i8(px[0], xmm_pkd_mask);    // Shuffle to obtain 4 RGB [R01|G01|B01|R11|G11|B11|R21|G21|B21|R31|G31|B31|00|00|00|00]
+}
+
+inline void rpp_generic_nn_load_u8pkd3_avx(Rpp8u *srcPtrChannel, Rpp32s *srcLoc, Rpp32s *invalidLoad, __m256i &p)
+{
+    __m128i px[7];
+    px[0] = invalidLoad[0] ? xmm_px0 : __lsx_vld((__m128i *)(srcPtrChannel + srcLoc[0]), 0); // LOC0 load [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|R05|G05|B05|R06] - Need RGB 01
+    px[1] = invalidLoad[1] ? xmm_px0 : __lsx_vld((__m128i *)(srcPtrChannel + srcLoc[1]), 0); // LOC1 load [R11|G11|B11|R12|G12|B12|R13|G13|B13|R14|G14|B14|R15|G15|B15|R16] - Need RGB 11
+    px[2] = invalidLoad[2] ? xmm_px0 : __lsx_vld((__m128i *)(srcPtrChannel + srcLoc[2]), 0); // LOC2 load [R21|G21|B21|R22|G22|B22|R23|G23|B23|R24|G24|B24|R25|G25|B25|R26] - Need RGB 21
+    px[3] = invalidLoad[3] ? xmm_px0 : __lsx_vld((__m128i *)(srcPtrChannel + srcLoc[3]), 0); // LOC3 load [R31|G31|B31|R32|G32|B32|R33|G33|B33|R34|G34|B34|R35|G35|B35|R36] - Need RGB 31
+    px[4] = __lsx_vilvl_d(__lsx_vilvl_w(px[3], px[2]), __lsx_vilvl_w(px[1], px[0]));    // Unpack to obtain [R01|G01|B01|R02|R11|G11|B11|R12|R21|G21|B21|R22|R31|G31|B31|R32]
+    px[4] = lsx_shuffle_i8(px[4], xmm_pkd_mask); // shuffle to obtain 4 RGB [R01|G01|B01|R11|G11|B11|R21|G21|B21|R31|G31|B31|00|00|00|00]
+
+    px[0] = invalidLoad[4] ? xmm_px0 : __lsx_vld((__m128i *)(srcPtrChannel + srcLoc[4]), 0); // LOC4 load [R41|G41|B41|R42|G42|B42|R43|G43|B43|R44|G44|B44|R45|G45|B45|R46] - Need RGB 41
+    px[1] = invalidLoad[5] ? xmm_px0 : __lsx_vld((__m128i *)(srcPtrChannel + srcLoc[5]), 0); // LOC5 load [R51|G51|B51|R52|G52|B52|R53|G53|B53|R54|G54|B54|R55|G55|B55|R56] - Need RGB 51
+    px[2] = invalidLoad[6] ? xmm_px0 : __lsx_vld((__m128i *)(srcPtrChannel + srcLoc[6]), 0); // LOC6 load [R61|G61|B61|R62|G62|B62|R63|G63|B63|R64|G64|B64|R65|G65|B65|R66] - Need RGB 61
+    px[3] = invalidLoad[7] ? xmm_px0 : __lsx_vld((__m128i *)(srcPtrChannel + srcLoc[7]), 0); // LOC7 load [R71|G71|B71|R72|G72|B72|R73|G73|B73|R74|G74|B74|R75|G75|B75|R76] - Need RGB 71
+    px[5] = __lsx_vilvl_d(__lsx_vilvl_w(px[3], px[2]), __lsx_vilvl_w(px[1], px[0]));    // Unpack to obtain [R41|G41|B41|R42|R51|G51|B51|R52|R61|G61|B61|R62|R71|G71|B71|R72]
+    px[5] = lsx_shuffle_i8(px[5], xmm_pkd_mask); // shuffle to obtain 4 RGB [R41|G41|B41|R51|G51|B51|R61|G61|B61|R71|G71|B71|00|00|00|00]
+
+    px[6] = lsx_shuffle_i8(px[5], xmm_pxMask00); // shuffle to move 0-3 of px[5] to 12-15
+    px[4] = __lsx_vadd_b(px[4], px[6]); // add px[4] and px[5]
+    px[5] = lsx_shuffle_i8(px[5], xmm_pxMask04To11); // shuffle to move values at 4-11 of px[5] to 0-7
+    p = lasx_setr_m128i(px[4], px[5]); // Merge to obtain 8 RGB [R01|G01|B01|R11|G11|B11|R21|G21|B21|R31|G31|B31|R41|G41|B41|R51|G51|B51|R61|G61|B61|R71|G71|B71|00|00|00|00|00|00|00|00]
+}
+
+inline void rpp_generic_nn_load_u8pln1(Rpp8u *srcPtrChannel, Rpp32s *srcLoc, Rpp32s *invalidLoad, __m128i &p)
+{
+    __m128i px[4];
+    px[0] = invalidLoad[0] ? xmm_px0 : __lsx_vld((__m128i *)(srcPtrChannel + srcLoc[0]), 0);  // LOC0 load [R01|R02|R03|R04|R05|R06...] - Need R01
+    px[1] = invalidLoad[1] ? xmm_px0 : __lsx_vld((__m128i *)(srcPtrChannel + srcLoc[1]), 0);  // LOC1 load [R11|R12|R13|R14|R15|R16...] - Need R11
+    px[2] = invalidLoad[2] ? xmm_px0 : __lsx_vld((__m128i *)(srcPtrChannel + srcLoc[2]), 0);  // LOC2 load [R21|R22|R23|R24|R25|R26...] - Need R21
+    px[3] = invalidLoad[3] ? xmm_px0 : __lsx_vld((__m128i *)(srcPtrChannel + srcLoc[3]), 0);  // LOC3 load [R31|R32|R33|R34|R35|R36...] - Need R31
+    px[0] = __lsx_vilvl_b(px[2], px[0]);    // unpack 8 lo-pixels of px[0] and px[2]
+    px[1] = __lsx_vilvl_b(px[3], px[1]);    // unpack 8 lo-pixels of px[1] and px[3]
+    p = __lsx_vilvl_b(px[1], px[0]);    // unpack to obtain [R01|R11|R21|R31|00|00|00|00|00|00|00|00|00|00|00|00]
+}
+
+inline void rpp_generic_nn_load_u8pln1_avx(Rpp8u *srcPtrChannel, Rpp32s *srcLoc, Rpp32s *invalidLoad, __m256i &p)
+{
+    Rpp8u buffer[16] = {0};
+    for(int i = 0; i < 8; i++)
+    {
+        if(!invalidLoad[i])
+            buffer[i] = *(srcPtrChannel + srcLoc[i]);
+    }
+    __m128i px = __lsx_vld((__m128i *)buffer, 0);
+    p = lasx_castm128i_m256i(px);
+}
+
+inline void rpp_generic_nn_load_f32pkd3_to_f32pln3(Rpp32f *srcPtrChannel, Rpp32s *srcLoc, Rpp32s *invalidLoad, __m128 *p)
+{
+    p[0] = invalidLoad[0] ? xmm_p0 : (__m128)__lsx_vld(srcPtrChannel + srcLoc[0], 0);  // LOC0 load [R01|G01|B01|R02] - Need RGB 01
+    p[1] = invalidLoad[1] ? xmm_p0 : (__m128)__lsx_vld(srcPtrChannel + srcLoc[1], 0);  // LOC1 load [R11|G11|B11|R12] - Need RGB 11
+    p[2] = invalidLoad[2] ? xmm_p0 : (__m128)__lsx_vld(srcPtrChannel + srcLoc[2], 0);  // LOC2 load [R21|G21|B21|R22] - Need RGB 21
+    __m128 pTemp = invalidLoad[3] ? xmm_p0 : (__m128)__lsx_vld(srcPtrChannel + srcLoc[3], 0);  // LOC2 load [R31|G31|B31|R32] - Need RGB 31
+    _MM_TRANSPOSE4_PS(p[0], p[1], p[2], pTemp); // Transpose to obtain RGB in each vector
+}
+
+inline void rpp_generic_nn_load_f32pkd3_to_f32pln3_avx(Rpp32f *srcPtrChannel, Rpp32s *srcLoc, Rpp32s *invalidLoad, __m256 *p)
+{
+    p[0] = lasx_setr_f32((!invalidLoad[0]) ? srcPtrChannel[srcLoc[0]]: 0, (!invalidLoad[1]) ? srcPtrChannel[srcLoc[1]]: 0,           // Get R01-R08. load the values from input using srcLoc buffer if invalidLoad is 0, else set the values to 0
+                          (!invalidLoad[2]) ? srcPtrChannel[srcLoc[2]]: 0, (!invalidLoad[3]) ? srcPtrChannel[srcLoc[3]]: 0,
+                          (!invalidLoad[4]) ? srcPtrChannel[srcLoc[4]]: 0, (!invalidLoad[5]) ? srcPtrChannel[srcLoc[5]]: 0,
+                          (!invalidLoad[6]) ? srcPtrChannel[srcLoc[6]]: 0, (!invalidLoad[7]) ? srcPtrChannel[srcLoc[7]]: 0);
+    p[1] = lasx_setr_f32((!invalidLoad[0]) ? srcPtrChannel[srcLoc[0] + 1]: 0, (!invalidLoad[1]) ? srcPtrChannel[srcLoc[1] + 1]: 0,   // Get G01-R08. load the values from input using srcLoc buffer if invalidLoad is 0, else set the values to 0
+                          (!invalidLoad[2]) ? srcPtrChannel[srcLoc[2] + 1]: 0, (!invalidLoad[3]) ? srcPtrChannel[srcLoc[3] + 1]: 0,
+                          (!invalidLoad[4]) ? srcPtrChannel[srcLoc[4] + 1]: 0, (!invalidLoad[5]) ? srcPtrChannel[srcLoc[5] + 1]: 0,
+                          (!invalidLoad[6]) ? srcPtrChannel[srcLoc[6] + 1]: 0, (!invalidLoad[7]) ? srcPtrChannel[srcLoc[7] + 1]: 0);
+    p[2] = lasx_setr_f32((!invalidLoad[0]) ? srcPtrChannel[srcLoc[0] + 2]: 0, (!invalidLoad[1]) ? srcPtrChannel[srcLoc[1] + 2]: 0,   // Get B01-R08. load the values from input using srcLoc buffer if invalidLoad is 0, else set the values to 0
+                          (!invalidLoad[2]) ? srcPtrChannel[srcLoc[2] + 2]: 0, (!invalidLoad[3]) ? srcPtrChannel[srcLoc[3] + 2]: 0,
+                          (!invalidLoad[4]) ? srcPtrChannel[srcLoc[4] + 2]: 0, (!invalidLoad[5]) ? srcPtrChannel[srcLoc[5] + 2]: 0,
+                          (!invalidLoad[6]) ? srcPtrChannel[srcLoc[6] + 2]: 0, (!invalidLoad[7]) ? srcPtrChannel[srcLoc[7] + 2]: 0);
+}
+
+inline void rpp_generic_nn_load_f32pkd3_to_f32pkd3(Rpp32f *srcPtrChannel, Rpp32s *srcLoc, Rpp32s *invalidLoad, __m128 *p)
+{
+    p[0] = invalidLoad[0] ? xmm_p0 : (__m128)__lsx_vld(srcPtrChannel + srcLoc[0], 0);  // LOC0 load [R01|G01|B01|R02] - Need RGB 01
+    p[1] = invalidLoad[1] ? xmm_p0 : (__m128)__lsx_vld(srcPtrChannel + srcLoc[1], 0);  // LOC1 load [R11|G11|B11|R12] - Need RGB 11
+    p[2] = invalidLoad[2] ? xmm_p0 : (__m128)__lsx_vld(srcPtrChannel + srcLoc[2], 0);  // LOC2 load [R21|G21|B21|R22] - Need RGB 21
+    p[3] = invalidLoad[3] ? xmm_p0 : (__m128)__lsx_vld(srcPtrChannel + srcLoc[3], 0);  // LOC2 load [R31|G31|B31|R32] - Need RGB 31
+}
+
+inline void rpp_generic_nn_load_f32pkd3_to_f32pkd3_avx(Rpp32f *srcPtrChannel, Rpp32s *srcLoc, Rpp32s *invalidLoad, __m256 *p)
+{
+    p[0] = lasx_setr_f32((!invalidLoad[0]) ? srcPtrChannel[srcLoc[0]]: 0, (!invalidLoad[0]) ? srcPtrChannel[srcLoc[0] + 1]: 0,        // Get R01|G01|B01|R02|B02|G02|R03|G03
+                          (!invalidLoad[0]) ? srcPtrChannel[srcLoc[0] + 2]: 0, (!invalidLoad[1]) ? srcPtrChannel[srcLoc[1]]: 0,        // load the values from input using srcLoc buffer if invalidLoad is 0, else set the values to 0
+                          (!invalidLoad[1]) ? srcPtrChannel[srcLoc[1] + 1]: 0, (!invalidLoad[1]) ? srcPtrChannel[srcLoc[1] + 2]: 0,
+                          (!invalidLoad[2]) ? srcPtrChannel[srcLoc[2]]: 0, (!invalidLoad[2]) ? srcPtrChannel[srcLoc[2] + 1]: 0);
+    p[1] = lasx_setr_f32((!invalidLoad[2]) ? srcPtrChannel[srcLoc[2] + 2]: 0, (!invalidLoad[3]) ? srcPtrChannel[srcLoc[3]]: 0,        // Get B03|R04|G04|B04|R05|G05|B05|R06
+                          (!invalidLoad[3]) ? srcPtrChannel[srcLoc[3] + 1]: 0, (!invalidLoad[3]) ? srcPtrChannel[srcLoc[3] + 2]: 0,    // load the values from input using srcLoc buffer if invalidLoad is 0, else set the values to 0
+                          (!invalidLoad[4]) ? srcPtrChannel[srcLoc[4]]: 0, (!invalidLoad[4]) ? srcPtrChannel[srcLoc[4] + 1]: 0,
+                          (!invalidLoad[4]) ? srcPtrChannel[srcLoc[4] + 2]: 0, (!invalidLoad[5]) ? srcPtrChannel[srcLoc[5]]: 0);
+    p[2] = lasx_setr_f32((!invalidLoad[5]) ? srcPtrChannel[srcLoc[5] + 1]: 0, (!invalidLoad[5]) ? srcPtrChannel[srcLoc[5] + 2]: 0,    // Get G06|B06|R07|G07|B07|R08|G08|B08
+                          (!invalidLoad[6]) ? srcPtrChannel[srcLoc[6]]: 0, (!invalidLoad[6]) ? srcPtrChannel[srcLoc[6] + 1]: 0,        // load the values from input using srcLoc buffer if invalidLoad is 0, else set the values to 0
+                          (!invalidLoad[6]) ? srcPtrChannel[srcLoc[6] + 2]: 0, (!invalidLoad[7]) ? srcPtrChannel[srcLoc[7]]: 0,
+                          (!invalidLoad[7]) ? srcPtrChannel[srcLoc[7] + 1]: 0, (!invalidLoad[7]) ? srcPtrChannel[srcLoc[7] + 2]: 0);
+}
+
+inline void rpp_generic_nn_load_f32pln1(Rpp32f *srcPtrChanel, Rpp32s *srcLoc, Rpp32s *invalidLoad, __m128 &p)
+{
+    __m128 pTemp[4];
+    pTemp[0] = invalidLoad[0] ? xmm_p0 : (__m128)__lsx_vld(srcPtrChanel + srcLoc[0], 0);  // LOC0 load [R01|R02|R03|R04] - Need R01
+    pTemp[1] = invalidLoad[1] ? xmm_p0 : (__m128)__lsx_vld(srcPtrChanel + srcLoc[1], 0);  // LOC1 load [R11|R12|R13|R14] - Need R11
+    pTemp[2] = invalidLoad[2] ? xmm_p0 : (__m128)__lsx_vld(srcPtrChanel + srcLoc[2], 0);  // LOC2 load [R21|R22|R23|R24] - Need R21
+    pTemp[3] = invalidLoad[3] ? xmm_p0 : (__m128)__lsx_vld(srcPtrChanel + srcLoc[3], 0);  // LOC3 load [R31|R32|R33|R34] - Need R31
+    pTemp[0] = (__m128)__lsx_vilvl_w((__m128i)pTemp[2], (__m128i)pTemp[0]);
+    pTemp[1] = (__m128)__lsx_vilvl_w((__m128i)pTemp[3], (__m128i)pTemp[1]);
+    p = (__m128)__lsx_vilvl_w((__m128i)pTemp[1], (__m128i)pTemp[0]);    // Unpack to obtain [R01|R11|R21|R31]
+}
+
+#if defined(__loongarch64)
+static __m256 _mm256_mask_i32gather_ps(__m256 a, const float* m, __m256i i, __m256 mask, const int s)
+{
+    float result[8];
+    int indices[8];
+    float mask_f[8];
+    float a_f[8];
+
+    //  i/mask/a 
+    __builtin_memcpy(indices, &i, sizeof(indices));
+    __builtin_memcpy(mask_f, &mask, sizeof(mask_f));
+    __builtin_memcpy(a_f, &a, sizeof(a_f));
+
+    for (int j = 0; j < 8; ++j) {
+        if (*(uint32_t*)&mask_f[j]) {
+            uintptr_t addr = (uintptr_t)m + indices[j] * s;
+            result[j] = *(float*)addr;
+        } else {
+            result[j] = a_f[j];
+        }
+    }
+
+    return (__m256)__lasx_xvld(result, 0);
+}
+#endif
+
+inline void rpp_generic_nn_load_f32pln1_avx(Rpp32f *srcPtrChannel, Rpp32s *srcLoc, Rpp32s *invalidLoad, __m256 &p)
+{
+    __m256 pLoadMask = (__m256)(lasx_setr_i32((!invalidLoad[0]) ? 0x80000000 : 0, (!invalidLoad[1]) ? 0x80000000 : 0,  // Set MSB of 32 bit value to 1 if invalidLoad value is 0
+                                                             (!invalidLoad[2]) ? 0x80000000 : 0, (!invalidLoad[3]) ? 0x80000000 : 0,
+                                                             (!invalidLoad[4]) ? 0x80000000 : 0, (!invalidLoad[5]) ? 0x80000000 : 0,
+                                                             (!invalidLoad[6]) ? 0x80000000 : 0, (!invalidLoad[7]) ? 0x80000000 : 0));
+    __m256i pxSrcLoc = __lasx_xvld((__m256i *)srcLoc, 0);   // Load the source location values passed
+    p = _mm256_mask_i32gather_ps(avx_p0, srcPtrChannel, pxSrcLoc, pLoadMask, 4);   // if the MSB of 32 bit value is set, then load from corresponding location value in pSrcLoc. Otherwise set the 32 bit value to 0
+}
+
+inline void rpp_generic_nn_load_i8pkd3(Rpp8s *srcPtrChannel, Rpp32s *srcLoc, Rpp32s *invalidLoad, __m128i &p)
+{
+    __m128i px[4];
+    px[0] = invalidLoad[0] ? xmm_px0 : __lsx_vld((__m128i *)(srcPtrChannel + srcLoc[0]), 0); // LOC0 load [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|R05|G05|B05|R06] - Need RGB 01
+    px[1] = invalidLoad[1] ? xmm_px0 : __lsx_vld((__m128i *)(srcPtrChannel + srcLoc[1]), 0); // LOC1 load [R11|G11|B11|R12|G12|B12|R13|G13|B13|R14|G14|B14|R15|G15|B15|R16] - Need RGB 11
+    px[2] = invalidLoad[2] ? xmm_px0 : __lsx_vld((__m128i *)(srcPtrChannel + srcLoc[2]), 0); // LOC2 load [R21|G21|B21|R22|G22|B22|R23|G23|B23|R24|G24|B24|R25|G25|B25|R26] - Need RGB 21
+    px[3] = invalidLoad[3] ? xmm_px0 : __lsx_vld((__m128i *)(srcPtrChannel + srcLoc[3]), 0); // LOC3 load [R31|G31|B31|R32|G32|B32|R33|G33|B33|R34|G34|B34|R35|G35|B35|R36] - Need RGB 31
+    px[0] = __lsx_vilvl_d(__lsx_vilvl_w(px[3], px[2]), __lsx_vilvl_w(px[1], px[0]));    // Unpack to obtain [R01|G01|B01|R02|R11|G11|B11|R12|R21|G21|B21|R22|R31|G31|B31|R32]
+    p = lsx_shuffle_i8(px[0], xmm_pkd_mask);    // Shuffle to obtain 4 RGB [R01|G01|B01|R11|G11|B11|R21|G21|B21|R31|G31|B31|00|00|00|00]
+}
+
+inline void rpp_generic_nn_load_i8pkd3_avx(Rpp8s *srcPtrChannel, Rpp32s *srcLoc, Rpp32s *invalidLoad, __m256i &p)
+{
+    __m128i px[7];
+    px[0] = invalidLoad[0] ? xmm_px0 : __lsx_vld((__m128i *)(srcPtrChannel + srcLoc[0]), 0); // LOC0 load [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|R05|G05|B05|R06] - Need RGB 01
+    px[1] = invalidLoad[1] ? xmm_px0 : __lsx_vld((__m128i *)(srcPtrChannel + srcLoc[1]), 0); // LOC1 load [R11|G11|B11|R12|G12|B12|R13|G13|B13|R14|G14|B14|R15|G15|B15|R16] - Need RGB 11
+    px[2] = invalidLoad[2] ? xmm_px0 : __lsx_vld((__m128i *)(srcPtrChannel + srcLoc[2]), 0); // LOC2 load [R21|G21|B21|R22|G22|B22|R23|G23|B23|R24|G24|B24|R25|G25|B25|R26] - Need RGB 21
+    px[3] = invalidLoad[3] ? xmm_px0 : __lsx_vld((__m128i *)(srcPtrChannel + srcLoc[3]), 0); // LOC3 load [R31|G31|B31|R32|G32|B32|R33|G33|B33|R34|G34|B34|R35|G35|B35|R36] - Need RGB 31
+    px[4] = __lsx_vilvl_d(__lsx_vilvl_w(px[3], px[2]), __lsx_vilvl_w(px[1], px[0]));    // Unpack to obtain [R01|G01|B01|R02|R11|G11|B11|R12|R21|G21|B21|R22|R31|G31|B31|R32]
+    px[4] = lsx_shuffle_i8(px[4], xmm_pkd_mask); // shuffle to obtain 4 RGB [R01|G01|B01|R11|G11|B11|R21|G21|B21|R31|G31|B31|00|00|00|00]
+
+    px[0] = invalidLoad[4] ? xmm_px0 : __lsx_vld((__m128i *)(srcPtrChannel + srcLoc[4]), 0); // LOC4 load [R41|G41|B41|R42|G42|B42|R43|G43|B43|R44|G44|B44|R45|G45|B45|R46] - Need RGB 41
+    px[1] = invalidLoad[5] ? xmm_px0 : __lsx_vld((__m128i *)(srcPtrChannel + srcLoc[5]), 0); // LOC5 load [R51|G51|B51|R52|G52|B52|R53|G53|B53|R54|G54|B54|R55|G55|B55|R56] - Need RGB 51
+    px[2] = invalidLoad[6] ? xmm_px0 : __lsx_vld((__m128i *)(srcPtrChannel + srcLoc[6]), 0); // LOC6 load [R61|G61|B61|R62|G62|B62|R63|G63|B63|R64|G64|B64|R65|G65|B65|R66] - Need RGB 61
+    px[3] = invalidLoad[7] ? xmm_px0 : __lsx_vld((__m128i *)(srcPtrChannel + srcLoc[7]), 0); // LOC7 load [R71|G71|B71|R72|G72|B72|R73|G73|B73|R74|G74|B74|R75|G75|B75|R76] - Need RGB 71
+    px[5] = __lsx_vilvl_d(__lsx_vilvl_w(px[3], px[2]), __lsx_vilvl_w(px[1], px[0]));    // Unpack to obtain [R41|G41|B41|R42|R51|G51|B51|R52|R61|G61|B61|R62|R71|G71|B71|R72]
+    px[5] = lsx_shuffle_i8(px[5], xmm_pkd_mask); // shuffle to obtain 4 RGB [R41|G41|B41|R51|G51|B51|R61|G61|B61|R71|G71|B71|00|00|00|00]
+
+    px[6] = lsx_shuffle_i8(px[5], xmm_pxMask00); // shuffle to move 0-3 of px[5] to 12-15
+    px[4] = __lsx_vadd_b(px[4], px[6]); // add px[4] and px[5]
+    px[5] = lsx_shuffle_i8(px[5], xmm_pxMask04To11); // shuffle to move values at 4-11 of px[5] to 0-7
+    p = lasx_setr_m128i(px[4], px[5]); // Merge to obtain 8 RGB [R01|G01|B01|R11|G11|B11|R21|G21|B21|R31|G31|B31|R41|G41|B41|R51|G51|B51|R61|G61|B61|R71|G71|B71|00|00|00|00|00|00|00|00]
+}
+
+inline void rpp_generic_nn_load_i8pln1(Rpp8s *srcPtrChannel, Rpp32s *srcLoc, Rpp32s *invalidLoad, __m128i &p)
+{
+    __m128i px[4];
+    px[0] = invalidLoad[0] ? xmm_px0 : __lsx_vld((__m128i *)(srcPtrChannel + srcLoc[0]), 0);  // LOC0 load [R01|R02|R03|R04|R05|R06...] - Need R01
+    px[1] = invalidLoad[1] ? xmm_px0 : __lsx_vld((__m128i *)(srcPtrChannel + srcLoc[1]), 0);  // LOC1 load [R11|R12|R13|R14|R15|R16...] - Need R11
+    px[2] = invalidLoad[2] ? xmm_px0 : __lsx_vld((__m128i *)(srcPtrChannel + srcLoc[2]), 0);  // LOC2 load [R21|R22|R23|R24|R25|R26...] - Need R21
+    px[3] = invalidLoad[3] ? xmm_px0 : __lsx_vld((__m128i *)(srcPtrChannel + srcLoc[3]), 0);  // LOC3 load [R31|R32|R33|R34|R35|R36...] - Need R31
+    px[0] = __lsx_vilvl_b(px[2], px[0]);    // unpack 8 lo-pixels of px[0] and px[2]
+    px[1] = __lsx_vilvl_b(px[3], px[1]);    // unpack 8 lo-pixels of px[1] and px[3]
+    p = __lsx_vilvl_b(px[1], px[0]);    // unpack to obtain [R01|R11|R21|R31|00|00|00|00|00|00|00|00|00|00|00|00]
+}
+
+inline void rpp_generic_nn_load_i8pln1_avx(Rpp8s *srcPtrChannel, Rpp32s *srcLoc, Rpp32s *invalidLoad, __m256i &p)
+{
+    Rpp8s buffer[16] = {0};
+    for(int i = 0; i < 8; i++)
+    {
+        if(!invalidLoad[i])
+            buffer[i] = *(srcPtrChannel + srcLoc[i]);
+    }
+    __m128i px = __lsx_vld((__m128i *)buffer, 0);
+    p = lasx_setr_m128i(px, xmm_px0);
+}
+
+inline void rpp_generic_bilinear_load_mask_avx(__m256 &pSrcY, __m256 &pSrcX, __m256 *pRoiLTRB, Rpp32s *invalidLoadMask)
+{
+    __lasx_xvst(__lasx_xvftint_w_s((__m256)__lasx_xvor_v(                                    // Vectorized ROI boundary check for 8 locations
+        (__m256i)__lasx_xvor_v((__m256)__lasx_xvfcmp_xxx_s(pSrcX, pRoiLTRB[0], _CMP_LT_OQ), (__m256)__lasx_xvfcmp_xxx_s(pSrcY, pRoiLTRB[1], _CMP_LT_OQ)),
+        (__m256i)__lasx_xvor_v((__m256)__lasx_xvfcmp_xxx_s(__lasx_xvfrintrm_s(pSrcX), pRoiLTRB[2], _CMP_GT_OQ), (__m256)__lasx_xvfcmp_xxx_s(__lasx_xvfrintrm_s(pSrcY), pRoiLTRB[3], _CMP_GT_OQ)))), (__m256i*) invalidLoadMask, 0);
+}
+
+template <typename T>
+inline void rpp_generic_bilinear_load_1c_avx(T *srcPtrChannel, RpptDescPtr srcDescPtr, RpptBilinearNbhoodLocsVecLen8 &srcLocs, __m256 &pSrcY, __m256 &pSrcX, __m256 *pRoiLTRB, __m256 *pSrc)
+{
+    Rpp32s invalidLoadMask[8];
+    RpptBilinearNbhoodValsVecLen8 srcVals;
+    memset(&srcVals, 0, sizeof(RpptBilinearNbhoodValsVecLen8));
+    rpp_generic_bilinear_load_mask_avx(pSrcY, pSrcX, pRoiLTRB, invalidLoadMask);
+    for (int j = 0; j < 8; j++)
+    {
+        if (invalidLoadMask[j] == 0) // Loading specific pixels where invalidLoadMask is set to 0
+        {
+            srcVals.srcValsTL.data[j] = (Rpp32f) srcPtrChannel[srcLocs.srcLocsTL.data[j]];
+            srcVals.srcValsTR.data[j] = (Rpp32f) srcPtrChannel[srcLocs.srcLocsTR.data[j]];
+            srcVals.srcValsBL.data[j] = (Rpp32f) srcPtrChannel[srcLocs.srcLocsBL.data[j]];
+            srcVals.srcValsBR.data[j] = (Rpp32f) srcPtrChannel[srcLocs.srcLocsBR.data[j]];
+        }
+    }
+    pSrc[0] = (__m256)__lasx_xvld(&srcVals.srcValsTL.data[0], 0);      // R channel Top-Left
+    pSrc[1] = (__m256)__lasx_xvld(&srcVals.srcValsTR.data[0], 0);      // R channel Top-Right
+    pSrc[2] = (__m256)__lasx_xvld(&srcVals.srcValsBL.data[0], 0);      // R channel Bottom-Left
+    pSrc[3] = (__m256)__lasx_xvld(&srcVals.srcValsBR.data[0], 0);      // R channel Bottom-Right
+}
+
+template <typename T>
+inline void rpp_generic_bilinear_load_3c_avx(T *srcPtrChannel, RpptDescPtr srcDescPtr, RpptBilinearNbhoodLocsVecLen8 &srcLocs, __m256 &pSrcY, __m256 &pSrcX, __m256 *pRoiLTRB, __m256 *pSrc)
+{
+    Rpp32s invalidLoadMask[8];
+    RpptBilinearNbhoodValsVecLen8 srcVals;
+    memset(&srcVals, 0, sizeof(RpptBilinearNbhoodValsVecLen8));
+    rpp_generic_bilinear_load_mask_avx(pSrcY, pSrcX, pRoiLTRB, invalidLoadMask);
+    for (int j = 0; j < 8; j++)
+    {
+        if (invalidLoadMask[j] == 0) // Loading specific pixels where invalidLoadMask is set to 0
+        {
+            for (int c = 0; c < srcDescPtr->c * 8; c += 8)
+            {
+                Rpp32s pos = c + j;
+                srcVals.srcValsTL.data[pos] = (Rpp32f) srcPtrChannel[srcLocs.srcLocsTL.data[pos]];
+                srcVals.srcValsTR.data[pos] = (Rpp32f) srcPtrChannel[srcLocs.srcLocsTR.data[pos]];
+                srcVals.srcValsBL.data[pos] = (Rpp32f) srcPtrChannel[srcLocs.srcLocsBL.data[pos]];
+                srcVals.srcValsBR.data[pos] = (Rpp32f) srcPtrChannel[srcLocs.srcLocsBR.data[pos]];
+            }
+        }
+    }
+    pSrc[0] = (__m256)__lasx_xvld(&srcVals.srcValsTL.data[0], 0);      // R channel Top-Left
+    pSrc[1] = (__m256)__lasx_xvld(&srcVals.srcValsTR.data[0], 0);      // R channel Top-Right
+    pSrc[2] = (__m256)__lasx_xvld(&srcVals.srcValsBL.data[0], 0);      // R channel Bottom-Left
+    pSrc[3] = (__m256)__lasx_xvld(&srcVals.srcValsBR.data[0], 0);      // R channel Bottom-Right
+    pSrc[4] = (__m256)__lasx_xvld(&srcVals.srcValsTL.data[8], 0);      // G channel Top-Left
+    pSrc[5] = (__m256)__lasx_xvld(&srcVals.srcValsTR.data[8], 0);      // G channel Top-Right
+    pSrc[6] = (__m256)__lasx_xvld(&srcVals.srcValsBL.data[8], 0);      // G channel Bottom-Left
+    pSrc[7] = (__m256)__lasx_xvld(&srcVals.srcValsBR.data[8], 0);      // G channel Bottom-Right
+    pSrc[8] = (__m256)__lasx_xvld(&srcVals.srcValsTL.data[16], 0);     // B channel Top-Left
+    pSrc[9] = (__m256)__lasx_xvld(&srcVals.srcValsTR.data[16], 0);     // B channel Top-Right
+    pSrc[10] = (__m256)__lasx_xvld(&srcVals.srcValsBL.data[16], 0);    // B channel Bottom-Left
+    pSrc[11] = (__m256)__lasx_xvld(&srcVals.srcValsBR.data[16], 0);    // B channel Bottom-Right
+}
+
+/* Resize loads and stores */
+inline void rpp_bilinear_load_u8pkd3_to_f32pln3_avx(Rpp8u **srcRowPtrsForInterp, Rpp32s *loc, __m256* p, __m256i &pxSrcLoc, __m256i &pxMaxSrcLoc, Rpp32s maxSrcLoc)
+{
+    __m128i px[8];
+    px[0] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[0] + loc[0]), 0);  /* Top Row LOC0 load [R01|G01|B01|R02|G02|B02|R03|G03|B03|...] - Need RGB 01-02 */
+    px[1] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[0] + loc[1]), 0);  /* Top Row LOC1 load [R01|G01|B01|R02|G02|B02|R03|G03|B03|...] - Need RGB 01-02 */
+    px[0] = __lsx_vilvl_b(px[1], px[0]);                                /* unpack 8 lo-pixels of px[0] and px[1] */
+    px[2] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[0] + loc[2]), 0);  /* Top Row LOC2 load [R01|G01|B01|R02|G02|B02|R03|G03|B03|...] - Need RGB 01-02 */
+    px[3] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[0] + loc[3]), 0);  /* Top Row LOC3 load [R01|G01|B01|R02|G02|B02|R03|G03|B03|...] - Need RGB 01-02 */
+    px[1] = __lsx_vilvl_b(px[3], px[2]);    /* unpack 8 lo-pixels of px[2] and px[3] */
+    px[2] = __lsx_vilvl_h(px[1], px[0]);   /* unpack to obtain [R01|R01|R01|R01|G01|G01|G01|G01|B01|B01|B01|B01|R02|R02|R02|R02] */
+    px[3] = __lsx_vilvh_h(px[1], px[0]);   /* unpack to obtain [G02|G02|G02|G02|B02|B02|B02|B02|R03|R03|R03|R03|G03|G03|G03|G03] */
+
+    /* Repeat the above steps for next 4 dst locations*/
+    px[4] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[0] + loc[4]), 0);
+    px[5] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[0] + loc[5]), 0);
+    px[4] = __lsx_vilvl_b(px[5], px[4]);
+    px[6] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[0] + loc[6]), 0);
+    px[7] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[0] + loc[7]), 0);
+    px[5] = __lsx_vilvl_b(px[7], px[6]);
+    px[6] = __lsx_vilvl_h(px[5], px[4]);
+    px[7] = __lsx_vilvh_h(px[5], px[4]);
+
+    p[0] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMask00To03), lsx_shuffle_i8(px[6], xmm_pxMask00To03)));  /* Contains TopRow R01 for all the dst locations */
+    p[1] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMask12To15), lsx_shuffle_i8(px[6], xmm_pxMask12To15)));  /* Contains TopRow R02 for all the dst locations */
+    p[4] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMask04To07), lsx_shuffle_i8(px[6], xmm_pxMask04To07)));  /* Contains TopRow G01 for all the dst locations */
+    p[5] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[3], xmm_pxMask00To03), lsx_shuffle_i8(px[7], xmm_pxMask00To03)));  /* Contains TopRow G02 for all the dst locations */
+    p[8] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMask08To11), lsx_shuffle_i8(px[6], xmm_pxMask08To11)));  /* Contains TopRow B01 for all the dst locations */
+    p[9] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[3], xmm_pxMask04To07), lsx_shuffle_i8(px[7], xmm_pxMask04To07)));  /* Contains TopRow B02 for all the dst locations */
+
+    /* Repeat above steps to obtain pixels from the BottomRow*/
+    px[0] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[1] + loc[0]), 0);
+    px[1] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[1] + loc[1]), 0);
+    px[0] = __lsx_vilvl_b(px[1], px[0]);
+    px[2] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[1] + loc[2]), 0);
+    px[3] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[1] + loc[3]), 0);
+    px[1] = __lsx_vilvl_b(px[3], px[2]);
+    px[2] = __lsx_vilvl_h(px[1], px[0]);
+    px[3] = __lsx_vilvh_h(px[1], px[0]);
+
+    px[4] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[1] + loc[4]), 0);
+    px[5] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[1] + loc[5]), 0);
+    px[4] = __lsx_vilvl_b(px[5], px[4]);
+    px[6] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[1] + loc[6]), 0);
+    px[7] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[1] + loc[7]), 0);
+    px[5] = __lsx_vilvl_b(px[7], px[6]);
+    px[6] = __lsx_vilvl_h(px[5], px[4]);
+    px[7] = __lsx_vilvh_h(px[5], px[4]);
+
+    p[2] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMask00To03), lsx_shuffle_i8(px[6], xmm_pxMask00To03)));  /* Contains BottomRow R01 for all the dst locations */
+    p[3] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMask12To15), lsx_shuffle_i8(px[6], xmm_pxMask12To15)));  /* Contains BottomRow R02 for all the dst locations */
+    p[6] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMask04To07), lsx_shuffle_i8(px[6], xmm_pxMask04To07)));  /* Contains BottomRow G01 for all the dst locations */
+    p[7] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[3], xmm_pxMask00To03), lsx_shuffle_i8(px[7], xmm_pxMask00To03)));  /* Contains BottomRow G02 for all the dst locations */
+    p[10] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMask08To11), lsx_shuffle_i8(px[6], xmm_pxMask08To11))); /* Contains BottomRow B01 for all the dst locations */
+    p[11] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[3], xmm_pxMask04To07), lsx_shuffle_i8(px[7], xmm_pxMask04To07))); /* Contains BottomRow B02 for all the dst locations */
+
+    if(loc[0] < 0 || loc[7] < 0) // If any src location below min src location is encountered replace the source pixel loaded with first pixel of the row
+    {
+        __m256 pLowerBoundMask = (__m256)(__lasx_xvslt_w(pxSrcLoc, avx_px0)); // Mask set to true if the location is below min src location
+        p[0] = lasx_blendv_f32(p[0], p[1], pLowerBoundMask);
+        p[2] = lasx_blendv_f32(p[2], p[3], pLowerBoundMask);
+        p[4] = lasx_blendv_f32(p[4], p[5], pLowerBoundMask);
+        p[6] = lasx_blendv_f32(p[6], p[7], pLowerBoundMask);
+        p[8] = lasx_blendv_f32(p[8], p[9], pLowerBoundMask);
+        p[10] = lasx_blendv_f32(p[10], p[11], pLowerBoundMask);
+    }
+    else if(loc[7] > maxSrcLoc || loc[0] > maxSrcLoc) // If any src location beyond max src location -1 is encountered replace the source pixel loaded with first pixel of the row
+    {
+        __m256 pUpperBoundMask = (__m256)(__lasx_xvslt_w(pxMaxSrcLoc, pxSrcLoc)); // Mask set to true if the location is beyond max src location - 1
+        p[1] = lasx_blendv_f32(p[1], p[0], pUpperBoundMask);
+        p[3] = lasx_blendv_f32(p[3], p[2], pUpperBoundMask);
+        p[5] = lasx_blendv_f32(p[5], p[4], pUpperBoundMask);
+        p[7] = lasx_blendv_f32(p[7], p[6], pUpperBoundMask);
+        p[9] = lasx_blendv_f32(p[9], p[8], pUpperBoundMask);
+        p[11] = lasx_blendv_f32(p[11], p[10], pUpperBoundMask);
+    }
+}
+
+inline void rpp_bilinear_load_u8pln1_to_f32pln1_avx(Rpp8u **srcRowPtrsForInterp, Rpp32s *loc, __m256* p, __m256i &pxSrcLoc, __m256i &pxMaxSrcLoc, Rpp32s maxSrcLoc)
+{
+    __m128i pxTemp[8];
+    pxTemp[0] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[0] + loc[0]), 0);  /* Top Row load LOC0 [R01|R02|R03|R04|R05|R06|R07|...|R16] Need R01-02 */
+    pxTemp[1] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[0] + loc[1]), 0);  /* Top Row load LOC1 [R01|R02|R03|R04|R05|R06|R07|...|R16] Need R01-02 */
+    pxTemp[2] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[0] + loc[2]), 0);  /* Top Row load LOC2 [R01|R02|R03|R04|R05|R06|R07|...|R16] Need R01-02 */
+    pxTemp[3] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[0] + loc[3]), 0);  /* Top Row load LOC3 [R01|R02|R03|R04|R05|R06|R07|...|R16] Need R01-02 */
+    pxTemp[0] = __lsx_vilvl_b(pxTemp[1], pxTemp[0]);    /* unpack 8 lo-pixels of px[0] and px[1] */
+    pxTemp[1] = __lsx_vilvl_b(pxTemp[3], pxTemp[2]);    /* unpack 8 lo-pixels of px[2] and px[3] */
+    pxTemp[0] = __lsx_vilvl_h(pxTemp[1], pxTemp[0]);   /* unpack 8 lo-pixels to obtain 1st and 2nd pixels of TopRow*/
+
+    /* Repeat the above steps for next 4 dst locations*/
+    pxTemp[4] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[0] + loc[4]), 0);
+    pxTemp[5] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[0] + loc[5]), 0);
+    pxTemp[6] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[0] + loc[6]), 0);
+    pxTemp[7] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[0] + loc[7]), 0);
+    pxTemp[4] = __lsx_vilvl_b(pxTemp[5], pxTemp[4]);
+    pxTemp[5] = __lsx_vilvl_b(pxTemp[7], pxTemp[6]);
+    pxTemp[4] = __lsx_vilvl_h(pxTemp[5], pxTemp[4]);
+
+    p[0] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(pxTemp[0], xmm_pxMask00To03), lsx_shuffle_i8(pxTemp[4], xmm_pxMask00To03)));    /* Contains 1st pixels of 8 locations from Top row */
+    p[1] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(pxTemp[0], xmm_pxMask04To07), lsx_shuffle_i8(pxTemp[4], xmm_pxMask04To07)));    /* Contains 2nd pixels of 8 locations from Top row */
+
+    /* Repeat above steps to obtain pixels from the BottomRow*/
+    pxTemp[0] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[1] + loc[0]), 0);
+    pxTemp[1] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[1] + loc[1]), 0);
+    pxTemp[2] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[1] + loc[2]), 0);
+    pxTemp[3] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[1] + loc[3]), 0);
+    pxTemp[0] = __lsx_vilvl_b(pxTemp[1], pxTemp[0]);
+    pxTemp[1] = __lsx_vilvl_b(pxTemp[3], pxTemp[2]);
+    pxTemp[0] = __lsx_vilvl_h(pxTemp[1], pxTemp[0]);
+
+    pxTemp[4] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[1] + loc[4]), 0);
+    pxTemp[5] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[1] + loc[5]), 0);
+    pxTemp[6] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[1] + loc[6]), 0);
+    pxTemp[7] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[1] + loc[7]), 0);
+    pxTemp[4] = __lsx_vilvl_b(pxTemp[5], pxTemp[4]);
+    pxTemp[5] = __lsx_vilvl_b(pxTemp[7], pxTemp[6]);
+    pxTemp[4] = __lsx_vilvl_h(pxTemp[5], pxTemp[4]);
+
+    p[2] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(pxTemp[0], xmm_pxMask00To03), lsx_shuffle_i8(pxTemp[4], xmm_pxMask00To03)));    /* Contains 1st pixels of 8 locations from Bottom row */
+    p[3] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(pxTemp[0], xmm_pxMask04To07), lsx_shuffle_i8(pxTemp[4], xmm_pxMask04To07)));    /* Contains 2nd pixels of 8 locations from Bottom row */
+
+    if(loc[0] < 0 || loc[7] < 0) // If any src location below min src location is encountered replace the source pixel loaded with first pixel of the row
+    {
+        __m256 pLowerBoundMask = (__m256)(__lasx_xvslt_w(pxSrcLoc, avx_px0)); // Mask set to true if the location is below min src location
+        p[0] = lasx_blendv_f32(p[0], p[1], pLowerBoundMask);
+        p[2] = lasx_blendv_f32(p[2], p[3], pLowerBoundMask);
+    }
+    else if(loc[7] > maxSrcLoc || loc[0] > maxSrcLoc) // If any src location beyond max src location -1 is encountered replace the source pixel loaded with first pixel of the row
+    {
+        __m256 pUpperBoundMask = (__m256)(__lasx_xvslt_w(pxMaxSrcLoc, pxSrcLoc)); // Mask set to true if the location is beyond max src location - 1
+        p[1] = lasx_blendv_f32(p[1], p[0], pUpperBoundMask);
+        p[3] = lasx_blendv_f32(p[3], p[2], pUpperBoundMask);
+    }
+}
+
+inline void rpp_store24_f32pln3_to_u8pkd3_avx(Rpp8u* dstPtr, __m256* p)
+{
+    __m256i px1 = lasx_packus_i32(__lasx_xvftint_w_s(p[0]), __lasx_xvftint_w_s(p[1])); /* Pack p[0] and p[1] (R and G channels) */
+    __m256i px2 = lasx_packus_i32(__lasx_xvftint_w_s(p[2]), avx_px0);                  /* Pack p[2] and zeros (B channel)*/
+    px1 = lasx_packus_i16(px1, px2);                    /* Pack to obtain |R01|R02|R03|R04|G01|G02|G03|G04|B01|B02|B03|B04|00|...|R05|R06|R07|R08|G05|G06|G07|G08|B05|B06|B07|B08|00|... */
+    px1 = lasx_shuffle_i8(px1, avx_pxShufflePkd);       /* Shuffle to obtain in RGB packed format */
+    px1 = lasx_permutevar8x32_i32(px1, avx_pxPermPkd);  /* Permute to eliminate the zeros in between */
+    __lasx_xvst(px1, (__m256i *)(dstPtr), 0);          /* store the 24 U8 pixels in dst */
+}
+
+inline void rpp_store8_f32pln1_to_u8pln1_avx(Rpp8u* dstPtr, __m256 &p)
+{
+    __m256i px1 = __lasx_xvpermi_d(lasx_packus_i32(__lasx_xvftint_w_s(p), avx_px0), _MM_SHUFFLE(3,1,2,0));
+    px1 = lasx_packus_i16(px1, avx_px0);
+    rpp_storeu_si64((__m128i *)(dstPtr), __lasx_cvt_256_128(px1));
+}
+
+inline void rpp_store24_f32pln3_to_u8pln3_avx(Rpp8u* dstRPtr, Rpp8u* dstGPtr, Rpp8u* dstBPtr, __m256* p)
+{
+    rpp_store8_f32pln1_to_u8pln1_avx(dstRPtr, p[0]);
+    rpp_store8_f32pln1_to_u8pln1_avx(dstGPtr, p[1]);
+    rpp_store8_f32pln1_to_u8pln1_avx(dstBPtr, p[2]);
+}
+
+inline void rpp_bilinear_load_i8pkd3_to_f32pln3_avx(Rpp8s **srcRowPtrsForInterp, Rpp32s *loc, __m256* p, __m256i &pxSrcLoc, __m256i &pxMaxSrcLoc, Rpp32s maxSrcLoc)
+{
+    __m128i px[8];
+    px[0] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[0] + loc[0]), 0);  /* Top Row LOC0 load [R01|G01|B01|R02|G02|B02|R03|G03|B03|...] - Need RGB 01-02 */
+    px[1] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[0] + loc[1]), 0);  /* Top Row LOC1 load [R01|G01|B01|R02|G02|B02|R03|G03|B03|...] - Need RGB 01-02 */
+    px[0] = __lsx_vilvl_b(px[1], px[0]);                                /* unpack 8 lo-pixels of px[0] and px[1] */
+    px[2] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[0] + loc[2]), 0);  /* Top Row LOC2 load [R01|G01|B01|R02|G02|B02|R03|G03|B03|...] - Need RGB 01-02 */
+    px[3] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[0] + loc[3]), 0);  /* Top Row LOC3 load [R01|G01|B01|R02|G02|B02|R03|G03|B03|...] - Need RGB 01-02 */
+    px[1] = __lsx_vilvl_b(px[3], px[2]);                                /* unpack 8 lo-pixels of px[2] and px[3] */
+    px[2] = __lsx_vadd_b(__lsx_vilvl_h(px[1], px[0]), xmm_pxConvertI8);    /* unpack to obtain [R01|R01|R01|R01|G01|G01|G01|G01|B01|B01|B01|B01|R02|R02|R02|R02] */
+    px[3] = __lsx_vadd_b(__lsx_vilvh_h(px[1], px[0]), xmm_pxConvertI8);    /* unpack to obtain [G02|G02|G02|G02|B02|B02|B02|B02|R03|R03|R03|R03|G03|G03|G03|G03] */
+
+    /* Repeat the above steps for next 4 dst locations*/
+    px[4] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[0] + loc[4]), 0);
+    px[5] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[0] + loc[5]), 0);
+    px[4] = __lsx_vilvl_b(px[5], px[4]);
+    px[6] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[0] + loc[6]), 0);
+    px[7] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[0] + loc[7]), 0);
+    px[5] = __lsx_vilvl_b(px[7], px[6]);
+    px[6] = __lsx_vadd_b(__lsx_vilvl_h(px[5], px[4]), xmm_pxConvertI8);
+    px[7] = __lsx_vadd_b(__lsx_vilvh_h(px[5], px[4]), xmm_pxConvertI8);
+
+    p[0] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMask00To03), lsx_shuffle_i8(px[6], xmm_pxMask00To03)));  /* Contains TopRow 1st pixels R channel for all the dst locations */
+    p[1] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMask12To15), lsx_shuffle_i8(px[6], xmm_pxMask12To15)));  /* Contains TopRow 2nd pixels R channel for all the dst locations */
+    p[4] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMask04To07), lsx_shuffle_i8(px[6], xmm_pxMask04To07)));  /* Contains TopRow 1st pixels R channel for all the dst locations */
+    p[5] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[3], xmm_pxMask00To03), lsx_shuffle_i8(px[7], xmm_pxMask00To03)));  /* Contains TopRow 2nd pixels R channel for all the dst locations */
+    p[8] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMask08To11), lsx_shuffle_i8(px[6], xmm_pxMask08To11)));  /* Contains TopRow 1st pixels R channel for all the dst locations */
+    p[9] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[3], xmm_pxMask04To07), lsx_shuffle_i8(px[7], xmm_pxMask04To07)));  /* Contains TopRow 2nd pixels R channel for all the dst locations */
+
+    /* Repeat above steps to obtain pixels from the BottomRow*/
+    px[0] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[1] + loc[0]), 0);
+    px[1] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[1] + loc[1]), 0);
+    px[0] = __lsx_vilvl_b(px[1], px[0]);
+    px[2] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[1] + loc[2]), 0);
+    px[3] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[1] + loc[3]), 0);
+    px[1] = __lsx_vilvl_b(px[3], px[2]);
+    px[2] = __lsx_vadd_b(__lsx_vilvl_h(px[1], px[0]), xmm_pxConvertI8);
+    px[3] = __lsx_vadd_b(__lsx_vilvh_h(px[1], px[0]), xmm_pxConvertI8);
+
+    px[4] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[1] + loc[4]), 0);
+    px[5] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[1] + loc[5]), 0);
+    px[4] = __lsx_vilvl_b(px[5], px[4]);
+    px[6] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[1] + loc[6]), 0);
+    px[7] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[1] + loc[7]), 0);
+    px[5] = __lsx_vilvl_b(px[7], px[6]);
+    px[6] = __lsx_vadd_b(__lsx_vilvl_h(px[5], px[4]), xmm_pxConvertI8);
+    px[7] = __lsx_vadd_b(__lsx_vilvh_h(px[5], px[4]), xmm_pxConvertI8);
+
+    p[2] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMask00To03), lsx_shuffle_i8(px[6], xmm_pxMask00To03)));  /* Contains BottomRow 1st pixels R channel for all the dst locations */
+    p[3] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMask12To15), lsx_shuffle_i8(px[6], xmm_pxMask12To15)));  /* Contains BottomRow 2nd pixels R channel for all the dst locations */
+    p[6] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMask04To07), lsx_shuffle_i8(px[6], xmm_pxMask04To07)));  /* Contains BottomRow 1st pixels G channel for all the dst locations */
+    p[7] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[3], xmm_pxMask00To03), lsx_shuffle_i8(px[7], xmm_pxMask00To03)));  /* Contains BottomRow 2nd pixels G channel for all the dst locations */
+    p[10] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[2], xmm_pxMask08To11), lsx_shuffle_i8(px[6], xmm_pxMask08To11))); /* Contains BottomRow 1st pixels B channel for all the dst locations */
+    p[11] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(px[3], xmm_pxMask04To07), lsx_shuffle_i8(px[7], xmm_pxMask04To07))); /* Contains BottomRow 2nd pixels B channel for all the dst locations */
+
+    if(loc[0] < 0 || loc[7] < 0) // If any src location below min src location is encountered replace the source pixel loaded with first pixel of the row
+    {
+        __m256 pLowerBoundMask = (__m256)(__lasx_xvslt_w(pxSrcLoc, avx_px0)); // Mask set to true if the location is below min src location
+        p[0] = lasx_blendv_f32(p[0], p[1], pLowerBoundMask);
+        p[2] = lasx_blendv_f32(p[2], p[3], pLowerBoundMask);
+        p[4] = lasx_blendv_f32(p[4], p[5], pLowerBoundMask);
+        p[6] = lasx_blendv_f32(p[6], p[7], pLowerBoundMask);
+        p[8] = lasx_blendv_f32(p[8], p[9], pLowerBoundMask);
+        p[10] = lasx_blendv_f32(p[10], p[11], pLowerBoundMask);
+    }
+    else if(loc[7] > maxSrcLoc || loc[0] > maxSrcLoc) // If any src location beyond max src location -1 is encountered replace the source pixel loaded with first pixel of the row
+    {
+        __m256 pUpperBoundMask = (__m256)(__lasx_xvslt_w(pxMaxSrcLoc, pxSrcLoc)); // Mask set to true if the location is beyond max src location - 1
+        p[1] = lasx_blendv_f32(p[1], p[0], pUpperBoundMask);
+        p[3] = lasx_blendv_f32(p[3], p[2], pUpperBoundMask);
+        p[5] = lasx_blendv_f32(p[5], p[4], pUpperBoundMask);
+        p[7] = lasx_blendv_f32(p[7], p[6], pUpperBoundMask);
+        p[9] = lasx_blendv_f32(p[9], p[8], pUpperBoundMask);
+        p[11] = lasx_blendv_f32(p[11], p[10], pUpperBoundMask);
+    }
+}
+
+inline void rpp_bilinear_load_i8pln1_to_f32pln1_avx(Rpp8s **srcRowPtrsForInterp, Rpp32s *loc, __m256* p, __m256i &pxSrcLoc, __m256i &pxMaxSrcLoc, Rpp32s maxSrcLoc)
+{
+    __m128i pxTemp[8];
+    pxTemp[0] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[0] + loc[0]), 0);  /* Top Row load LOC0 [R01|R02|R03|R04|R05|R06|R07|...|R16] Need R01-02 */
+    pxTemp[1] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[0] + loc[1]), 0);  /* Top Row load LOC1 [R01|R02|R03|R04|R05|R06|R07|...|R16] Need R01-02 */
+    pxTemp[2] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[0] + loc[2]), 0);  /* Top Row load LOC2 [R01|R02|R03|R04|R05|R06|R07|...|R16] Need R01-02 */
+    pxTemp[3] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[0] + loc[3]), 0);  /* Top Row load LOC3 [R01|R02|R03|R04|R05|R06|R07|...|R16] Need R01-02 */
+    pxTemp[0] = __lsx_vilvl_b(pxTemp[1], pxTemp[0]);    /* unpack 8 lo-pixels of px[0] and px[1] */
+    pxTemp[1] = __lsx_vilvl_b(pxTemp[3], pxTemp[2]);    /* unpack 8 lo-pixels of px[2] and px[3] */
+    pxTemp[0] = __lsx_vilvl_h(pxTemp[1], pxTemp[0]);   /* unpack 8 lo-pixels to obtain 1st and 2nd pixels of TopRow*/
+
+    /* Repeat the above steps for next 4 dst locations*/
+    pxTemp[4] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[0] + loc[4]), 0);
+    pxTemp[5] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[0] + loc[5]), 0);
+    pxTemp[6] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[0] + loc[6]), 0);
+    pxTemp[7] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[0] + loc[7]), 0);
+    pxTemp[4] = __lsx_vilvl_b(pxTemp[5], pxTemp[4]);
+    pxTemp[5] = __lsx_vilvl_b(pxTemp[7], pxTemp[6]);
+    pxTemp[4] = __lsx_vilvl_h(pxTemp[5], pxTemp[4]);
+
+    pxTemp[0] = __lsx_vadd_b(pxTemp[0], xmm_pxConvertI8);   /* add I8 conversion param */
+    pxTemp[4] = __lsx_vadd_b(pxTemp[4], xmm_pxConvertI8);   /* add I8 conversion param */
+
+    p[0] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(pxTemp[0], xmm_pxMask00To03), lsx_shuffle_i8(pxTemp[4], xmm_pxMask00To03)));    /* Contains 1st pixels of 8 locations from Top row */
+    p[1] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(pxTemp[0], xmm_pxMask04To07), lsx_shuffle_i8(pxTemp[4], xmm_pxMask04To07)));    /* Contains 2nd pixels of 8 locations from Top row */
+
+    /* Repeat above steps to obtain pixels from the BottomRow*/
+    pxTemp[0] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[1] + loc[0]), 0);
+    pxTemp[1] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[1] + loc[1]), 0);
+    pxTemp[2] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[1] + loc[2]), 0);
+    pxTemp[3] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[1] + loc[3]), 0);
+    pxTemp[0] = __lsx_vilvl_b(pxTemp[1], pxTemp[0]);
+    pxTemp[1] = __lsx_vilvl_b(pxTemp[3], pxTemp[2]);
+    pxTemp[0] = __lsx_vilvl_h(pxTemp[1], pxTemp[0]);
+
+    pxTemp[4] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[1] + loc[4]), 0);
+    pxTemp[5] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[1] + loc[5]), 0);
+    pxTemp[6] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[1] + loc[6]), 0);
+    pxTemp[7] = __lsx_vld((__m128i *)(srcRowPtrsForInterp[1] + loc[7]), 0);
+    pxTemp[4] = __lsx_vilvl_b(pxTemp[5], pxTemp[4]);
+    pxTemp[5] = __lsx_vilvl_b(pxTemp[7], pxTemp[6]);
+    pxTemp[4] = __lsx_vilvl_h(pxTemp[5], pxTemp[4]);
+
+    pxTemp[0] = __lsx_vadd_b(pxTemp[0], xmm_pxConvertI8);
+    pxTemp[4] = __lsx_vadd_b(pxTemp[4], xmm_pxConvertI8);
+
+    p[2] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(pxTemp[0], xmm_pxMask00To03), lsx_shuffle_i8(pxTemp[4], xmm_pxMask00To03)));  /* Contains 1st pixels of 8 locations from Bottom row */
+    p[3] = __lasx_xvffint_s_w(lasx_setr_m128i(lsx_shuffle_i8(pxTemp[0], xmm_pxMask04To07), lsx_shuffle_i8(pxTemp[4], xmm_pxMask04To07)));  /* Contains 2nd pixels of 8 locations from Bottom row */
+
+    if(loc[0] < 0 || loc[7] < 0) // If any src location below min src location is encountered replace the source pixel loaded with first pixel of the row
+    {
+        __m256 pLowerBoundMask = (__m256)(__lasx_xvslt_w(pxSrcLoc, avx_px0)); // Mask set to true if the location is below min src location
+        p[0] = lasx_blendv_f32(p[0], p[1], pLowerBoundMask);
+        p[2] = lasx_blendv_f32(p[2], p[3], pLowerBoundMask);
+    }
+    else if(loc[7] > maxSrcLoc || loc[0] > maxSrcLoc) // If any src location beyond max src location -1 is encountered replace the source pixel loaded with first pixel of the row
+    {
+        __m256 pUpperBoundMask = (__m256)(__lasx_xvslt_w(pxMaxSrcLoc, pxSrcLoc)); // Mask set to true if the location is beyond max src location - 1
+        p[1] = lasx_blendv_f32(p[1], p[0], pUpperBoundMask);
+        p[3] = lasx_blendv_f32(p[3], p[2], pUpperBoundMask);
+    }
+}
+
+inline void rpp_store24_f32pln3_to_i8pkd3_avx(Rpp8s* dstPtr, __m256* p)
+{
+    __m256i px1 = lasx_packus_i32(__lasx_xvftint_w_s(p[0]), __lasx_xvftint_w_s(p[1]));  /* Pack the R and G channels to single vector*/
+    __m256i px2 = lasx_packus_i32(__lasx_xvftint_w_s(p[2]), avx_px0);                   /* Pack the B channel with zeros to single vector */
+    px1 = lasx_packus_i16(px1, px2);
+    px1 = lasx_shuffle_i8(px1, avx_pxShufflePkd);       /* Shuffle the pixels to obtain RGB in packed format */
+    px1 = lasx_permutevar8x32_i32(px1, avx_pxPermPkd);  /* Permute to get continuous RGB pixels */
+    px1 = __lasx_xvsub_b(px1, avx_pxConvertI8);            /* add I8 conversion param */
+    __lasx_xvst(px1, (__m256i *)(dstPtr), 0);          /* store the 12 U8 pixels in dst */
+}
+
+inline void rpp_store8_f32pln1_to_i8pln1_avx(Rpp8s* dstPtr, __m256 &p)
+{
+    __m256i px1 = __lasx_xvpermi_d(lasx_packus_i32(__lasx_xvftint_w_s(p), avx_px0), _MM_SHUFFLE(3,1,2,0));
+    px1 = __lasx_xvsub_b(lasx_packus_i16(px1, avx_px0), avx_pxConvertI8);  /* Pack and add I8 conversion param */
+    rpp_storeu_si64((__m128i *)(dstPtr), __lasx_cvt_256_128(px1));          /* store the 4 pixels in dst */
+}
+
+inline void rpp_store24_f32pln3_to_i8pln3_avx(Rpp8s* dstRPtr, Rpp8s* dstGPtr, Rpp8s* dstBPtr, __m256* p)
+{
+    rpp_store8_f32pln1_to_i8pln1_avx(dstRPtr, p[0]);
+    rpp_store8_f32pln1_to_i8pln1_avx(dstGPtr, p[1]);
+    rpp_store8_f32pln1_to_i8pln1_avx(dstBPtr, p[2]);
+}
+
+inline void rpp_bilinear_load_f32pkd3_to_f32pln3_avx(Rpp32f **srcRowPtrsForInterp, Rpp32s *loc, __m256* p, __m256i &pxSrcLoc, __m256i &pxMaxSrcLoc, Rpp32s maxSrcLoc)
+{
+    __m256 pTemp[10];
+    pTemp[0] = (__m256)__lasx_xvld(srcRowPtrsForInterp[0] + loc[0], 0);   /* Top Row load LOC0 [R01|G01|B01|R02|G02|B02|XX|XX] Need RGB 01-02 */
+    pTemp[1] = (__m256)__lasx_xvld(srcRowPtrsForInterp[0] + loc[1], 0);   /* Top Row load LOC1 [R01|G01|B01|R02|G02|B02|XX|XX] Need RGB 01-02 */
+    pTemp[2] = (__m256)__lasx_xvld(srcRowPtrsForInterp[0] + loc[2], 0);   /* Top Row load LOC2 [R01|G01|B01|R02|G02|B02|XX|XX] Need RGB 01-02 */
+    pTemp[3] = (__m256)__lasx_xvld(srcRowPtrsForInterp[0] + loc[3], 0);   /* Top Row load LOC3 [R01|G01|B01|R02|G02|B02|XX|XX] Need RGB 01-02 */
+
+    pTemp[4] = (__m256)__lasx_xvilvl_w((__m256i)pTemp[1], (__m256i)pTemp[0]);  /* Unpack to obtain [R01|R01|G01|G01|G02|G02|B02|B02] of LOC0 & LOC1 */
+    pTemp[5] = (__m256)__lasx_xvilvh_w((__m256i)pTemp[1], (__m256i)pTemp[0]);  /* Unpack to obtain [B01|B01|R02|R02|XX|XX|XX|XX] of LOC0 & LOC1*/
+    pTemp[6] = (__m256)__lasx_xvilvl_w((__m256i)pTemp[3], (__m256i)pTemp[2]);  /* Unpack to obtain [R01|R01|G01|G01|G02|G02|B02|B02] of LOC2 & LOC3 */
+    pTemp[7] = (__m256)__lasx_xvilvh_w((__m256i)pTemp[3], (__m256i)pTemp[2]);  /* Unpack to obtain [B01|B01|R02|R02|XX|XX|XX|XX] of LOC2 & LOC3 */
+
+    pTemp[8] = (__m256)((__m256d)__lasx_xvilvl_d((__m256i)(__m256d)(pTemp[6]), (__m256i)(__m256d)(pTemp[4])));  /* Unpack to obtain [R01|R01|R01|R01|G02|G02|G02|G02] */
+    pTemp[9] = (__m256)((__m256d)__lasx_xvilvh_d((__m256i)(__m256d)(pTemp[6]), (__m256i)(__m256d)(pTemp[4])));  /* Unpack to obtain [G01|G01|G01|G01|B02|B02|B02|B02] */
+    p[8] = (__m256)((__m256d)__lasx_xvilvl_d((__m256i)(__m256d)(pTemp[7]), (__m256i)(__m256d)(pTemp[5])));    /* Unpack to obtain [B01|B01|B01|B01|XX|XX|XX|XX] */
+    p[1] = (__m256)((__m256d)__lasx_xvilvh_d((__m256i)(__m256d)(pTemp[7]), (__m256i)(__m256d)(pTemp[5])));    /* Unpack to obtain [R02|R02|R02|R02|XX|XX|XX|XX] */
+
+    /* Repeat the above steps for next 4 dst locations*/
+    pTemp[0] = (__m256)__lasx_xvld(srcRowPtrsForInterp[0] + loc[4], 0);
+    pTemp[1] = (__m256)__lasx_xvld(srcRowPtrsForInterp[0] + loc[5], 0);
+    pTemp[2] = (__m256)__lasx_xvld(srcRowPtrsForInterp[0] + loc[6], 0);
+    pTemp[3] = (__m256)__lasx_xvld(srcRowPtrsForInterp[0] + loc[7], 0);
+
+    pTemp[4] = (__m256)__lasx_xvilvl_w((__m256i)pTemp[1], (__m256i)pTemp[0]);
+    pTemp[5] = (__m256)__lasx_xvilvh_w((__m256i)pTemp[1], (__m256i)pTemp[0]);
+    pTemp[6] = (__m256)__lasx_xvilvl_w((__m256i)pTemp[3], (__m256i)pTemp[2]);
+    pTemp[7] = (__m256)__lasx_xvilvh_w((__m256i)pTemp[3], (__m256i)pTemp[2]);
+
+    pTemp[0] = (__m256)((__m256d)__lasx_xvilvl_d((__m256i)(__m256d)(pTemp[6]), (__m256i)(__m256d)(pTemp[4])));
+    pTemp[1] = (__m256)((__m256d)__lasx_xvilvh_d((__m256i)(__m256d)(pTemp[6]), (__m256i)(__m256d)(pTemp[4])));
+    pTemp[2] = (__m256)((__m256d)__lasx_xvilvl_d((__m256i)(__m256d)(pTemp[7]), (__m256i)(__m256d)(pTemp[5])));
+    pTemp[3] = (__m256)((__m256d)__lasx_xvilvh_d((__m256i)(__m256d)(pTemp[7]), (__m256i)(__m256d)(pTemp[5])));
+
+    p[0] = lasx_permute2f128_f32(pTemp[8], pTemp[0], 32);  /* Permute to obtain R01 of 8 dst pixels in Top Row*/
+    p[4] = lasx_permute2f128_f32(pTemp[9], pTemp[1], 32);  /* Permute to obtain G01 of 8 dst pixels in Top Row*/
+    p[8] = lasx_permute2f128_f32(p[8], pTemp[2], 32);      /* Permute to obtain B01 of 8 dst pixels in Top Row*/
+
+    p[1] = lasx_permute2f128_f32(p[1], pTemp[3], 32);      /* Permute to obtain R02 of 8 dst pixels in Top Row*/
+    p[5] = lasx_permute2f128_f32(pTemp[8], pTemp[0], 49);  /* Permute to obtain G02 of 8 dst pixels in Top Row*/
+    p[9] = lasx_permute2f128_f32(pTemp[9], pTemp[1], 49);  /* Permute to obtain B02 of 8 dst pixels in Top Row*/
+
+    /* Repeat above steps to obtain pixels from the BottomRow*/
+    pTemp[0] = (__m256)__lasx_xvld(srcRowPtrsForInterp[1] + loc[0], 0);
+    pTemp[1] = (__m256)__lasx_xvld(srcRowPtrsForInterp[1] + loc[1], 0);
+    pTemp[2] = (__m256)__lasx_xvld(srcRowPtrsForInterp[1] + loc[2], 0);
+    pTemp[3] = (__m256)__lasx_xvld(srcRowPtrsForInterp[1] + loc[3], 0);
+
+    pTemp[4] = (__m256)__lasx_xvilvl_w((__m256i)pTemp[1], (__m256i)pTemp[0]);
+    pTemp[5] = (__m256)__lasx_xvilvh_w((__m256i)pTemp[1], (__m256i)pTemp[0]);
+    pTemp[6] = (__m256)__lasx_xvilvl_w((__m256i)pTemp[3], (__m256i)pTemp[2]);
+    pTemp[7] = (__m256)__lasx_xvilvh_w((__m256i)pTemp[3], (__m256i)pTemp[2]);
+
+    pTemp[8] = (__m256)((__m256d)__lasx_xvilvl_d((__m256i)(__m256d)(pTemp[6]), (__m256i)(__m256d)(pTemp[4])));
+    pTemp[9] = (__m256)((__m256d)__lasx_xvilvh_d((__m256i)(__m256d)(pTemp[6]), (__m256i)(__m256d)(pTemp[4])));
+    p[10] = (__m256)((__m256d)__lasx_xvilvl_d((__m256i)(__m256d)(pTemp[7]), (__m256i)(__m256d)(pTemp[5])));
+    p[3] = (__m256)((__m256d)__lasx_xvilvh_d((__m256i)(__m256d)(pTemp[7]), (__m256i)(__m256d)(pTemp[5])));
+
+    pTemp[0] = (__m256)__lasx_xvld(srcRowPtrsForInterp[1] + loc[4], 0);
+    pTemp[1] = (__m256)__lasx_xvld(srcRowPtrsForInterp[1] + loc[5], 0);
+    pTemp[2] = (__m256)__lasx_xvld(srcRowPtrsForInterp[1] + loc[6], 0);
+    pTemp[3] = (__m256)__lasx_xvld(srcRowPtrsForInterp[1] + loc[7], 0);
+
+    pTemp[4] = (__m256)__lasx_xvilvl_w((__m256i)pTemp[1], (__m256i)pTemp[0]);
+    pTemp[5] = (__m256)__lasx_xvilvh_w((__m256i)pTemp[1], (__m256i)pTemp[0]);
+    pTemp[6] = (__m256)__lasx_xvilvl_w((__m256i)pTemp[3], (__m256i)pTemp[2]);
+    pTemp[7] = (__m256)__lasx_xvilvh_w((__m256i)pTemp[3], (__m256i)pTemp[2]);
+
+    pTemp[0] = (__m256)((__m256d)__lasx_xvilvl_d((__m256i)(__m256d)(pTemp[6]), (__m256i)(__m256d)(pTemp[4])));
+    pTemp[1] = (__m256)((__m256d)__lasx_xvilvh_d((__m256i)(__m256d)(pTemp[6]), (__m256i)(__m256d)(pTemp[4])));
+    pTemp[2] = (__m256)((__m256d)__lasx_xvilvl_d((__m256i)(__m256d)(pTemp[7]), (__m256i)(__m256d)(pTemp[5])));
+    pTemp[3] = (__m256)((__m256d)__lasx_xvilvh_d((__m256i)(__m256d)(pTemp[7]), (__m256i)(__m256d)(pTemp[5])));
+
+    p[2] = lasx_permute2f128_f32(pTemp[8], pTemp[0], 32);  /* Permute to obtain R01 of 8 dst pixels in Bottom Row*/
+    p[6] = lasx_permute2f128_f32(pTemp[9], pTemp[1], 32);  /* Permute to obtain G01 of 8 dst pixels in Bottom Row*/
+    p[10] = lasx_permute2f128_f32(p[10], pTemp[2], 32);    /* Permute to obtain B01 of 8 dst pixels in Bottom Row*/
+
+    p[3] = lasx_permute2f128_f32(p[3], pTemp[3], 32);      /* Permute to obtain R02 of 8 dst pixels in Botttom Row*/
+    p[7] = lasx_permute2f128_f32(pTemp[8], pTemp[0], 49);  /* Permute to obtain G02 of 8 dst pixels in Bottom Row*/
+    p[11] = lasx_permute2f128_f32(pTemp[9], pTemp[1], 49); /* Permute to obtain B02 of 8 dst pixels in Bottom Row*/
+
+    if(loc[0] < 0 || loc[7] < 0) // If any src location below min src location is encountered replace the source pixel loaded with first pixel of the row
+    {
+        __m256 pLowerBoundMask = (__m256)(__lasx_xvslt_w(pxSrcLoc, avx_px0)); // Mask set to true if the location is below min src location
+        p[0] = lasx_blendv_f32(p[0], p[1], pLowerBoundMask);
+        p[2] = lasx_blendv_f32(p[2], p[3], pLowerBoundMask);
+        p[4] = lasx_blendv_f32(p[4], p[5], pLowerBoundMask);
+        p[6] = lasx_blendv_f32(p[6], p[7], pLowerBoundMask);
+        p[8] = lasx_blendv_f32(p[8], p[9], pLowerBoundMask);
+        p[10] = lasx_blendv_f32(p[10], p[11], pLowerBoundMask);
+    }
+    else if(loc[7] > maxSrcLoc || loc[0] > maxSrcLoc) // If any src location beyond max src location -1 is encountered replace the source pixel loaded with first pixel of the row
+    {
+        __m256 pUpperBoundMask = (__m256)(__lasx_xvslt_w(pxMaxSrcLoc, pxSrcLoc)); // Mask set to true if the location is beyond max src location - 1
+        p[1] = lasx_blendv_f32(p[1], p[0], pUpperBoundMask);
+        p[3] = lasx_blendv_f32(p[3], p[2], pUpperBoundMask);
+        p[5] = lasx_blendv_f32(p[5], p[4], pUpperBoundMask);
+        p[7] = lasx_blendv_f32(p[7], p[6], pUpperBoundMask);
+        p[9] = lasx_blendv_f32(p[9], p[8], pUpperBoundMask);
+        p[11] = lasx_blendv_f32(p[11], p[10], pUpperBoundMask);
+    }
+}
+
+inline void rpp_bilinear_load_f32pln1_to_f32pln1_avx(Rpp32f **srcRowPtrsForInterp, Rpp32s *loc, __m256* p, __m256i &pxSrcLoc, __m256i &pxMaxSrcLoc, Rpp32s maxSrcLoc)
+{
+    __m128 pTemp[6];
+    pTemp[0] = (__m128)__lsx_vld(srcRowPtrsForInterp[0] + loc[0], 0);   /* Top Row load LOC0 [R01|R02|R03|R04] Need R01-02 */
+    pTemp[1] = (__m128)__lsx_vld(srcRowPtrsForInterp[0] + loc[1], 0);   /* Top Row load LOC1 [R01|R02|R03|R04] Need R01-02 */
+    pTemp[2] = (__m128)__lsx_vld(srcRowPtrsForInterp[0] + loc[2], 0);   /* Top Row load LOC2 [R01|R02|R03|R04] Need R01-02 */
+    pTemp[3] = (__m128)__lsx_vld(srcRowPtrsForInterp[0] + loc[3], 0);   /* Top Row load LOC3 [R01|R02|R03|R04] Need R01-02 */
+    _MM_TRANSPOSE4_PS(pTemp[0], pTemp[1], pTemp[2], pTemp[3]);  /* Transpose to obtain the R01 and R02 pixels of 4 locations in same vector*/
+
+    /* Repeat the above steps for next 4 dst locations*/
+    pTemp[2] = (__m128)__lsx_vld(srcRowPtrsForInterp[0] + loc[4], 0);
+    pTemp[3] = (__m128)__lsx_vld(srcRowPtrsForInterp[0] + loc[5], 0);
+    pTemp[4] = (__m128)__lsx_vld(srcRowPtrsForInterp[0] + loc[6], 0);
+    pTemp[5] = (__m128)__lsx_vld(srcRowPtrsForInterp[0] + loc[7], 0);
+    _MM_TRANSPOSE4_PS(pTemp[2], pTemp[3], pTemp[4], pTemp[5]);
+    p[0] = lasx_setr_m128(pTemp[0], pTemp[2]);    /* Set to obtain the 1st pixels of 8 dst locations in single vector */
+    p[1] = lasx_setr_m128(pTemp[1], pTemp[3]);    /* Set to obtain the 2nd pixels of 8 dst locations in single vector */
+
+    /* Repeat above steps to obtain pixels from the BottomRow*/
+    pTemp[0] = (__m128)__lsx_vld(srcRowPtrsForInterp[1] + loc[0], 0);
+    pTemp[1] = (__m128)__lsx_vld(srcRowPtrsForInterp[1] + loc[1], 0);
+    pTemp[2] = (__m128)__lsx_vld(srcRowPtrsForInterp[1] + loc[2], 0);
+    pTemp[3] = (__m128)__lsx_vld(srcRowPtrsForInterp[1] + loc[3], 0);
+    _MM_TRANSPOSE4_PS(pTemp[0], pTemp[1], pTemp[2], pTemp[3]);
+
+    pTemp[2] = (__m128)__lsx_vld(srcRowPtrsForInterp[1] + loc[4], 0);
+    pTemp[3] = (__m128)__lsx_vld(srcRowPtrsForInterp[1] + loc[5], 0);
+    pTemp[4] = (__m128)__lsx_vld(srcRowPtrsForInterp[1] + loc[6], 0);
+    pTemp[5] = (__m128)__lsx_vld(srcRowPtrsForInterp[1] + loc[7], 0);
+    _MM_TRANSPOSE4_PS(pTemp[2], pTemp[3], pTemp[4], pTemp[5]);
+    p[2] = lasx_setr_m128(pTemp[0], pTemp[2]);
+    p[3] = lasx_setr_m128(pTemp[1], pTemp[3]);
+
+    if(loc[0] < 0 || loc[7] < 0) // If any src location below min src location is encountered replace the source pixel loaded with first pixel of the row
+    {
+        __m256 pLowerBoundMask = (__m256)(__lasx_xvslt_w(pxSrcLoc, avx_px0)); // Mask set to true if the location is below min src location
+        p[0] = lasx_blendv_f32(p[0], p[1], pLowerBoundMask);
+        p[2] = lasx_blendv_f32(p[2], p[3], pLowerBoundMask);
+    }
+    else if(loc[7] > maxSrcLoc || loc[0] > maxSrcLoc) // If any src location beyond max src location -1 is encountered replace the source pixel loaded with first pixel of the row
+    {
+        __m256 pUpperBoundMask = (__m256)(__lasx_xvslt_w(pxMaxSrcLoc, pxSrcLoc)); // Mask set to true if the location is beyond max src location - 1
+        p[1] = lasx_blendv_f32(p[1], p[0], pUpperBoundMask);
+        p[3] = lasx_blendv_f32(p[3], p[2], pUpperBoundMask);
+    }
+}
+
+inline void rpp_store8_f32pln1_to_f32pln1_avx(Rpp32f* dstPtr, __m256 p)
+{
+    __lasx_xvst((__m256i)p, dstPtr, 0);   /* store the 8 pixels in dst*/
+}
+
+inline void rpp_bilinear_load_f16pkd3_to_f32pln3_avx(Rpp16f **srcRowPtrsForInterp, Rpp32s *loc, __m256* p, __m256i &pxSrcLoc, __m256i &pxMaxSrcLoc, Rpp32s maxSrcLoc)
+{
+    Rpp32f topRow0[3][8], topRow1[3][8], bottomRow0[3][8], bottomRow1[3][8];
+    for(int cnt = 0; cnt < 8; cnt++)
+    {
+        *(topRow0[0] + cnt) = (Rpp32f) *(srcRowPtrsForInterp[0] + loc[cnt]);
+        *(topRow0[1] + cnt) = (Rpp32f) *(srcRowPtrsForInterp[0] + loc[cnt] + 1);
+        *(topRow0[2] + cnt) = (Rpp32f) *(srcRowPtrsForInterp[0] + loc[cnt] + 2);
+        *(topRow1[0] + cnt) = (Rpp32f) *(srcRowPtrsForInterp[0] + loc[cnt] + 3);
+        *(topRow1[1] + cnt) = (Rpp32f) *(srcRowPtrsForInterp[0] + loc[cnt] + 4);
+        *(topRow1[2] + cnt) = (Rpp32f) *(srcRowPtrsForInterp[0] + loc[cnt] + 5);
+
+        *(bottomRow0[0] + cnt) = (Rpp32f) *(srcRowPtrsForInterp[1] + loc[cnt]);
+        *(bottomRow0[1] + cnt) = (Rpp32f) *(srcRowPtrsForInterp[1] + loc[cnt] + 1);
+        *(bottomRow0[2] + cnt) = (Rpp32f) *(srcRowPtrsForInterp[1] + loc[cnt] + 2);
+        *(bottomRow1[0] + cnt) = (Rpp32f) *(srcRowPtrsForInterp[1] + loc[cnt] + 3);
+        *(bottomRow1[1] + cnt) = (Rpp32f) *(srcRowPtrsForInterp[1] + loc[cnt] + 4);
+        *(bottomRow1[2] + cnt) = (Rpp32f) *(srcRowPtrsForInterp[1] + loc[cnt] + 5);
+    }
+
+    p[0] = (__m256)__lasx_xvld(topRow0[0], 0);
+    p[4] = (__m256)__lasx_xvld(topRow0[1], 0);
+    p[8] = (__m256)__lasx_xvld(topRow0[2], 0);
+
+    p[1] = (__m256)__lasx_xvld(topRow1[0], 0);
+    p[5] = (__m256)__lasx_xvld(topRow1[1], 0);
+    p[9] = (__m256)__lasx_xvld(topRow1[2], 0);
+
+    p[2] = (__m256)__lasx_xvld(bottomRow0[0], 0);
+    p[6] = (__m256)__lasx_xvld(bottomRow0[1], 0);
+    p[10] = (__m256)__lasx_xvld(bottomRow0[2], 0);
+
+    p[3] = (__m256)__lasx_xvld(bottomRow1[0], 0);
+    p[7] = (__m256)__lasx_xvld(bottomRow1[1], 0);
+    p[11] = (__m256)__lasx_xvld(bottomRow1[2], 0);
+
+    if(loc[0] < 0 || loc[7] < 0) // If any src location below min src location is encountered replace the source pixel loaded with first pixel of the row
+    {
+        __m256 pLowerBoundMask = (__m256)(__lasx_xvslt_w(pxSrcLoc, avx_px0)); // Mask set to true if the location is below min src location
+        p[0] = lasx_blendv_f32(p[0], p[1], pLowerBoundMask);
+        p[2] = lasx_blendv_f32(p[2], p[3], pLowerBoundMask);
+        p[4] = lasx_blendv_f32(p[4], p[5], pLowerBoundMask);
+        p[6] = lasx_blendv_f32(p[6], p[7], pLowerBoundMask);
+        p[8] = lasx_blendv_f32(p[8], p[9], pLowerBoundMask);
+        p[10] = lasx_blendv_f32(p[10], p[11], pLowerBoundMask);
+    }
+    else if(loc[7] > maxSrcLoc || loc[0] > maxSrcLoc) // If any src location beyond max src location -1 is encountered replace the source pixel loaded with first pixel of the row
+    {
+        __m256 pUpperBoundMask = (__m256)(__lasx_xvslt_w(pxMaxSrcLoc, pxSrcLoc)); // Mask set to true if the location is beyond max src location - 1
+        p[1] = lasx_blendv_f32(p[1], p[0], pUpperBoundMask);
+        p[3] = lasx_blendv_f32(p[3], p[2], pUpperBoundMask);
+        p[5] = lasx_blendv_f32(p[5], p[4], pUpperBoundMask);
+        p[7] = lasx_blendv_f32(p[7], p[6], pUpperBoundMask);
+        p[9] = lasx_blendv_f32(p[9], p[8], pUpperBoundMask);
+        p[11] = lasx_blendv_f32(p[11], p[10], pUpperBoundMask);
+    }
+}
+
+inline void rpp_bilinear_load_f16pln1_to_f32pln1_avx(Rpp16f **srcRowPtrsForInterp, Rpp32s *loc, __m256* p, __m256i &pxSrcLoc, __m256i &pxMaxSrcLoc, Rpp32s maxSrcLoc)
+{
+    Rpp32f topRow0[8], topRow1[8], bottomRow0[8], bottomRow1[8];
+    for(int cnt = 0; cnt < 8; cnt++)
+    {
+        *(topRow0 + cnt) = (Rpp32f) *(srcRowPtrsForInterp[0] + loc[cnt]);
+        *(topRow1 + cnt) = (Rpp32f) *(srcRowPtrsForInterp[0] + loc[cnt] + 1);
+        *(bottomRow0 + cnt) = (Rpp32f) *(srcRowPtrsForInterp[1] + loc[cnt]);
+        *(bottomRow1 + cnt) = (Rpp32f) *(srcRowPtrsForInterp[1] + loc[cnt] + 1);
+    }
+    p[0] = (__m256)__lasx_xvld(topRow0, 0);
+    p[1] = (__m256)__lasx_xvld(topRow1, 0);
+    p[2] = (__m256)__lasx_xvld(bottomRow0, 0);
+    p[3] = (__m256)__lasx_xvld(bottomRow1, 0);
+
+    if(loc[0] < 0 || loc[7] < 0) // If any src location below min src location is encountered replace the source pixel loaded with first pixel of the row
+    {
+       __m256 pLowerBoundMask = (__m256)(__lasx_xvslt_w(pxSrcLoc, avx_px0)); // Mask set to true if the location is below min src location
+        p[0] = lasx_blendv_f32(p[0], p[1], pLowerBoundMask);
+        p[2] = lasx_blendv_f32(p[2], p[3], pLowerBoundMask);
+    }
+    else if(loc[7] > maxSrcLoc || loc[0] > maxSrcLoc) // If any src location beyond max src location -1 is encountered replace the source pixel loaded with first pixel of the row
+    {
+        __m256 pUpperBoundMask = (__m256)(__lasx_xvslt_w(pxMaxSrcLoc, pxSrcLoc)); // Mask set to true if the location is beyond max src location - 1
+        p[1] = lasx_blendv_f32(p[1], p[0], pUpperBoundMask);
+        p[3] = lasx_blendv_f32(p[3], p[2], pUpperBoundMask);
+    }
+}
+
+inline void rpp_store24_f32pln3_to_f16pkd3_avx(Rpp16f* dstPtr, __m256* p)
+{
+    __m128 p128[4];
+    p128[0] = lasx_extractf128_f32(p[0], 0);
+    p128[1] = lasx_extractf128_f32(p[1], 0);
+    p128[2] = lasx_extractf128_f32(p[2], 0);
+    _MM_TRANSPOSE4_PS(p128[0], p128[1], p128[2], p128[3]);
+
+    __m128i px128[4];
+    px128[0] = __lsx_vfcvtrxxx_h_s(p128[0], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    px128[1] = __lsx_vfcvtrxxx_h_s(p128[1], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    px128[2] = __lsx_vfcvtrxxx_h_s(p128[2], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    px128[3] = __lsx_vfcvtrxxx_h_s(p128[3], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    __lsx_vst(px128[0], (__m128i *)dstPtr, 0);
+    __lsx_vst(px128[1], (__m128i *)(dstPtr + 3), 0);
+    __lsx_vst(px128[2], (__m128i *)(dstPtr + 6), 0);
+    __lsx_vst(px128[3], (__m128i *)(dstPtr + 9), 0);
+
+    p128[0] = lasx_extractf128_f32(p[0], 1);
+    p128[1] = lasx_extractf128_f32(p[1], 1);
+    p128[2] = lasx_extractf128_f32(p[2], 1);
+    _MM_TRANSPOSE4_PS(p128[0], p128[1], p128[2], p128[3]);
+
+    px128[0] = __lsx_vfcvtrxxx_h_s(p128[0], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    px128[1] = __lsx_vfcvtrxxx_h_s(p128[1], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    px128[2] = __lsx_vfcvtrxxx_h_s(p128[2], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    px128[3] = __lsx_vfcvtrxxx_h_s(p128[3], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    __lsx_vst(px128[0], (__m128i *)(dstPtr + 12), 0);
+    __lsx_vst(px128[1], (__m128i *)(dstPtr + 15), 0);
+    __lsx_vst(px128[2], (__m128i *)(dstPtr + 18), 0);
+    __lsx_vst(px128[3], (__m128i *)(dstPtr + 21), 0);
+}
+
+inline void rpp_store8_f32pln1_to_f16pln1_avx(Rpp16f* dstPtr, __m256 p)
+{
+    __m128i px128 = lasx_cvtf32_ph(p, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    __lsx_vst(px128, (__m128i *)dstPtr, 0);
+}
+
+inline void rpp_store24_f32pln3_to_f16pln3_avx(Rpp16f* dstRPtr, Rpp16f* dstGPtr, Rpp16f* dstBPtr, __m256* p)
+{
+    __m128i px128[3];
+    px128[0] = lasx_cvtf32_ph(p[0], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    px128[1] = lasx_cvtf32_ph(p[1], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    px128[2] = lasx_cvtf32_ph(p[2], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    __lsx_vst(px128[0], (__m128i *)dstRPtr, 0);
+    __lsx_vst(px128[1], (__m128i *)dstGPtr, 0);
+    __lsx_vst(px128[2], (__m128i *)dstBPtr, 0);
+}
+
+inline void rpp_resize_load(Rpp8u *srcPtr, __m128 *p)
+{
+    rpp_load16_u8_to_f32(srcPtr, p);
+}
+
+inline void rpp_resize_load(Rpp32f *srcPtr, __m128 *p)
+{
+    rpp_load4_f32_to_f32(srcPtr, p);
+}
+
+inline void rpp_resize_load(Rpp8s *srcPtr, __m128 *p)
+{
+    rpp_load16_i8_to_f32(srcPtr, p);
+}
+
+inline void rpp_resize_load(Rpp16f *srcPtr, __m128 *p)
+{
+    Rpp32f srcPtrTemp_ps[8];
+    for(int cnt = 0; cnt < 8; cnt ++)
+        *(srcPtrTemp_ps + cnt) = (Rpp32f) *(srcPtr + cnt);
+
+    rpp_load4_f32_to_f32(srcPtrTemp_ps, p);
+    rpp_load4_f32_to_f32(srcPtrTemp_ps + 4, p + 1);
+}
+
+inline void rpp_resize_store(Rpp8u *dstPtr, __m128 *p)
+{
+    rpp_store16_f32_to_u8(dstPtr, p);
+}
+
+inline void rpp_resize_store(Rpp32f *dstPtr, __m128 *p)
+{
+    rpp_store4_f32_to_f32(dstPtr, p);
+}
+
+inline void rpp_resize_store(Rpp8s *dstPtr, __m128 *p)
+{
+    rpp_store16_f32_to_i8(dstPtr, p);
+}
+
+inline void rpp_resize_store(Rpp16f *dstPtr, __m128 *p)
+{
+    Rpp32f dstPtrTemp_ps[8];
+    rpp_store4_f32_to_f32(dstPtrTemp_ps, p);
+    rpp_store4_f32_to_f32(dstPtrTemp_ps + 4, p + 1);
+    for(int cnt = 0; cnt < 8; cnt ++)
+        *(dstPtr + cnt) = (Rpp16f) *(dstPtrTemp_ps + cnt);
+}
+
+inline void rpp_resize_store_pln3(Rpp8u *dstPtrR, Rpp8u *dstPtrG, Rpp8u *dstPtrB, __m128 *p)
+{
+    rpp_store48_f32pln3_to_u8pln3(dstPtrR, dstPtrG, dstPtrB, p);
+}
+
+inline void rpp_resize_store_pln3(Rpp32f *dstPtrR, Rpp32f *dstPtrG, Rpp32f *dstPtrB, __m128 *p)
+{
+    rpp_store12_f32pln3_to_f32pln3(dstPtrR, dstPtrG, dstPtrB, p);
+}
+
+inline void rpp_resize_store_pln3(Rpp16f *dstPtrR, Rpp16f *dstPtrG, Rpp16f *dstPtrB, __m128 *p)
+{
+    __m128 temp[3];
+    Rpp32f dstPtrTempR_ps[8], dstPtrTempG_ps[8], dstPtrTempB_ps[8];
+    temp[0] = p[0]; /* R channel */
+    temp[1] = p[2]; /* G channel */
+    temp[2] = p[4]; /* B channel */
+    rpp_store12_f32pln3_to_f32pln3(dstPtrTempR_ps, dstPtrTempG_ps, dstPtrTempB_ps, temp);
+    temp[0] = p[1]; /* R channel */
+    temp[1] = p[3]; /* R channel */
+    temp[2] = p[5]; /* R channel */
+    rpp_store12_f32pln3_to_f32pln3(dstPtrTempR_ps + 4, dstPtrTempG_ps + 4, dstPtrTempB_ps + 4, temp);
+
+    for(int cnt = 0; cnt < 8; cnt++)
+    {
+        *(dstPtrR + cnt) = (Rpp16f) *(dstPtrTempR_ps + cnt);
+        *(dstPtrG + cnt) = (Rpp16f) *(dstPtrTempG_ps + cnt);
+        *(dstPtrB + cnt) = (Rpp16f) *(dstPtrTempB_ps + cnt);
+    }
+}
+
+inline void rpp_resize_store_pln3(Rpp8s *dstPtrR, Rpp8s *dstPtrG, Rpp8s *dstPtrB, __m128 *p)
+{
+    rpp_store48_f32pln3_to_i8pln3(dstPtrR, dstPtrG, dstPtrB, p);
+}
+
+inline void rpp_resize_store_pkd3(Rpp8u *dstPtr, __m128 *p)
+{
+    rpp_store48_f32pln3_to_u8pkd3(dstPtr, p);
+}
+
+inline void rpp_resize_store_pkd3(Rpp32f *dstPtr, __m128 *p)
+{
+    rpp_store12_f32pln3_to_f32pkd3(dstPtr, p);
+}
+
+inline void rpp_resize_store_pkd3(Rpp16f *dstPtr, __m128 *p)
+{
+    __m128 temp[3];
+    Rpp32f dstPtrTempR_ps[8], dstPtrTempG_ps[8], dstPtrTempB_ps[8];
+    temp[0] = p[0]; /* R channel */
+    temp[1] = p[2]; /* G channel */
+    temp[2] = p[4]; /* B channel */
+    rpp_store12_f32pln3_to_f32pln3(dstPtrTempR_ps, dstPtrTempG_ps, dstPtrTempB_ps, temp);
+    temp[0] = p[1]; /* R channel */
+    temp[1] = p[3]; /* G channel */
+    temp[2] = p[5]; /* B channel */
+    rpp_store12_f32pln3_to_f32pln3(dstPtrTempR_ps + 4, dstPtrTempG_ps + 4, dstPtrTempB_ps + 4, temp);
+
+    for(int cnt = 0; cnt < 8; cnt++)
+    {
+        *dstPtr++ = (Rpp16f) *(dstPtrTempR_ps + cnt);
+        *dstPtr++ = (Rpp16f) *(dstPtrTempG_ps + cnt);
+        *dstPtr++ = (Rpp16f) *(dstPtrTempB_ps + cnt);
+    }
+}
+
+inline void rpp_resize_store_pkd3(Rpp8s *dstPtr, __m128 *p)
+{
+    rpp_store48_f32pln3_to_i8pkd3(dstPtr, p);
+}
+
+inline void rpp_resize_nn_load_u8pkd3(Rpp8u *srcRowPtrsForInterp, Rpp32s *loc, __m128i &p)
+{
+    __m128i px[4];
+    px[0] = __lsx_vld((__m128i *)(srcRowPtrsForInterp + loc[0]), 0);  // LOC0 load [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|R05|G05|B05|R06] - Need RGB 01
+    px[1] = __lsx_vld((__m128i *)(srcRowPtrsForInterp + loc[1]), 0);  // LOC1 load [R11|G11|B11|R12|G12|B12|R13|G13|B13|R14|G14|B14|R15|G15|B15|R16] - Need RGB 11
+    px[2] = __lsx_vld((__m128i *)(srcRowPtrsForInterp + loc[2]), 0);  // LOC2 load [R21|G21|B21|R22|G22|B22|R23|G23|B23|R24|G24|B24|R25|G25|B25|R26] - Need RGB 21
+    px[3] = __lsx_vld((__m128i *)(srcRowPtrsForInterp + loc[3]), 0);  // LOC3 load [R31|G31|B31|R32|G32|B32|R33|G33|B33|R34|G34|B34|R35|G35|B35|R36] - Need RGB 31
+    px[0] = __lsx_vilvl_d(__lsx_vilvl_w(px[3], px[2]), __lsx_vilvl_w(px[1], px[0]));    // Unpack to obtain [R01|G01|B01|R02|R11|G11|B11|R12|R21|G21|B21|R22|R31|G31|B31|R32]
+    p = lsx_shuffle_i8(px[0], xmm_pkd_mask);    // Shuffle to obtain 4 RGB [R01|G01|B01|R11|G11|B11|R21|G21|B21|R31|G31|B31|00|00|00|00]
+}
+
+template<typename T>
+inline void rpp_resize_nn_extract_pkd3_avx(T *srcRowPtrsForInterp, Rpp32s *loc, __m256i &p)
+{
+    p = lasx_setr_i8(*(srcRowPtrsForInterp + loc[0]), *(srcRowPtrsForInterp + loc[0] + 1), *(srcRowPtrsForInterp + loc[0] + 2), 
+                         *(srcRowPtrsForInterp + loc[1]), *(srcRowPtrsForInterp + loc[1] + 1), *(srcRowPtrsForInterp + loc[1] + 2), 
+                         *(srcRowPtrsForInterp + loc[2]), *(srcRowPtrsForInterp + loc[2] + 1), *(srcRowPtrsForInterp + loc[2] + 2),
+                         *(srcRowPtrsForInterp + loc[3]), *(srcRowPtrsForInterp + loc[3] + 1), *(srcRowPtrsForInterp + loc[3] + 2),
+                         *(srcRowPtrsForInterp + loc[4]), *(srcRowPtrsForInterp + loc[4] + 1), *(srcRowPtrsForInterp + loc[4] + 2),
+                         *(srcRowPtrsForInterp + loc[5]), *(srcRowPtrsForInterp + loc[5] + 1), *(srcRowPtrsForInterp + loc[5] + 2),
+                         *(srcRowPtrsForInterp + loc[6]), *(srcRowPtrsForInterp + loc[6] + 1), *(srcRowPtrsForInterp + loc[6] + 2),
+                         *(srcRowPtrsForInterp + loc[7]), *(srcRowPtrsForInterp + loc[7] + 1), *(srcRowPtrsForInterp + loc[7] + 2),
+                         0, 0, 0, 0, 0, 0, 0, 0);
+}
+
+inline void rpp_resize_nn_load_u8pln1(Rpp8u *srcRowPtrsForInterp, Rpp32s *loc, __m128i &p)
+{
+    __m128i px[4];
+    px[0] = __lsx_vld((__m128i *)(srcRowPtrsForInterp + loc[0]), 0);  // LOC0 load [R01|R02|R03|R04|R05|R06...] - Need R01
+    px[1] = __lsx_vld((__m128i *)(srcRowPtrsForInterp + loc[1]), 0);  // LOC1 load [R11|R12|R13|R14|R15|R16...] - Need R11
+    px[2] = __lsx_vld((__m128i *)(srcRowPtrsForInterp + loc[2]), 0);  // LOC2 load [R21|R22|R23|R24|R25|R26...] - Need R21
+    px[3] = __lsx_vld((__m128i *)(srcRowPtrsForInterp + loc[3]), 0);  // LOC3 load [R31|R32|R33|R34|R35|R36...] - Need R31
+    px[0] = __lsx_vilvl_b(px[2], px[0]);    // unpack 8 lo-pixels of px[0] and px[2]
+    px[1] = __lsx_vilvl_b(px[3], px[1]);    // unpack 8 lo-pixels of px[1] and px[3]
+    p = __lsx_vilvl_b(px[1], px[0]);    // unpack to obtain [R01|R11|R21|R31|00|00|00|00|00|00|00|00|00|00|00|00]
+}
+
+template<typename T>
+inline void rpp_resize_nn_extract_pln1_avx(T *srcRowPtrsForInterp, Rpp32s *loc, __m256i &p)
+{
+    p = lasx_setr_i8(*(srcRowPtrsForInterp + loc[0]), *(srcRowPtrsForInterp + loc[1]), 
+                         *(srcRowPtrsForInterp + loc[2]), *(srcRowPtrsForInterp + loc[3]),
+                         *(srcRowPtrsForInterp + loc[4]), *(srcRowPtrsForInterp + loc[5]),
+                         *(srcRowPtrsForInterp + loc[6]), *(srcRowPtrsForInterp + loc[7]),
+                         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
+}
+
+inline void rpp_resize_nn_load_f32pkd3_to_f32pln3(Rpp32f *srcRowPtrsForInterp, Rpp32s *loc, __m128 *p)
+{
+    p[0] = (__m128)__lsx_vld(srcRowPtrsForInterp + loc[0], 0);  // LOC0 load [R01|G01|B01|R02] - Need RGB 01
+    p[1] = (__m128)__lsx_vld(srcRowPtrsForInterp + loc[1], 0);  // LOC1 load [R11|G11|B11|R12] - Need RGB 11
+    p[2] = (__m128)__lsx_vld(srcRowPtrsForInterp + loc[2], 0);  // LOC2 load [R21|G21|B21|R22] - Need RGB 21
+    __m128 pTemp = (__m128)__lsx_vld(srcRowPtrsForInterp + loc[3], 0);  // LOC2 load [R31|G31|B31|R32]  - Need RGB 31
+    _MM_TRANSPOSE4_PS(p[0], p[1], p[2], pTemp); // Transpose to obtain RGB in each vector
+}
+
+inline void rpp_resize_nn_load_f32pkd3_to_f32pln3_avx(Rpp32f *srcRowPtrsForInterp, Rpp32s *loc, __m256 *p)
+{
+    __m128 p128[8];
+    p128[0] = (__m128)__lsx_vld(srcRowPtrsForInterp + loc[0], 0);
+    p128[1] = (__m128)__lsx_vld(srcRowPtrsForInterp + loc[1], 0);
+    p128[2] = (__m128)__lsx_vld(srcRowPtrsForInterp + loc[2], 0);
+    p128[3] = (__m128)__lsx_vld(srcRowPtrsForInterp + loc[3], 0);
+    _MM_TRANSPOSE4_PS(p128[0], p128[1], p128[2], p128[3]);
+    p128[4] = (__m128)__lsx_vld(srcRowPtrsForInterp + loc[4], 0);
+    p128[5] = (__m128)__lsx_vld(srcRowPtrsForInterp + loc[5], 0);
+    p128[6] = (__m128)__lsx_vld(srcRowPtrsForInterp + loc[6], 0);
+    p128[7] = (__m128)__lsx_vld(srcRowPtrsForInterp + loc[7], 0);
+    _MM_TRANSPOSE4_PS(p128[4], p128[5], p128[6], p128[7]);
+    p[0] = lasx_setr_m128(p128[0], p128[4]);
+    p[1] = lasx_setr_m128(p128[1], p128[5]);
+    p[2] = lasx_setr_m128(p128[2], p128[6]);
+}
+
+inline void rpp_resize_nn_load_f16pkd3_to_f32pln3_avx(Rpp16f *srcRowPtrsForInterp, Rpp32s *loc, __m256 *p)
+{
+    p[0] = lasx_setr_f32((Rpp32f)*(srcRowPtrsForInterp + loc[0]), (Rpp32f)*(srcRowPtrsForInterp + loc[1]),
+                          (Rpp32f)*(srcRowPtrsForInterp + loc[2]), (Rpp32f)*(srcRowPtrsForInterp + loc[3]),
+                          (Rpp32f)*(srcRowPtrsForInterp + loc[4]), (Rpp32f)*(srcRowPtrsForInterp + loc[5]),
+                          (Rpp32f)*(srcRowPtrsForInterp + loc[6]), (Rpp32f)*(srcRowPtrsForInterp + loc[7]));
+
+    p[1] = lasx_setr_f32((Rpp32f)*(srcRowPtrsForInterp + loc[0] + 1), (Rpp32f)*(srcRowPtrsForInterp + loc[1] + 1),
+                          (Rpp32f)*(srcRowPtrsForInterp + loc[2] + 1), (Rpp32f)*(srcRowPtrsForInterp + loc[3] + 1),
+                          (Rpp32f)*(srcRowPtrsForInterp + loc[4] + 1), (Rpp32f)*(srcRowPtrsForInterp + loc[5] + 1),
+                          (Rpp32f)*(srcRowPtrsForInterp + loc[6] + 1), (Rpp32f)*(srcRowPtrsForInterp + loc[7] + 1));
+
+    p[2] = lasx_setr_f32((Rpp32f)*(srcRowPtrsForInterp + loc[0] + 2), (Rpp32f)*(srcRowPtrsForInterp + loc[1] + 2),
+                          (Rpp32f)*(srcRowPtrsForInterp + loc[2] + 2), (Rpp32f)*(srcRowPtrsForInterp + loc[3] + 2),
+                          (Rpp32f)*(srcRowPtrsForInterp + loc[4] + 2), (Rpp32f)*(srcRowPtrsForInterp + loc[5] + 2),
+                          (Rpp32f)*(srcRowPtrsForInterp + loc[6] + 2), (Rpp32f)*(srcRowPtrsForInterp + loc[7] + 2));
+}
+
+inline void rpp_generic_nn_load_f16pkd3_to_f32pln3_avx(Rpp16f *srcRowPtrsForInterp, Rpp32s *loc, Rpp32s *invalidLoad, __m256 *p)
+{
+    p[0] = lasx_setr_f32((!invalidLoad[0]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[0]) : 0, (!invalidLoad[1]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[1]) : 0,
+                          (!invalidLoad[2]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[2]) : 0, (!invalidLoad[3]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[3]) : 0,
+                          (!invalidLoad[4]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[4]) : 0, (!invalidLoad[5]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[5]) : 0,
+                          (!invalidLoad[6]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[6]) : 0, (!invalidLoad[7]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[7]) : 0);
+
+    p[1] = lasx_setr_f32((!invalidLoad[0]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[0] + 1) : 0, (!invalidLoad[1]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[1] + 1) : 0,
+                          (!invalidLoad[2]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[2] + 1) : 0, (!invalidLoad[3]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[3] + 1) : 0,
+                          (!invalidLoad[4]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[4] + 1) : 0, (!invalidLoad[5]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[5] + 1) : 0,
+                          (!invalidLoad[6]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[6] + 1) : 0, (!invalidLoad[7]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[7] + 1) : 0);
+
+    p[2] = lasx_setr_f32((!invalidLoad[0]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[0] + 2) : 0, (!invalidLoad[1]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[1] + 2) : 0,
+                          (!invalidLoad[2]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[2] + 2) : 0, (!invalidLoad[3]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[3] + 2) : 0,
+                          (!invalidLoad[4]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[4] + 2) : 0, (!invalidLoad[5]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[5] + 2) : 0,
+                          (!invalidLoad[6]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[6] + 2) : 0, (!invalidLoad[7]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[7] + 2) : 0);
+}
+
+inline void rpp_generic_nn_load_f16pkd3_to_f32pkd3_avx(Rpp16f *srcRowPtrsForInterp, Rpp32s *loc, Rpp32s *invalidLoad, __m256 *p)
+{
+    p[0] = lasx_setr_f32((!invalidLoad[0]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[0]): 0, (!invalidLoad[0]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[0] + 1): 0,        // Get R01|G01|B01|R02|B02|G02|R03|G03
+                          (!invalidLoad[0]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[0] + 2): 0, (!invalidLoad[1]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[1]): 0,        // load the values from input using srcLoc buffer if invalidLoad is 0, else set the values to 0
+                          (!invalidLoad[1]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[1] + 1): 0, (!invalidLoad[1]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[1] + 2): 0,
+                          (!invalidLoad[2]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[2]): 0, (!invalidLoad[2]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[2] + 1): 0);
+    p[1] = lasx_setr_f32((!invalidLoad[2]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[2] + 2): 0, (!invalidLoad[3]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[3]): 0,        // Get B03|R04|G04|B04|R05|G05|B05|R06
+                          (!invalidLoad[3]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[3] + 1): 0, (!invalidLoad[3]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[3] + 2): 0,    // load the values from input using srcLoc buffer if invalidLoad is 0, else set the values to 0
+                          (!invalidLoad[4]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[4]): 0, (!invalidLoad[4]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[4] + 1): 0,
+                          (!invalidLoad[4]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[4] + 2): 0, (!invalidLoad[5]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[5]): 0);
+    p[2] = lasx_setr_f32((!invalidLoad[5]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[5] + 1): 0, (!invalidLoad[5]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[5] + 2): 0,    // Get G06|B06|R07|G07|B07|R08|G08|B08
+                          (!invalidLoad[6]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[6]): 0, (!invalidLoad[6]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[6] + 1): 0,        // load the values from input using srcLoc buffer if invalidLoad is 0, else set the values to 0
+                          (!invalidLoad[6]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[6] + 2): 0, (!invalidLoad[7]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[7]): 0,
+                          (!invalidLoad[7]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[7] + 1): 0, (!invalidLoad[7]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[7] + 2): 0);
+}
+
+inline void rpp_resize_nn_load_f32pln1(Rpp32f *srcRowPtrsForInterp, Rpp32s *loc, __m128 &p)
+{
+    __m128 pTemp[4];
+    pTemp[0] = (__m128)__lsx_vld(srcRowPtrsForInterp + loc[0], 0);  // LOC0 load [R01|R02|R03|R04] - Need R01
+    pTemp[1] = (__m128)__lsx_vld(srcRowPtrsForInterp + loc[1], 0);  // LOC1 load [R11|R12|R13|R14] - Need R11
+    pTemp[2] = (__m128)__lsx_vld(srcRowPtrsForInterp + loc[2], 0);  // LOC2 load [R21|R22|R23|R24] - Need R21
+    pTemp[3] = (__m128)__lsx_vld(srcRowPtrsForInterp + loc[3], 0);  // LOC3 load [R31|R32|R33|R34] - Need R31
+    pTemp[0] = (__m128)__lsx_vilvl_w((__m128i)pTemp[2], (__m128i)pTemp[0]);
+    pTemp[1] = (__m128)__lsx_vilvl_w((__m128i)pTemp[3], (__m128i)pTemp[1]);
+    p = (__m128)__lsx_vilvl_w((__m128i)pTemp[1], (__m128i)pTemp[0]);    // Unpack to obtain [R01|R11|R21|R31]
+}
+
+inline void rpp_resize_nn_load_f32pln1_avx(Rpp32f *srcRowPtrsForInterp, Rpp32s *loc, __m256 &p)
+{
+    p = lasx_setr_f32(*(srcRowPtrsForInterp + loc[0]), *(srcRowPtrsForInterp + loc[1]),
+                       *(srcRowPtrsForInterp + loc[2]), *(srcRowPtrsForInterp + loc[3]),
+                       *(srcRowPtrsForInterp + loc[4]), *(srcRowPtrsForInterp + loc[5]),
+                       *(srcRowPtrsForInterp + loc[6]), *(srcRowPtrsForInterp + loc[7]));
+}
+
+inline void rpp_resize_nn_load_f16pln1_avx(Rpp16f *srcRowPtrsForInterp, Rpp32s *loc, __m256 &p)
+{
+    p = lasx_setr_f32((Rpp32f)*(srcRowPtrsForInterp + loc[0]), (Rpp32f)*(srcRowPtrsForInterp + loc[1]),
+                       (Rpp32f)*(srcRowPtrsForInterp + loc[2]), (Rpp32f)*(srcRowPtrsForInterp + loc[3]),
+                       (Rpp32f)*(srcRowPtrsForInterp + loc[4]), (Rpp32f)*(srcRowPtrsForInterp + loc[5]),
+                       (Rpp32f)*(srcRowPtrsForInterp + loc[6]), (Rpp32f)*(srcRowPtrsForInterp + loc[7]));
+}
+
+inline void rpp_generic_nn_load_f16pln1_avx(Rpp16f *srcRowPtrsForInterp, Rpp32s *loc, Rpp32s *invalidLoad, __m256 &p)
+{
+    p = lasx_setr_f32((!invalidLoad[0]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[0]) : 0, (!invalidLoad[1]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[1]) : 0,
+                       (!invalidLoad[2]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[2]) : 0, (!invalidLoad[3]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[3]) : 0,
+                        (!invalidLoad[4]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[4]) : 0, (!invalidLoad[5]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[5]) : 0,
+                        (!invalidLoad[6]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[6]) : 0, (!invalidLoad[7]) ? (Rpp32f)*(srcRowPtrsForInterp + loc[7]) : 0);
+}
+
+inline void rpp_resize_nn_load_i8pkd3(Rpp8s *srcRowPtrsForInterp, Rpp32s *loc, __m128i &p)
+{
+    __m128i px[4];
+    px[0] = __lsx_vld((__m128i *)(srcRowPtrsForInterp + loc[0]), 0);  // LOC0 load [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|R05|G05|B05|R06] - Need RGB 01
+    px[1] = __lsx_vld((__m128i *)(srcRowPtrsForInterp + loc[1]), 0);  // LOC1 load [R11|G11|B11|R12|G12|B12|R13|G13|B13|R14|G14|B14|R15|G15|B15|R16] - Need RGB 11
+    px[2] = __lsx_vld((__m128i *)(srcRowPtrsForInterp + loc[2]), 0);  // LOC2 load [R21|G21|B21|R22|G22|B22|R23|G23|B23|R24|G24|B24|R25|G25|B25|R26] - Need RGB 21
+    px[3] = __lsx_vld((__m128i *)(srcRowPtrsForInterp + loc[3]), 0);  // LOC3 load [R31|G31|B31|R32|G32|B32|R33|G33|B33|R34|G34|B34|R35|G35|B35|R36] - Need RGB 31
+    px[0] = __lsx_vilvl_d(__lsx_vilvl_w(px[3], px[2]), __lsx_vilvl_w(px[1], px[0]));    // Unpack to obtain [R01|G01|B01|R02|R11|G11|B11|R12|R21|G21|B21|R22|R31|G31|B31|R32]
+    p = lsx_shuffle_i8(px[0], xmm_pkd_mask);    // Shuffle to obtain 4 RGB [R01|G01|B01|R11|G11|B11|R21|G21|B21|R31|G31|B31|00|00|00|00]
+}
+
+inline void rpp_resize_nn_load_i8pln1(Rpp8s *srcRowPtrsForInterp, Rpp32s *loc, __m128i &p)
+{
+    __m128i px[4];
+    px[0] = __lsx_vld((__m128i *)(srcRowPtrsForInterp + loc[0]), 0);  // LOC0 load [R01|R02|R03|R04|R05|R06...] - Need R01
+    px[1] = __lsx_vld((__m128i *)(srcRowPtrsForInterp + loc[1]), 0);  // LOC1 load [R11|R12|R13|R14|R15|R16...] - Need R11
+    px[2] = __lsx_vld((__m128i *)(srcRowPtrsForInterp + loc[2]), 0);  // LOC2 load [R21|R22|R23|R24|R25|R26...] - Need R21
+    px[3] = __lsx_vld((__m128i *)(srcRowPtrsForInterp + loc[3]), 0);  // LOC3 load [R31|R32|R33|R34|R35|R36...] - Need R31
+    px[0] = __lsx_vilvl_b(px[2], px[0]);    // unpack 8 lo-pixels of px[0] and px[2]
+    px[1] = __lsx_vilvl_b(px[3], px[1]);    // unpack 8 lo-pixels of px[1] and px[3]
+    p = __lsx_vilvl_b(px[1], px[0]);    // unpack to obtain [R01|R11|R21|R31|00|00|00|00|00|00|00|00|00|00|00|00]
+}
+
+inline void rpp_store12_u8pkd3_to_u8pln3(Rpp8u* dstPtrR, Rpp8u* dstPtrG, Rpp8u* dstPtrB, __m128i &p)
+{
+    rpp_storeu_si32((__m128i *)(dstPtrR), lsx_shuffle_i8(p, xmm_char_maskR)); /* Shuffle and extract the R pixels*/
+    rpp_storeu_si32((__m128i *)(dstPtrG), lsx_shuffle_i8(p, xmm_char_maskG)); /* Shuffle and extract the G pixels*/
+    rpp_storeu_si32((__m128i *)(dstPtrB), lsx_shuffle_i8(p, xmm_char_maskB)); /* Shuffle and extract the B pixels*/
+}
+
+inline void rpp_store24_u8pkd3_to_u8pln3_avx(Rpp8u* dstPtrR, Rpp8u* dstPtrG, Rpp8u* dstPtrB, __m256i &p)
+{
+    __m128i p128[2];
+    p128[0] = __lasx_cvt_256_128(p); /* R01|G01|B01|R11|G11|B11|R21|G21|B21|R31|G31|B31|R41|G41|B41|R51 */
+    rpp_storeu_si32((__m128i *)(dstPtrR), lsx_shuffle_i8(p128[0], xmm_char_maskR)); /* shuffle to get R01-R04*/
+    rpp_storeu_si32((__m128i *)(dstPtrG), lsx_shuffle_i8(p128[0], xmm_char_maskG)); /* shuffle to get G01-G04*/
+    rpp_storeu_si32((__m128i *)(dstPtrB), lsx_shuffle_i8(p128[0], xmm_char_maskB)); /* shuffle to get B01-B04*/
+
+    p128[1] = lasx_extractf128_m256i(p, 1); /* G51|B51|R61|G61|B61|R71|G71|B71|00|00|00|00|00|00|00|00 */
+    const __m128i shuffleMask = lsx_setr_i8(12, 13, 14, 15, 0, 1, 2, 3, 4, 5, 6, 7, 0x80, 0x80, 0x80, 0x80);
+    p128[0] = __lsx_vilvh_d(xmm_px0, p128[0]); /* B21|R31|G31|B31|R41|G41|B41|R51|00|00|00|00|00|00|00|00 */
+    p128[1] = __lsx_vilvl_d(p128[0], p128[1]); /* G51|B51|R61|G61|B61|R71|G71|B71|B21|R31|G31|B31|R41|G41|B41|R51 */
+    p128[1] = lsx_shuffle_i8(p128[1], shuffleMask); /* R41|G41|B41|R51|G51|B51|R61|G61|B61|R71|G71|B71|00|00|00|00 */
+    rpp_storeu_si32((__m128i *)(dstPtrR + 4), lsx_shuffle_i8(p128[1], xmm_char_maskR)); /* shuffle to get R05-R08*/
+    rpp_storeu_si32((__m128i *)(dstPtrG + 4), lsx_shuffle_i8(p128[1], xmm_char_maskG)); /* shuffle to get G05-G08*/
+    rpp_storeu_si32((__m128i *)(dstPtrB + 4), lsx_shuffle_i8(p128[1], xmm_char_maskB)); /* shuffle to get B05-B08*/
+}
+
+inline void rpp_store24_i8pkd3_to_i8pln3_avx(Rpp8s* dstPtrR, Rpp8s* dstPtrG, Rpp8s* dstPtrB, __m256i &p)
+{
+    __m128i p128[2];
+    p128[0] = __lasx_cvt_256_128(p); /* R01|G01|B01|R11|G11|B11|R21|G21|B21|R31|G31|B31|R41|G41|B41|R51 */
+    rpp_storeu_si32((__m128i *)(dstPtrR), lsx_shuffle_i8(p128[0], xmm_char_maskR)); /* shuffle to get R01-R04*/
+    rpp_storeu_si32((__m128i *)(dstPtrG), lsx_shuffle_i8(p128[0], xmm_char_maskG)); /* shuffle to get G01-G04*/
+    rpp_storeu_si32((__m128i *)(dstPtrB), lsx_shuffle_i8(p128[0], xmm_char_maskB)); /* shuffle to get B01-B04*/
+
+    p128[1] = lasx_extractf128_m256i(p, 1); /* G51|B51|R61|G61|B61|R71|G71|B71|00|00|00|00|00|00|00|00 */
+    const __m128i shuffleMask = lsx_setr_i8(12, 13, 14, 15, 0, 1, 2, 3, 4, 5, 6, 7, 0x80, 0x80, 0x80, 0x80);
+    p128[0] = __lsx_vilvh_d(xmm_px0, p128[0]); /* B21|R31|G31|B31|R41|G41|B41|R51|00|00|00|00|00|00|00|00 */
+    p128[1] = __lsx_vilvl_d(p128[0], p128[1]); /* G51|B51|R61|G61|B61|R71|G71|B71|B21|R31|G31|B31|R41|G41|B41|R51 */
+    p128[1] = lsx_shuffle_i8(p128[1], shuffleMask); /* R41|G41|B41|R51|G51|B51|R61|G61|B61|R71|G71|B71|00|00|00|00 */
+    rpp_storeu_si32((__m128i *)(dstPtrR + 4), lsx_shuffle_i8(p128[1], xmm_char_maskR)); /* shuffle to get R05-R08*/
+    rpp_storeu_si32((__m128i *)(dstPtrG + 4), lsx_shuffle_i8(p128[1], xmm_char_maskG)); /* shuffle to get G05-G08*/
+    rpp_storeu_si32((__m128i *)(dstPtrB + 4), lsx_shuffle_i8(p128[1], xmm_char_maskB)); /* shuffle to get B05-B08*/
+}
+
+inline void rpp_store12_u8_to_u8(Rpp8u* dstPtr, __m128i &p)
+{
+    __lsx_vst(p, (__m128i *)(dstPtr), 0);
+}
+
+inline void rpp_store4_u8pln1_to_u8pln1(Rpp8u* dstPtr, __m128i &p)
+{
+    rpp_storeu_si32((__m128i *)(dstPtr), p);
+}
+
+inline void rpp_store4_i8pln1_to_i8pln1(Rpp8s* dstPtr, __m128i &p)
+{
+    rpp_storeu_si32((__m128i *)(dstPtr), p);
+}
+
+inline void rpp_store24_u8_to_u8_avx(Rpp8u* dstPtr, __m256i &p)
+{
+    __lasx_xvst(p, (__m256i *)(dstPtr), 0);
+}
+
+inline void rpp_store24_i8_to_i8_avx(Rpp8s* dstPtr, __m256i &p)
+{
+    __lasx_xvst(p, (__m256i *)(dstPtr), 0);
+}
+
+inline void rpp_store12_u8pln3_to_u8pkd3(Rpp8u* dstPtr, __m128i *p)
+{
+    __m128i px[4];
+    px[0] = __lsx_vilvl_b(p[1], p[0]);
+    px[1] = __lsx_vilvl_d(px[2], p[0]);
+    __lsx_vst(lsx_shuffle_i8(px[1], xmm_store4_pkd_pixels), (__m128i *)(dstPtr), 0);
+}
+
+inline void rpp_store24_u8pln3_to_u8pkd3_avx(Rpp8u* dstPtr, __m256i *p)
+{
+    __m128i px[5];
+    px[0] = __lasx_cvt_256_128(p[0]);            /* R01|R11|R21|R31|R41|R51|R61|R71|00|00|00|00|00|00|00|00] */
+    px[1] = __lasx_cvt_256_128(p[1]);            /* G01|G11|G21|G31|G41|G51|G61|G71|00|00|00|00|00|00|00|00] */
+    px[2] = __lasx_cvt_256_128(p[2]);            /* B01|B11|B21|B31|B41|B51|B61|B71|00|00|00|00|00|00|00|00] */
+
+    px[3] = __lsx_vilvl_b(px[1], px[0]);         /* unpack as R01|G01|R11|G11|R21|G21|R31|G31|R41|G41|R51|G51|R61|G61|R71|G71 */
+    px[4] = __lsx_vilvl_d(px[2], px[3]);        /* unpack as R01|G01|R11|G11|R21|G21|R31|G31|B01|B11|B21|B31|B41|B51|B61|B71 */
+    __lsx_vst(lsx_shuffle_i8(px[4], xmm_store4_pkd_pixels), (__m128i *)(dstPtr), 0); /* shuffle to get RGB 00-03 */
+
+    const __m128i xmm_shuffle_mask = lsx_setr_i8(0, 1, 12, 2, 3, 13, 4, 5, 14, 6, 7, 15, 0x80, 0x80, 0x80, 0x80);
+    px[4] = __lsx_vilvh_d(px[4], px[3]);        /* unpack as R41|G41|R51|G51|R61|G61|R71|G71|B01|B11|B21|B31|B41|B51|B61|B71] */
+    __lsx_vst(lsx_shuffle_i8(px[4], xmm_shuffle_mask), (__m128i *)(dstPtr + 12), 0); /* shuffle to get RGB 04-07 */
+}
+
+inline void rpp_store12_i8pkd3_to_i8pln3(Rpp8s* dstPtrR, Rpp8s* dstPtrG, Rpp8s* dstPtrB, __m128i &p)
+{
+    rpp_storeu_si32((__m128i *)(dstPtrR), lsx_shuffle_i8(p, xmm_char_maskR)); /* Shuffle and extract the R pixels*/
+    rpp_storeu_si32((__m128i *)(dstPtrG), lsx_shuffle_i8(p, xmm_char_maskG)); /* Shuffle and extract the G pixels*/
+    rpp_storeu_si32((__m128i *)(dstPtrB), lsx_shuffle_i8(p, xmm_char_maskB)); /* Shuffle and extract the B pixels*/
+}
+
+inline void rpp_store12_i8_to_i8(Rpp8s* dstPtr, __m128i &p)
+{
+    __lsx_vst(p, (__m128i *)(dstPtr), 0);
+}
+
+inline void rpp_store12_i8pln3_to_i8pkd3(Rpp8s* dstPtr, __m128i *p)
+{
+    __m128i px[4];
+    px[0] = __lsx_vilvl_b(p[1], p[0]);
+    px[1] = __lsx_vilvl_d(px[2], p[0]);
+    __lsx_vst(lsx_shuffle_i8(px[1], xmm_store4_pkd_pixels), (__m128i *)(dstPtr), 0);
+}
+
+inline void rpp_store24_i8pln3_to_i8pkd3_avx(Rpp8s* dstPtr, __m256i *p)
+{
+    __m128i px[5];
+    px[0] = __lasx_cvt_256_128(p[0]);            // [R01|R11|R21|R31|R41|R51|R61|R71|00|00|00|00|00|00|00|00]
+    px[1] = __lasx_cvt_256_128(p[1]);            // [G01|G11|G21|G31|G41|G51|G61|G71|00|00|00|00|00|00|00|00]
+    px[2] = __lasx_cvt_256_128(p[2]);            // [B01|B11|B21|B31|B41|B51|B61|B71|00|00|00|00|00|00|00|00]
+
+    px[3] = __lsx_vilvl_b(px[1], px[0]);         // [R01|G01|R11|G11|R21|G21|R31|G31|R41|G41|R51|G51|R61|G61|R71|G71]
+    px[4] = __lsx_vilvl_d(px[2], px[3]);        // [R01|G01|R11|G11|R21|G21|R31|G31|B01|B11|B21|B31|B41|B51|B61|B71]
+    __lsx_vst(lsx_shuffle_i8(px[4], xmm_store4_pkd_pixels), (__m128i *)(dstPtr), 0); // shuffle to get RGB 00-03
+
+    const __m128i xmm_shuffle_mask = lsx_setr_i8(0, 1, 12, 2, 3, 13, 4, 5, 14, 6, 7, 15, 0x80, 0x80, 0x80, 0x80);
+    px[4] = __lsx_vilvh_d(px[4], px[3]);        // [R41|G41|R51|G51|R61|G61|R71|G71|B01|B11|B21|B31|B41|B51|B61|B71]
+    __lsx_vst(lsx_shuffle_i8(px[4], xmm_shuffle_mask), (__m128i *)(dstPtr + 12), 0); // shuffle to get RGB 04-07
+}
+
+inline void rpp_store12_f32pkd3_to_f32pkd3(Rpp32f* dstPtr, __m128 *p)
+{
+    __lsx_vst(p[0], dstPtr, 0); /* Store RGB set 1 */
+    __lsx_vst(p[1], dstPtr + 3, 0); /* Store RGB set 2 */
+    __lsx_vst(p[2], dstPtr + 6, 0); /* Store RGB set 3 */
+    __lsx_vst(p[3], dstPtr + 9, 0); /* Store RGB set 4 */
+}
+
+inline void rpp_store24_f32pkd3_to_f32pkd3_avx(Rpp32f* dstPtr, __m256 *p)
+{
+    __lasx_xvst((__m256i)p[0], dstPtr, 0); /* Store RGB set 1 */
+    __lasx_xvst((__m256i)p[1], dstPtr + 8, 0); /* Store RGB set 2 */
+    __lasx_xvst((__m256i)p[2], dstPtr + 16, 0); /* Store RGB set 3 */
+}
+
+inline void rpp_store24_f32pkd3_to_f16pkd3_avx(Rpp16f* dstPtr, __m256* p)
+{
+    __m128i px128[3];
+    px128[0] = lasx_cvtf32_ph(p[0], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    px128[1] = lasx_cvtf32_ph(p[1], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    px128[2] = lasx_cvtf32_ph(p[2], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    __lsx_vst(px128[0], (__m128i *)dstPtr, 0);
+    __lsx_vst(px128[1], (__m128i *)(dstPtr + 8), 0);
+    __lsx_vst(px128[2], (__m128i *)(dstPtr + 16), 0);
+}
+
+inline void rpp_convert24_pkd3_to_pln3(__m128i &pxLower, __m128i &pxUpper, __m128i *pxDstChn)
+{
+    // pxLower = R1 G1 B1 R2 G2 B2 R3 G3 B3 R4 G4 B4 R5 G5 B5 R6
+    // pxUpper = G6 B6 R7 G7 B7 R8 G8 B8 0  0  0  0  0  0  0  0
+    // shuffle1 - R1 R2 R3 R4 0 0 0 0 0 0 0 0 0 0 0 0
+    // shuffle2 - G1 G2 G3 G4 0 0 0 0 0 0 0 0 0 0 0 0
+    // shuffle3 - B1 B2 B3 B4 0 0 0 0 0 0 0 0 0 0 0 0
+    // blend    - G6 B6 R7 G7 B7 R8 G8 B8 0 0 0 0 R5 G5 B5 R6
+    // R5 R6 R7 R8 G5 G6 G7 G8 B5 B6 B7 B8 0 0 0 0
+    // R1 R2 R3 R4 R5 R6 R7 R8 0  0  0  0  0  0  0  0
+    // G1 G2 G3 G4 G5 G6 G7 G8 0  0  0  0  0  0  0  0
+    // B1 B2 B3 B4 0  0  0  0  B5 B6 B7 B8 0  0  0  0
+    // B1 B2 B3 B4 B5 B6 B7 B8 0  0  0  0  0  0  0  0
+
+    __m128i pxTempUpper = lsx_blend_i16(pxUpper, pxLower, 192);
+    __m128i xmm_shuffle_mask = lsx_setr_i8(12, 15, 2, 5, 13, 0, 3, 6, 14, 1, 4, 7, 0x80, 0x80, 0x80, 0x80);
+    pxTempUpper = lsx_shuffle_i8(pxTempUpper, xmm_shuffle_mask);
+
+    pxDstChn[0] = __lsx_vilvl_w(pxTempUpper, lsx_shuffle_i8(pxLower, xmm_char_maskR));
+    pxDstChn[1] = lsx_blend_i16(lsx_shuffle_i8(pxLower, xmm_char_maskG), pxTempUpper, 12);
+
+    xmm_shuffle_mask = lsx_setr_i8(0, 1, 2, 3, 8, 9, 10, 11, 0x80, 0x80, 0x80, 0x80,0x80, 0x80, 0x80, 0x80);
+    pxDstChn[2] = lsx_shuffle_i8(lsx_blend_i16(lsx_shuffle_i8(pxLower, xmm_char_maskB), pxTempUpper, 48), xmm_shuffle_mask);
+}
+
+inline void rpp_convert72_pln3_to_pkd3(__m256i *pxSrc, __m128i *pxDst)
+{
+    const __m128i pxMask = lsx_setr_i8(0, 1, 12, 2, 3, 13, 4, 5, 14, 6, 7, 15, 0x80, 0x80, 0x80, 0x80);
+
+    __m256i px[2];
+    px[0] = __lasx_xvilvl_b(pxSrc[1], pxSrc[0]);
+    px[1] = __lasx_xvilvh_b(pxSrc[1], pxSrc[0]);
+
+    __m128i pxTemp[4];
+    // RGB 1-8
+    pxTemp[0] = __lasx_cvt_256_128(px[0]);
+    pxTemp[1] = __lasx_cvt_256_128(pxSrc[2]);
+
+    // RGB 1-4, shuffle to get correct order
+    // RGB 5-8, shuffle to get correct order
+    pxTemp[2] = __lsx_vilvl_d(pxTemp[1], pxTemp[0]);
+    pxTemp[3] = __lsx_vilvl_d(pxTemp[1], lsx_srli_m128i(pxTemp[0], 8));
+    pxDst[0] = lsx_shuffle_i8(pxTemp[2], xmm_store4_pkd_pixels);
+    pxDst[1] = lsx_shuffle_i8(pxTemp[3], pxMask);
+
+    // RGB 9-16
+    pxTemp[0] = __lasx_cvt_256_128(px[1]),
+    pxTemp[1] = __lasx_cvt_256_128(pxSrc[2]);
+
+    // RGB 9-12, shuffle to get correct order
+    // RGB 13-15, shuffle to get correct order
+    pxTemp[2] =  __lsx_vilvl_d(lsx_srli_m128i(pxTemp[1], 8), pxTemp[0]);
+    pxTemp[3] = __lsx_vilvh_d(pxTemp[1], pxTemp[0]);
+    pxDst[2] = lsx_shuffle_i8(pxTemp[2], xmm_store4_pkd_pixels);
+    pxDst[3] = lsx_shuffle_i8(pxTemp[3], pxMask);
+
+    // RGB 17-24
+    pxTemp[0] = lasx_extracti128_m256i(px[0], 1),
+    pxTemp[1] = lasx_extracti128_m256i(pxSrc[2], 1);
+
+    // RGB 17-20, shuffle to get correct order
+    // RGB 21-24, shuffle to get correct order
+    pxTemp[2] = __lsx_vilvl_d(pxTemp[1], pxTemp[0]);
+    pxTemp[3] = __lsx_vilvl_d(pxTemp[1], lsx_srli_m128i(pxTemp[0], 8));
+    pxDst[4] = lsx_shuffle_i8(pxTemp[2], xmm_store4_pkd_pixels);
+    pxDst[5] = lsx_shuffle_i8(pxTemp[3], pxMask);
+}
+
+inline void rpp_convert48_pln3_to_pkd3(__m128i *pxSrc, __m128i *pxDst)
+{
+    const __m128i pxMask = lsx_setr_i8(0, 1, 12, 2, 3, 13, 4, 5, 14, 6, 7, 15, 0x80, 0x80, 0x80, 0x80);
+
+    __m128i pxTemp[3];
+    pxTemp[0] = __lsx_vilvl_b(pxSrc[1], pxSrc[0]);
+
+    // RGB 1-4, shuffle to get correct order
+    // RGB 5-8, shuffle to get correct order
+    pxTemp[1] = __lsx_vilvl_d(pxSrc[2], pxTemp[0]);
+    pxTemp[2] = __lsx_vilvl_d(pxSrc[2], lsx_srli_m128i(pxTemp[0], 8));
+    pxDst[0] = lsx_shuffle_i8(pxTemp[1], xmm_store4_pkd_pixels);
+    pxDst[1] = lsx_shuffle_i8(pxTemp[2], pxMask);
+
+    pxTemp[0] = __lsx_vilvh_b(pxSrc[1], pxSrc[0]);
+
+    // RGB 9-12, shuffle to get correct order
+    // RGB 13-16, shuffle to get correct order
+    pxTemp[1] = __lsx_vilvh_d(pxSrc[2], lsx_slli_m128i(pxTemp[0], 8));
+    pxTemp[2] = __lsx_vilvh_d(pxSrc[2], pxTemp[0]);
+    pxDst[2] = lsx_shuffle_i8(pxTemp[1], xmm_store4_pkd_pixels);
+    pxDst[3] = lsx_shuffle_i8(pxTemp[2], pxMask);
+}
+
+inline void rpp_convert12_f32pkd3_to_f32pln3(__m256 *pSrc, __m128 *pDst)
+{
+    __m128 pSrcPkd[3], pTemp;
+    pSrcPkd[0] = (__m128)__lasx_cvt_256_128((__m256i)pSrc[0]);
+    pSrcPkd[1] = lasx_extractf128_f32(pSrc[0], 1);
+    pSrcPkd[2] = (__m128)__lasx_cvt_256_128((__m256i)pSrc[1]);
+
+    pTemp = lsx_blend_f32(pSrcPkd[0], pSrcPkd[1], 4);
+    pTemp = lsx_blend_f32(pTemp, pSrcPkd[2], 2);
+    pDst[0] = (__m128)__lsx_vpermi_w((__m128i)pTemp, (__m128i)pTemp, 108);
+
+    pTemp = lsx_blend_f32(pSrcPkd[0], pSrcPkd[1], 9);
+    pTemp = lsx_blend_f32(pTemp, pSrcPkd[2], 4);
+    pDst[1] = (__m128)__lsx_vpermi_w((__m128i)pTemp, (__m128i)pTemp, 177);
+
+    pTemp = lsx_blend_f32(pSrcPkd[0], pSrcPkd[1], 2);
+    pTemp = lsx_blend_f32(pTemp, pSrcPkd[2], 9);
+    pDst[2] = (__m128)__lsx_vpermi_w((__m128i)pTemp, (__m128i)pTemp, 198);
+}
+
+inline void rpp_store16_float(Rpp32f *dstPtrTemp, __m256 *pDst)
+{
+    __lasx_xvst((__m256i)pDst[0], dstPtrTemp, 0);
+    __lasx_xvst((__m256i)pDst[1], dstPtrTemp + 8, 0);
+}
+
+inline void rpp_store16_float(Rpp16f *dstPtrTemp, __m256 *pDst)
+{
+    __m128i pxDst[2];
+    pxDst[0] = lasx_cvtf32_ph(pDst[0], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    pxDst[1] = lasx_cvtf32_ph(pDst[1], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    __lsx_vst(pxDst[0], (__m128i *)dstPtrTemp, 0);
+    __lsx_vst(pxDst[1], (__m128i *)(dstPtrTemp + 8), 0);
+}
+
+inline void rpp_load16_f16_to_f32_avx(Rpp16f *srcPtr, __m256 *p)
+{
+    p[0] = lasx_cvtph_f32((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtr), 0)));
+    p[1] = lasx_cvtph_f32((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtr + 8), 0)));
+}
+
+inline void rpp_load24_f16_to_f32_avx(Rpp16f *srcPtr, __m256 *p)
+{
+    p[0] = lasx_cvtph_f32((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtr), 0)));
+    p[1] = lasx_cvtph_f32((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtr + 8), 0)));
+    p[2] = lasx_cvtph_f32((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtr + 16), 0)));
+}
+
+inline void rpp_load32_f16_to_f32_avx(Rpp16f *srcPtr, __m256 *p)
+{
+    p[0] = lasx_cvtph_f32((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtr), 0)));
+    p[1] = lasx_cvtph_f32((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtr + 8), 0)));
+    p[2] = lasx_cvtph_f32((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtr + 16), 0)));
+    p[3] = lasx_cvtph_f32((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtr + 24), 0)));
+}
+
+inline void rpp_load40_f16_to_f32_avx(Rpp16f *srcPtr, __m256 *p)
+{
+    p[0] = lasx_cvtph_f32((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtr), 0)));
+    p[1] = lasx_cvtph_f32((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtr + 8), 0)));
+    p[2] = lasx_cvtph_f32((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtr + 16), 0)));
+    p[3] = lasx_cvtph_f32((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtr + 24), 0)));
+    p[4] = lasx_cvtph_f32((__m128i)((__m128)__lsx_vld(reinterpret_cast<Rpp32f *>(srcPtr + 32), 0)));
+}
+
+inline void rpp_store12_float_pkd_pln(Rpp32f **dstPtrTempChannels, __m128 *pDst)
+{
+    __lsx_vst(pDst[0], dstPtrTempChannels[0], 0);
+    __lsx_vst(pDst[1], dstPtrTempChannels[1], 0);
+    __lsx_vst(pDst[2], dstPtrTempChannels[2], 0);
+}
+
+inline void rpp_store12_float_pkd_pln(Rpp16f **dstPtrTempChannels, __m128 *pDst)
+{
+    __m128i pxDst[3];
+    pxDst[0] = __lsx_vfcvtrxxx_h_s(pDst[0], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    pxDst[1] = __lsx_vfcvtrxxx_h_s(pDst[1], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    pxDst[2] = __lsx_vfcvtrxxx_h_s(pDst[2], _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
+    __lsx_vst(pxDst[0], (__m128i *)(dstPtrTempChannels[0]), 0);
+    __lsx_vst(pxDst[1], (__m128i *)(dstPtrTempChannels[1]), 0);
+    __lsx_vst(pxDst[2], (__m128i *)(dstPtrTempChannels[2]), 0);
+}
+
+inline void rpp_store12_float_pkd_pln(Rpp8u **dstPtrTempChannels, __m128 *pDst)
+{
+    __m128i px[4];
+    for(int i = 0; i < 3; i++)
+    {
+        px[0] = __lsx_vftint_w_s(pDst[i]);    /* pixels 0-3 */
+        px[1] = __lsx_vftint_w_s(xmm_p0);    /* pixels 4-7 */
+        px[2] = __lsx_vftint_w_s(xmm_p0);    /* pixels 8-11 */
+        px[3] = __lsx_vftint_w_s(xmm_p0);    /* pixels 12-15 */
+        px[0] = lsx_packus_i32(px[0], px[1]);    /* pixels 0-7 */
+        px[1] = lsx_packus_i32(px[2], px[3]);    /* pixels 8-15 */
+        px[0] = lsx_packus_i16(px[0], px[1]);    /* pixels 0-15 */
+        __lsx_vstelm_w(px[0], (__m128i *)dstPtrTempChannels[i], 0, 0);    /* store pixels 0-15 */
+    }
+}
+
+inline void rpp_store12_float_pkd_pln(Rpp8s **dstPtrTempChannels, __m128 *pDst)
+{
+    __m128i px[4];
+    for(int i = 0; i < 3; i++)
+    {
+        px[0] = __lsx_vftint_w_s(pDst[i]);    /* pixels 0-3 */
+        px[1] = __lsx_vftint_w_s(xmm_p0);    /* pixels 4-7 */
+        px[2] = __lsx_vftint_w_s(xmm_p0);    /* pixels 8-11 */
+        px[3] = __lsx_vftint_w_s(xmm_p0);    /* pixels 12-15 */
+        px[0] = lsx_packus_i32(px[0], px[1]);    /* pixels 0-7 */
+        px[1] = lsx_packus_i32(px[2], px[3]);    /* pixels 8-15 */
+        px[0] = lsx_packus_i16(px[0], px[1]);    /* pixels 0-15 */
+        px[0] = __lsx_vsub_b(px[0], xmm_pxConvertI8);    /* convert back to i8 for px0 store */
+        __lsx_vstelm_w(px[0], (__m128i *)dstPtrTempChannels[i], 0, 0);    /* store pixels 0-15 */
+    }
+}
+
+inline void rpp_store8_f32_to_u8_avx(Rpp8u *dstPtrTemp, __m256 pDst)
+{
+    __m256i px1 = __lasx_xvftint_w_s(pDst);
+    // Pack int32 values to uint16
+    __m128i px2 = lsx_packus_i32(__lasx_cvt_256_128(px1), lasx_extracti128_m256i(px1, 1));
+    // Pack uint16 values to uint8
+    __m128i px3 = lsx_packus_i16(px2, __lsx_vldi(0));
+    // Store the result to dst
+    __lsx_vstelm_d(px3, (__m128i*)dstPtrTemp, 0, 0);
+}
+
+inline void rpp_store8_f32_to_i8_avx(Rpp8s *dstPtrTemp, __m256 pDst)
+{
+    __m256i px1 = __lasx_xvftint_w_s(pDst);
+    __m128i px2 = lsx_packus_i32(__lasx_cvt_256_128(px1), lasx_extracti128_m256i(px1, 1));
+    __m128i px3 = lsx_packus_i16(px2, __lsx_vldi(0));
+    px3 = __lsx_vsub_b(px3, xmm_pxConvertI8);    /* convert back to i8 for px0 store */
+    // Store the result to dst
+    __lsx_vstelm_d(px3, (__m128i*)dstPtrTemp, 0, 0);
+}
+
+#endif //AMD_RPP_RPP_CPU_SIMD_HPP
diff --git a/src/modules/CMakeLists.txt b/src/modules/CMakeLists.txt
index 50dfb6d8..aacf4fb7 100644
--- a/src/modules/CMakeLists.txt
+++ b/src/modules/CMakeLists.txt
@@ -95,8 +95,10 @@ if( "${BACKEND}" STREQUAL "HIP")
 
     # Set compiler flags
     set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} ${HIP_HIPCC_FLAGS}")
-    set_source_files_properties(rppt_tensor_audio_augmentations.cpp PROPERTIES COMPILE_FLAGS -mno-fma)
-    set_source_files_properties(rppt_tensor_statistical_operations.cpp PROPERTIES COMPILE_FLAGS -mno-fma) # no-fma flag added to get the exact output match with golden outputs
+    if(NOT CMAKE_SYSTEM_PROCESSOR MATCHES "loongarch")
+        set_source_files_properties(rppt_tensor_audio_augmentations.cpp PROPERTIES COMPILE_FLAGS -mno-fma)
+        set_source_files_properties(rppt_tensor_statistical_operations.cpp PROPERTIES COMPILE_FLAGS -mno-fma) # no-fma flag added to get the exact output match with golden outputs
+    endif()
 
     # Add HIP specific preprocessor flags
     add_definitions(-DHIP_COMPILE)
diff --git a/src/modules/cpu/host_arithmetic_operations.hpp b/src/modules/cpu/host_arithmetic_operations.hpp
index a9bc5073..422e813f 100644
--- a/src/modules/cpu/host_arithmetic_operations.hpp
+++ b/src/modules/cpu/host_arithmetic_operations.hpp
@@ -25,7 +25,11 @@ SOFTWARE.
 #ifndef HOST_ARITHMETIC_OPERATIONS_HPP
 #define HOST_ARITHMETIC_OPERATIONS_HPP
 
+#ifdef __loongarch_sx
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_common.hpp"
+#endif
 
 /**************** absolute_difference ***************/
 
diff --git a/src/modules/cpu/host_computer_vision.hpp b/src/modules/cpu/host_computer_vision.hpp
index 2920e73f..47fbeb4d 100644
--- a/src/modules/cpu/host_computer_vision.hpp
+++ b/src/modules/cpu/host_computer_vision.hpp
@@ -25,7 +25,11 @@ SOFTWARE.
 #ifndef HOST_COMPUTER_VISION_HPP
 #define HOST_COMPUTER_VISION_HPP
 
+#ifdef __loongarch_sx
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_common.hpp"
+#endif
 
 /**************** data_object_copy ***************/
 
diff --git a/src/modules/cpu/host_filter_operations.hpp b/src/modules/cpu/host_filter_operations.hpp
index aad71ab5..699c38ef 100644
--- a/src/modules/cpu/host_filter_operations.hpp
+++ b/src/modules/cpu/host_filter_operations.hpp
@@ -25,7 +25,11 @@ SOFTWARE.
 #ifndef HOST_FILTER_OPERATIONS_HPP
 #define HOST_FILTER_OPERATIONS_HPP
 
+#ifdef __loongarch_sx
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_common.hpp"
+#endif
 
 /**************** box_filter ***************/
 
diff --git a/src/modules/cpu/host_logical_operations.hpp b/src/modules/cpu/host_logical_operations.hpp
index c9a5034b..2de8ede2 100644
--- a/src/modules/cpu/host_logical_operations.hpp
+++ b/src/modules/cpu/host_logical_operations.hpp
@@ -25,7 +25,11 @@ SOFTWARE.
 #ifndef HOST_LOGICAL_OPERATIONS_HPP
 #define HOST_LOGICAL_OPERATIONS_HPP
 
+#ifdef __loongarch_sx
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_common.hpp"
+#endif
 
 /**************** bitwise_AND ***************/
 
diff --git a/src/modules/cpu/host_morphological_transforms.hpp b/src/modules/cpu/host_morphological_transforms.hpp
index 41008d38..0bc8d11b 100644
--- a/src/modules/cpu/host_morphological_transforms.hpp
+++ b/src/modules/cpu/host_morphological_transforms.hpp
@@ -25,7 +25,11 @@ SOFTWARE.
 #ifndef HOST_MORPHOLOGICAL_TRANSFORMS_HPP
 #define HOST_MORPHOLOGICAL_TRANSFORMS_HPP
 
+#ifdef __loongarch_sx
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_common.hpp"
+#endif
 
 /**************** erode ***************/
 
diff --git a/src/modules/cpu/host_statistical_operations.hpp b/src/modules/cpu/host_statistical_operations.hpp
index d6f90d61..f0a578a7 100644
--- a/src/modules/cpu/host_statistical_operations.hpp
+++ b/src/modules/cpu/host_statistical_operations.hpp
@@ -26,7 +26,11 @@ SOFTWARE.
 #define STATISTICAL_OPERATIONS_HPP
 
 #include <limits>
+#ifdef __loongarch_sx
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_common.hpp"
+#endif
 
 /**************** min ***************/
 
diff --git a/src/modules/cpu/host_tensor_filter_augmentations.hpp b/src/modules/cpu/host_tensor_filter_augmentations.hpp
index 04e2e60d..62306964 100644
--- a/src/modules/cpu/host_tensor_filter_augmentations.hpp
+++ b/src/modules/cpu/host_tensor_filter_augmentations.hpp
@@ -26,6 +26,10 @@ SOFTWARE.
 #define HOST_TENSOR_FILTER_AUGMENTATIONS_HPP
 
 #include "kernel/gaussian_filter.hpp"
+#if __loongarch64
+#include "kernel/box_filter_loongarch.hpp"
+#else
 #include "kernel/box_filter.hpp"
+#endif
 
 #endif // HOST_TENSOR_FILTER_AUGMENTATIONS_HPP
diff --git a/src/modules/cpu/host_tensor_geometric_augmentations.hpp b/src/modules/cpu/host_tensor_geometric_augmentations.hpp
index 9248e6f2..52c26203 100644
--- a/src/modules/cpu/host_tensor_geometric_augmentations.hpp
+++ b/src/modules/cpu/host_tensor_geometric_augmentations.hpp
@@ -28,7 +28,11 @@ SOFTWARE.
 #include "kernel/crop.hpp"
 #include "kernel/crop_mirror_normalize.hpp"
 #include "kernel/flip.hpp"
+#ifdef __loongarch64
+#include "kernel/remap_loongarch.hpp"
+#else
 #include "kernel/remap.hpp"
+#endif
 #include "kernel/resize.hpp"
 #include "kernel/resize_mirror_normalize.hpp"
 #include "kernel/resize_crop_mirror.hpp"
diff --git a/src/modules/cpu/kernel/add_scalar.hpp b/src/modules/cpu/kernel/add_scalar.hpp
index d0179d4e..415c8a56 100644
--- a/src/modules/cpu/kernel/add_scalar.hpp
+++ b/src/modules/cpu/kernel/add_scalar.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 RppStatus add_scalar_f32_f32_host_tensor(Rpp32f *srcPtr,
                                          RpptGenericDescPtr srcGenericDescPtr,
@@ -62,7 +67,11 @@ RppStatus add_scalar_f32_f32_host_tensor(Rpp32f *srcPtr,
         Rpp32u vectorIncrement = 16;
         Rpp32u bufferLength = roi.xyzwhdROI.roiWidth * layoutParams.bufferMultiplier;
         Rpp32u alignedLength = (bufferLength / vectorIncrement) * vectorIncrement;
+#ifdef __loongarch_asx
+        __m256 pAddParam = lasx_set1_f32(addParam);
+#else
         __m256 pAddParam = _mm256_set1_ps(addParam);
+#endif
 
         // Add without fused output-layout toggle (NCDHW -> NCDHW)
         if((srcGenericDescPtr->layout == RpptLayout::NCDHW) && (dstGenericDescPtr->layout == RpptLayout::NCDHW))
diff --git a/src/modules/cpu/kernel/bitwise_and.hpp b/src/modules/cpu/kernel/bitwise_and.hpp
index fb8dfc2d..ffd859db 100644
--- a/src/modules/cpu/kernel/bitwise_and.hpp
+++ b/src/modules/cpu/kernel/bitwise_and.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 RppStatus bitwise_and_u8_u8_host_tensor(Rpp8u *srcPtr1,
                                         Rpp8u *srcPtr2,
@@ -89,10 +94,16 @@ RppStatus bitwise_and_u8_u8_host_tensor(Rpp8u *srcPtr1,
 
                     rpp_simd_load(rpp_load48_u8pkd3_to_u8pln3, srcPtr1Temp, p1);    // simd loads
                     rpp_simd_load(rpp_load48_u8pkd3_to_u8pln3, srcPtr2Temp, p2);    // simd loads
+#if defined(__loongarch_sx)
+                    p1[0] = __lsx_vand_v(p1[0], p2[0]);    // bitwise_and computation
+                    p1[1] = __lsx_vand_v(p1[1], p2[1]);    // bitwise_and computation
+                    p1[2] = __lsx_vand_v(p1[2], p2[2]);    // bitwise_and computation
+#else
                     p1[0] = _mm_and_si128(p1[0], p2[0]);    // bitwise_and computation
                     p1[1] = _mm_and_si128(p1[1], p2[1]);    // bitwise_and computation
                     p1[2] = _mm_and_si128(p1[2], p2[2]);    // bitwise_and computation
                     rpp_simd_store(rpp_store48_u8pln3_to_u8pln3, dstPtrTempR, dstPtrTempG, dstPtrTempB, p1);    // simd stores
+#endif
 
                     srcPtr1Temp += vectorIncrement;
                     srcPtr2Temp += vectorIncrement;
@@ -149,9 +160,15 @@ RppStatus bitwise_and_u8_u8_host_tensor(Rpp8u *srcPtr1,
 
                     rpp_simd_load(rpp_load48_u8pln3_to_u8pln3, srcPtr1TempR, srcPtr1TempG, srcPtr1TempB, p1);    // simd loads
                     rpp_simd_load(rpp_load48_u8pln3_to_u8pln3, srcPtr2TempR, srcPtr2TempG, srcPtr2TempB, p2);    // simd loads
+#if defined(__loongarch_sx)
+                    p1[0] = __lsx_vand_v(p1[0], p2[0]);    // bitwise_and computation
+                    p1[1] = __lsx_vand_v(p1[1], p2[1]);    // bitwise_and computation
+                    p1[2] = __lsx_vand_v(p1[2], p2[2]);    // bitwise_and computation
+#else
                     p1[0] = _mm_and_si128(p1[0], p2[0]);    // bitwise_and computation
                     p1[1] = _mm_and_si128(p1[1], p2[1]);    // bitwise_and computation
                     p1[2] = _mm_and_si128(p1[2], p2[2]);    // bitwise_and computation
+#endif
                     rpp_simd_store(rpp_store48_u8pln3_to_u8pkd3, dstPtrTemp, p1);    // simd stores
 
                     srcPtr1TempR += vectorIncrementPerChannel;
@@ -212,11 +229,17 @@ RppStatus bitwise_and_u8_u8_host_tensor(Rpp8u *srcPtr1,
                     {
                         __m128i p1, p2;
 
+#if defined(__loongarch_sx)
+                        p1 = __lsx_vld((__m128i *)srcPtr1Temp, 0);   // simd loads
+                        p2 = __lsx_vld((__m128i *)srcPtr2Temp, 0);   // simd loads
+                        p1 = __lsx_vand_v(p1, p2);    // bitwise_and computation
+                        __lsx_vst(p1, (__m128i *)dstPtrTemp, 0);    // simd stores
+#else
                         p1 = _mm_loadu_si128((__m128i *)srcPtr1Temp);   // simd loads
                         p2 = _mm_loadu_si128((__m128i *)srcPtr2Temp);   // simd loads
                         p1 = _mm_and_si128(p1, p2);    // bitwise_and computation
                         _mm_storeu_si128((__m128i *)dstPtrTemp, p1);    // simd stores
-
+#endif
                         srcPtr1Temp += vectorIncrementPerChannel;
                         srcPtr2Temp += vectorIncrementPerChannel;
                         dstPtrTemp += vectorIncrementPerChannel;
@@ -282,7 +305,7 @@ RppStatus bitwise_and_f32_f32_host_tensor(Rpp32f *srcPtr1,
         srcPtr2Channel = srcPtr2Image + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
         dstPtrChannel = dstPtrImage;
 
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
         Rpp32u alignedLength = (bufferLength / 24) * 24;
         Rpp32u vectorIncrement = 24;
         Rpp32u vectorIncrementPerChannel = 8;
@@ -308,7 +331,7 @@ RppStatus bitwise_and_f32_f32_host_tensor(Rpp32f *srcPtr1,
                 dstPtrTempB = dstPtrRowB;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256 p1[3], p2[3];
@@ -323,6 +346,27 @@ RppStatus bitwise_and_f32_f32_host_tensor(Rpp32f *srcPtr1,
                     p1[2] = _mm256_mul_ps(p1[2], avx_p1op255);
                     rpp_simd_store(rpp_store24_f32pln3_to_f32pln3_avx, dstPtrTempR, dstPtrTempG, dstPtrTempB, p1);    // simd stores
 
+                    srcPtr1Temp += vectorIncrement;
+                    srcPtr2Temp += vectorIncrement;
+                    dstPtrTempR += vectorIncrementPerChannel;
+                    dstPtrTempG += vectorIncrementPerChannel;
+                    dstPtrTempB += vectorIncrementPerChannel;
+                }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
+                {
+                    __m256 p1[3], p2[3];
+
+                    rpp_simd_load(rpp_load24_f32pkd3_to_f32pln3_avx, srcPtr1Temp, p1);    // simd loads
+                    rpp_simd_load(rpp_load24_f32pkd3_to_f32pln3_avx, srcPtr2Temp, p2);    // simd loads
+                    p1[0] = __lasx_xvffint_s_w(__lasx_xvand_v(__lasx_xvftintrz_w_s(__lasx_xvfmul_s(p1[0], avx_p255)), __lasx_xvftintrz_w_s(__lasx_xvfmul_s(p2[0], avx_p255))));    // bitwise_and computation
+                    p1[1] = __lasx_xvffint_s_w(__lasx_xvand_v(__lasx_xvftintrz_w_s(__lasx_xvfmul_s(p1[1], avx_p255)), __lasx_xvftintrz_w_s(__lasx_xvfmul_s(p2[1], avx_p255))));    // bitwise_and computation
+                    p1[2] = __lasx_xvffint_s_w(__lasx_xvand_v(__lasx_xvftintrz_w_s(__lasx_xvfmul_s(p1[2], avx_p255)), __lasx_xvftintrz_w_s(__lasx_xvfmul_s(p2[2], avx_p255))));    // bitwise_and computation
+                    p1[0] = __lasx_xvfmul_s(p1[0], avx_p1op255);
+                    p1[1] = __lasx_xvfmul_s(p1[1], avx_p1op255);
+                    p1[2] = __lasx_xvfmul_s(p1[2], avx_p1op255);
+                    rpp_simd_store(rpp_store24_f32pln3_to_f32pln3_avx, dstPtrTempR, dstPtrTempG, dstPtrTempB, p1);    // simd stores
+
                     srcPtr1Temp += vectorIncrement;
                     srcPtr2Temp += vectorIncrement;
                     dstPtrTempR += vectorIncrementPerChannel;
@@ -372,7 +416,7 @@ RppStatus bitwise_and_f32_f32_host_tensor(Rpp32f *srcPtr1,
                 dstPtrTemp = dstPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 p1[3], p2[3];
@@ -387,6 +431,29 @@ RppStatus bitwise_and_f32_f32_host_tensor(Rpp32f *srcPtr1,
                     p1[2] = _mm256_mul_ps(p1[2], avx_p1op255);
                     rpp_simd_store(rpp_store24_f32pln3_to_f32pkd3_avx, dstPtrTemp, p1);    // simd stores
 
+                    srcPtr1TempR += vectorIncrementPerChannel;
+                    srcPtr1TempG += vectorIncrementPerChannel;
+                    srcPtr1TempB += vectorIncrementPerChannel;
+                    srcPtr2TempR += vectorIncrementPerChannel;
+                    srcPtr2TempG += vectorIncrementPerChannel;
+                    srcPtr2TempB += vectorIncrementPerChannel;
+                    dstPtrTemp += vectorIncrement;
+                }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256 p1[3], p2[3];
+
+                    rpp_simd_load(rpp_load24_f32pln3_to_f32pln3_avx, srcPtr1TempR, srcPtr1TempG, srcPtr1TempB, p1);    // simd loads
+                    rpp_simd_load(rpp_load24_f32pln3_to_f32pln3_avx, srcPtr2TempR, srcPtr2TempG, srcPtr2TempB, p2);    // simd loads
+                    p1[0] = __lasx_xvffint_s_w(__lasx_xvand_v(__lasx_xvftintrz_w_s(__lasx_xvfmul_s(p1[0], avx_p255)), __lasx_xvftintrz_w_s(__lasx_xvfmul_s(p2[0], avx_p255))));    // bitwise_and computation
+                    p1[1] = __lasx_xvffint_s_w(__lasx_xvand_v(__lasx_xvftintrz_w_s(__lasx_xvfmul_s(p1[1], avx_p255)), __lasx_xvftintrz_w_s(__lasx_xvfmul_s(p2[1], avx_p255))));    // bitwise_and computation
+                    p1[2] = __lasx_xvffint_s_w(__lasx_xvand_v(__lasx_xvftintrz_w_s(__lasx_xvfmul_s(p1[2], avx_p255)), __lasx_xvftintrz_w_s(__lasx_xvfmul_s(p2[2], avx_p255))));    // bitwise_and computation
+                    p1[0] = __lasx_xvfmul_s(p1[0], avx_p1op255);
+                    p1[1] = __lasx_xvfmul_s(p1[1], avx_p1op255);
+                    p1[2] = __lasx_xvfmul_s(p1[2], avx_p1op255);
+                    rpp_simd_store(rpp_store24_f32pln3_to_f32pkd3_avx, dstPtrTemp, p1);    // simd stores
+
                     srcPtr1TempR += vectorIncrementPerChannel;
                     srcPtr1TempG += vectorIncrementPerChannel;
                     srcPtr1TempB += vectorIncrementPerChannel;
@@ -424,7 +491,7 @@ RppStatus bitwise_and_f32_f32_host_tensor(Rpp32f *srcPtr1,
         // Bitwise AND without fused output-layout toggle (NHWC -> NHWC or NCHW -> NCHW)
         else
         {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             alignedLength = bufferLength & ~7;
 #endif
 
@@ -443,7 +510,7 @@ RppStatus bitwise_and_f32_f32_host_tensor(Rpp32f *srcPtr1,
                     dstPtrTemp = dstPtrRow;
 
                     int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                     {
                         __m256 p1[1], p2[1];
@@ -454,6 +521,21 @@ RppStatus bitwise_and_f32_f32_host_tensor(Rpp32f *srcPtr1,
                         p1[0] = _mm256_mul_ps(p1[0], avx_p1op255);
                         rpp_simd_store(rpp_store8_f32_to_f32_avx, dstPtrTemp, p1);    // simd stores
 
+                        srcPtr1Temp += vectorIncrementPerChannel;
+                        srcPtr2Temp += vectorIncrementPerChannel;
+                        dstPtrTemp += vectorIncrementPerChannel;
+                    }
+#elif defined(__loongarch_asx)
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                    {
+                        __m256 p1[1], p2[1];
+
+                        rpp_simd_load(rpp_load8_f32_to_f32_avx, srcPtr1Temp, p1);    // simd loads
+                        rpp_simd_load(rpp_load8_f32_to_f32_avx, srcPtr2Temp, p2);    // simd loads
+                        p1[0] = __lasx_xvffint_s_w(__lasx_xvand_v(__lasx_xvftintrz_w_s(__lasx_xvfmul_s(p1[0], avx_p255)), __lasx_xvftintrz_w_s(__lasx_xvfmul_s(p2[0], avx_p255))));    // bitwise_and computation
+                        p1[0] = __lasx_xvfmul_s(p1[0], avx_p1op255);
+                        rpp_simd_store(rpp_store8_f32_to_f32_avx, dstPtrTemp, p1);    // simd stores
+
                         srcPtr1Temp += vectorIncrementPerChannel;
                         srcPtr2Temp += vectorIncrementPerChannel;
                         dstPtrTemp += vectorIncrementPerChannel;
@@ -515,7 +597,7 @@ RppStatus bitwise_and_f16_f16_host_tensor(Rpp16f *srcPtr1,
         srcPtr2Channel = srcPtr2Image + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
         dstPtrChannel = dstPtrImage;
 
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
         Rpp32u alignedLength = (bufferLength / 24) * 24;
         Rpp32u vectorIncrement = 24;
         Rpp32u vectorIncrementPerChannel = 8;
@@ -541,7 +623,7 @@ RppStatus bitwise_and_f16_f16_host_tensor(Rpp16f *srcPtr1,
                 dstPtrTempB = dstPtrRowB;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     Rpp32f srcPtr1Temp_ps[24], srcPtr2Temp_ps[24];
@@ -564,6 +646,35 @@ RppStatus bitwise_and_f16_f16_host_tensor(Rpp16f *srcPtr1,
                     p1[2] = _mm256_mul_ps(p1[2], avx_p1op255);
                     rpp_simd_store(rpp_store24_f32pln3_to_f16pln3_avx, dstPtrTempR, dstPtrTempG, dstPtrTempB, p1);    // simd stores
 
+                    srcPtr1Temp += vectorIncrement;
+                    srcPtr2Temp += vectorIncrement;
+                    dstPtrTempR += vectorIncrementPerChannel;
+                    dstPtrTempG += vectorIncrementPerChannel;
+                    dstPtrTempB += vectorIncrementPerChannel;
+                }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
+                {
+                    Rpp32f srcPtr1Temp_ps[24], srcPtr2Temp_ps[24];
+
+                    for(int cnt = 0; cnt < vectorIncrement; cnt++)
+                    {
+                        srcPtr1Temp_ps[cnt] = static_cast<Rpp32f>(srcPtr1Temp[cnt]);
+                        srcPtr2Temp_ps[cnt] = static_cast<Rpp32f>(srcPtr2Temp[cnt]);
+                    }
+
+                    __m256 p1[3], p2[3];
+
+                    rpp_simd_load(rpp_load24_f32pkd3_to_f32pln3_avx, srcPtr1Temp_ps, p1);    // simd loads
+                    rpp_simd_load(rpp_load24_f32pkd3_to_f32pln3_avx, srcPtr2Temp_ps, p2);    // simd loads
+                    p1[0] = __lasx_xvffint_s_w(__lasx_xvand_v(__lasx_xvftintrz_w_s(__lasx_xvfmul_s(p1[0], avx_p255)), __lasx_xvftintrz_w_s(__lasx_xvfmul_s(p2[0], avx_p255))));    // bitwise_and computation
+                    p1[1] = __lasx_xvffint_s_w(__lasx_xvand_v(__lasx_xvftintrz_w_s(__lasx_xvfmul_s(p1[1], avx_p255)), __lasx_xvftintrz_w_s(__lasx_xvfmul_s(p2[1], avx_p255))));    // bitwise_and computation
+                    p1[2] = __lasx_xvffint_s_w(__lasx_xvand_v(__lasx_xvftintrz_w_s(__lasx_xvfmul_s(p1[2], avx_p255)), __lasx_xvftintrz_w_s(__lasx_xvfmul_s(p2[2], avx_p255))));    // bitwise_and computation
+                    p1[0] = __lasx_xvfmul_s(p1[0], avx_p1op255);
+                    p1[1] = __lasx_xvfmul_s(p1[1], avx_p1op255);
+                    p1[2] = __lasx_xvfmul_s(p1[2], avx_p1op255);
+                    rpp_simd_store(rpp_store24_f32pln3_to_f16pln3_avx, dstPtrTempR, dstPtrTempG, dstPtrTempB, p1);    // simd stores
+
                     srcPtr1Temp += vectorIncrement;
                     srcPtr2Temp += vectorIncrement;
                     dstPtrTempR += vectorIncrementPerChannel;
@@ -613,7 +724,7 @@ RppStatus bitwise_and_f16_f16_host_tensor(Rpp16f *srcPtr1,
                 dstPtrTemp = dstPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     Rpp32f srcPtr1Temp_ps[24], srcPtr2Temp_ps[24];
@@ -641,6 +752,42 @@ RppStatus bitwise_and_f16_f16_host_tensor(Rpp16f *srcPtr1,
                     p1[2] = _mm256_mul_ps(p1[2], avx_p1op255);
                     rpp_simd_store(rpp_store24_f32pln3_to_f16pkd3_avx, dstPtrTemp, p1);    // simd stores
 
+                    srcPtr1TempR += vectorIncrementPerChannel;
+                    srcPtr1TempG += vectorIncrementPerChannel;
+                    srcPtr1TempB += vectorIncrementPerChannel;
+                    srcPtr2TempR += vectorIncrementPerChannel;
+                    srcPtr2TempG += vectorIncrementPerChannel;
+                    srcPtr2TempB += vectorIncrementPerChannel;
+                    dstPtrTemp += vectorIncrement;
+                }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    Rpp32f srcPtr1Temp_ps[24], srcPtr2Temp_ps[24];
+
+                    for(int cnt = 0; cnt < vectorIncrementPerChannel; cnt++)
+                    {
+                        srcPtr1Temp_ps[cnt] = static_cast<Rpp32f>(srcPtr1TempR[cnt]);
+                        srcPtr1Temp_ps[cnt + 8] = static_cast<Rpp32f>(srcPtr1TempG[cnt]);
+                        srcPtr1Temp_ps[cnt + 16] = static_cast<Rpp32f>(srcPtr1TempB[cnt]);
+
+                        srcPtr2Temp_ps[cnt] = static_cast<Rpp32f>(srcPtr2TempR[cnt]);
+                        srcPtr2Temp_ps[cnt + 8] = static_cast<Rpp32f>(srcPtr2TempG[cnt]);
+                        srcPtr2Temp_ps[cnt + 16] = static_cast<Rpp32f>(srcPtr2TempB[cnt]);
+                    }
+
+                    __m256 p1[4], p2[4];
+
+                    rpp_simd_load(rpp_load24_f32pln3_to_f32pln3_avx, srcPtr1Temp_ps, srcPtr1Temp_ps + 8, srcPtr1Temp_ps + 16, p1);    // simd loads
+                    rpp_simd_load(rpp_load24_f32pln3_to_f32pln3_avx, srcPtr2Temp_ps, srcPtr2Temp_ps + 8, srcPtr2Temp_ps + 16, p2);    // simd loads
+                    p1[0] = __lasx_xvffint_s_w(__lasx_xvand_v(__lasx_xvftintrz_w_s(__lasx_xvfmul_s(p1[0], avx_p255)), __lasx_xvftintrz_w_s(__lasx_xvfmul_s(p2[0], avx_p255))));    // bitwise_and computation
+                    p1[1] = __lasx_xvffint_s_w(__lasx_xvand_v(__lasx_xvftintrz_w_s(__lasx_xvfmul_s(p1[1], avx_p255)), __lasx_xvftintrz_w_s(__lasx_xvfmul_s(p2[1], avx_p255))));    // bitwise_and computation
+                    p1[2] = __lasx_xvffint_s_w(__lasx_xvand_v(__lasx_xvftintrz_w_s(__lasx_xvfmul_s(p1[2], avx_p255)), __lasx_xvftintrz_w_s(__lasx_xvfmul_s(p2[2], avx_p255))));    // bitwise_and computation
+                    p1[0] = __lasx_xvfmul_s(p1[0], avx_p1op255);
+                    p1[1] = __lasx_xvfmul_s(p1[1], avx_p1op255);
+                    p1[2] = __lasx_xvfmul_s(p1[2], avx_p1op255);
+                    rpp_simd_store(rpp_store24_f32pln3_to_f16pkd3_avx, dstPtrTemp, p1);    // simd stores
+
                     srcPtr1TempR += vectorIncrementPerChannel;
                     srcPtr1TempG += vectorIncrementPerChannel;
                     srcPtr1TempB += vectorIncrementPerChannel;
@@ -678,7 +825,7 @@ RppStatus bitwise_and_f16_f16_host_tensor(Rpp16f *srcPtr1,
         // Bitwise AND without fused output-layout toggle (NHWC -> NHWC or NCHW -> NCHW)
         else
         {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             alignedLength = bufferLength & ~7;
 #endif
 
@@ -697,7 +844,7 @@ RppStatus bitwise_and_f16_f16_host_tensor(Rpp16f *srcPtr1,
                     dstPtrTemp = dstPtrRow;
 
                     int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                     {
                         Rpp32f srcPtr1Temp_ps[8], srcPtr2Temp_ps[8];
@@ -716,6 +863,29 @@ RppStatus bitwise_and_f16_f16_host_tensor(Rpp16f *srcPtr1,
                         p1[0] = _mm256_mul_ps(p1[0], avx_p1op255);
                         rpp_simd_store(rpp_store8_f32_to_f16_avx, dstPtrTemp, p1);    // simd stores
 
+                        srcPtr1Temp += vectorIncrementPerChannel;
+                        srcPtr2Temp += vectorIncrementPerChannel;
+                        dstPtrTemp += vectorIncrementPerChannel;
+                    }
+#elif defined(__loongarch_asx)
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                    {
+                        Rpp32f srcPtr1Temp_ps[8], srcPtr2Temp_ps[8];
+
+                        for(int cnt = 0; cnt < vectorIncrementPerChannel; cnt++)
+                        {
+                            srcPtr1Temp_ps[cnt] = static_cast<Rpp32f>(srcPtr1Temp[cnt]);
+                            srcPtr2Temp_ps[cnt] = static_cast<Rpp32f>(srcPtr2Temp[cnt]);
+                        }
+
+                        __m256 p1[1], p2[1];
+
+                        rpp_simd_load(rpp_load8_f32_to_f32_avx, srcPtr1Temp_ps, p1);    // simd loads
+                        rpp_simd_load(rpp_load8_f32_to_f32_avx, srcPtr2Temp_ps, p2);    // simd loads
+                        p1[0] = __lasx_xvffint_s_w(__lasx_xvand_v(__lasx_xvftintrz_w_s(__lasx_xvfmul_s(p1[0], avx_p255)), __lasx_xvftintrz_w_s(__lasx_xvfmul_s(p2[0], avx_p255))));    // bitwise_and computation
+                        p1[0] = __lasx_xvfmul_s(p1[0], avx_p1op255);
+                        rpp_simd_store(rpp_store8_f32_to_f16_avx, dstPtrTemp, p1);    // simd stores
+
                         srcPtr1Temp += vectorIncrementPerChannel;
                         srcPtr2Temp += vectorIncrementPerChannel;
                         dstPtrTemp += vectorIncrementPerChannel;
@@ -807,9 +977,15 @@ RppStatus bitwise_and_i8_i8_host_tensor(Rpp8s *srcPtr1,
 
                     rpp_simd_load(rpp_load48_i8pkd3_to_u8pln3, srcPtr1Temp, p1);    // simd loads
                     rpp_simd_load(rpp_load48_i8pkd3_to_u8pln3, srcPtr2Temp, p2);    // simd loads
+#if defined(__loongarch_sx)
+                    p1[0] = __lsx_vand_v(p1[0], p2[0]);    // bitwise_and computation
+                    p1[1] = __lsx_vand_v(p1[1], p2[1]);    // bitwise_and computation
+                    p1[2] = __lsx_vand_v(p1[2], p2[2]);    // bitwise_and computation
+#else
                     p1[0] = _mm_and_si128(p1[0], p2[0]);    // bitwise_and computation
                     p1[1] = _mm_and_si128(p1[1], p2[1]);    // bitwise_and computation
                     p1[2] = _mm_and_si128(p1[2], p2[2]);    // bitwise_and computation
+#endif
                     rpp_simd_store(rpp_store48_u8pln3_to_i8pln3, dstPtrTempR, dstPtrTempG, dstPtrTempB, p1);    // simd stores
 
                     srcPtr1Temp += vectorIncrement;
@@ -868,9 +1044,15 @@ RppStatus bitwise_and_i8_i8_host_tensor(Rpp8s *srcPtr1,
 
                     rpp_simd_load(rpp_load48_i8pln3_to_u8pln3, srcPtr1TempR, srcPtr1TempG, srcPtr1TempB, p1);    // simd loads
                     rpp_simd_load(rpp_load48_i8pln3_to_u8pln3, srcPtr2TempR, srcPtr2TempG, srcPtr2TempB, p2);    // simd loads
+#if defined(__loongarch_sx)
+                    p1[0] = __lsx_vand_v(p1[0], p2[0]);    // bitwise_and computation
+                    p1[1] = __lsx_vand_v(p1[1], p2[1]);    // bitwise_and computation
+                    p1[2] = __lsx_vand_v(p1[2], p2[2]);    // bitwise_and computation
+#else
                     p1[0] = _mm_and_si128(p1[0], p2[0]);    // bitwise_and computation
                     p1[1] = _mm_and_si128(p1[1], p2[1]);    // bitwise_and computation
                     p1[2] = _mm_and_si128(p1[2], p2[2]);    // bitwise_and computation
+#endif
                     rpp_simd_store(rpp_store48_u8pln3_to_i8pkd3, dstPtrTemp, p1);    // simd stores
 
 
@@ -932,10 +1114,17 @@ RppStatus bitwise_and_i8_i8_host_tensor(Rpp8s *srcPtr1,
                     {
                         __m128i p1, p2;
 
+#if defined(__loongarch_sx)
+                        p1 = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)srcPtr1Temp, 0));   // simd loads
+                        p2 = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)srcPtr2Temp, 0));   // simd loads
+                        p1 = __lsx_vand_v(p1, p2);    // bitwise_and computation
+                        __lsx_vst(__lsx_vsub_b(p1, xmm_pxConvertI8), (__m128i *)dstPtrTemp, 0);    // simd stores
+#else
                         p1 = _mm_add_epi8(xmm_pxConvertI8, _mm_loadu_si128((__m128i *)srcPtr1Temp));   // simd loads
                         p2 = _mm_add_epi8(xmm_pxConvertI8, _mm_loadu_si128((__m128i *)srcPtr2Temp));   // simd loads
                         p1 = _mm_and_si128(p1, p2);    // bitwise_and computation
                         _mm_storeu_si128((__m128i *)dstPtrTemp, _mm_sub_epi8(p1, xmm_pxConvertI8));    // simd stores
+#endif
 
                         srcPtr1Temp += vectorIncrementPerChannel;
                         srcPtr2Temp += vectorIncrementPerChannel;
diff --git a/src/modules/cpu/kernel/bitwise_or.hpp b/src/modules/cpu/kernel/bitwise_or.hpp
index 8fb0b49f..8d96946e 100644
--- a/src/modules/cpu/kernel/bitwise_or.hpp
+++ b/src/modules/cpu/kernel/bitwise_or.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 RppStatus bitwise_or_u8_u8_host_tensor(Rpp8u *srcPtr1,
                                        Rpp8u *srcPtr2,
@@ -89,9 +94,15 @@ RppStatus bitwise_or_u8_u8_host_tensor(Rpp8u *srcPtr1,
 
                     rpp_simd_load(rpp_load48_u8pkd3_to_u8pln3, srcPtr1Temp, p1);    // simd loads
                     rpp_simd_load(rpp_load48_u8pkd3_to_u8pln3, srcPtr2Temp, p2);    // simd loads
+#if defined(__loongarch_sx)
+                    p1[0] = __lsx_vor_v(p1[0], p2[0]);    // bitwise_or computation
+                    p1[1] = __lsx_vor_v(p1[1], p2[1]);    // bitwise_or computation
+                    p1[2] = __lsx_vor_v(p1[2], p2[2]);    // bitwise_or computation
+#else
                     p1[0] = _mm_or_si128(p1[0], p2[0]);    // bitwise_or computation
                     p1[1] = _mm_or_si128(p1[1], p2[1]);    // bitwise_or computation
                     p1[2] = _mm_or_si128(p1[2], p2[2]);    // bitwise_or computation
+#endif
                     rpp_simd_store(rpp_store48_u8pln3_to_u8pln3, dstPtrTempR, dstPtrTempG, dstPtrTempB, p1);    // simd stores
 
                     srcPtr1Temp += vectorIncrement;
@@ -149,9 +160,15 @@ RppStatus bitwise_or_u8_u8_host_tensor(Rpp8u *srcPtr1,
 
                     rpp_simd_load(rpp_load48_u8pln3_to_u8pln3, srcPtr1TempR, srcPtr1TempG, srcPtr1TempB, p1);    // simd loads
                     rpp_simd_load(rpp_load48_u8pln3_to_u8pln3, srcPtr2TempR, srcPtr2TempG, srcPtr2TempB, p2);    // simd loads
+#if defined(__loongarch_sx)
+                    p1[0] = __lsx_vor_v(p1[0], p2[0]);    // bitwise_or computation
+                    p1[1] = __lsx_vor_v(p1[1], p2[1]);    // bitwise_or computation
+                    p1[2] = __lsx_vor_v(p1[2], p2[2]);    // bitwise_or computation
+#else
                     p1[0] = _mm_or_si128(p1[0], p2[0]);    // bitwise_or computation
                     p1[1] = _mm_or_si128(p1[1], p2[1]);    // bitwise_or computation
                     p1[2] = _mm_or_si128(p1[2], p2[2]);    // bitwise_or computation
+#endif
                     rpp_simd_store(rpp_store48_u8pln3_to_u8pkd3, dstPtrTemp, p1);    // simd stores
 
                     srcPtr1TempR += vectorIncrementPerChannel;
@@ -212,10 +229,17 @@ RppStatus bitwise_or_u8_u8_host_tensor(Rpp8u *srcPtr1,
                     {
                         __m128i p1, p2;
 
+#if defined(__loongarch_sx)
+                        p1 = __lsx_vld((__m128i *)srcPtr1Temp, 0);   // simd loads
+                        p2 = __lsx_vld((__m128i *)srcPtr2Temp, 0);   // simd loads
+                        p1 = __lsx_vor_v(p1, p2);    // bitwise_or computation
+                        __lsx_vst(p1, (__m128i *)dstPtrTemp, 0);    // simd stores
+#else
                         p1 = _mm_loadu_si128((__m128i *)srcPtr1Temp);   // simd loads
                         p2 = _mm_loadu_si128((__m128i *)srcPtr2Temp);   // simd loads
                         p1 = _mm_or_si128(p1, p2);    // bitwise_or computation
                         _mm_storeu_si128((__m128i *)dstPtrTemp, p1);    // simd stores
+#endif
 
                         srcPtr1Temp += vectorIncrementPerChannel;
                         srcPtr2Temp += vectorIncrementPerChannel;
@@ -282,7 +306,7 @@ RppStatus bitwise_or_f32_f32_host_tensor(Rpp32f *srcPtr1,
         srcPtr2Channel = srcPtr2Image + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
         dstPtrChannel = dstPtrImage;
 
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
         Rpp32u alignedLength = (bufferLength / 24) * 24;
         Rpp32u vectorIncrement = 24;
         Rpp32u vectorIncrementPerChannel = 8;
@@ -308,7 +332,7 @@ RppStatus bitwise_or_f32_f32_host_tensor(Rpp32f *srcPtr1,
                 dstPtrTempB = dstPtrRowB;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256 p1[3], p2[3];
@@ -323,6 +347,27 @@ RppStatus bitwise_or_f32_f32_host_tensor(Rpp32f *srcPtr1,
                     p1[2] = _mm256_mul_ps(p1[2], avx_p1op255);
                     rpp_simd_store(rpp_store24_f32pln3_to_f32pln3_avx, dstPtrTempR, dstPtrTempG, dstPtrTempB, p1);    // simd stores
 
+                    srcPtr1Temp += vectorIncrement;
+                    srcPtr2Temp += vectorIncrement;
+                    dstPtrTempR += vectorIncrementPerChannel;
+                    dstPtrTempG += vectorIncrementPerChannel;
+                    dstPtrTempB += vectorIncrementPerChannel;
+                }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
+                {
+                    __m256 p1[3], p2[3];
+
+                    rpp_simd_load(rpp_load24_f32pkd3_to_f32pln3_avx, srcPtr1Temp, p1);    // simd loads
+                    rpp_simd_load(rpp_load24_f32pkd3_to_f32pln3_avx, srcPtr2Temp, p2);    // simd loads
+                    p1[0] = __lasx_xvffint_s_w(__lasx_xvor_v(__lasx_xvftintrz_w_s(__lasx_xvfmul_s(p1[0], avx_p255)), __lasx_xvftintrz_w_s(__lasx_xvfmul_s(p2[0], avx_p255))));    // bitwise_or computation
+                    p1[1] = __lasx_xvffint_s_w(__lasx_xvor_v(__lasx_xvftintrz_w_s(__lasx_xvfmul_s(p1[1], avx_p255)), __lasx_xvftintrz_w_s(__lasx_xvfmul_s(p2[1], avx_p255))));    // bitwise_or computation
+                    p1[2] = __lasx_xvffint_s_w(__lasx_xvor_v(__lasx_xvftintrz_w_s(__lasx_xvfmul_s(p1[2], avx_p255)), __lasx_xvftintrz_w_s(__lasx_xvfmul_s(p2[2], avx_p255))));    // bitwise_or computation
+                    p1[0] = __lasx_xvfmul_s(p1[0], avx_p1op255);
+                    p1[1] = __lasx_xvfmul_s(p1[1], avx_p1op255);
+                    p1[2] = __lasx_xvfmul_s(p1[2], avx_p1op255);
+                    rpp_simd_store(rpp_store24_f32pln3_to_f32pln3_avx, dstPtrTempR, dstPtrTempG, dstPtrTempB, p1);    // simd stores
+
                     srcPtr1Temp += vectorIncrement;
                     srcPtr2Temp += vectorIncrement;
                     dstPtrTempR += vectorIncrementPerChannel;
@@ -372,7 +417,7 @@ RppStatus bitwise_or_f32_f32_host_tensor(Rpp32f *srcPtr1,
                 dstPtrTemp = dstPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 p1[3], p2[3];
@@ -387,6 +432,29 @@ RppStatus bitwise_or_f32_f32_host_tensor(Rpp32f *srcPtr1,
                     p1[2] = _mm256_mul_ps(p1[2], avx_p1op255);
                     rpp_simd_store(rpp_store24_f32pln3_to_f32pkd3_avx, dstPtrTemp, p1);    // simd stores
 
+                    srcPtr1TempR += vectorIncrementPerChannel;
+                    srcPtr1TempG += vectorIncrementPerChannel;
+                    srcPtr1TempB += vectorIncrementPerChannel;
+                    srcPtr2TempR += vectorIncrementPerChannel;
+                    srcPtr2TempG += vectorIncrementPerChannel;
+                    srcPtr2TempB += vectorIncrementPerChannel;
+                    dstPtrTemp += vectorIncrement;
+                }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256 p1[3], p2[3];
+
+                    rpp_simd_load(rpp_load24_f32pln3_to_f32pln3_avx, srcPtr1TempR, srcPtr1TempG, srcPtr1TempB, p1);    // simd loads
+                    rpp_simd_load(rpp_load24_f32pln3_to_f32pln3_avx, srcPtr2TempR, srcPtr2TempG, srcPtr2TempB, p2);    // simd loads
+                    p1[0] = __lasx_xvffint_s_w(__lasx_xvor_v(__lasx_xvftintrz_w_s(__lasx_xvfmul_s(p1[0], avx_p255)), __lasx_xvftintrz_w_s(__lasx_xvfmul_s(p2[0], avx_p255))));    // bitwise_or computation
+                    p1[1] = __lasx_xvffint_s_w(__lasx_xvor_v(__lasx_xvftintrz_w_s(__lasx_xvfmul_s(p1[1], avx_p255)), __lasx_xvftintrz_w_s(__lasx_xvfmul_s(p2[1], avx_p255))));    // bitwise_or computation
+                    p1[2] = __lasx_xvffint_s_w(__lasx_xvor_v(__lasx_xvftintrz_w_s(__lasx_xvfmul_s(p1[2], avx_p255)), __lasx_xvftintrz_w_s(__lasx_xvfmul_s(p2[2], avx_p255))));    // bitwise_or computation
+                    p1[0] = __lasx_xvfmul_s(p1[0], avx_p1op255);
+                    p1[1] = __lasx_xvfmul_s(p1[1], avx_p1op255);
+                    p1[2] = __lasx_xvfmul_s(p1[2], avx_p1op255);
+                    rpp_simd_store(rpp_store24_f32pln3_to_f32pkd3_avx, dstPtrTemp, p1);    // simd stores
+
                     srcPtr1TempR += vectorIncrementPerChannel;
                     srcPtr1TempG += vectorIncrementPerChannel;
                     srcPtr1TempB += vectorIncrementPerChannel;
@@ -424,7 +492,7 @@ RppStatus bitwise_or_f32_f32_host_tensor(Rpp32f *srcPtr1,
         // Bitwise OR without fused output-layout toggle (NHWC -> NHWC or NCHW -> NCHW)
         else
         {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             alignedLength = bufferLength & ~7;
 #endif
 
@@ -443,7 +511,7 @@ RppStatus bitwise_or_f32_f32_host_tensor(Rpp32f *srcPtr1,
                     dstPtrTemp = dstPtrRow;
 
                     int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                     {
                         __m256 p1[1], p2[1];
@@ -454,6 +522,21 @@ RppStatus bitwise_or_f32_f32_host_tensor(Rpp32f *srcPtr1,
                         p1[0] = _mm256_mul_ps(p1[0], avx_p1op255);
                         rpp_simd_store(rpp_store8_f32_to_f32_avx, dstPtrTemp, p1);    // simd stores
 
+                        srcPtr1Temp += vectorIncrementPerChannel;
+                        srcPtr2Temp += vectorIncrementPerChannel;
+                        dstPtrTemp += vectorIncrementPerChannel;
+                    }
+#elif defined(__loongarch_asx)
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                    {
+                        __m256 p1[1], p2[1];
+
+                        rpp_simd_load(rpp_load8_f32_to_f32_avx, srcPtr1Temp, p1);    // simd loads
+                        rpp_simd_load(rpp_load8_f32_to_f32_avx, srcPtr2Temp, p2);    // simd loads
+                        p1[0] = __lasx_xvffint_s_w(__lasx_xvor_v(__lasx_xvftintrz_w_s(__lasx_xvfmul_s(p1[0], avx_p255)), __lasx_xvftintrz_w_s(__lasx_xvfmul_s(p2[0], avx_p255))));    // bitwise_or computation
+                        p1[0] = __lasx_xvfmul_s(p1[0], avx_p1op255);
+                        rpp_simd_store(rpp_store8_f32_to_f32_avx, dstPtrTemp, p1);    // simd stores
+
                         srcPtr1Temp += vectorIncrementPerChannel;
                         srcPtr2Temp += vectorIncrementPerChannel;
                         dstPtrTemp += vectorIncrementPerChannel;
@@ -515,7 +598,7 @@ RppStatus bitwise_or_f16_f16_host_tensor(Rpp16f *srcPtr1,
         srcPtr2Channel = srcPtr2Image + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
         dstPtrChannel = dstPtrImage;
 
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
         Rpp32u alignedLength = (bufferLength / 24) * 24;
         Rpp32u vectorIncrement = 24;
         Rpp32u vectorIncrementPerChannel = 8;
@@ -541,7 +624,7 @@ RppStatus bitwise_or_f16_f16_host_tensor(Rpp16f *srcPtr1,
                 dstPtrTempB = dstPtrRowB;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     Rpp32f srcPtr1Temp_ps[24], srcPtr2Temp_ps[24];
@@ -564,6 +647,35 @@ RppStatus bitwise_or_f16_f16_host_tensor(Rpp16f *srcPtr1,
                     p1[2] = _mm256_mul_ps(p1[2], avx_p1op255);
                     rpp_simd_store(rpp_store24_f32pln3_to_f16pln3_avx, dstPtrTempR, dstPtrTempG, dstPtrTempB, p1);    // simd stores
 
+                    srcPtr1Temp += vectorIncrement;
+                    srcPtr2Temp += vectorIncrement;
+                    dstPtrTempR += vectorIncrementPerChannel;
+                    dstPtrTempG += vectorIncrementPerChannel;
+                    dstPtrTempB += vectorIncrementPerChannel;
+                }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
+                {
+                    Rpp32f srcPtr1Temp_ps[24], srcPtr2Temp_ps[24];
+
+                    for(int cnt = 0; cnt < vectorIncrement; cnt++)
+                    {
+                        srcPtr1Temp_ps[cnt] = static_cast<Rpp32f>(srcPtr1Temp[cnt]);
+                        srcPtr2Temp_ps[cnt] = static_cast<Rpp32f>(srcPtr2Temp[cnt]);
+                    }
+
+                    __m256 p1[3], p2[3];
+
+                    rpp_simd_load(rpp_load24_f32pkd3_to_f32pln3_avx, srcPtr1Temp_ps, p1);    // simd loads
+                    rpp_simd_load(rpp_load24_f32pkd3_to_f32pln3_avx, srcPtr2Temp_ps, p2);    // simd loads
+                    p1[0] = __lasx_xvffint_s_w(__lasx_xvor_v(__lasx_xvftintrz_w_s(__lasx_xvfmul_s(p1[0], avx_p255)), __lasx_xvftintrz_w_s(__lasx_xvfmul_s(p2[0], avx_p255))));    // bitwise_or computation
+                    p1[1] = __lasx_xvffint_s_w(__lasx_xvor_v(__lasx_xvftintrz_w_s(__lasx_xvfmul_s(p1[1], avx_p255)), __lasx_xvftintrz_w_s(__lasx_xvfmul_s(p2[1], avx_p255))));    // bitwise_or computation
+                    p1[2] = __lasx_xvffint_s_w(__lasx_xvor_v(__lasx_xvftintrz_w_s(__lasx_xvfmul_s(p1[2], avx_p255)), __lasx_xvftintrz_w_s(__lasx_xvfmul_s(p2[2], avx_p255))));    // bitwise_or computation
+                    p1[0] = __lasx_xvfmul_s(p1[0], avx_p1op255);
+                    p1[1] = __lasx_xvfmul_s(p1[1], avx_p1op255);
+                    p1[2] = __lasx_xvfmul_s(p1[2], avx_p1op255);
+                    rpp_simd_store(rpp_store24_f32pln3_to_f16pln3_avx, dstPtrTempR, dstPtrTempG, dstPtrTempB, p1);    // simd stores
+
                     srcPtr1Temp += vectorIncrement;
                     srcPtr2Temp += vectorIncrement;
                     dstPtrTempR += vectorIncrementPerChannel;
@@ -613,7 +725,7 @@ RppStatus bitwise_or_f16_f16_host_tensor(Rpp16f *srcPtr1,
                 dstPtrTemp = dstPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     Rpp32f srcPtr1Temp_ps[24], srcPtr2Temp_ps[24];
@@ -641,6 +753,42 @@ RppStatus bitwise_or_f16_f16_host_tensor(Rpp16f *srcPtr1,
                     p1[2] = _mm256_mul_ps(p1[2], avx_p1op255);
                     rpp_simd_store(rpp_store24_f32pln3_to_f16pkd3_avx, dstPtrTemp, p1);    // simd stores
 
+                    srcPtr1TempR += vectorIncrementPerChannel;
+                    srcPtr1TempG += vectorIncrementPerChannel;
+                    srcPtr1TempB += vectorIncrementPerChannel;
+                    srcPtr2TempR += vectorIncrementPerChannel;
+                    srcPtr2TempG += vectorIncrementPerChannel;
+                    srcPtr2TempB += vectorIncrementPerChannel;
+                    dstPtrTemp += vectorIncrement;
+                }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    Rpp32f srcPtr1Temp_ps[24], srcPtr2Temp_ps[24];
+
+                    for(int cnt = 0; cnt < vectorIncrementPerChannel; cnt++)
+                    {
+                        srcPtr1Temp_ps[cnt] = static_cast<Rpp32f>(srcPtr1TempR[cnt]);
+                        srcPtr1Temp_ps[cnt + 8] = static_cast<Rpp32f>(srcPtr1TempG[cnt]);
+                        srcPtr1Temp_ps[cnt + 16] = static_cast<Rpp32f>(srcPtr1TempB[cnt]);
+
+                        srcPtr2Temp_ps[cnt] = static_cast<Rpp32f>(srcPtr2TempR[cnt]);
+                        srcPtr2Temp_ps[cnt + 8] = static_cast<Rpp32f>(srcPtr2TempG[cnt]);
+                        srcPtr2Temp_ps[cnt + 16] = static_cast<Rpp32f>(srcPtr2TempB[cnt]);
+                    }
+
+                    __m256 p1[4], p2[4];
+
+                    rpp_simd_load(rpp_load24_f32pln3_to_f32pln3_avx, srcPtr1Temp_ps, srcPtr1Temp_ps + 8, srcPtr1Temp_ps + 16, p1);    // simd loads
+                    rpp_simd_load(rpp_load24_f32pln3_to_f32pln3_avx, srcPtr2Temp_ps, srcPtr2Temp_ps + 8, srcPtr2Temp_ps + 16, p2);    // simd loads
+                    p1[0] = __lasx_xvffint_s_w(__lasx_xvor_v(__lasx_xvftintrz_w_s(__lasx_xvfmul_s(p1[0], avx_p255)), __lasx_xvftintrz_w_s(__lasx_xvfmul_s(p2[0], avx_p255))));    // bitwise_or computation
+                    p1[1] = __lasx_xvffint_s_w(__lasx_xvor_v(__lasx_xvftintrz_w_s(__lasx_xvfmul_s(p1[1], avx_p255)), __lasx_xvftintrz_w_s(__lasx_xvfmul_s(p2[1], avx_p255))));    // bitwise_or computation
+                    p1[2] = __lasx_xvffint_s_w(__lasx_xvor_v(__lasx_xvftintrz_w_s(__lasx_xvfmul_s(p1[2], avx_p255)), __lasx_xvftintrz_w_s(__lasx_xvfmul_s(p2[2], avx_p255))));    // bitwise_or computation
+                    p1[0] = __lasx_xvfmul_s(p1[0], avx_p1op255);
+                    p1[1] = __lasx_xvfmul_s(p1[1], avx_p1op255);
+                    p1[2] = __lasx_xvfmul_s(p1[2], avx_p1op255);
+                    rpp_simd_store(rpp_store24_f32pln3_to_f16pkd3_avx, dstPtrTemp, p1);    // simd stores
+
                     srcPtr1TempR += vectorIncrementPerChannel;
                     srcPtr1TempG += vectorIncrementPerChannel;
                     srcPtr1TempB += vectorIncrementPerChannel;
@@ -678,7 +826,7 @@ RppStatus bitwise_or_f16_f16_host_tensor(Rpp16f *srcPtr1,
         // Bitwise OR without fused output-layout toggle (NHWC -> NHWC or NCHW -> NCHW)
         else
         {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             alignedLength = bufferLength & ~7;
 #endif
 
@@ -697,7 +845,7 @@ RppStatus bitwise_or_f16_f16_host_tensor(Rpp16f *srcPtr1,
                     dstPtrTemp = dstPtrRow;
 
                     int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                     {
                         Rpp32f srcPtr1Temp_ps[8], srcPtr2Temp_ps[8];
@@ -716,6 +864,29 @@ RppStatus bitwise_or_f16_f16_host_tensor(Rpp16f *srcPtr1,
                         p1[0] = _mm256_mul_ps(p1[0], avx_p1op255);
                         rpp_simd_store(rpp_store8_f32_to_f16_avx, dstPtrTemp, p1);    // simd stores
 
+                        srcPtr1Temp += vectorIncrementPerChannel;
+                        srcPtr2Temp += vectorIncrementPerChannel;
+                        dstPtrTemp += vectorIncrementPerChannel;
+                    }
+#elif defined(__loongarch_asx)
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                    {
+                        Rpp32f srcPtr1Temp_ps[8], srcPtr2Temp_ps[8];
+
+                        for(int cnt = 0; cnt < vectorIncrementPerChannel; cnt++)
+                        {
+                            srcPtr1Temp_ps[cnt] = static_cast<Rpp32f>(srcPtr1Temp[cnt]);
+                            srcPtr2Temp_ps[cnt] = static_cast<Rpp32f>(srcPtr2Temp[cnt]);
+                        }
+
+                        __m256 p1[1], p2[1];
+
+                        rpp_simd_load(rpp_load8_f32_to_f32_avx, srcPtr1Temp_ps, p1);    // simd loads
+                        rpp_simd_load(rpp_load8_f32_to_f32_avx, srcPtr2Temp_ps, p2);    // simd loads
+                        p1[0] = __lasx_xvffint_s_w(__lasx_xvor_v(__lasx_xvftintrz_w_s(__lasx_xvfmul_s(p1[0], avx_p255)), __lasx_xvftintrz_w_s(__lasx_xvfmul_s(p2[0], avx_p255))));    // bitwise_or computation
+                        p1[0] = __lasx_xvfmul_s(p1[0], avx_p1op255);
+                        rpp_simd_store(rpp_store8_f32_to_f16_avx, dstPtrTemp, p1);    // simd stores
+
                         srcPtr1Temp += vectorIncrementPerChannel;
                         srcPtr2Temp += vectorIncrementPerChannel;
                         dstPtrTemp += vectorIncrementPerChannel;
@@ -807,9 +978,15 @@ RppStatus bitwise_or_i8_i8_host_tensor(Rpp8s *srcPtr1,
 
                     rpp_simd_load(rpp_load48_i8pkd3_to_u8pln3, srcPtr1Temp, p1);    // simd loads
                     rpp_simd_load(rpp_load48_i8pkd3_to_u8pln3, srcPtr2Temp, p2);    // simd loads
+#if defined(__loongarch_sx)
+                    p1[0] = __lsx_vor_v(p1[0], p2[0]);    // bitwise_or computation
+                    p1[1] = __lsx_vor_v(p1[1], p2[1]);    // bitwise_or computation
+                    p1[2] = __lsx_vor_v(p1[2], p2[2]);    // bitwise_or computation
+#else
                     p1[0] = _mm_or_si128(p1[0], p2[0]);    // bitwise_or computation
                     p1[1] = _mm_or_si128(p1[1], p2[1]);    // bitwise_or computation
                     p1[2] = _mm_or_si128(p1[2], p2[2]);    // bitwise_or computation
+#endif
                     rpp_simd_store(rpp_store48_u8pln3_to_i8pln3, dstPtrTempR, dstPtrTempG, dstPtrTempB, p1);    // simd stores
 
                     srcPtr1Temp += vectorIncrement;
@@ -868,9 +1045,15 @@ RppStatus bitwise_or_i8_i8_host_tensor(Rpp8s *srcPtr1,
 
                     rpp_simd_load(rpp_load48_i8pln3_to_u8pln3, srcPtr1TempR, srcPtr1TempG, srcPtr1TempB, p1);    // simd loads
                     rpp_simd_load(rpp_load48_i8pln3_to_u8pln3, srcPtr2TempR, srcPtr2TempG, srcPtr2TempB, p2);    // simd loads
+#if defined(__loongarch_sx)
+                    p1[0] = __lsx_vor_v(p1[0], p2[0]);    // bitwise_or computation
+                    p1[1] = __lsx_vor_v(p1[1], p2[1]);    // bitwise_or computation
+                    p1[2] = __lsx_vor_v(p1[2], p2[2]);    // bitwise_or computation
+#else
                     p1[0] = _mm_or_si128(p1[0], p2[0]);    // bitwise_or computation
                     p1[1] = _mm_or_si128(p1[1], p2[1]);    // bitwise_or computation
                     p1[2] = _mm_or_si128(p1[2], p2[2]);    // bitwise_or computation
+#endif
                     rpp_simd_store(rpp_store48_u8pln3_to_i8pkd3, dstPtrTemp, p1);    // simd stores
 
 
@@ -932,10 +1115,17 @@ RppStatus bitwise_or_i8_i8_host_tensor(Rpp8s *srcPtr1,
                     {
                         __m128i p1, p2;
 
+#if defined(__loongarch_sx)
+                        p1 = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)srcPtr1Temp, 0));   // simd loads
+                        p2 = __lsx_vadd_b(xmm_pxConvertI8, __lsx_vld((__m128i *)srcPtr2Temp, 0));   // simd loads
+                        p1 = __lsx_vor_v(p1, p2);    // bitwise_or computation
+                        __lsx_vst(__lsx_vsub_b(p1, xmm_pxConvertI8), (__m128i *)dstPtrTemp, 0);    // simd stores
+#else
                         p1 = _mm_add_epi8(xmm_pxConvertI8, _mm_loadu_si128((__m128i *)srcPtr1Temp));   // simd loads
                         p2 = _mm_add_epi8(xmm_pxConvertI8, _mm_loadu_si128((__m128i *)srcPtr2Temp));   // simd loads
                         p1 = _mm_or_si128(p1, p2);    // bitwise_or computation
                         _mm_storeu_si128((__m128i *)dstPtrTemp, _mm_sub_epi8(p1, xmm_pxConvertI8));    // simd stores
+#endif
 
                         srcPtr1Temp += vectorIncrementPerChannel;
                         srcPtr2Temp += vectorIncrementPerChannel;
diff --git a/src/modules/cpu/kernel/bitwise_xor.hpp b/src/modules/cpu/kernel/bitwise_xor.hpp
index 4a53fe83..d5a8e78d 100644
--- a/src/modules/cpu/kernel/bitwise_xor.hpp
+++ b/src/modules/cpu/kernel/bitwise_xor.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 /* bitwiseXOR is logical operation only on U8/I8 types.*/
 
@@ -61,7 +66,7 @@ RppStatus bitwise_xor_u8_u8_host_tensor(Rpp8u *srcPtr1,
         srcPtr2Channel = srcPtr2Image + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
         dstPtrChannel = dstPtrImage;
 
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
         Rpp32u alignedLength = (bufferLength / 96) * 96;
         Rpp32u vectorIncrement = 96;
         Rpp32u vectorIncrementPerChannel = 32;
@@ -87,7 +92,7 @@ RppStatus bitwise_xor_u8_u8_host_tensor(Rpp8u *srcPtr1,
                 dstPtrTempB = dstPtrRowB;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256i p1[3], p2[3];
@@ -99,6 +104,25 @@ RppStatus bitwise_xor_u8_u8_host_tensor(Rpp8u *srcPtr1,
                     p1[2] = _mm256_xor_si256(p1[2], p2[2]);    // bitwise_xor computation
                     rpp_simd_store(rpp_store96_u8pln3_to_u8pln3, dstPtrTempR, dstPtrTempG, dstPtrTempB, p1);    // simd stores
 
+                    srcPtr1Temp += vectorIncrement;
+                    srcPtr2Temp += vectorIncrement;
+                    dstPtrTempR += vectorIncrementPerChannel;
+                    dstPtrTempG += vectorIncrementPerChannel;
+                    dstPtrTempB += vectorIncrementPerChannel;
+                }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
+                {
+                    __m256i p1[3], p2[3];
+
+                    rpp_simd_load(rpp_load96_u8pkd3_to_u8pln3, srcPtr1Temp, p1);    // simd loads
+                    rpp_simd_load(rpp_load96_u8pkd3_to_u8pln3, srcPtr2Temp, p2);    // simd loads
+
+                    p1[0] = __lasx_xvxor_v(p1[0], p2[0]);    // bitwise_xor computation
+                    p1[1] = __lasx_xvxor_v(p1[1], p2[1]);    // bitwise_xor computation
+                    p1[2] = __lasx_xvxor_v(p1[2], p2[2]);    // bitwise_xor computation
+                    rpp_simd_store(rpp_store96_u8pln3_to_u8pln3, dstPtrTempR, dstPtrTempG, dstPtrTempB, p1);    // simd stores
+
                     srcPtr1Temp += vectorIncrement;
                     srcPtr2Temp += vectorIncrement;
                     dstPtrTempR += vectorIncrementPerChannel;
@@ -148,7 +172,7 @@ RppStatus bitwise_xor_u8_u8_host_tensor(Rpp8u *srcPtr1,
                 dstPtrTemp = dstPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256i p1[3], p2[3];
@@ -160,6 +184,26 @@ RppStatus bitwise_xor_u8_u8_host_tensor(Rpp8u *srcPtr1,
                     p1[2] = _mm256_xor_si256(p1[2], p2[2]);    // bitwise_xor computation
                     rpp_simd_store(rpp_store96_u8pln3_to_u8pkd3, dstPtrTemp, p1);    // simd stores
 
+                    srcPtr1TempR += vectorIncrementPerChannel;
+                    srcPtr1TempG += vectorIncrementPerChannel;
+                    srcPtr1TempB += vectorIncrementPerChannel;
+                    srcPtr2TempR += vectorIncrementPerChannel;
+                    srcPtr2TempG += vectorIncrementPerChannel;
+                    srcPtr2TempB += vectorIncrementPerChannel;
+                    dstPtrTemp += vectorIncrement;
+                }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256i p1[3], p2[3];
+
+                    rpp_simd_load(rpp_load96_u8_avx, srcPtr1TempR, srcPtr1TempG, srcPtr1TempB, p1);    // simd loads
+                    rpp_simd_load(rpp_load96_u8_avx, srcPtr2TempR, srcPtr2TempG, srcPtr2TempB, p2);    // simd loads
+                    p1[0] = __lasx_xvxor_v(p1[0], p2[0]);    // bitwise_xor computation
+                    p1[1] = __lasx_xvxor_v(p1[1], p2[1]);    // bitwise_xor computation
+                    p1[2] = __lasx_xvxor_v(p1[2], p2[2]);    // bitwise_xor computation
+                    rpp_simd_store(rpp_store96_u8pln3_to_u8pkd3, dstPtrTemp, p1);    // simd stores
+
                     srcPtr1TempR += vectorIncrementPerChannel;
                     srcPtr1TempG += vectorIncrementPerChannel;
                     srcPtr1TempB += vectorIncrementPerChannel;
@@ -197,7 +241,7 @@ RppStatus bitwise_xor_u8_u8_host_tensor(Rpp8u *srcPtr1,
         // Bitwise XOR without fused output-layout toggle (NCHW -> NCHW for 3 channel)
         else if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NCHW))
         {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             alignedLength = bufferLength & ~31;
 #endif
 
@@ -225,7 +269,7 @@ RppStatus bitwise_xor_u8_u8_host_tensor(Rpp8u *srcPtr1,
                 dstPtrTempB = dstPtrRowB;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256i p1[3], p2[3];
@@ -237,6 +281,28 @@ RppStatus bitwise_xor_u8_u8_host_tensor(Rpp8u *srcPtr1,
                     p1[2] = _mm256_xor_si256(p1[2], p2[2]);    // bitwise_xor computation
                     rpp_simd_store(rpp_store96_u8pln3_to_u8pln3, dstPtrTempR, dstPtrTempG, dstPtrTempB, p1);
 
+                    srcPtr1TempR += vectorIncrementPerChannel;
+                    srcPtr1TempG += vectorIncrementPerChannel;
+                    srcPtr1TempB += vectorIncrementPerChannel;
+                    srcPtr2TempR += vectorIncrementPerChannel;
+                    srcPtr2TempG += vectorIncrementPerChannel;
+                    srcPtr2TempB += vectorIncrementPerChannel;
+                    dstPtrTempR += vectorIncrementPerChannel;
+                    dstPtrTempG += vectorIncrementPerChannel;
+                    dstPtrTempB += vectorIncrementPerChannel;
+                }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256i p1[3], p2[3];
+
+                    rpp_simd_load(rpp_load96_u8_avx, srcPtr1TempR, srcPtr1TempG, srcPtr1TempB, p1);    // simd loads
+                    rpp_simd_load(rpp_load96_u8_avx, srcPtr2TempR, srcPtr2TempG, srcPtr2TempB, p2);    // simd loads
+                    p1[0] = __lasx_xvxor_v(p1[0], p2[0]);    // bitwise_xor computation
+                    p1[1] = __lasx_xvxor_v(p1[1], p2[1]);    // bitwise_xor computation
+                    p1[2] = __lasx_xvxor_v(p1[2], p2[2]);    // bitwise_xor computation
+                    rpp_simd_store(rpp_store96_u8pln3_to_u8pln3, dstPtrTempR, dstPtrTempG, dstPtrTempB, p1);
+
                     srcPtr1TempR += vectorIncrementPerChannel;
                     srcPtr1TempG += vectorIncrementPerChannel;
                     srcPtr1TempB += vectorIncrementPerChannel;
@@ -280,7 +346,7 @@ RppStatus bitwise_xor_u8_u8_host_tensor(Rpp8u *srcPtr1,
         // Bitwise XOR without fused output-layout toggle (NHWC -> NHWC or NCHW -> NCHW for 1 channel)
         else
         {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             alignedLength = bufferLength & ~31;
 #endif
 
@@ -297,7 +363,7 @@ RppStatus bitwise_xor_u8_u8_host_tensor(Rpp8u *srcPtr1,
                 dstPtrTemp = dstPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256i p1, p2;
@@ -307,6 +373,20 @@ RppStatus bitwise_xor_u8_u8_host_tensor(Rpp8u *srcPtr1,
                     p1 = _mm256_xor_si256(p1, p2);    // exclusive_or computation
                     _mm256_storeu_si256((__m256i *)dstPtrTemp, p1);    // simd stores
 
+                    srcPtr1Temp += vectorIncrementPerChannel;
+                    srcPtr2Temp += vectorIncrementPerChannel;
+                    dstPtrTemp += vectorIncrementPerChannel;
+                }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256i p1, p2;
+
+                    p1 = __lasx_xvld((const __m256i *)srcPtr1Temp, 0);   // simd loads
+                    p2 = __lasx_xvld((const __m256i *)srcPtr2Temp, 0);   // simd loads
+                    p1 = __lasx_xvxor_v(p1, p2);    // exclusive_or computation
+                    __lasx_xvst(p1, (__m256i *)dstPtrTemp, 0);    // simd stores
+
                     srcPtr1Temp += vectorIncrementPerChannel;
                     srcPtr2Temp += vectorIncrementPerChannel;
                     dstPtrTemp += vectorIncrementPerChannel;
diff --git a/src/modules/cpu/kernel/blend.hpp b/src/modules/cpu/kernel/blend.hpp
index 148b7a4f..8fdf178e 100644
--- a/src/modules/cpu/kernel/blend.hpp
+++ b/src/modules/cpu/kernel/blend.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 RppStatus blend_u8_u8_host_tensor(Rpp8u *srcPtr1,
                                   Rpp8u *srcPtr2,
@@ -57,7 +62,11 @@ RppStatus blend_u8_u8_host_tensor(Rpp8u *srcPtr1,
 
         Rpp32u bufferLength = roi.xywhROI.roiWidth * layoutParams.bufferMultiplier;
 
+#ifdef __loongarch_sx
+        __m128 pMul = lsx_set1_f32(alpha);
+#else
         __m128 pMul = _mm_set1_ps(alpha);
+#endif
 
         Rpp8u *srcPtr1Channel, *srcPtr2Channel, *dstPtrChannel;
         srcPtr1Channel = srcPtr1Image + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
@@ -88,6 +97,25 @@ RppStatus blend_u8_u8_host_tensor(Rpp8u *srcPtr1,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 48)
                 {
+#ifdef __loongarch_sx
+                    __m128 p1[12], p2[12];
+
+                    rpp_simd_load(rpp_load48_u8pkd3_to_f32pln3, srcPtr1Temp, p1);    // simd loads
+                    rpp_simd_load(rpp_load48_u8pkd3_to_f32pln3, srcPtr2Temp, p2);    // simd loads
+                    p1[0] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[0], p2[0]), pMul, p2[0]);    // alpha-blending adjustment
+                    p1[1] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[1], p2[1]), pMul, p2[1]);    // alpha-blending adjustment
+                    p1[2] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[2], p2[2]), pMul, p2[2]);    // alpha-blending adjustment
+                    p1[3] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[3], p2[3]), pMul, p2[3]);    // alpha-blending adjustment
+                    p1[4] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[4], p2[4]), pMul, p2[4]);    // alpha-blending adjustment
+                    p1[5] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[5], p2[5]), pMul, p2[5]);    // alpha-blending adjustment
+                    p1[6] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[6], p2[6]), pMul, p2[6]);    // alpha-blending adjustment
+                    p1[7] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[7], p2[7]), pMul, p2[7]);    // alpha-blending adjustment
+                    p1[8] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[8], p2[8]), pMul, p2[8]);    // alpha-blending adjustment
+                    p1[9] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[9], p2[9]), pMul, p2[9]);    // alpha-blending adjustment
+                    p1[10] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[10], p2[10]), pMul, p2[10]);    // alpha-blending adjustment
+                    p1[11] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[11], p2[11]), pMul, p2[11]);    // alpha-blending adjustment
+                    rpp_simd_store(rpp_store48_f32pln3_to_u8pln3, dstPtrTempR, dstPtrTempG, dstPtrTempB, p1);    // simd stores
+#else
                     __m128 p1[12], p2[12];
 
                     rpp_simd_load(rpp_load48_u8pkd3_to_f32pln3, srcPtr1Temp, p1);    // simd loads
@@ -105,6 +133,7 @@ RppStatus blend_u8_u8_host_tensor(Rpp8u *srcPtr1,
                     p1[10] = _mm_fmadd_ps(_mm_sub_ps(p1[10], p2[10]), pMul, p2[10]);    // alpha-blending adjustment
                     p1[11] = _mm_fmadd_ps(_mm_sub_ps(p1[11], p2[11]), pMul, p2[11]);    // alpha-blending adjustment
                     rpp_simd_store(rpp_store48_f32pln3_to_u8pln3, dstPtrTempR, dstPtrTempG, dstPtrTempB, p1);    // simd stores
+#endif
 
                     srcPtr1Temp += 48;
                     srcPtr2Temp += 48;
@@ -161,6 +190,25 @@ RppStatus blend_u8_u8_host_tensor(Rpp8u *srcPtr1,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 16)
                 {
+#ifdef __loongarch_sx
+                    __m128 p1[12], p2[12];
+
+                    rpp_simd_load(rpp_load48_u8pln3_to_f32pln3, srcPtr1TempR, srcPtr1TempG, srcPtr1TempB, p1);    // simd loads
+                    rpp_simd_load(rpp_load48_u8pln3_to_f32pln3, srcPtr2TempR, srcPtr2TempG, srcPtr2TempB, p2);    // simd loads
+                    p1[0] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[0], p2[0]), pMul, p2[0]);    // alpha-blending adjustment
+                    p1[1] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[1], p2[1]), pMul, p2[1]);    // alpha-blending adjustment
+                    p1[2] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[2], p2[2]), pMul, p2[2]);    // alpha-blending adjustment
+                    p1[3] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[3], p2[3]), pMul, p2[3]);    // alpha-blending adjustment
+                    p1[4] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[4], p2[4]), pMul, p2[4]);    // alpha-blending adjustment
+                    p1[5] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[5], p2[5]), pMul, p2[5]);    // alpha-blending adjustment
+                    p1[6] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[6], p2[6]), pMul, p2[6]);    // alpha-blending adjustment
+                    p1[7] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[7], p2[7]), pMul, p2[7]);    // alpha-blending adjustment
+                    p1[8] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[8], p2[8]), pMul, p2[8]);    // alpha-blending adjustment
+                    p1[9] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[9], p2[9]), pMul, p2[9]);    // alpha-blending adjustment
+                    p1[10] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[10], p2[10]), pMul, p2[10]);    // alpha-blending adjustment
+                    p1[11] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[11], p2[11]), pMul, p2[11]);    // alpha-blending adjustment
+                    rpp_simd_store(rpp_store48_f32pln3_to_u8pkd3, dstPtrTemp, p1);    // simd stores
+#else
                     __m128 p1[12], p2[12];
 
                     rpp_simd_load(rpp_load48_u8pln3_to_f32pln3, srcPtr1TempR, srcPtr1TempG, srcPtr1TempB, p1);    // simd loads
@@ -178,6 +226,7 @@ RppStatus blend_u8_u8_host_tensor(Rpp8u *srcPtr1,
                     p1[10] = _mm_fmadd_ps(_mm_sub_ps(p1[10], p2[10]), pMul, p2[10]);    // alpha-blending adjustment
                     p1[11] = _mm_fmadd_ps(_mm_sub_ps(p1[11], p2[11]), pMul, p2[11]);    // alpha-blending adjustment
                     rpp_simd_store(rpp_store48_f32pln3_to_u8pkd3, dstPtrTemp, p1);    // simd stores
+#endif
 
                     srcPtr1TempR += 16;
                     srcPtr1TempG += 16;
@@ -234,6 +283,17 @@ RppStatus blend_u8_u8_host_tensor(Rpp8u *srcPtr1,
                     int vectorLoopCount = 0;
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += 16)
                     {
+#ifdef __loongarch_sx
+                        __m128 p1[4], p2[4];
+
+                        rpp_simd_load(rpp_load16_u8_to_f32, srcPtr1Temp, p1);    // simd loads
+                        rpp_simd_load(rpp_load16_u8_to_f32, srcPtr2Temp, p2);    // simd loads
+                        p1[0] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[0], p2[0]), pMul, p2[0]);    // alpha-blending adjustment
+                        p1[1] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[1], p2[1]), pMul, p2[1]);    // alpha-blending adjustment
+                        p1[2] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[2], p2[2]), pMul, p2[2]);    // alpha-blending adjustment
+                        p1[3] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[3], p2[3]), pMul, p2[3]);    // alpha-blending adjustment
+                        rpp_simd_store(rpp_store16_f32_to_u8, dstPtrTemp, p1);    // simd stores
+#else
                         __m128 p1[4], p2[4];
 
                         rpp_simd_load(rpp_load16_u8_to_f32, srcPtr1Temp, p1);    // simd loads
@@ -243,6 +303,7 @@ RppStatus blend_u8_u8_host_tensor(Rpp8u *srcPtr1,
                         p1[2] = _mm_fmadd_ps(_mm_sub_ps(p1[2], p2[2]), pMul, p2[2]);    // alpha-blending adjustment
                         p1[3] = _mm_fmadd_ps(_mm_sub_ps(p1[3], p2[3]), pMul, p2[3]);    // alpha-blending adjustment
                         rpp_simd_store(rpp_store16_f32_to_u8, dstPtrTemp, p1);    // simd stores
+#endif
 
                         srcPtr1Temp +=16;
                         srcPtr2Temp +=16;
@@ -303,7 +364,11 @@ RppStatus blend_f32_f32_host_tensor(Rpp32f *srcPtr1,
 
         Rpp32u bufferLength = roi.xywhROI.roiWidth * layoutParams.bufferMultiplier;
 
+#ifdef __loongarch_sx
+        __m128 pMul = lsx_set1_f32(alpha);
+#else
         __m128 pMul = _mm_set1_ps(alpha);
+#endif
 
         Rpp32f *srcPtr1Channel, *srcPtr2Channel, *dstPtrChannel;
         srcPtr1Channel = srcPtr1Image + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
@@ -334,6 +399,16 @@ RppStatus blend_f32_f32_host_tensor(Rpp32f *srcPtr1,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 12)
                 {
+#ifdef __loongarch_sx
+                    __m128 p1[4], p2[4];
+
+                    rpp_simd_load(rpp_load12_f32pkd3_to_f32pln3, srcPtr1Temp, p1);    // simd loads
+                    rpp_simd_load(rpp_load12_f32pkd3_to_f32pln3, srcPtr2Temp, p2);    // simd loads
+                    p1[0] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[0], p2[0]), pMul, p2[0]);    // alpha-blending adjustment
+                    p1[1] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[1], p2[1]), pMul, p2[1]);    // alpha-blending adjustment
+                    p1[2] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[2], p2[2]), pMul, p2[2]);    // alpha-blending adjustment
+                    rpp_simd_store(rpp_store12_f32pln3_to_f32pln3, dstPtrTempR, dstPtrTempG, dstPtrTempB, p1);    // simd stores
+#else
                     __m128 p1[4], p2[4];
 
                     rpp_simd_load(rpp_load12_f32pkd3_to_f32pln3, srcPtr1Temp, p1);    // simd loads
@@ -342,6 +417,7 @@ RppStatus blend_f32_f32_host_tensor(Rpp32f *srcPtr1,
                     p1[1] = _mm_fmadd_ps(_mm_sub_ps(p1[1], p2[1]), pMul, p2[1]);    // alpha-blending adjustment
                     p1[2] = _mm_fmadd_ps(_mm_sub_ps(p1[2], p2[2]), pMul, p2[2]);    // alpha-blending adjustment
                     rpp_simd_store(rpp_store12_f32pln3_to_f32pln3, dstPtrTempR, dstPtrTempG, dstPtrTempB, p1);    // simd stores
+#endif
 
                     srcPtr1Temp += 12;
                     srcPtr2Temp += 12;
@@ -398,6 +474,16 @@ RppStatus blend_f32_f32_host_tensor(Rpp32f *srcPtr1,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 4)
                 {
+#ifdef __loongarch_sx
+                    __m128 p1[4], p2[4];
+
+                    rpp_simd_load(rpp_load12_f32pln3_to_f32pln3, srcPtr1TempR, srcPtr1TempG, srcPtr1TempB, p1);    // simd loads
+                    rpp_simd_load(rpp_load12_f32pln3_to_f32pln3, srcPtr2TempR, srcPtr2TempG, srcPtr2TempB, p2);    // simd loads
+                    p1[0] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[0], p2[0]), pMul, p2[0]);    // alpha-blending adjustment
+                    p1[1] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[1], p2[1]), pMul, p2[1]);    // alpha-blending adjustment
+                    p1[2] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[2], p2[2]), pMul, p2[2]);    // alpha-blending adjustment
+                    rpp_simd_store(rpp_store12_f32pln3_to_f32pkd3, dstPtrTemp, p1);    // simd stores
+#else
                     __m128 p1[4], p2[4];
 
                     rpp_simd_load(rpp_load12_f32pln3_to_f32pln3, srcPtr1TempR, srcPtr1TempG, srcPtr1TempB, p1);    // simd loads
@@ -406,6 +492,7 @@ RppStatus blend_f32_f32_host_tensor(Rpp32f *srcPtr1,
                     p1[1] = _mm_fmadd_ps(_mm_sub_ps(p1[1], p2[1]), pMul, p2[1]);    // alpha-blending adjustment
                     p1[2] = _mm_fmadd_ps(_mm_sub_ps(p1[2], p2[2]), pMul, p2[2]);    // alpha-blending adjustment
                     rpp_simd_store(rpp_store12_f32pln3_to_f32pkd3, dstPtrTemp, p1);    // simd stores
+#endif
 
                     srcPtr1TempR += 4;
                     srcPtr1TempG += 4;
@@ -462,12 +549,21 @@ RppStatus blend_f32_f32_host_tensor(Rpp32f *srcPtr1,
                     int vectorLoopCount = 0;
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += 4)
                     {
+#ifdef __loongarch_sx
+                        __m128 p1[1], p2[1];
+
+                        rpp_simd_load(rpp_load4_f32_to_f32, srcPtr1Temp, p1);    // simd loads
+                        rpp_simd_load(rpp_load4_f32_to_f32, srcPtr2Temp, p2);    // simd loads
+                        p1[0] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[0], p2[0]), pMul, p2[0]);    // alpha-blending adjustment
+                        rpp_simd_store(rpp_store4_f32_to_f32, dstPtrTemp, p1);    // simd stores
+#else
                         __m128 p1[1], p2[1];
 
                         rpp_simd_load(rpp_load4_f32_to_f32, srcPtr1Temp, p1);    // simd loads
                         rpp_simd_load(rpp_load4_f32_to_f32, srcPtr2Temp, p2);    // simd loads
                         p1[0] = _mm_fmadd_ps(_mm_sub_ps(p1[0], p2[0]), pMul, p2[0]);    // alpha-blending adjustment
                         rpp_simd_store(rpp_store4_f32_to_f32, dstPtrTemp, p1);    // simd stores
+#endif
 
                         srcPtr1Temp += 4;
                         srcPtr2Temp += 4;
@@ -528,7 +624,11 @@ RppStatus blend_f16_f16_host_tensor(Rpp16f *srcPtr1,
 
         Rpp32u bufferLength = roi.xywhROI.roiWidth * layoutParams.bufferMultiplier;
 
+#ifdef __loongarch_sx
+        __m128 pMul = lsx_set1_f32(alpha);
+#else
         __m128 pMul = _mm_set1_ps(alpha);
+#endif
 
         Rpp16f *srcPtr1Channel, *srcPtr2Channel, *dstPtrChannel;
         srcPtr1Channel = srcPtr1Image + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
@@ -567,6 +667,16 @@ RppStatus blend_f16_f16_host_tensor(Rpp16f *srcPtr1,
                         *(srcPtr2Temp_ps + cnt) = (Rpp32f) *(srcPtr2Temp + cnt);
                     }
 
+#ifdef __loongarch_sx
+                    __m128 p1[4], p2[4];
+
+                    rpp_simd_load(rpp_load12_f32pkd3_to_f32pln3, srcPtr1Temp_ps, p1);    // simd loads
+                    rpp_simd_load(rpp_load12_f32pkd3_to_f32pln3, srcPtr2Temp_ps, p2);    // simd loads
+                    p1[0] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[0], p2[0]), pMul, p2[0]);    // alpha-blending adjustment
+                    p1[1] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[1], p2[1]), pMul, p2[1]);    // alpha-blending adjustment
+                    p1[2] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[2], p2[2]), pMul, p2[2]);    // alpha-blending adjustment
+                    rpp_simd_store(rpp_store12_f32pln3_to_f32pln3, dstPtrTemp_ps, dstPtrTemp_ps + 4, dstPtrTemp_ps + 8, p1);    // simd stores
+#else
                     __m128 p1[4], p2[4];
 
                     rpp_simd_load(rpp_load12_f32pkd3_to_f32pln3, srcPtr1Temp_ps, p1);    // simd loads
@@ -575,6 +685,7 @@ RppStatus blend_f16_f16_host_tensor(Rpp16f *srcPtr1,
                     p1[1] = _mm_fmadd_ps(_mm_sub_ps(p1[1], p2[1]), pMul, p2[1]);    // alpha-blending adjustment
                     p1[2] = _mm_fmadd_ps(_mm_sub_ps(p1[2], p2[2]), pMul, p2[2]);    // alpha-blending adjustment
                     rpp_simd_store(rpp_store12_f32pln3_to_f32pln3, dstPtrTemp_ps, dstPtrTemp_ps + 4, dstPtrTemp_ps + 8, p1);    // simd stores
+#endif
 
                     for(int cnt = 0; cnt < 4; cnt++)
                     {
@@ -651,6 +762,16 @@ RppStatus blend_f16_f16_host_tensor(Rpp16f *srcPtr1,
                         *(srcPtr2Temp_ps + 8 + cnt) = (Rpp32f) *(srcPtr2TempB + cnt);
                     }
 
+#ifdef __loongarch_sx
+                    __m128 p1[4], p2[4];
+
+                    rpp_simd_load(rpp_load12_f32pln3_to_f32pln3, srcPtr1Temp_ps, srcPtr1Temp_ps + 4, srcPtr1Temp_ps + 8, p1);    // simd loads
+                    rpp_simd_load(rpp_load12_f32pln3_to_f32pln3, srcPtr2Temp_ps, srcPtr2Temp_ps + 4, srcPtr2Temp_ps + 8, p2);    // simd loads
+                    p1[0] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[0], p2[0]), pMul, p2[0]);    // alpha-blending adjustment
+                    p1[1] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[1], p2[1]), pMul, p2[1]);    // alpha-blending adjustment
+                    p1[2] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[2], p2[2]), pMul, p2[2]);    // alpha-blending adjustment
+                    rpp_simd_store(rpp_store12_f32pln3_to_f32pkd3, dstPtrTemp_ps, p1);    // simd stores
+#else
                     __m128 p1[4], p2[4];
 
                     rpp_simd_load(rpp_load12_f32pln3_to_f32pln3, srcPtr1Temp_ps, srcPtr1Temp_ps + 4, srcPtr1Temp_ps + 8, p1);    // simd loads
@@ -659,6 +780,7 @@ RppStatus blend_f16_f16_host_tensor(Rpp16f *srcPtr1,
                     p1[1] = _mm_fmadd_ps(_mm_sub_ps(p1[1], p2[1]), pMul, p2[1]);    // alpha-blending adjustment
                     p1[2] = _mm_fmadd_ps(_mm_sub_ps(p1[2], p2[2]), pMul, p2[2]);    // alpha-blending adjustment
                     rpp_simd_store(rpp_store12_f32pln3_to_f32pkd3, dstPtrTemp_ps, p1);    // simd stores
+#endif
 
                     for(int cnt = 0; cnt < 12; cnt++)
                     {
@@ -728,12 +850,21 @@ RppStatus blend_f16_f16_host_tensor(Rpp16f *srcPtr1,
                             *(srcPtr2Temp_ps + cnt) = (Rpp16f) *(srcPtr2Temp + cnt);
                         }
 
+#ifdef __loongarch_sx
+                        __m128 p1[1], p2[1];
+
+                        rpp_simd_load(rpp_load4_f32_to_f32, srcPtr1Temp_ps, p1);    // simd loads
+                        rpp_simd_load(rpp_load4_f32_to_f32, srcPtr2Temp_ps, p2);    // simd loads
+                        p1[0] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[0], p2[0]), pMul, p2[0]);    // alpha-blending adjustment
+                        rpp_simd_store(rpp_store4_f32_to_f32, dstPtrTemp_ps, p1);    // simd stores
+#else
                         __m128 p1[1], p2[1];
 
                         rpp_simd_load(rpp_load4_f32_to_f32, srcPtr1Temp_ps, p1);    // simd loads
                         rpp_simd_load(rpp_load4_f32_to_f32, srcPtr2Temp_ps, p2);    // simd loads
                         p1[0] = _mm_fmadd_ps(_mm_sub_ps(p1[0], p2[0]), pMul, p2[0]);    // alpha-blending adjustment
                         rpp_simd_store(rpp_store4_f32_to_f32, dstPtrTemp_ps, p1);    // simd stores
+#endif
 
                         for(int cnt = 0; cnt < 4; cnt++)
                         {
@@ -799,7 +930,11 @@ RppStatus blend_i8_i8_host_tensor(Rpp8s *srcPtr1,
 
         Rpp32u bufferLength = roi.xywhROI.roiWidth * layoutParams.bufferMultiplier;
 
+#ifdef __loongarch_sx
+        __m128 pMul = lsx_set1_f32(alpha);
+#else
         __m128 pMul = _mm_set1_ps(alpha);
+#endif
 
         Rpp8s *srcPtr1Channel, *srcPtr2Channel, *dstPtrChannel;
         srcPtr1Channel = srcPtr1Image + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
@@ -828,6 +963,27 @@ RppStatus blend_i8_i8_host_tensor(Rpp8s *srcPtr1,
                 dstPtrTempB = dstPtrRowB;
 
                 int vectorLoopCount = 0;
+#ifdef __loongarch_sx
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += 48)
+                {
+                    __m128 p1[12], p2[12];
+
+                    rpp_simd_load(rpp_load48_i8pkd3_to_f32pln3, srcPtr1Temp, p1);    // simd loads
+                    rpp_simd_load(rpp_load48_i8pkd3_to_f32pln3, srcPtr2Temp, p2);    // simd loads
+                    p1[0] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[0], p2[0]), pMul, p2[0]);    // alpha-blending adjustment
+                    p1[1] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[1], p2[1]), pMul, p2[1]);    // alpha-blending adjustment
+                    p1[2] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[2], p2[2]), pMul, p2[2]);    // alpha-blending adjustment
+                    p1[3] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[3], p2[3]), pMul, p2[3]);    // alpha-blending adjustment
+                    p1[4] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[4], p2[4]), pMul, p2[4]);    // alpha-blending adjustment
+                    p1[5] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[5], p2[5]), pMul, p2[5]);    // alpha-blending adjustment
+                    p1[6] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[6], p2[6]), pMul, p2[6]);    // alpha-blending adjustment
+                    p1[7] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[7], p2[7]), pMul, p2[7]);    // alpha-blending adjustment
+                    p1[8] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[8], p2[8]), pMul, p2[8]);    // alpha-blending adjustment
+                    p1[9] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[9], p2[9]), pMul, p2[9]);    // alpha-blending adjustment
+                    p1[10] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[10], p2[10]), pMul, p2[10]);    // alpha-blending adjustment
+                    p1[11] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[11], p2[11]), pMul, p2[11]);    // alpha-blending adjustment
+                    rpp_simd_store(rpp_store48_f32pln3_to_i8pln3, dstPtrTempR, dstPtrTempG, dstPtrTempB, p1);    // simd stores
+#else
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 48)
                 {
                     __m128 p1[12], p2[12];
@@ -847,6 +1003,7 @@ RppStatus blend_i8_i8_host_tensor(Rpp8s *srcPtr1,
                     p1[10] = _mm_fmadd_ps(_mm_sub_ps(p1[10], p2[10]), pMul, p2[10]);    // alpha-blending adjustment
                     p1[11] = _mm_fmadd_ps(_mm_sub_ps(p1[11], p2[11]), pMul, p2[11]);    // alpha-blending adjustment
                     rpp_simd_store(rpp_store48_f32pln3_to_i8pln3, dstPtrTempR, dstPtrTempG, dstPtrTempB, p1);    // simd stores
+#endif
 
                     srcPtr1Temp += 48;
                     srcPtr2Temp += 48;
@@ -903,6 +1060,25 @@ RppStatus blend_i8_i8_host_tensor(Rpp8s *srcPtr1,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 16)
                 {
+#ifdef __loongarch_sx
+                    __m128 p1[12], p2[12];
+
+                    rpp_simd_load(rpp_load48_i8pln3_to_f32pln3, srcPtr1TempR, srcPtr1TempG, srcPtr1TempB, p1);    // simd loads
+                    rpp_simd_load(rpp_load48_i8pln3_to_f32pln3, srcPtr2TempR, srcPtr2TempG, srcPtr2TempB, p2);    // simd loads
+                    p1[0] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[0], p2[0]), pMul, p2[0]);    // alpha-blending adjustment
+                    p1[1] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[1], p2[1]), pMul, p2[1]);    // alpha-blending adjustment
+                    p1[2] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[2], p2[2]), pMul, p2[2]);    // alpha-blending adjustment
+                    p1[3] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[3], p2[3]), pMul, p2[3]);    // alpha-blending adjustment
+                    p1[4] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[4], p2[4]), pMul, p2[4]);    // alpha-blending adjustment
+                    p1[5] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[5], p2[5]), pMul, p2[5]);    // alpha-blending adjustment
+                    p1[6] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[6], p2[6]), pMul, p2[6]);    // alpha-blending adjustment
+                    p1[7] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[7], p2[7]), pMul, p2[7]);    // alpha-blending adjustment
+                    p1[8] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[8], p2[8]), pMul, p2[8]);    // alpha-blending adjustment
+                    p1[9] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[9], p2[9]), pMul, p2[9]);    // alpha-blending adjustment
+                    p1[10] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[10], p2[10]), pMul, p2[10]);    // alpha-blending adjustment
+                    p1[11] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[11], p2[11]), pMul, p2[11]);    // alpha-blending adjustment
+                    rpp_simd_store(rpp_store48_f32pln3_to_i8pkd3, dstPtrTemp, p1);    // simd stores
+#else
                     __m128 p1[12], p2[12];
 
                     rpp_simd_load(rpp_load48_i8pln3_to_f32pln3, srcPtr1TempR, srcPtr1TempG, srcPtr1TempB, p1);    // simd loads
@@ -920,6 +1096,7 @@ RppStatus blend_i8_i8_host_tensor(Rpp8s *srcPtr1,
                     p1[10] = _mm_fmadd_ps(_mm_sub_ps(p1[10], p2[10]), pMul, p2[10]);    // alpha-blending adjustment
                     p1[11] = _mm_fmadd_ps(_mm_sub_ps(p1[11], p2[11]), pMul, p2[11]);    // alpha-blending adjustment
                     rpp_simd_store(rpp_store48_f32pln3_to_i8pkd3, dstPtrTemp, p1);    // simd stores
+#endif
 
                     srcPtr1TempR += 16;
                     srcPtr1TempG += 16;
@@ -976,6 +1153,17 @@ RppStatus blend_i8_i8_host_tensor(Rpp8s *srcPtr1,
                     int vectorLoopCount = 0;
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += 16)
                     {
+#ifdef __loongarch_sx
+                        __m128 p1[4], p2[4];
+
+                        rpp_simd_load(rpp_load16_i8_to_f32, srcPtr1Temp, p1);    // simd loads
+                        rpp_simd_load(rpp_load16_i8_to_f32, srcPtr2Temp, p2);    // simd loads
+                        p1[0] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[0], p2[0]), pMul, p2[0]);    // alpha-blending adjustment
+                        p1[1] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[1], p2[1]), pMul, p2[1]);    // alpha-blending adjustment
+                        p1[2] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[2], p2[2]), pMul, p2[2]);    // alpha-blending adjustment
+                        p1[3] = __lsx_vfmadd_s(__lsx_vfsub_s(p1[3], p2[3]), pMul, p2[3]);    // alpha-blending adjustment
+                        rpp_simd_store(rpp_store16_f32_to_i8, dstPtrTemp, p1);    // simd stores
+#else
                         __m128 p1[4], p2[4];
 
                         rpp_simd_load(rpp_load16_i8_to_f32, srcPtr1Temp, p1);    // simd loads
@@ -985,6 +1173,7 @@ RppStatus blend_i8_i8_host_tensor(Rpp8s *srcPtr1,
                         p1[2] = _mm_fmadd_ps(_mm_sub_ps(p1[2], p2[2]), pMul, p2[2]);    // alpha-blending adjustment
                         p1[3] = _mm_fmadd_ps(_mm_sub_ps(p1[3], p2[3]), pMul, p2[3]);    // alpha-blending adjustment
                         rpp_simd_store(rpp_store16_f32_to_i8, dstPtrTemp, p1);    // simd stores
+#endif
 
                         srcPtr1Temp +=16;
                         srcPtr2Temp +=16;
diff --git a/src/modules/cpu/kernel/box_filter_loongarch.hpp b/src/modules/cpu/kernel/box_filter_loongarch.hpp
new file mode 100644
index 00000000..36d3fa51
--- /dev/null
+++ b/src/modules/cpu/kernel/box_filter_loongarch.hpp
@@ -0,0 +1,2881 @@
+/*
+MIT License
+
+Copyright (c) 2019 - 2024 Advanced Micro Devices, Inc.
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
+*/
+
+#include "rppdefs.h"
+#include "rpp_loongarch_common.hpp"
+#include "rpp_loongarch_filter.hpp"
+
+/* box filter algorithm explanation for U8 PLN1 3x3 kernel size variant
+Lets take an example input of 3x32 image
+x x x x x x x x x x  .. x x
+x 1 2 3 4 5 6 7 8 9 .. 32 x
+x 1 2 3 4 5 6 7 8 9 .. 32 x
+x 1 2 3 4 5 6 7 8 9 .. 32 x
+x x x x x x x x x x  .. x x
+padLength = 1 (kernelSize / 2)
+
+Below steps are followed for getting outputs for the first 0-16 locations in 1st row
+1. Process padLength number of columns in each row using raw c code (outputs for 0th location)
+2. Process remaining alignedLength number of columns in each row using SSE/AVX code (outputs for 1-16 locations)
+    - load kernel size number of rows
+    1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 .. | 32
+    1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 .. | 32
+    1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 .. | 32
+
+    - unpack lower half to 16 bits
+    1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16
+    1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16
+    1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16
+
+    - unpack higher half to 16 bits
+    16 | 17 | 18 | 19 | 20 | 21 | 22 | 23 | 24 | 25 | 26 | 27 | 28 | 29 | 30 | 31 | 32
+    16 | 17 | 18 | 19 | 20 | 21 | 22 | 23 | 24 | 25 | 26 | 27 | 28 | 29 | 30 | 31 | 32
+    16 | 17 | 18 | 19 | 20 | 21 | 22 | 23 | 24 | 25 | 26 | 27 | 28 | 29 | 30 | 31 | 32
+
+    - add the unpacked values for both lower and higher half for all the unpacked 3 rows
+    1+1+1 | 2+2+2 | 3+3+3 | ... | 16+16..16
+    17+17+17 | 18+18+18 | 19+19+19 | ... |32+32+32
+
+    - blend and shuffle and above accumalted lower half and higher values to get below outputs
+    2+2+2 | 3+3+3 | 4+4+4 | ... | 17+17..17
+    3+3+3 | 4+4+4 | 5+5+5 | ... | 18+18..18
+
+    - add 3 registers for getting outputs desired outputs
+    1+1+1 | 2+2+2 | 3+3+3 | ... | 16+16..16
+    2+2+2 | 3+3+3 | 4+4+4 | ... | 17+17..17
+    3+3+3 | 4+4+4 | 5+5+5 | ... | 18+18..18
+    result = ((1+1+1)+(2+2+2)+(3+3+3)) | ((2+2+2)+(3+3+3)+(4+4+4)) | ... | ((16+16+16)+(17+17+17)+(18+18+18))
+
+    - multiply with convolution factor
+    (1/9)*((1+1+1)+(2+2+2)+(3+3+3)) | (1/9)*((2+2+2)+(3+3+3)+(4+4+4)) | ... | (1/9)*((16+16+16)+(17+17+17)+(18+18+18))
+
+    - convert back to 8 bit and store in output
+    2 | 3 | 4 | ... | 17
+
+    Repeat the same process for remaining alignedLength columns and store in output
+3. Process remaining non aligned columns in each row again using raw c code*/
+
+// generic raw c code for box filter 
+template<typename T>
+inline void box_filter_generic_tensor(T **srcPtrTemp, T *dstPtrTemp, Rpp32s columnIndex,
+                                      Rpp32u kernelSize, Rpp32u padLength, Rpp32u unpaddedWidth, Rpp32s rowKernelLoopLimit,
+                                      Rpp32f kernelSizeInverseSquare, Rpp32u channels = 1)
+{
+    Rpp32f accum = 0.0f;
+    Rpp32s columnKernelLoopLimit = kernelSize;
+
+    // find the colKernelLoopLimit based on columnIndex
+    get_kernel_loop_limit(columnIndex, columnKernelLoopLimit, padLength, unpaddedWidth);
+    if constexpr (std::is_same<T, Rpp8s>::value)
+    {
+        for (int i = 0; i < rowKernelLoopLimit; i++)
+            for (int j = 0, k = 0 ; j < columnKernelLoopLimit; j++, k += channels)
+                accum += static_cast<Rpp32f>(srcPtrTemp[i][k] + 128);
+    }
+    else
+    {
+        for (int i = 0; i < rowKernelLoopLimit; i++)
+            for (int j = 0, k = 0 ; j < columnKernelLoopLimit; j++, k += channels)
+                accum += static_cast<Rpp32f>(srcPtrTemp[i][k]);
+
+    }
+    accum *= kernelSizeInverseSquare;
+    saturate_pixel(accum, dstPtrTemp);
+}
+
+// process padLength number of columns in each row for PLN-PLN case
+// left border pixels in image which does not have required pixels in 3x3/5x5/7x7/9x9 box, process them separately
+template<typename T>
+inline void process_left_border_columns_pln_pln(T **srcPtrTemp, T *dstPtrTemp, Rpp32u kernelSize, Rpp32u padLength,
+                                                Rpp32u unpaddedWidth, Rpp32s rowKernelLoopLimit, Rpp32f kernelSizeInverseSquare)
+{
+    for (int k = 0; k < padLength; k++)
+    {
+        box_filter_generic_tensor(srcPtrTemp, dstPtrTemp, k, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+        dstPtrTemp++;
+    }
+}
+
+// process padLength * 3 number of columns in each row for PKD-PKD case
+// left border pixels in image which does not have required pixels in 3x3/5x5/7x7/9x9 box, process them separately
+template<typename T>
+inline void process_left_border_columns_pkd_pkd(T **srcPtrTemp, T **srcPtrRow, T *dstPtrTemp, Rpp32u kernelSize, Rpp32u padLength,
+                                                Rpp32u unpaddedWidth, Rpp32s rowKernelLoopLimit, Rpp32f kernelSizeInverseSquare)
+{
+    for (int c = 0; c < 3; c++)
+    {
+        T *dstPtrTempChannel = dstPtrTemp + c;
+        for (int k = 0; k < padLength; k++)
+        {
+            box_filter_generic_tensor(srcPtrTemp, dstPtrTempChannel, k, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare, 3);
+            dstPtrTempChannel += 3;
+        }
+        increment_row_ptrs(srcPtrTemp, kernelSize, 1);
+    }
+    // reset source to initial position
+    for (int k = 0; k < kernelSize; k++)
+        srcPtrTemp[k] = srcPtrRow[k];
+}
+
+// process padLength * 3 number of columns in each row for PKD-PLN case
+// left border pixels in image which does not have required pixels in 3x3/5x5/7x7/9x9 box, process them separately
+template<typename T>
+inline void process_left_border_columns_pkd_pln(T **srcPtrTemp, T **srcPtrRow, T **dstPtrTempChannels, Rpp32u kernelSize, Rpp32u padLength,
+                                                Rpp32u unpaddedWidth, Rpp32s rowKernelLoopLimit, Rpp32f kernelSizeInverseSquare)
+{
+    for (int c = 0; c < 3; c++)
+    {
+        for (int k = 0; k < padLength; k++)
+        {
+            box_filter_generic_tensor(srcPtrTemp, dstPtrTempChannels[c], k, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare, 3);
+            dstPtrTempChannels[c] += 1;
+        }
+        increment_row_ptrs(srcPtrTemp, kernelSize, 1);
+    }
+
+    // reset source to initial position
+    for (int k = 0; k < kernelSize; k++)
+        srcPtrTemp[k] = srcPtrRow[k];
+}
+
+// -------------------- Set 0 box_filter compute functions --------------------
+
+// unpack lower half of 3 256 bit registers and add (used for 3x3 kernel size U8/I8 variants)
+inline void unpacklo_and_add_3x3_host(__m256i *pxRow, __m256i *pxDst)
+{
+    pxDst[0] = __lasx_xvilvl_b(avx_px0, pxRow[0]);
+    pxDst[0] = __lasx_xvadd_h(pxDst[0], __lasx_xvilvl_b(avx_px0, pxRow[1]));
+    pxDst[0] = __lasx_xvadd_h(pxDst[0], __lasx_xvilvl_b(avx_px0, pxRow[2]));
+}
+
+// unpack higher half of 3 256 bit registers and add (used for 3x3 kernel size U8/I8 variants)
+inline void unpackhi_and_add_3x3_host(__m256i *pxRow, __m256i *pxDst)
+{
+    pxDst[0] = __lasx_xvilvh_b(avx_px0, pxRow[0]);
+    pxDst[0] = __lasx_xvadd_h(pxDst[0], __lasx_xvilvh_b(avx_px0, pxRow[1]));
+    pxDst[0] = __lasx_xvadd_h(pxDst[0], __lasx_xvilvh_b(avx_px0, pxRow[2]));
+}
+
+// unpack lower half of 5 256 bit registers and add (used for 5x5 kernel size U8/I8 variants)
+inline void unpacklo_and_add_5x5_host(__m256i *pxRow, __m256i *pxDst)
+{
+    pxDst[0] = __lasx_xvilvh_b(avx_px0, pxRow[0]);
+    pxDst[0] = __lasx_xvadd_h(pxDst[0], __lasx_xvilvl_b(avx_px0, pxRow[1]));
+    pxDst[0] = __lasx_xvadd_h(pxDst[0], __lasx_xvilvl_b(avx_px0, pxRow[2]));
+    pxDst[0] = __lasx_xvadd_h(pxDst[0], __lasx_xvilvl_b(avx_px0, pxRow[3]));
+    pxDst[0] = __lasx_xvadd_h(pxDst[0], __lasx_xvilvl_b(avx_px0, pxRow[4]));
+}
+
+// unpack higher half of 5 256 bit registers and add (used for 5x5 kernel size U8/I8 variants)
+inline void unpackhi_and_add_5x5_host(__m256i *pxRow, __m256i *pxDst)
+{
+    pxDst[0] = __lasx_xvilvh_b(avx_px0, pxRow[0]);
+    pxDst[0] = __lasx_xvadd_h(pxDst[0], __lasx_xvilvh_b(avx_px0, pxRow[1]));
+    pxDst[0] = __lasx_xvadd_h(pxDst[0], __lasx_xvilvh_b(avx_px0, pxRow[2]));
+    pxDst[0] = __lasx_xvadd_h(pxDst[0], __lasx_xvilvh_b(avx_px0, pxRow[3]));
+    pxDst[0] = __lasx_xvadd_h(pxDst[0], __lasx_xvilvh_b(avx_px0, pxRow[4]));
+}
+
+// unpack lower half of 7 256 bit registers and add (used for 7x7 kernel size U8/I8 variants)
+inline void unpacklo_and_add_7x7_host(__m256i *pxRow, __m256i *pxDst)
+{
+    pxDst[0] = __lasx_xvilvl_b(avx_px0, pxRow[0]);
+    pxDst[0] = __lasx_xvadd_h(pxDst[0], __lasx_xvilvl_b(avx_px0, pxRow[1]));
+    pxDst[0] = __lasx_xvadd_h(pxDst[0], __lasx_xvilvl_b(avx_px0, pxRow[2]));
+    pxDst[0] = __lasx_xvadd_h(pxDst[0], __lasx_xvilvl_b(avx_px0, pxRow[3]));
+    pxDst[0] = __lasx_xvadd_h(pxDst[0], __lasx_xvilvl_b(avx_px0, pxRow[4]));
+    pxDst[0] = __lasx_xvadd_h(pxDst[0], __lasx_xvilvl_b(avx_px0, pxRow[5]));
+    pxDst[0] = __lasx_xvadd_h(pxDst[0], __lasx_xvilvl_b(avx_px0, pxRow[6]));
+}
+
+// unpack higher half of 7 256 bit registers and add (used for 7x7 kernel size U8/I8 variants)
+inline void unpackhi_and_add_7x7_host(__m256i *pxRow, __m256i *pxDst)
+{
+    pxDst[0] = __lasx_xvilvh_b(avx_px0, pxRow[0]);
+    pxDst[0] = __lasx_xvadd_h(pxDst[0], __lasx_xvilvh_b(avx_px0, pxRow[1]));
+    pxDst[0] = __lasx_xvadd_h(pxDst[0], __lasx_xvilvh_b(avx_px0, pxRow[2]));
+    pxDst[0] = __lasx_xvadd_h(pxDst[0], __lasx_xvilvh_b(avx_px0, pxRow[3]));
+    pxDst[0] = __lasx_xvadd_h(pxDst[0], __lasx_xvilvh_b(avx_px0, pxRow[4]));
+    pxDst[0] = __lasx_xvadd_h(pxDst[0], __lasx_xvilvh_b(avx_px0, pxRow[5]));
+    pxDst[0] = __lasx_xvadd_h(pxDst[0], __lasx_xvilvh_b(avx_px0, pxRow[6]));
+}
+
+// unpack lower half of 9 256 bit registers and add (used for 9x9 kernel size U8/I8 variants)
+inline void unpacklo_and_add_9x9_host(__m256i *pxRow, __m256i *pxDst)
+{
+    pxDst[0] = __lasx_xvilvl_b(avx_px0, pxRow[0]);
+    pxDst[0] = __lasx_xvadd_h(pxDst[0], __lasx_xvilvl_b(avx_px0, pxRow[1]));
+    pxDst[0] = __lasx_xvadd_h(pxDst[0], __lasx_xvilvl_b(avx_px0, pxRow[2]));
+    pxDst[0] = __lasx_xvadd_h(pxDst[0], __lasx_xvilvl_b(avx_px0, pxRow[3]));
+    pxDst[0] = __lasx_xvadd_h(pxDst[0], __lasx_xvilvl_b(avx_px0, pxRow[4]));
+    pxDst[0] = __lasx_xvadd_h(pxDst[0], __lasx_xvilvl_b(avx_px0, pxRow[5]));
+    pxDst[0] = __lasx_xvadd_h(pxDst[0], __lasx_xvilvl_b(avx_px0, pxRow[6]));
+    pxDst[0] = __lasx_xvadd_h(pxDst[0], __lasx_xvilvl_b(avx_px0, pxRow[7]));
+    pxDst[0] = __lasx_xvadd_h(pxDst[0], __lasx_xvilvl_b(avx_px0, pxRow[8]));
+}
+
+// unpack higher half of 9 256 bit registers and add (used for 9x9 kernel size U8/I8 variants)
+inline void unpackhi_and_add_9x9_host(__m256i *pxRow, __m256i *pxDst)
+{
+    pxDst[0] = __lasx_xvilvh_b(avx_px0, pxRow[0]);
+    pxDst[0] = __lasx_xvadd_h(pxDst[0], __lasx_xvilvh_b(avx_px0, pxRow[1]));
+    pxDst[0] = __lasx_xvadd_h(pxDst[0], __lasx_xvilvh_b(avx_px0, pxRow[2]));
+    pxDst[0] = __lasx_xvadd_h(pxDst[0], __lasx_xvilvh_b(avx_px0, pxRow[3]));
+    pxDst[0] = __lasx_xvadd_h(pxDst[0], __lasx_xvilvh_b(avx_px0, pxRow[4]));
+    pxDst[0] = __lasx_xvadd_h(pxDst[0], __lasx_xvilvh_b(avx_px0, pxRow[5]));
+    pxDst[0] = __lasx_xvadd_h(pxDst[0], __lasx_xvilvh_b(avx_px0, pxRow[6]));
+    pxDst[0] = __lasx_xvadd_h(pxDst[0], __lasx_xvilvh_b(avx_px0, pxRow[7]));
+    pxDst[0] = __lasx_xvadd_h(pxDst[0], __lasx_xvilvh_b(avx_px0, pxRow[8]));
+}
+
+// add 3 256 bit registers (used for 3x3 kernel size F32/F16 variants)
+inline void add_rows_3x3(__m256 *pRow, __m256 *pDst)
+{
+    pDst[0] = __lasx_xvfadd_s(pRow[0], pRow[1]);
+    pDst[0] = __lasx_xvfadd_s(pDst[0], pRow[2]);
+}
+
+// add 5 256 bit registers (used for 5x5 kernel size F32/F16 variants)
+inline void add_rows_5x5(__m256 *pRow, __m256 *pDst)
+{
+    pDst[0] = __lasx_xvfadd_s(__lasx_xvfadd_s(pRow[0], pRow[1]), pRow[2]);
+    pDst[0] = __lasx_xvfadd_s(pDst[0], __lasx_xvfadd_s(pRow[3], pRow[4]));
+}
+
+// add 7 256 bit registers (used for 7x7 kernel size F32/F16 variants)
+inline void add_rows_7x7(__m256 *pRow, __m256 *pDst)
+{
+    pDst[0] = __lasx_xvfadd_s(__lasx_xvfadd_s(pRow[0], pRow[1]), pRow[2]);
+    pDst[0] = __lasx_xvfadd_s(pDst[0], __lasx_xvfadd_s(__lasx_xvfadd_s(pRow[3], pRow[4]), pRow[5]));
+    pDst[0] = __lasx_xvfadd_s(pDst[0], pRow[6]);
+}
+
+// add 9 256 bit registers (used for 9x9 kernel size F32/F16 variants)
+inline void add_rows_9x9(__m256 *pRow, __m256 *pDst)
+{
+    pDst[0] = __lasx_xvfadd_s(__lasx_xvfadd_s(pRow[0], pRow[1]), pRow[2]);
+    pDst[0] = __lasx_xvfadd_s(pDst[0], __lasx_xvfadd_s(__lasx_xvfadd_s(pRow[3], pRow[4]), pRow[5]));
+    pDst[0] = __lasx_xvfadd_s(pDst[0], __lasx_xvfadd_s(__lasx_xvfadd_s(pRow[6], pRow[7]), pRow[8]));
+}
+
+template<typename T>
+RppStatus box_filter_char_host_tensor(T *srcPtr,
+                                      RpptDescPtr srcDescPtr,
+                                      T *dstPtr,
+                                      RpptDescPtr dstDescPtr,
+                                      Rpp32u kernelSize,
+                                      RpptROIPtr roiTensorPtrSrc,
+                                      RpptRoiType roiType,
+                                      RppLayoutParams layoutParams,
+                                      rpp::Handle& handle)
+{
+    RpptROI roiDefault = {0, 0, (Rpp32s)srcDescPtr->w, (Rpp32s)srcDescPtr->h};
+    Rpp32u numThreads = handle.GetNumThreads();
+    static_assert((std::is_same<T, Rpp8u>::value || std::is_same<T, Rpp8s>::value), "T must be Rpp8u or Rpp8s");
+
+    if ((kernelSize != 3) && (kernelSize != 5) && (kernelSize != 7) && (kernelSize != 9))
+        return box_filter_generic_host_tensor(srcPtr, srcDescPtr, dstPtr, dstDescPtr, kernelSize, roiTensorPtrSrc, roiType, layoutParams, handle);
+
+    // set the required masks array needed for shuffle operations 
+#if __loongarch_asx
+    __m128i pxMaskPln[7] = {xmm_pxMaskRotate0To1, xmm_pxMaskRotate0To3, xmm_pxMaskRotate0To5, xmm_pxMaskRotate0To7, xmm_pxMaskRotate0To9, xmm_pxMaskRotate0To11, xmm_pxMaskRotate0To13};
+    __m128i pxMaskPkd[7] = {xmm_pxMaskRotate0To5, xmm_pxMaskRotate0To11, xmm_pxMaskRotate0To1, xmm_pxMaskRotate0To7, xmm_pxMaskRotate0To13, xmm_pxMaskRotate0To3, xmm_pxMaskRotate0To9};
+#endif
+
+    omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+    for(int batchCount = 0; batchCount < dstDescPtr->n; batchCount++)
+    {
+        RpptROI roi;
+        RpptROIPtr roiPtrInput = &roiTensorPtrSrc[batchCount];
+        compute_roi_validation_host(roiPtrInput, &roi, &roiDefault, roiType);
+
+        T *srcPtrImage, *dstPtrImage;
+        srcPtrImage = srcPtr + batchCount * srcDescPtr->strides.nStride;
+        dstPtrImage = dstPtr + batchCount * dstDescPtr->strides.nStride;
+
+        Rpp32u padLength = kernelSize / 2;
+        Rpp32u bufferLength = roi.xywhROI.roiWidth * layoutParams.bufferMultiplier;
+        Rpp32u unpaddedHeight = roi.xywhROI.roiHeight - padLength;
+        Rpp32u unpaddedWidth = roi.xywhROI.roiWidth - padLength;
+
+        Rpp32f kernelSizeInverseSquare = 1.0 / (kernelSize * kernelSize);
+        Rpp16s convolutionFactor = (Rpp16s) std::ceil(65536 * kernelSizeInverseSquare);
+#if __loongarch_asx
+        const __m128i pxConvolutionFactor = __lsx_vreplgr2vr_h(convolutionFactor);
+        // set the register order needed for blend operations 
+        Rpp32u blendRegisterOrder[7] = {0, 0, 1, 1, 1, 2, 2};
+        if (srcDescPtr->layout == RpptLayout::NCHW)
+            std::fill_n(blendRegisterOrder, 7, 0);
+#endif
+        T *srcPtrChannel, *dstPtrChannel;
+        srcPtrChannel = srcPtrImage + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
+        dstPtrChannel = dstPtrImage;
+        if (kernelSize == 3)
+        {
+            T *srcPtrRow[3], *dstPtrRow;
+            for (int i = 0; i < 3; i++)
+                srcPtrRow[i] = srcPtrChannel + i * srcDescPtr->strides.hStride;
+            dstPtrRow = dstPtrChannel;
+
+            // box filter without fused output-layout toggle (NCHW -> NCHW)
+            if ((srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NCHW))
+            {
+                /* exclude 2 * padLength number of columns from alignedLength calculation
+                   since padLength number of columns from the beginning and end of each row will be computed using raw c code */
+                Rpp32u alignedLength = ((bufferLength - (2 * padLength)) / 24) * 24;
+                for (int c = 0; c < srcDescPtr->c; c++)
+                {
+                    srcPtrRow[0] = srcPtrChannel;
+                    srcPtrRow[1] = srcPtrRow[0] + srcDescPtr->strides.hStride;
+                    srcPtrRow[2] = srcPtrRow[1] + srcDescPtr->strides.hStride;
+                    dstPtrRow = dstPtrChannel;
+                    for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+                    {
+                        int vectorLoopCount = 0;
+                        bool padLengthRows = (i < padLength) ? 1: 0;
+                        T *srcPtrTemp[3] = {srcPtrRow[0], srcPtrRow[1], srcPtrRow[2]};
+                        T *dstPtrTemp = dstPtrRow;
+
+                        // get the number of rows needs to be loaded for the corresponding row
+                        Rpp32s rowKernelLoopLimit = kernelSize;
+                        get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
+                        process_left_border_columns_pln_pln(srcPtrTemp, dstPtrTemp, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                        dstPtrTemp += padLength;
+#if __loongarch_asx
+                        // process alignedLength number of columns in each row
+                        for (; vectorLoopCount < alignedLength; vectorLoopCount += 24)
+                        {
+                            __m256i pxRow[3], pxRowHalf[2], pxResult;
+                            rpp_load_box_filter_char_3x3_host(pxRow, srcPtrTemp, rowKernelLoopLimit);
+
+                            // unpack lower half and higher half of each of 3 loaded row values from 8 bit to 16 bit and add
+                            unpacklo_and_add_3x3_host(pxRow, &pxRowHalf[0]);
+                            unpackhi_and_add_3x3_host(pxRow, &pxRowHalf[1]);
+
+                            // perform blend and shuffle operations to get required order and add them
+                            __m128i pxTemp[4];
+                            extract_4sse_registers(pxRowHalf, pxTemp);
+                            blend_shuffle_add_3x3_host<1, 3>(&pxTemp[0], pxMaskPln, blendRegisterOrder);
+                            blend_shuffle_add_3x3_host<1, 3>(&pxTemp[1], pxMaskPln, blendRegisterOrder);
+                            blend_shuffle_add_3x3_host<1, 3>(&pxTemp[2], pxMaskPln, blendRegisterOrder);
+
+                            __m128i pxDst[2];
+                            pxTemp[0] = __lsx_vmuh_h(pxTemp[0], pxConvolutionFactor);
+                            pxTemp[1] = __lsx_vmuh_h(pxTemp[1], pxConvolutionFactor);
+                            pxTemp[2] = __lsx_vmuh_h(pxTemp[2], pxConvolutionFactor);
+                            pxDst[0] = lsx_packus_i16(pxTemp[0], pxTemp[1]);
+                            pxDst[1] = lsx_packus_i16(pxTemp[2], xmm_px0);
+
+                            pxResult = lasx_setr_m128i(pxDst[0], pxDst[1]);
+                            if constexpr (std::is_same<T, Rpp8s>::value)
+                                pxResult = __lasx_xvsub_b(pxResult, avx_pxConvertI8);
+
+                            __lasx_xvst(pxResult, (__m256i *)dstPtrTemp, 0);
+                            increment_row_ptrs(srcPtrTemp, kernelSize, 24);
+                            dstPtrTemp += 24;
+                        }
+#endif
+                        vectorLoopCount += padLength;
+                        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                        {
+                            box_filter_generic_tensor(srcPtrTemp, dstPtrTemp, vectorLoopCount, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                            increment_row_ptrs(srcPtrTemp, kernelSize, 1);
+                            dstPtrTemp++;
+                        }
+                        // for the first padLength rows, we need not increment the src row pointers to next rows
+                        increment_row_ptrs(srcPtrRow, kernelSize, (!padLengthRows) ? srcDescPtr->strides.hStride : 0);
+                        dstPtrRow += dstDescPtr->strides.hStride;
+                    }
+                    srcPtrChannel += srcDescPtr->strides.cStride;
+                    dstPtrChannel += dstDescPtr->strides.cStride;
+                }
+            }
+            else if ((srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NHWC))
+            {
+                /* exclude ((2 * padLength) * 3) number of columns from alignedLength calculation
+                   since (padLength * 3) number of columns from the beginning and end of each row will be computed using raw c code */
+                Rpp32u alignedLength = ((bufferLength - (2 * padLength) * 3) / 24) * 24;
+                for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+                {
+                    int vectorLoopCount = 0;
+                    bool padLengthRows = (i < padLength) ? 1: 0;
+                    T *srcPtrTemp[3] = {srcPtrRow[0], srcPtrRow[1], srcPtrRow[2]};
+                    T *dstPtrTemp = dstPtrRow;
+
+                    Rpp32s rowKernelLoopLimit = kernelSize;
+                    get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
+                    process_left_border_columns_pkd_pkd(srcPtrTemp, srcPtrRow, dstPtrTemp, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                    dstPtrTemp += padLength * 3;
+#if __loongarch_asx
+                    // process remaining columns in each row
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount += 24)
+                    {
+                        __m256i pxRow[3], pxRowHalf[2], pxResult;
+                        rpp_load_box_filter_char_3x3_host(pxRow, srcPtrTemp, rowKernelLoopLimit);
+
+                        // unpack lower half and higher half of each of 3 loaded row values from 8 bit to 16 bit and add
+                        unpacklo_and_add_3x3_host(pxRow, &pxRowHalf[0]);
+                        unpackhi_and_add_3x3_host(pxRow, &pxRowHalf[1]);
+
+                        // perform blend and shuffle operations for the first 8 output values to get required order and add them
+                        __m128i pxTemp[4];
+                        extract_4sse_registers(pxRowHalf, pxTemp);
+                        blend_shuffle_add_3x3_host<7, 63>(&pxTemp[0], pxMaskPkd, blendRegisterOrder);
+                        blend_shuffle_add_3x3_host<7, 63>(&pxTemp[1], pxMaskPkd, blendRegisterOrder);
+                        blend_shuffle_add_3x3_host<7, 63>(&pxTemp[2], pxMaskPkd, blendRegisterOrder);
+
+                        __m128i pxDst[2];
+                        pxTemp[0] = __lsx_vmuh_h(pxTemp[0], pxConvolutionFactor);
+                        pxTemp[1] = __lsx_vmuh_h(pxTemp[1], pxConvolutionFactor);
+                        pxTemp[2] = __lsx_vmuh_h(pxTemp[2], pxConvolutionFactor);
+                        pxDst[0] = lsx_packus_i16(pxTemp[0], pxTemp[1]);
+                        pxDst[1] = lsx_packus_i16(pxTemp[2], xmm_px0);
+
+                        pxResult = lasx_setr_m128i(pxDst[0], pxDst[1]);
+                        if constexpr (std::is_same<T, Rpp8s>::value)
+                            pxResult = __lasx_xvsub_b(pxResult, avx_pxConvertI8);
+
+                        __lasx_xvst(pxResult, (__m256i *)dstPtrTemp, 0);
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 24);
+                        dstPtrTemp += 24;
+                    }
+#endif
+                    vectorLoopCount += padLength * 3;
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                    {
+                        box_filter_generic_tensor(srcPtrTemp, dstPtrTemp, vectorLoopCount / 3, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare, 3);
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 1);
+                        dstPtrTemp++;
+                    }
+                    // for the first padLength rows, we need not increment the src row pointers to next rows
+                    increment_row_ptrs(srcPtrRow, kernelSize, (!padLengthRows) ? srcDescPtr->strides.hStride : 0);
+                    dstPtrRow += dstDescPtr->strides.hStride;
+                }
+            }
+            else if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
+            {
+                /* exclude ((2 * padLength) * 3) number of columns from alignedLength calculation
+                   since (padLength * 3) number of columns from the beginning and end of each row will be computed using raw c code */
+                Rpp32u alignedLength = ((bufferLength - (2 * padLength) * 3) / 24) * 24;
+                T *dstPtrChannels[3];
+                for (int i = 0; i < 3; i++)
+                    dstPtrChannels[i] = dstPtrChannel + i * dstDescPtr->strides.cStride;
+
+                for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+                {
+                    int vectorLoopCount = 0;
+                    bool padLengthRows = (i < padLength) ? 1: 0;
+                    T *srcPtrTemp[3] = {srcPtrRow[0], srcPtrRow[1], srcPtrRow[2]};
+                    T *dstPtrTempChannels[3] = {dstPtrChannels[0], dstPtrChannels[1], dstPtrChannels[2]};
+
+                    Rpp32s rowKernelLoopLimit = kernelSize;
+                    get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
+                    process_left_border_columns_pkd_pln(srcPtrTemp, srcPtrRow, dstPtrTempChannels, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+#if __loongarch_asx
+                    // process remaining columns in each row
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount += 24)
+                    {
+                        __m256i pxRow[3], pxRowHalf[2];
+                        rpp_load_box_filter_char_3x3_host(pxRow, srcPtrTemp, rowKernelLoopLimit);
+
+                        // unpack lower half and higher half of each of 3 loaded row values from 8 bit to 16 bit and add
+                        unpacklo_and_add_3x3_host(pxRow, &pxRowHalf[0]);
+                        unpackhi_and_add_3x3_host(pxRow, &pxRowHalf[1]);
+
+                        // perform blend and shuffle operations for the first 8 output values to get required order and add them
+                        __m128i pxTemp[4];
+                        extract_4sse_registers(pxRowHalf, pxTemp);
+                        blend_shuffle_add_3x3_host<7, 63>(&pxTemp[0], pxMaskPkd, blendRegisterOrder);
+                        blend_shuffle_add_3x3_host<7, 63>(&pxTemp[1], pxMaskPkd, blendRegisterOrder);
+                        blend_shuffle_add_3x3_host<7, 63>(&pxTemp[2], pxMaskPkd, blendRegisterOrder);
+
+                        __m128i pxDst[2];
+                        pxTemp[0] = __lsx_vmuh_h(pxTemp[0], pxConvolutionFactor);
+                        pxTemp[1] = __lsx_vmuh_h(pxTemp[1], pxConvolutionFactor);
+                        pxTemp[2] = __lsx_vmuh_h(pxTemp[2], pxConvolutionFactor);
+                        pxDst[0] = lsx_packus_i16(pxTemp[0], pxTemp[1]);
+                        pxDst[1] = lsx_packus_i16(pxTemp[2], xmm_px0);
+                        if constexpr (std::is_same<T, Rpp8s>::value)
+                        {
+                            pxDst[0] = __lsx_vsub_b(pxDst[0], xmm_pxConvertI8);
+                            pxDst[1] = __lsx_vsub_b(pxDst[1], xmm_pxConvertI8);
+                        }
+
+                        // convert from PKD3 to PLN3 and store channelwise
+                        __m128i pxDstChn[3];
+                        rpp_convert24_pkd3_to_pln3(pxDst[0], pxDst[1], pxDstChn);
+                        rpp_storeu_si64((__m128i *)(dstPtrTempChannels[0]), pxDstChn[0]);
+                        rpp_storeu_si64((__m128i *)(dstPtrTempChannels[1]), pxDstChn[1]);
+                        rpp_storeu_si64((__m128i *)(dstPtrTempChannels[2]), pxDstChn[2]);
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 24);
+                        increment_row_ptrs(dstPtrTempChannels, kernelSize, 8);
+                    }
+#endif
+                    vectorLoopCount += padLength * 3;
+                    for (int c = 0; vectorLoopCount < bufferLength; vectorLoopCount++, c++)
+                    {
+                        int channel = c % 3;
+                        box_filter_generic_tensor(srcPtrTemp, dstPtrTempChannels[channel], vectorLoopCount / 3, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare, 3);
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 1);
+                        dstPtrTempChannels[channel]++;
+                    }
+                    // for the first padLength rows, we need not increment the src row pointers to next rows
+                    increment_row_ptrs(srcPtrRow, kernelSize, (!padLengthRows) ? srcDescPtr->strides.hStride : 0);
+                    increment_row_ptrs(dstPtrChannels, kernelSize, dstDescPtr->strides.hStride);
+                }
+            }
+            else if ((srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NHWC))
+            {
+                /* exclude (2 * padLength) number of columns from alignedLength calculation
+                   since padLength number of columns from the beginning and end of each row will be computed using raw c code */
+                Rpp32u alignedLength = ((bufferLength - (2 * padLength)) / 24) * 24;
+                for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+                {
+                    int vectorLoopCount = 0;
+                    bool padLengthRows = (i < padLength) ? 1: 0;
+                    T *srcPtrTemp[3][3] = {
+                                            {srcPtrRow[0], srcPtrRow[1], srcPtrRow[2]},
+                                            {srcPtrRow[0] + srcDescPtr->strides.cStride, srcPtrRow[1] + srcDescPtr->strides.cStride, srcPtrRow[2] + srcDescPtr->strides.cStride},
+                                            {srcPtrRow[0] + 2 * srcDescPtr->strides.cStride, srcPtrRow[1] + 2 * srcDescPtr->strides.cStride, srcPtrRow[2] + 2 * srcDescPtr->strides.cStride}
+                                          };
+
+                    T *dstPtrTemp = dstPtrRow;
+                    // get the number of rows needs to be loaded for the corresponding row
+                    Rpp32s rowKernelLoopLimit = kernelSize;
+                    get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
+
+                    // process padLength number of columns in each row
+                    // left border pixels in image which does not have required pixels in 3x3 box, process them separately
+                    for (int k = 0; k < padLength; k++)
+                    {
+                        for (int c = 0; c < 3; c++)
+                        {
+                            box_filter_generic_tensor(srcPtrTemp[c], dstPtrTemp, k, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                            dstPtrTemp++;
+                        }
+                    }
+#if __loongarch_asx
+                    // process alignedLength number of columns in each row
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount += 24)
+                    {
+                        __m256i pxResultPln[3];
+                        for (int c = 0; c < 3; c++)
+                        {
+                            __m256i pxRow[3], pxRowHalf[2];
+                            rpp_load_box_filter_char_3x3_host(pxRow, srcPtrTemp[c], rowKernelLoopLimit);
+
+                            // unpack lower half and higher half of each of 3 loaded row values from 8 bit to 16 bit and add
+                            unpacklo_and_add_3x3_host(pxRow, &pxRowHalf[0]);
+                            unpackhi_and_add_3x3_host(pxRow, &pxRowHalf[1]);
+
+                            // perform blend and shuffle operations for the first 8 output values to get required order and add them
+                            __m128i pxTemp[4];
+                            extract_4sse_registers(pxRowHalf, pxTemp);
+                            blend_shuffle_add_3x3_host<1, 3>(&pxTemp[0], pxMaskPln, blendRegisterOrder);
+                            blend_shuffle_add_3x3_host<1, 3>(&pxTemp[1], pxMaskPln, blendRegisterOrder);
+                            blend_shuffle_add_3x3_host<1, 3>(&pxTemp[2], pxMaskPln, blendRegisterOrder);
+
+                            __m128i pxDst[2];
+                            pxTemp[0] = __lsx_vmuh_h(pxTemp[0], pxConvolutionFactor);
+                            pxTemp[1] = __lsx_vmuh_h(pxTemp[1], pxConvolutionFactor);
+                            pxTemp[2] = __lsx_vmuh_h(pxTemp[2], pxConvolutionFactor);
+                            pxDst[0] = lsx_packus_i16(pxTemp[0], pxTemp[1]);
+                            pxDst[1] = lsx_packus_i16(pxTemp[2], xmm_px0);
+
+                            pxResultPln[c] = lasx_setr_m128i(pxDst[0], pxDst[1]);
+                            increment_row_ptrs(srcPtrTemp[c], kernelSize, 24);
+                        }
+                        if constexpr (std::is_same<T, Rpp8s>::value)
+                        {
+                            pxResultPln[0] = __lasx_xvsub_b(pxResultPln[0], avx_pxConvertI8);
+                            pxResultPln[1] = __lasx_xvsub_b(pxResultPln[1], avx_pxConvertI8);
+                            pxResultPln[2] = __lasx_xvsub_b(pxResultPln[2], avx_pxConvertI8);
+                        }
+
+                        __m128i pxResultPkd[6];
+                        // convert result from pln to pkd format and store in output buffer
+                        rpp_convert72_pln3_to_pkd3(pxResultPln, pxResultPkd);
+                        __lsx_vst(pxResultPkd[0], (__m128i *)dstPtrTemp, 0);
+                        __lsx_vst(pxResultPkd[1], (__m128i *)(dstPtrTemp + 12), 0);
+                        __lsx_vst(pxResultPkd[2], (__m128i *)(dstPtrTemp + 24), 0);
+                        __lsx_vst(pxResultPkd[3], (__m128i *)(dstPtrTemp + 36), 0);
+                        __lsx_vst(pxResultPkd[4], (__m128i *)(dstPtrTemp + 48), 0);
+                        __lsx_vst(pxResultPkd[5], (__m128i *)(dstPtrTemp + 60), 0);
+                        dstPtrTemp += 72;
+                    }
+#endif
+                    vectorLoopCount += padLength;
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                    {
+                        for (int c = 0; c < 3; c++)
+                        {
+                            box_filter_generic_tensor(srcPtrTemp[c], dstPtrTemp, vectorLoopCount, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                            increment_row_ptrs(srcPtrTemp[c], kernelSize, 1);
+                            dstPtrTemp++;
+                        }
+                    }
+                    // for the first padLength rows, we need not increment the src row pointers to next rows
+                    increment_row_ptrs(srcPtrRow, kernelSize, (!padLengthRows) ? srcDescPtr->strides.hStride : 0);
+                    dstPtrRow += dstDescPtr->strides.hStride;
+                }
+            }
+        }
+        else if (kernelSize == 5)
+        {
+            T *srcPtrRow[5], *dstPtrRow;
+            for (int i = 0; i < 5; i++)
+                srcPtrRow[i] = srcPtrChannel + i * srcDescPtr->strides.hStride;
+            dstPtrRow = dstPtrChannel;
+
+            // box filter without fused output-layout toggle (NCHW -> NCHW)
+            if ((srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NCHW))
+            {
+                /* exclude (2 * padLength) number of columns from alignedLength calculation
+                   since padLength number of columns from the beginning and end of each row will be computed using raw c code */
+                Rpp32u alignedLength = ((bufferLength - (2 * padLength)) / 24) * 24;
+                for (int c = 0; c < srcDescPtr->c; c++)
+                {
+                    srcPtrRow[0] = srcPtrChannel;
+                    srcPtrRow[1] = srcPtrRow[0] + srcDescPtr->strides.hStride;
+                    srcPtrRow[2] = srcPtrRow[1] + srcDescPtr->strides.hStride;
+                    srcPtrRow[3] = srcPtrRow[2] + srcDescPtr->strides.hStride;
+                    srcPtrRow[4] = srcPtrRow[3] + srcDescPtr->strides.hStride;
+                    dstPtrRow = dstPtrChannel;
+                    for (int i = 0; i < roi.xywhROI.roiHeight; i++)
+                    {
+                        int vectorLoopCount = 0;
+                        bool padLengthRows = (i < padLength) ? 1 : 0;
+                        T *srcPtrTemp[5] = {srcPtrRow[0], srcPtrRow[1], srcPtrRow[2], srcPtrRow[3], srcPtrRow[4]};
+                        T *dstPtrTemp = dstPtrRow;
+
+                        // get the number of rows needs to be loaded for the corresponding row
+                        Rpp32s rowKernelLoopLimit = kernelSize;
+                        get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
+                        process_left_border_columns_pln_pln(srcPtrTemp, dstPtrTemp, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                        dstPtrTemp += padLength;
+#if __loongarch_asx
+                        // process alignedLength number of columns in each row
+                        for (; vectorLoopCount < alignedLength; vectorLoopCount += 24)
+                        {
+                            __m256i pxRow[5], pxRowHalf[2], pxResult;
+                            rpp_load_box_filter_char_5x5_host(pxRow, srcPtrTemp, rowKernelLoopLimit);
+
+                            // pack lower and higher half of each of 5 loaded row values from 8 bit to 16 bit and add
+                            unpacklo_and_add_5x5_host(pxRow, &pxRowHalf[0]);
+                            unpackhi_and_add_5x5_host(pxRow, &pxRowHalf[1]);
+
+                            __m128i pxTemp[4], pxDst[2];
+                            extract_4sse_registers(pxRowHalf, pxTemp);
+                            blend_shuffle_add_5x5_host<1, 3, 7, 15>(&pxTemp[0], pxMaskPln, blendRegisterOrder);
+                            blend_shuffle_add_5x5_host<1, 3, 7, 15>(&pxTemp[1], pxMaskPln, blendRegisterOrder);
+                            blend_shuffle_add_5x5_host<1, 3, 7, 15>(&pxTemp[2], pxMaskPln, blendRegisterOrder);
+
+                            pxTemp[0] = __lsx_vmuh_h(pxTemp[0], pxConvolutionFactor);
+                            pxTemp[1] = __lsx_vmuh_h(pxTemp[1], pxConvolutionFactor);
+                            pxTemp[2] = __lsx_vmuh_h(pxTemp[2], pxConvolutionFactor);
+                            pxDst[0] = lsx_packus_i16(pxTemp[0], pxTemp[1]);
+                            pxDst[1] = lsx_packus_i16(pxTemp[2], xmm_px0);
+                            pxResult = lasx_setr_m128i(pxDst[0], pxDst[1]);
+                            if constexpr (std::is_same<T, Rpp8s>::value)
+                                pxResult = __lasx_xvsub_b(pxResult, avx_pxConvertI8);
+
+                            __lasx_xvst(pxResult, (__m256i *)dstPtrTemp, 0);
+                            increment_row_ptrs(srcPtrTemp, kernelSize, 24);
+                            dstPtrTemp += 24;
+                        }
+#endif
+                        vectorLoopCount += padLength;
+                        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                        {
+                            box_filter_generic_tensor(srcPtrTemp, dstPtrTemp, vectorLoopCount, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                            increment_row_ptrs(srcPtrTemp, kernelSize, 1);
+                            dstPtrTemp++;
+                        }
+                        // for the first padLength rows, we need not increment the src row pointers to next rows
+                        increment_row_ptrs(srcPtrRow, kernelSize, (!padLengthRows) ? srcDescPtr->strides.hStride : 0);
+                        dstPtrRow += dstDescPtr->strides.hStride;
+                    }
+                    srcPtrChannel += srcDescPtr->strides.cStride;
+                    dstPtrChannel += dstDescPtr->strides.cStride;
+                }
+            }
+            else if ((srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NHWC))
+            {
+                /* exclude ((2 * padLength) * 3) number of columns from alignedLength calculation
+                   since (padLength * 3) number of columns from the beginning and end of each row will be computed using raw c code */
+                Rpp32u alignedLength = ((bufferLength - (2 * padLength) * 3) / 18) * 18;
+                for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+                {
+                    int vectorLoopCount = 0;
+                    bool padLengthRows = (i < padLength) ? 1: 0;
+                    T *srcPtrTemp[5] = {srcPtrRow[0], srcPtrRow[1], srcPtrRow[2], srcPtrRow[3], srcPtrRow[4]};
+                    T *dstPtrTemp = dstPtrRow;
+
+                    Rpp32s rowKernelLoopLimit = kernelSize;
+                    get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
+                    process_left_border_columns_pkd_pkd(srcPtrTemp, srcPtrRow, dstPtrTemp, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                    dstPtrTemp += padLength * 3;
+#if __loongarch_asx
+                    // process remaining columns in each row
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount += 18)
+                    {
+                        __m256i pxRow[5], pxRowHalf[2], pxResult;
+                        rpp_load_box_filter_char_5x5_host(pxRow, srcPtrTemp, rowKernelLoopLimit);
+
+                        // pack lower and higher half of each of 5 loaded row values from 8 bit to 16 bit and add
+                        unpacklo_and_add_5x5_host(pxRow, &pxRowHalf[0]);
+                        unpackhi_and_add_5x5_host(pxRow, &pxRowHalf[1]);
+
+                        __m128i pxTemp[5], pxDst[2];
+                        extract_4sse_registers(pxRowHalf, pxTemp);
+                        pxTemp[4] = xmm_px0;
+                        blend_shuffle_add_5x5_host<7, 63, 1, 15>(&pxTemp[0], pxMaskPkd, blendRegisterOrder);
+                        blend_shuffle_add_5x5_host<7, 63, 1, 15>(&pxTemp[1], pxMaskPkd, blendRegisterOrder);
+                        blend_shuffle_add_5x5_host<7, 63, 1, 15>(&pxTemp[2], pxMaskPkd, blendRegisterOrder);
+
+                        pxTemp[0] = __lsx_vmuh_h(pxTemp[0], pxConvolutionFactor);
+                        pxTemp[1] = __lsx_vmuh_h(pxTemp[1], pxConvolutionFactor);
+                        pxTemp[2] = __lsx_vmuh_h(pxTemp[2], pxConvolutionFactor);
+                        pxDst[0] = lsx_packus_i16(pxTemp[0], pxTemp[1]);
+                        pxDst[1] = lsx_packus_i16(pxTemp[2], xmm_px0);
+                        pxResult = lasx_setr_m128i(pxDst[0], pxDst[1]);
+                        if constexpr (std::is_same<T, Rpp8s>::value)
+                            pxResult = __lasx_xvsub_b(pxResult, avx_pxConvertI8);
+
+                        __lasx_xvst(pxResult, (__m256i *)dstPtrTemp, 0);
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 18);
+                        dstPtrTemp += 18;
+                    }
+#endif
+                    vectorLoopCount += padLength * 3;
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                    {
+                        box_filter_generic_tensor(srcPtrTemp, dstPtrTemp, vectorLoopCount / 3, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare, 3);
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 1);
+                        dstPtrTemp++;
+                    }
+                    // for the first padLength rows, we need not increment the src row pointers to next rows
+                    increment_row_ptrs(srcPtrRow, kernelSize, (!padLengthRows) ? srcDescPtr->strides.hStride : 0);
+                    dstPtrRow += dstDescPtr->strides.hStride;
+                }
+            }
+            else if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
+            {
+                /* exclude ((2 * padLength) * 3) number of columns from alignedLength calculation
+                   since (padLength * 3) number of columns from the beginning and end of each row will be computed using raw c code */
+                Rpp32u alignedLength = ((bufferLength - (2 * padLength) * 3) / 18) * 18;
+                T *dstPtrChannels[3];
+                for (int i = 0; i < 3; i++)
+                    dstPtrChannels[i] = dstPtrChannel + i * dstDescPtr->strides.cStride;
+
+                for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+                {
+                    int vectorLoopCount = 0;
+                    bool padLengthRows = (i < padLength) ? 1: 0;
+                    T *srcPtrTemp[5] = {srcPtrRow[0], srcPtrRow[1], srcPtrRow[2], srcPtrRow[3], srcPtrRow[4]};
+                    T *dstPtrTempChannels[3] = {dstPtrChannels[0], dstPtrChannels[1], dstPtrChannels[2]};
+
+                    Rpp32s rowKernelLoopLimit = kernelSize;
+                    get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
+                    process_left_border_columns_pkd_pln(srcPtrTemp, srcPtrRow, dstPtrTempChannels, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+#if __loongarch_asx
+                    // process remaining columns in each row
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount += 18)
+                    {
+                        __m256i pxRow[5], pxRowHalf[2];
+                        rpp_load_box_filter_char_5x5_host(pxRow, srcPtrTemp, rowKernelLoopLimit);
+
+                        // pack lower and higher half of each of 5 loaded row values from 8 bit to 16 bit and add
+                        unpacklo_and_add_5x5_host(pxRow, &pxRowHalf[0]);
+                        unpackhi_and_add_5x5_host(pxRow, &pxRowHalf[1]);
+
+                        __m128i pxTemp[5], pxDst[2];
+                        extract_4sse_registers(pxRowHalf, pxTemp);
+                        pxTemp[4] = xmm_px0;
+                        blend_shuffle_add_5x5_host<7, 63, 1, 15>(&pxTemp[0], pxMaskPkd, blendRegisterOrder);
+                        blend_shuffle_add_5x5_host<7, 63, 1, 15>(&pxTemp[1], pxMaskPkd, blendRegisterOrder);
+                        blend_shuffle_add_5x5_host<7, 63, 1, 15>(&pxTemp[2], pxMaskPkd, blendRegisterOrder);
+
+                        pxTemp[0] = __lsx_vmuh_h(pxTemp[0], pxConvolutionFactor);
+                        pxTemp[1] = __lsx_vmuh_h(pxTemp[1], pxConvolutionFactor);
+                        pxTemp[2] = __lsx_vmuh_h(pxTemp[2], pxConvolutionFactor);
+                        pxDst[0] = lsx_packus_i16(pxTemp[0], pxTemp[1]);
+                        pxDst[1] = lsx_packus_i16(pxTemp[2], xmm_px0);
+                        if constexpr (std::is_same<T, Rpp8s>::value)
+                        {
+                            pxDst[0] = __lsx_vsub_b(pxDst[0], xmm_pxConvertI8);
+                            pxDst[1] = __lsx_vsub_b(pxDst[1], xmm_pxConvertI8);
+                        }
+
+                        // convert from PKD3 to PLN3 and store channelwise
+                        __m128i pxDstChn[3];
+                        rpp_convert24_pkd3_to_pln3(pxDst[0], pxDst[1], pxDstChn);
+                        rpp_storeu_si64((__m128i *)(dstPtrTempChannels[0]), pxDstChn[0]);
+                        rpp_storeu_si64((__m128i *)(dstPtrTempChannels[1]), pxDstChn[1]);
+                        rpp_storeu_si64((__m128i *)(dstPtrTempChannels[2]), pxDstChn[2]);
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 18);
+                        increment_row_ptrs(dstPtrTempChannels, kernelSize, 6);
+                    }
+#endif
+                    vectorLoopCount += padLength * 3;
+                    for (int c = 0; vectorLoopCount < bufferLength; vectorLoopCount++, c++)
+                    {
+                        int channel = c % 3;
+                        box_filter_generic_tensor(srcPtrTemp, dstPtrTempChannels[channel], vectorLoopCount / 3, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare, 3);
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 1);
+                        dstPtrTempChannels[channel]++;
+                    }
+                    // for the first padLength rows, we need not increment the src row pointers to next rows
+                    increment_row_ptrs(srcPtrRow, kernelSize, (!padLengthRows) ? srcDescPtr->strides.hStride : 0);
+                    increment_row_ptrs(dstPtrChannels, kernelSize, dstDescPtr->strides.hStride);
+                }
+            }
+            else if ((srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NHWC))
+            {
+                /* exclude (2 * padLength) number of columns from alignedLength calculation
+                   since padLength * 3 number of columns from the beginning and end of each row will be computed using raw c code */
+                Rpp32u alignedLength = ((bufferLength - (2 * padLength)) / 24) * 24;
+                for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+                {
+                    int vectorLoopCount = 0;
+                    bool padLengthRows = (i < padLength) ? 1: 0;
+                    T *srcPtrTemp[3][5];
+                    for (int c = 0; c < 3; c++)
+                    {
+                        Rpp32u channelStride = c * srcDescPtr->strides.cStride;
+                        for (int k = 0; k < 5; k++)
+                            srcPtrTemp[c][k] = srcPtrRow[k] + channelStride;
+                    }
+                    T *dstPtrTemp = dstPtrRow;
+                    // get the number of rows needs to be loaded for the corresponding row
+                    Rpp32s rowKernelLoopLimit = kernelSize;
+                    get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
+
+                    // process padLength number of columns in each row
+                    // left border pixels in image which does not have required pixels in 5x5 box, process them separately
+                    for (int k = 0; k < padLength; k++)
+                    {
+                        for (int c = 0; c < 3; c++)
+                        {
+                            box_filter_generic_tensor(srcPtrTemp[c], dstPtrTemp, k, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                            dstPtrTemp++;
+                        }
+                    }
+#if __loongarch_asx
+                    // process alignedLength number of columns in each row
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount += 24)
+                    {
+                        __m256i pxResultPln[3];
+                        for (int c = 0; c < 3; c++)
+                        {
+                            __m256i pxRow[5], pxRowHalf[2], pxResult;
+                            rpp_load_box_filter_char_5x5_host(pxRow, srcPtrTemp[c], rowKernelLoopLimit);
+
+                            // pack lower and higher half of each of 5 loaded row values from 8 bit to 16 bit and add
+                            unpacklo_and_add_5x5_host(pxRow, &pxRowHalf[0]);
+                            unpackhi_and_add_5x5_host(pxRow, &pxRowHalf[1]);
+
+                            __m128i pxTemp[4], pxDst[2];
+                            extract_4sse_registers(pxRowHalf, pxTemp);
+                            blend_shuffle_add_5x5_host<1, 3, 7, 15>(&pxTemp[0], pxMaskPln, blendRegisterOrder);
+                            blend_shuffle_add_5x5_host<1, 3, 7, 15>(&pxTemp[1], pxMaskPln, blendRegisterOrder);
+                            blend_shuffle_add_5x5_host<1, 3, 7, 15>(&pxTemp[2], pxMaskPln, blendRegisterOrder);
+
+                            pxTemp[0] = __lsx_vmuh_h(pxTemp[0], pxConvolutionFactor);
+                            pxTemp[1] = __lsx_vmuh_h(pxTemp[1], pxConvolutionFactor);
+                            pxTemp[2] = __lsx_vmuh_h(pxTemp[2], pxConvolutionFactor);
+                            pxDst[0] = lsx_packus_i16(pxTemp[0], pxTemp[1]);
+                            pxDst[1] = lsx_packus_i16(pxTemp[2], xmm_px0);
+                            pxResultPln[c] = lasx_setr_m128i(pxDst[0], pxDst[1]);
+                            increment_row_ptrs(srcPtrTemp[c], kernelSize, 24);
+                        }
+                        if constexpr (std::is_same<T, Rpp8s>::value)
+                        {
+                            pxResultPln[0] = __lasx_xvsub_b(pxResultPln[0], avx_pxConvertI8);
+                            pxResultPln[1] = __lasx_xvsub_b(pxResultPln[1], avx_pxConvertI8);
+                            pxResultPln[2] = __lasx_xvsub_b(pxResultPln[2], avx_pxConvertI8);
+                        }
+
+                        __m128i pxResultPkd[6];
+                        // convert result from pln to pkd format and store in output buffer
+                        rpp_convert72_pln3_to_pkd3(pxResultPln, pxResultPkd);
+                        __lsx_vst(pxResultPkd[0], (__m128i *)dstPtrTemp, 0);
+                        __lsx_vst(pxResultPkd[1], (__m128i *)(dstPtrTemp + 12), 0);
+                        __lsx_vst(pxResultPkd[2], (__m128i *)(dstPtrTemp + 24), 0);
+                        __lsx_vst(pxResultPkd[3], (__m128i *)(dstPtrTemp + 36), 0);
+                        __lsx_vst(pxResultPkd[4], (__m128i *)(dstPtrTemp + 48), 0);
+                        __lsx_vst(pxResultPkd[5], (__m128i *)(dstPtrTemp + 60), 0);
+                        dstPtrTemp += 72;
+                    }
+#endif
+                    vectorLoopCount += padLength;
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                    {
+                        for (int c = 0; c < 3; c++)
+                        {
+                            box_filter_generic_tensor(srcPtrTemp[c], dstPtrTemp, vectorLoopCount, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                            increment_row_ptrs(srcPtrTemp[c], kernelSize, 1);
+                            dstPtrTemp++;
+                        }
+                    }
+                    // for the first padLength rows, we need not increment the src row pointers to next rows
+                    increment_row_ptrs(srcPtrRow, kernelSize, (!padLengthRows) ? srcDescPtr->strides.hStride : 0);
+                    dstPtrRow += dstDescPtr->strides.hStride;
+                }
+            }
+        }
+        else if (kernelSize == 7)
+        {
+            T *srcPtrRow[7], *dstPtrRow;
+            for (int i = 0; i < 7; i++)
+                srcPtrRow[i] = srcPtrChannel + i * srcDescPtr->strides.hStride;
+            dstPtrRow = dstPtrChannel;
+
+            // box filter without fused output-layout toggle (NCHW -> NCHW)
+            if ((srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NCHW))
+            {
+                /* exclude (2 * padLength) number of columns from alignedLength calculation
+                   since padLength number of columns from the beginning and end of each row will be computed using raw c code */
+                Rpp32u alignedLength = ((bufferLength - (2 * padLength)) / 24) * 24;
+                for (int c = 0; c < srcDescPtr->c; c++)
+                {
+                    srcPtrRow[0] = srcPtrChannel;
+                    for (int k = 1; k < 7; k++)
+                        srcPtrRow[k] = srcPtrRow[k - 1] + srcDescPtr->strides.hStride;
+                    dstPtrRow = dstPtrChannel;
+                    for (int i = 0; i < roi.xywhROI.roiHeight; i++)
+                    {
+                        int vectorLoopCount = 0;
+                        bool padLengthRows = (i < padLength) ? 1 : 0;
+                        T *srcPtrTemp[7];
+                        for (int k = 0; k < 7; k++)
+                            srcPtrTemp[k] = srcPtrRow[k];
+                        T *dstPtrTemp = dstPtrRow;
+
+                        // get the number of rows needs to be loaded for the corresponding row
+                        Rpp32s rowKernelLoopLimit = kernelSize;
+                        get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
+                        process_left_border_columns_pln_pln(srcPtrTemp, dstPtrTemp, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                        dstPtrTemp += padLength;
+#if __loongarch_asx
+                        // process alignedLength number of columns in each row
+                        for (; vectorLoopCount < alignedLength; vectorLoopCount += 24)
+                        {
+                            __m256i pxRow[7], pxRowHalf[2], pxResult;
+                            rpp_load_box_filter_char_7x7_host(pxRow, srcPtrTemp, rowKernelLoopLimit);
+
+                            // unpack lower and higher half of each of 7 loaded row values from 8 bit to 16 bit and add
+                            unpacklo_and_add_7x7_host(pxRow, &pxRowHalf[0]);
+                            unpackhi_and_add_7x7_host(pxRow, &pxRowHalf[1]);
+
+                            __m128i pxTemp[4], pxDst[2];
+                            extract_4sse_registers(pxRowHalf, pxTemp);
+                            blend_shuffle_add_7x7_host<1, 3, 7, 15, 31, 63>(&pxTemp[0], pxMaskPln, blendRegisterOrder);
+                            blend_shuffle_add_7x7_host<1, 3, 7, 15, 31, 63>(&pxTemp[1], pxMaskPln, blendRegisterOrder);
+                            blend_shuffle_add_7x7_host<1, 3, 7, 15, 31, 63>(&pxTemp[2], pxMaskPln, blendRegisterOrder);
+
+                            pxTemp[0] = __lsx_vmuh_h(pxTemp[0], pxConvolutionFactor);
+                            pxTemp[1] = __lsx_vmuh_h(pxTemp[1], pxConvolutionFactor);
+                            pxTemp[2] = __lsx_vmuh_h(pxTemp[2], pxConvolutionFactor);
+                            pxDst[0] = lsx_packus_i16(pxTemp[0], pxTemp[1]);
+                            pxDst[1] = lsx_packus_i16(pxTemp[2], xmm_px0);
+                            pxResult = lasx_setr_m128i(pxDst[0], pxDst[1]);
+                            if constexpr (std::is_same<T, Rpp8s>::value)
+                                pxResult = __lasx_xvsub_b(pxResult, avx_pxConvertI8);
+
+                            __lasx_xvst(pxResult, (__m256i *)dstPtrTemp, 0);
+                            increment_row_ptrs(srcPtrTemp, kernelSize, 24);
+                            dstPtrTemp += 24;
+                        }
+#endif
+                        vectorLoopCount += padLength;
+                        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                        {
+                            box_filter_generic_tensor(srcPtrTemp, dstPtrTemp, vectorLoopCount, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                            increment_row_ptrs(srcPtrTemp, kernelSize, 1);
+                            dstPtrTemp++;
+                        }
+                        // for the first padLength rows, we need not increment the src row pointers to next rows
+                        increment_row_ptrs(srcPtrRow, kernelSize, (!padLengthRows) ? srcDescPtr->strides.hStride : 0);
+                        dstPtrRow += dstDescPtr->strides.hStride;
+                    }
+                    srcPtrChannel += srcDescPtr->strides.cStride;
+                    dstPtrChannel += dstDescPtr->strides.cStride;
+                }
+            }
+            else if ((srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NHWC))
+            {
+                /* exclude (2 * padLength) number of columns from alignedLength calculation
+                   since padLength number of columns from the beginning and end of each row will be computed using raw c code */
+                Rpp32u alignedLength = ((bufferLength - (2 * padLength)) / 24) * 24;
+                for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+                {
+                    int vectorLoopCount = 0;
+                    bool padLengthRows = (i < padLength) ? 1: 0;
+                    T *srcPtrTemp[3][7];
+                    for (int c = 0; c < 3; c++)
+                    {
+                        Rpp32u channelStride = c * srcDescPtr->strides.cStride;
+                        for (int k = 0; k < 7; k++)
+                            srcPtrTemp[c][k] = srcPtrRow[k] + channelStride;
+                    }
+                    T *dstPtrTemp = dstPtrRow;
+                    // get the number of rows needs to be loaded for the corresponding row
+                    Rpp32s rowKernelLoopLimit = kernelSize;
+                    get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
+
+                    // process padLength number of columns in each row
+                    // left border pixels in image which does not have required pixels in 7x7 box, process them separately
+                    for (int k = 0; k < padLength; k++)
+                    {
+                        for (int c = 0; c < 3; c++)
+                        {
+                            box_filter_generic_tensor(srcPtrTemp[c], dstPtrTemp, k, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                            dstPtrTemp++;
+                        }
+                    }
+#if __loongarch_asx
+                    // process alignedLength number of columns in each row
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount += 24)
+                    {
+                        __m256i pxResultPln[3];
+                        for (int c = 0; c < 3; c++)
+                        {
+                            __m256i pxRow[7], pxRowHalf[2], pxResult;
+                            rpp_load_box_filter_char_7x7_host(pxRow, srcPtrTemp[c], rowKernelLoopLimit);
+
+                            // unpack lower and higher half of each of 7 loaded row values from 8 bit to 16 bit and add
+                            unpacklo_and_add_7x7_host(pxRow, &pxRowHalf[0]);
+                            unpackhi_and_add_7x7_host(pxRow, &pxRowHalf[1]);
+
+                            __m128i pxTemp[4], pxDst[2];
+                            extract_4sse_registers(pxRowHalf, pxTemp);
+                            blend_shuffle_add_7x7_host<1, 3, 7, 15, 31, 63>(&pxTemp[0], pxMaskPln, blendRegisterOrder);
+                            blend_shuffle_add_7x7_host<1, 3, 7, 15, 31, 63>(&pxTemp[1], pxMaskPln, blendRegisterOrder);
+                            blend_shuffle_add_7x7_host<1, 3, 7, 15, 31, 63>(&pxTemp[2], pxMaskPln, blendRegisterOrder);
+
+                            pxTemp[0] = __lsx_vmuh_h(pxTemp[0], pxConvolutionFactor);
+                            pxTemp[1] = __lsx_vmuh_h(pxTemp[1], pxConvolutionFactor);
+                            pxTemp[2] = __lsx_vmuh_h(pxTemp[2], pxConvolutionFactor);
+                            pxDst[0] = lsx_packus_i16(pxTemp[0], pxTemp[1]);
+                            pxDst[1] = lsx_packus_i16(pxTemp[2], xmm_px0);
+                            pxResultPln[c] = lasx_setr_m128i(pxDst[0], pxDst[1]);
+                            increment_row_ptrs(srcPtrTemp[c], kernelSize, 24);
+                        }
+                        if constexpr (std::is_same<T, Rpp8s>::value)
+                        {
+                            pxResultPln[0] = __lasx_xvsub_b(pxResultPln[0], avx_pxConvertI8);
+                            pxResultPln[1] = __lasx_xvsub_b(pxResultPln[1], avx_pxConvertI8);
+                            pxResultPln[2] = __lasx_xvsub_b(pxResultPln[2], avx_pxConvertI8);
+                        }
+
+                        __m128i pxResultPkd[6];
+                        // convert result from pln to pkd format and store in output buffer
+                        rpp_convert72_pln3_to_pkd3(pxResultPln, pxResultPkd);
+                        __lsx_vst(pxResultPkd[0], (__m128i *)dstPtrTemp, 0);
+                        __lsx_vst(pxResultPkd[1], (__m128i *)(dstPtrTemp + 12), 0);
+                        __lsx_vst(pxResultPkd[2], (__m128i *)(dstPtrTemp + 24), 0);
+                        __lsx_vst(pxResultPkd[3], (__m128i *)(dstPtrTemp + 36), 0);
+                        __lsx_vst(pxResultPkd[4], (__m128i *)(dstPtrTemp + 48), 0);
+                        __lsx_vst(pxResultPkd[5], (__m128i *)(dstPtrTemp + 60), 0);
+                        dstPtrTemp += 72;
+                    }
+#endif
+                    vectorLoopCount += padLength;
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                    {
+                        for (int c = 0; c < 3; c++)
+                        {
+                            box_filter_generic_tensor(srcPtrTemp[c], dstPtrTemp, vectorLoopCount, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                            increment_row_ptrs(srcPtrTemp[c], kernelSize, 1);
+                            dstPtrTemp++;
+                        }
+                    }
+                    // for the first padLength rows, we need not increment the src row pointers to next rows
+                    increment_row_ptrs(srcPtrRow, kernelSize, (!padLengthRows) ? srcDescPtr->strides.hStride : 0);
+                    dstPtrRow += dstDescPtr->strides.hStride;
+                }
+            }
+            else if ((srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NHWC))
+            {
+                /* exclude ((2 * padLength) * 3) number of columns from alignedLength calculation
+                   since (padLength * 3) number of columns from the beginning and end of each row will be computed using raw c code */
+                Rpp32u alignedLength = ((bufferLength - 2 * padLength * 3) / 12) * 12;
+                for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+                {
+                    int vectorLoopCount = 0;
+                    bool padLengthRows = (i < padLength) ? 1: 0;
+                    T *srcPtrTemp[7];
+                    for (int k = 0; k < 7; k++)
+                        srcPtrTemp[k] = srcPtrRow[k];
+                    T *dstPtrTemp = dstPtrRow;
+
+                    Rpp32s rowKernelLoopLimit = kernelSize;
+                    get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
+                    process_left_border_columns_pkd_pkd(srcPtrTemp, srcPtrRow, dstPtrTemp, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                    dstPtrTemp += padLength * 3;
+#if __loongarch_asx
+                    // process remaining columns in each row
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount += 12)
+                    {
+                        __m256i pxRow[7], pxRowHalf[2];
+                        rpp_load_box_filter_char_7x7_host(pxRow, srcPtrTemp, rowKernelLoopLimit);
+
+                        // unpack lower and higher half of each of 7 loaded row values from 8 bit to 16 bit and add
+                        unpacklo_and_add_7x7_host(pxRow, &pxRowHalf[0]);
+                        unpackhi_and_add_7x7_host(pxRow, &pxRowHalf[1]);
+
+                        __m128i pxTemp[4], pxResult;
+                        extract_4sse_registers(pxRowHalf, pxTemp);
+                        blend_shuffle_add_7x7_host<7, 63, 1, 15, 127, 3>(&pxTemp[0], pxMaskPkd, blendRegisterOrder);
+                        blend_shuffle_add_7x7_host<7, 63, 1, 15, 127, 3>(&pxTemp[1], pxMaskPkd, blendRegisterOrder);
+                        pxTemp[0] = __lsx_vmuh_h(pxTemp[0], pxConvolutionFactor);
+                        pxTemp[1] = __lsx_vmuh_h(pxTemp[1], pxConvolutionFactor);
+                        pxResult = lsx_packus_i16(pxTemp[0], pxTemp[1]);
+                        if constexpr (std::is_same<T, Rpp8s>::value)
+                            pxResult = __lsx_vsub_b(pxResult, xmm_pxConvertI8);
+
+                        __lsx_vst(pxResult, (__m128i*)dstPtrTemp, 0);
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 12);
+                        dstPtrTemp += 12;
+                    }
+#endif
+                    vectorLoopCount += padLength * 3;
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                    {
+                        box_filter_generic_tensor(srcPtrTemp, dstPtrTemp, vectorLoopCount / 3, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare, 3);
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 1);
+                        dstPtrTemp++;
+                    }
+                    // for the first padLength rows, we need not increment the src row pointers to next rows
+                    increment_row_ptrs(srcPtrRow, kernelSize, (!padLengthRows) ? srcDescPtr->strides.hStride : 0);
+                    dstPtrRow += dstDescPtr->strides.hStride;
+                }
+            }
+            else if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
+            {
+                /* exclude ((2 * padLength) * 3) number of columns from alignedLength calculation
+                   since (padLength * 3) number of columns from the beginning and end of each row will be computed using raw c code */
+                Rpp32u alignedLength = ((bufferLength - 2 * padLength * 3) / 12) * 12;
+                T *dstPtrChannels[3];
+                for (int i = 0; i < 3; i++)
+                    dstPtrChannels[i] = dstPtrChannel + i * dstDescPtr->strides.cStride;
+
+                for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+                {
+                    int vectorLoopCount = 0;
+                    bool padLengthRows = (i < padLength) ? 1: 0;
+                    T *srcPtrTemp[7] = {srcPtrRow[0], srcPtrRow[1], srcPtrRow[2], srcPtrRow[3], srcPtrRow[4], srcPtrRow[5], srcPtrRow[6]};
+                    T *dstPtrTempChannels[3] = {dstPtrChannels[0], dstPtrChannels[1], dstPtrChannels[2]};
+
+                    Rpp32s rowKernelLoopLimit = kernelSize;
+                    get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
+                    process_left_border_columns_pkd_pln(srcPtrTemp, srcPtrRow, dstPtrTempChannels, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+#if __loongarch_asx
+                    // process remaining columns in each row
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount += 12)
+                    {
+                        __m256i pxRow[7], pxRowHalf[2];
+                        rpp_load_box_filter_char_7x7_host(pxRow, srcPtrTemp, rowKernelLoopLimit);
+
+                        // unpack lower and higher half of each of 7 loaded row values from 8 bit to 16 bit and add
+                        unpacklo_and_add_7x7_host(pxRow, &pxRowHalf[0]);
+                        unpackhi_and_add_7x7_host(pxRow, &pxRowHalf[1]);
+
+                        __m128i pxTemp[4], pxResult[2];
+                        extract_4sse_registers(pxRowHalf, pxTemp);
+                        blend_shuffle_add_7x7_host<7, 63, 1, 15, 127, 3>(&pxTemp[0], pxMaskPkd, blendRegisterOrder);
+                        blend_shuffle_add_7x7_host<7, 63, 1, 15, 127, 3>(&pxTemp[1], pxMaskPkd, blendRegisterOrder);
+                        pxTemp[0] = __lsx_vmuh_h(pxTemp[0], pxConvolutionFactor);
+                        pxTemp[1] = __lsx_vmuh_h(pxTemp[1], pxConvolutionFactor);
+                        pxResult[0] = lsx_packus_i16(pxTemp[0], pxTemp[1]);
+                        pxResult[1] = xmm_px0;
+                        if constexpr (std::is_same<T, Rpp8s>::value)
+                            pxResult[0] = __lsx_vsub_b(pxResult[0], xmm_pxConvertI8);
+
+                        // convert from PKD3 to PLN3 and store channelwise
+                        __m128i pxDstChn[3];
+                        rpp_convert24_pkd3_to_pln3(pxResult[0], pxResult[1], pxDstChn);
+                        rpp_storeu_si64((__m128i *)(dstPtrTempChannels[0]), pxDstChn[0]);
+                        rpp_storeu_si64((__m128i *)(dstPtrTempChannels[1]), pxDstChn[1]);
+                        rpp_storeu_si64((__m128i *)(dstPtrTempChannels[2]), pxDstChn[2]);
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 12);
+                        increment_row_ptrs(dstPtrTempChannels, kernelSize, 4);
+                    }
+#endif
+                    vectorLoopCount += padLength * 3;
+                    for (int c = 0; vectorLoopCount < bufferLength; vectorLoopCount++, c++)
+                    {
+                        int channel = c % 3;
+                        box_filter_generic_tensor(srcPtrTemp, dstPtrTempChannels[channel], vectorLoopCount / 3, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare, 3);
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 1);
+                        dstPtrTempChannels[channel]++;
+                    }
+                    // for the first padLength rows, we need not increment the src row pointers to next rows
+                    increment_row_ptrs(srcPtrRow, kernelSize, (!padLengthRows) ? srcDescPtr->strides.hStride : 0);
+                    increment_row_ptrs(dstPtrChannels, kernelSize, dstDescPtr->strides.hStride);
+                }
+            }
+        }
+        else if (kernelSize == 9)
+        {
+            T *srcPtrRow[9], *dstPtrRow;
+            for (int i = 0; i < 9; i++)
+                srcPtrRow[i] = srcPtrChannel + i * srcDescPtr->strides.hStride;
+            dstPtrRow = dstPtrChannel;
+            if ((srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NCHW))
+            {
+                /* exclude (2 * padLength) number of columns from alignedLength calculation
+                   since padLength number of columns from the beginning and end of each row will be computed using raw c code */
+                Rpp32u alignedLength = ((bufferLength - (2 * padLength)) / 16) * 16;
+                for (int c = 0; c < srcDescPtr->c; c++)
+                {
+                    srcPtrRow[0] = srcPtrChannel;
+                    for (int k = 1; k < 9; k++)
+                        srcPtrRow[k] = srcPtrRow[k - 1] + srcDescPtr->strides.hStride;
+                    dstPtrRow = dstPtrChannel;
+                    for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+                    {
+                        int vectorLoopCount = 0;
+                        bool padLengthRows = (i < padLength) ? 1: 0;
+                        T *srcPtrTemp[9];
+                        for (int k = 0; k < 9; k++)
+                            srcPtrTemp[k] = srcPtrRow[k];
+                        T *dstPtrTemp = dstPtrRow;
+
+                        Rpp32s rowKernelLoopLimit = kernelSize;
+                        get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
+                        process_left_border_columns_pln_pln(srcPtrTemp, dstPtrTemp, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                        dstPtrTemp += padLength;
+#if __loongarch_asx
+                        // process alignedLength number of columns in each row
+                        for (; vectorLoopCount < alignedLength; vectorLoopCount += 16)
+                        {
+                            __m256i pxRow[9], pxRowHalf[2];
+                            rpp_load_box_filter_char_9x9_host(pxRow, srcPtrTemp, rowKernelLoopLimit);
+
+                            // unpack lower half and higher half of each of 9 loaded row values from 8 bit to 16 bit and add
+                            unpacklo_and_add_9x9_host(pxRow, &pxRowHalf[0]);
+                            unpackhi_and_add_9x9_host(pxRow, &pxRowHalf[1]);
+
+                            __m128i pxTemp[3], pxDst;
+                            extract_3sse_registers(pxRowHalf, pxTemp);
+                            blend_shuffle_add_9x9_host<1, 3, 7, 15, 31, 63, 127>(&pxTemp[0], pxMaskPln, blendRegisterOrder);
+                            blend_shuffle_add_9x9_host<1, 3, 7, 15, 31, 63, 127>(&pxTemp[1], pxMaskPln, blendRegisterOrder);
+                            pxTemp[0] = __lsx_vmuh_h(pxTemp[0], pxConvolutionFactor);
+                            pxTemp[1] = __lsx_vmuh_h(pxTemp[1], pxConvolutionFactor);
+                            pxDst = lsx_packus_i16(pxTemp[0], pxTemp[1]);
+                            if constexpr (std::is_same<T, Rpp8s>::value)
+                                pxDst = __lsx_vsub_b(pxDst, xmm_pxConvertI8);
+
+                            __lsx_vst(pxDst, (__m128i *)dstPtrTemp, 0);
+                            increment_row_ptrs(srcPtrTemp, kernelSize, 16);
+                            dstPtrTemp += 16;
+                        }
+#endif
+                        vectorLoopCount += padLength;
+                        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                        {
+                            box_filter_generic_tensor(srcPtrTemp, dstPtrTemp, vectorLoopCount, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                            increment_row_ptrs(srcPtrTemp, kernelSize, 1);
+                            dstPtrTemp++;
+                        }
+                        // for the first padLength rows, we need not increment the src row pointers to next rows
+                        increment_row_ptrs(srcPtrRow, kernelSize, (!padLengthRows) ? srcDescPtr->strides.hStride : 0);
+                        dstPtrRow += dstDescPtr->strides.hStride;
+                    }
+                    srcPtrChannel += srcDescPtr->strides.cStride;
+                    dstPtrChannel += dstDescPtr->strides.cStride;
+                }
+            }
+            else if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NHWC))
+            {
+                /* exclude ((2 * padLength) * 3) number of columns from alignedLength calculation
+                   since (padLength * 3) number of columns from the beginning and end of each row will be computed using raw c code */
+                Rpp32u alignedLength = ((bufferLength - (2 * padLength) * 3) / 64) * 64;
+                for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+                {
+                    int vectorLoopCount = 0;
+                    bool padLengthRows = (i < padLength) ? 1: 0;
+                    T *srcPtrTemp[9];
+                    for (int k = 0; k < 9; k++)
+                        srcPtrTemp[k] = srcPtrRow[k];
+                    T *dstPtrTemp = dstPtrRow;
+
+                    Rpp32s rowKernelLoopLimit = kernelSize;
+                    get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
+                    process_left_border_columns_pkd_pkd(srcPtrTemp, srcPtrRow, dstPtrTemp, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                    dstPtrTemp += padLength * 3;
+#if __loongarch_asx
+                    // load first 32 elements elements
+                    __m256i pxRow[9];
+                    if (alignedLength)
+                        rpp_load_box_filter_char_9x9_host(pxRow, srcPtrTemp, rowKernelLoopLimit);
+
+                    // process alignedLength number of columns in each row
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount += 32)
+                    {
+                        __m256i pxRowHalf[2], pxResult;
+                        unpacklo_and_add_9x9_host(pxRow, &pxRowHalf[0]);
+                        unpackhi_and_add_9x9_host(pxRow, &pxRowHalf[1]);
+
+                        // get the accumalated result for first 8 elements
+                        __m128i px128[8], pxTemp[7], pxDst[4];
+                        extract_4sse_registers(pxRowHalf, &px128[0]);
+                        blend_shuffle_add_9x9_host<7, 63, 1, 15, 127, 3, 31>(&px128[0], pxMaskPkd, blendRegisterOrder);
+
+                        // compute for next 8 elements
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 32);
+                        rpp_load_box_filter_char_9x9_host(pxRow, srcPtrTemp, rowKernelLoopLimit);
+                        unpacklo_and_add_9x9_host(pxRow, &pxRowHalf[0]);
+                        unpackhi_and_add_9x9_host(pxRow, &pxRowHalf[1]);
+
+                        // get the accumalated result for next 24 elements
+                        extract_4sse_registers(pxRowHalf, &px128[4]);
+                        blend_shuffle_add_9x9_host<7, 63, 1, 15, 127, 3, 31>(&px128[1], pxMaskPkd, blendRegisterOrder);
+                        blend_shuffle_add_9x9_host<7, 63, 1, 15, 127, 3, 31>(&px128[2], pxMaskPkd, blendRegisterOrder);
+                        blend_shuffle_add_9x9_host<7, 63, 1, 15, 127, 3, 31>(&px128[3], pxMaskPkd, blendRegisterOrder);
+
+                        // compute final result
+                        pxDst[0] = __lsx_vmuh_h(px128[0], pxConvolutionFactor);
+                        pxDst[1] = __lsx_vmuh_h(px128[1], pxConvolutionFactor);
+                        pxDst[2] = __lsx_vmuh_h(px128[2], pxConvolutionFactor);
+                        pxDst[3] = __lsx_vmuh_h(px128[3], pxConvolutionFactor);
+                        pxDst[0] = lsx_packus_i16(pxDst[0], pxDst[1]);
+                        pxDst[1] = lsx_packus_i16(pxDst[2], pxDst[3]);
+                        pxResult = lasx_setr_m128i(pxDst[0], pxDst[1]);
+                        if constexpr (std::is_same<T, Rpp8s>::value)
+                            pxResult = __lasx_xvsub_b(pxResult, avx_pxConvertI8);
+
+                        __lasx_xvst(pxResult, (__m256i *)dstPtrTemp, 0);
+                        dstPtrTemp += 32;
+                    }
+#endif
+                    vectorLoopCount += padLength * 3;
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                    {
+                        box_filter_generic_tensor(srcPtrTemp, dstPtrTemp, vectorLoopCount / 3, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare, 3);
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 1);
+                        dstPtrTemp++;
+                    }
+                    // for the first padLength rows, we need not increment the src row pointers to next rows
+                    increment_row_ptrs(srcPtrRow, kernelSize, (!padLengthRows) ? srcDescPtr->strides.hStride : 0);
+                    dstPtrRow += dstDescPtr->strides.hStride;
+                }
+            }
+            else if ((srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NHWC))
+            {
+                /* exclude (2 * padLength) number of columns from alignedLength calculation
+                   since padLength number of columns from the beginning and end of each row will be computed using raw c code */
+                Rpp32u alignedLength = ((bufferLength - (2 * padLength)) / 16) * 16;
+                for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+                {
+                    int vectorLoopCount = 0;
+                    bool padLengthRows = (i < padLength) ? 1: 0;
+                    T *srcPtrTemp[3][9];
+                    for (int c = 0; c < 3; c++)
+                    {
+                        Rpp32u channelStride = c * srcDescPtr->strides.cStride;
+                        for (int k = 0; k < 9; k++)
+                            srcPtrTemp[c][k] = srcPtrRow[k] + channelStride;
+                    }
+                    T *dstPtrTemp = dstPtrRow;
+
+                    Rpp32s rowKernelLoopLimit = kernelSize;
+                    get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
+
+                    // process padLength number of columns in each row
+                    for (int k = 0; k < padLength; k++)
+                    {
+                        for (int c = 0; c < 3; c++)
+                        {
+                            box_filter_generic_tensor(srcPtrTemp[c], dstPtrTemp, k, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                            dstPtrTemp++;
+                        }
+                    }
+#if __loongarch_asx
+                    // process alignedLength number of columns in each row
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount += 16)
+                    {
+                        __m128i pxResultPln[3];
+                        for (int c = 0; c < 3; c++)
+                        {
+                            __m256i pxRow[9], pxRowHalf[2];
+                            rpp_load_box_filter_char_9x9_host(pxRow, srcPtrTemp[c], rowKernelLoopLimit);
+
+                            // unpack lower half and higher half of each of 9 loaded row values from 8 bit to 16 bit and add
+                            unpacklo_and_add_9x9_host(pxRow, &pxRowHalf[0]);
+                            unpackhi_and_add_9x9_host(pxRow, &pxRowHalf[1]);
+
+                            __m128i pxTemp[3], pxDst;
+                            extract_3sse_registers(pxRowHalf, pxTemp);
+                            blend_shuffle_add_9x9_host<1, 3, 7, 15, 31, 63, 127>(&pxTemp[0], pxMaskPln, blendRegisterOrder);
+                            blend_shuffle_add_9x9_host<1, 3, 7, 15, 31, 63, 127>(&pxTemp[1], pxMaskPln, blendRegisterOrder);
+                            pxTemp[0] = __lsx_vmuh_h(pxTemp[0], pxConvolutionFactor);
+                            pxTemp[1] = __lsx_vmuh_h(pxTemp[1], pxConvolutionFactor);
+                            pxResultPln[c] = lsx_packus_i16(pxTemp[0], pxTemp[1]);
+                            increment_row_ptrs(srcPtrTemp[c], kernelSize, 16);
+                        }
+                        if constexpr (std::is_same<T, Rpp8s>::value)
+                        {
+                            pxResultPln[0] = __lsx_vsub_b(pxResultPln[0], xmm_pxConvertI8);
+                            pxResultPln[1] = __lsx_vsub_b(pxResultPln[1], xmm_pxConvertI8);
+                            pxResultPln[2] = __lsx_vsub_b(pxResultPln[2], xmm_pxConvertI8);
+                        }
+
+                        __m128i pxResultPkd[4];
+                        rpp_convert48_pln3_to_pkd3(pxResultPln, pxResultPkd);
+                        __lsx_vst(pxResultPkd[0], (__m128i *)(dstPtrTemp), 0);
+                        __lsx_vst(pxResultPkd[1], (__m128i *)(dstPtrTemp + 12), 0);
+                        __lsx_vst(pxResultPkd[2], (__m128i *)(dstPtrTemp + 24), 0);
+                        __lsx_vst(pxResultPkd[3], (__m128i *)(dstPtrTemp + 36), 0);
+                        dstPtrTemp += 48;
+                    }
+#endif
+                    vectorLoopCount += padLength;
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                    {
+                        for (int c = 0; c < srcDescPtr->c; c++)
+                        {
+                            box_filter_generic_tensor(srcPtrTemp[c], dstPtrTemp, vectorLoopCount, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                            increment_row_ptrs(srcPtrTemp[c], kernelSize, 1);
+                            dstPtrTemp++;
+                        }
+                    }
+                    // for the first padLength rows, we need not increment the src row pointers to next rows
+                    increment_row_ptrs(srcPtrRow, kernelSize, (!padLengthRows) ? srcDescPtr->strides.hStride : 0);
+                    dstPtrRow += dstDescPtr->strides.hStride;
+                }
+            }
+            else if ((srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
+            {
+                /* exclude ((2 * padLength) * 3) number of columns from alignedLength calculation
+                   since (padLength * 3) number of columns from the beginning and end of each row will be computed using raw c code */
+                Rpp32u alignedLength = ((bufferLength - (2 * padLength) * 3) / 64) * 64;
+                T *dstPtrChannels[3];
+                for (int c = 0; c < 3; c++)
+                    dstPtrChannels[c] = dstPtrChannel + c * dstDescPtr->strides.cStride;
+                for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+                {
+                    int vectorLoopCount = 0;
+                    bool padLengthRows = (i < padLength) ? 1: 0;
+                    T *srcPtrTemp[9];
+                    for (int k = 0; k < 9; k++)
+                        srcPtrTemp[k] = srcPtrRow[k];
+                    T *dstPtrTempChannels[3] = {dstPtrChannels[0], dstPtrChannels[1], dstPtrChannels[2]};
+
+                    Rpp32s rowKernelLoopLimit = kernelSize;
+                    get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
+                    process_left_border_columns_pkd_pln(srcPtrTemp, srcPtrRow, dstPtrTempChannels, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+#if __loongarch_asx
+                    // process alignedLength number of columns in each row
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount += 24)
+                    {
+                        // load first 32 elements elements
+                        __m256i pxRow[9], pxRowHalf[2];
+                        rpp_load_box_filter_char_9x9_host(pxRow, srcPtrTemp, rowKernelLoopLimit);
+
+                        // get the accumalated result for first 8 elements
+                        unpacklo_and_add_9x9_host(pxRow, &pxRowHalf[0]);
+                        unpackhi_and_add_9x9_host(pxRow, &pxRowHalf[1]);
+
+                        // get the accumalated result for first 8 elements
+                        __m128i px128[8], pxTemp[7], pxDst[4];
+                        extract_4sse_registers(pxRowHalf, &px128[0]);
+                        blend_shuffle_add_9x9_host<7, 63, 1, 15, 127, 3, 31>(&px128[0], pxMaskPkd, blendRegisterOrder);
+
+                        // compute for next 8 elements
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 32);
+                        rpp_load_box_filter_char_9x9_host(pxRow, srcPtrTemp, rowKernelLoopLimit);
+                        unpacklo_and_add_9x9_host(pxRow, &pxRowHalf[0]);
+                        unpackhi_and_add_9x9_host(pxRow, &pxRowHalf[1]);
+
+                        // get the accumalated result for next 24 elements
+                        extract_4sse_registers(pxRowHalf, &px128[4]);
+                        blend_shuffle_add_9x9_host<7, 63, 1, 15, 127, 3, 31>(&px128[1], pxMaskPkd, blendRegisterOrder);
+                        blend_shuffle_add_9x9_host<7, 63, 1, 15, 127, 3, 31>(&px128[2], pxMaskPkd, blendRegisterOrder);
+                        blend_shuffle_add_9x9_host<7, 63, 1, 15, 127, 3, 31>(&px128[3], pxMaskPkd, blendRegisterOrder);
+                        pxDst[0] = __lsx_vmuh_h(px128[0], pxConvolutionFactor);
+                        pxDst[1] = __lsx_vmuh_h(px128[1], pxConvolutionFactor);
+                        pxDst[2] = __lsx_vmuh_h(px128[2], pxConvolutionFactor);
+                        pxDst[3] = __lsx_vmuh_h(px128[3], pxConvolutionFactor);
+                        pxDst[0] = lsx_packus_i16(pxDst[0], pxDst[1]);
+                        pxDst[1] = lsx_packus_i16(pxDst[2], pxDst[3]);
+                        if constexpr (std::is_same<T, Rpp8s>::value)
+                        {
+                            pxDst[0] = __lsx_vsub_b(pxDst[0], xmm_pxConvertI8);
+                            pxDst[1] = __lsx_vsub_b(pxDst[1], xmm_pxConvertI8);
+                        }
+
+                        // convert from PKD3 to PLN3 and store
+                        __m128i pxDstChn[3];
+                        rpp_convert24_pkd3_to_pln3(pxDst[0], pxDst[1], pxDstChn);
+                        rpp_storeu_si64((__m128i *)(dstPtrTempChannels[0]), pxDstChn[0]);
+                        rpp_storeu_si64((__m128i *)(dstPtrTempChannels[1]), pxDstChn[1]);
+                        rpp_storeu_si64((__m128i *)(dstPtrTempChannels[2]), pxDstChn[2]);
+                        increment_row_ptrs(srcPtrTemp, kernelSize, -8);
+                        increment_row_ptrs(dstPtrTempChannels, 3, 8);
+                    }
+#endif
+                    vectorLoopCount += padLength * 3;
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                    {
+                        int channel = vectorLoopCount % 3;
+                        box_filter_generic_tensor(srcPtrTemp, dstPtrTempChannels[channel], vectorLoopCount / 3, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare, 3);
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 1);
+                        dstPtrTempChannels[channel]++;
+                    }
+                    // for the first padLength rows, we need not increment the src row pointers to next rows
+                    increment_row_ptrs(srcPtrRow, kernelSize, (!padLengthRows) ? srcDescPtr->strides.hStride : 0);
+                    increment_row_ptrs(dstPtrChannels, 3, dstDescPtr->strides.hStride);
+                }
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+// F32 and F16 bitdepth
+template<typename T>
+RppStatus box_filter_float_host_tensor(T *srcPtr,
+                                       RpptDescPtr srcDescPtr,
+                                       T *dstPtr,
+                                       RpptDescPtr dstDescPtr,
+                                       Rpp32u kernelSize,
+                                       RpptROIPtr roiTensorPtrSrc,
+                                       RpptRoiType roiType,
+                                       RppLayoutParams layoutParams,
+                                       rpp::Handle& handle)
+{
+    RpptROI roiDefault = {0, 0, (Rpp32s)srcDescPtr->w, (Rpp32s)srcDescPtr->h};
+    Rpp32u numThreads = handle.GetNumThreads();
+    static_assert((std::is_same<T, Rpp32f>::value || std::is_same<T, Rpp16f>::value), "T must be Rpp32f or Rpp16f");
+
+    if ((kernelSize != 3) && (kernelSize != 5) && (kernelSize != 7) && (kernelSize != 9))
+        return box_filter_generic_host_tensor(srcPtr, srcDescPtr, dstPtr, dstDescPtr, kernelSize, roiTensorPtrSrc, roiType, layoutParams, handle);
+
+    // set the required masks array needed for permute operations 
+#if __loongarch_asx
+    __m256i pxMaskPln[7] = {avx_pxMaskRotate0To1, avx_pxMaskRotate0To2, avx_pxMaskRotate0To3, avx_pxMaskRotate0To4, avx_pxMaskRotate0To5, avx_pxMaskRotate0To6, avx_pxMaskRotate0To7};
+    __m256i pxMaskPkd[7] = {avx_pxMaskRotate0To3, avx_pxMaskRotate0To6, avx_pxMaskRotate0To1, avx_pxMaskRotate0To4, avx_pxMaskRotate0To7, avx_pxMaskRotate0To2, avx_pxMaskRotate0To5};
+#endif
+
+    omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+    for(int batchCount = 0; batchCount < dstDescPtr->n; batchCount++)
+    {
+        RpptROI roi;
+        RpptROIPtr roiPtrInput = &roiTensorPtrSrc[batchCount];
+        compute_roi_validation_host(roiPtrInput, &roi, &roiDefault, roiType);
+
+        T *srcPtrImage, *dstPtrImage;
+        srcPtrImage = srcPtr + batchCount * srcDescPtr->strides.nStride;
+        dstPtrImage = dstPtr + batchCount * dstDescPtr->strides.nStride;
+
+        Rpp32u padLength = kernelSize / 2;
+        Rpp32u bufferLength = roi.xywhROI.roiWidth * layoutParams.bufferMultiplier;
+        Rpp32u unpaddedHeight = roi.xywhROI.roiHeight - padLength;
+        Rpp32u unpaddedWidth = roi.xywhROI.roiWidth - padLength;
+        Rpp32f kernelSizeInverseSquare = 1.0 / (kernelSize * kernelSize);
+#if __loongarch_asx
+        const __m256 pConvolutionFactor = lasx_set1_f32(kernelSizeInverseSquare);
+        // set the register order needed for blend operations 
+        Rpp32u blendRegisterOrder[7] = {0, 0, 1, 1, 1, 2, 2};
+        if (srcDescPtr->layout == RpptLayout::NCHW)
+            std::fill_n(blendRegisterOrder, 7, 0);
+#endif
+
+        T *srcPtrChannel, *dstPtrChannel;
+        srcPtrChannel = srcPtrImage + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
+        dstPtrChannel = dstPtrImage;
+        if (kernelSize == 3)
+        {
+            T *srcPtrRow[3], *dstPtrRow;
+            for (int i = 0; i < 3; i++)
+                srcPtrRow[i] = srcPtrChannel + i * srcDescPtr->strides.hStride;
+            dstPtrRow = dstPtrChannel;
+
+            // box filter without fused output-layout toggle (NCHW -> NCHW)
+            if ((srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NCHW))
+            {
+                /* exclude (2 * padLength) number of columns from alignedLength calculation
+                   since padLength number of columns from the beginning and end of each row will be computed using raw c code */
+                Rpp32u alignedLength = ((bufferLength - (2 * padLength)) / 16) * 16;
+                for (int c = 0; c < srcDescPtr->c; c++)
+                {
+                    srcPtrRow[0] = srcPtrChannel;
+                    srcPtrRow[1] = srcPtrRow[0] + srcDescPtr->strides.hStride;
+                    srcPtrRow[2] = srcPtrRow[1] + srcDescPtr->strides.hStride;
+                    dstPtrRow = dstPtrChannel;
+                    for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+                    {
+                        int vectorLoopCount = 0;
+                        bool padLengthRows = (i < padLength) ? 1: 0;
+                        T *srcPtrTemp[3] = {srcPtrRow[0], srcPtrRow[1], srcPtrRow[2]};
+                        T *dstPtrTemp = dstPtrRow;
+
+                        // get the number of rows needs to be loaded for the corresponding row
+                        Rpp32s rowKernelLoopLimit = kernelSize;
+                        get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
+                        process_left_border_columns_pln_pln(srcPtrTemp, dstPtrTemp, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                        dstPtrTemp += padLength;
+#if __loongarch_asx
+                        // process alignedLength number of columns in each row
+                        for (; vectorLoopCount < alignedLength; vectorLoopCount += 14)
+                        {
+                            __m256 pRow[3], pTemp[3], pDst[2];
+                            rpp_load_box_filter_float_3x3_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+                            add_rows_3x3(pRow, &pTemp[0]);
+
+                            increment_row_ptrs(srcPtrTemp, kernelSize, 8);
+                            rpp_load_box_filter_float_3x3_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+                            add_rows_3x3(pRow, &pTemp[1]);
+                            pTemp[2] = avx_p0;
+
+                            blend_permute_add_mul_3x3_host<1, 3>(&pTemp[0], &pDst[0], pConvolutionFactor, pxMaskPln, blendRegisterOrder);
+                            blend_permute_add_mul_3x3_host<1, 3>(&pTemp[1], &pDst[1], pConvolutionFactor, pxMaskPln, blendRegisterOrder);
+                            rpp_store16_float(dstPtrTemp, pDst);
+
+                            increment_row_ptrs(srcPtrTemp, kernelSize, 6);
+                            dstPtrTemp += 14;
+                        }
+#endif
+                        vectorLoopCount += padLength;
+                        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                        {
+                            box_filter_generic_tensor(srcPtrTemp, dstPtrTemp, vectorLoopCount, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                            increment_row_ptrs(srcPtrTemp, kernelSize, 1);
+                            dstPtrTemp++;
+                        }
+                        // for the first padLength rows, we need not increment the src row pointers to next rows
+                        increment_row_ptrs(srcPtrRow, kernelSize, (!padLengthRows) ? srcDescPtr->strides.hStride : 0);
+                        dstPtrRow += dstDescPtr->strides.hStride;
+                    }
+                    srcPtrChannel += srcDescPtr->strides.cStride;
+                    dstPtrChannel += dstDescPtr->strides.cStride;
+                }
+            }
+            else if ((srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NHWC))
+            {
+                /* exclude ((2 * padLength) * 3) number of columns from alignedLength calculation
+                   since (padLength * 3) number of columns from the beginning and end of each row will be computed using raw c code */
+                Rpp32u alignedLength = ((bufferLength - (2 * padLength) * 3) / 24) * 24;
+                for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+                {
+                    int vectorLoopCount = 0;
+                    bool padLengthRows = (i < padLength) ? 1: 0;
+                    T *srcPtrTemp[3] = {srcPtrRow[0], srcPtrRow[1], srcPtrRow[2]};
+                    T *dstPtrTemp = dstPtrRow;
+
+                    Rpp32s rowKernelLoopLimit = kernelSize;
+                    get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
+                    process_left_border_columns_pkd_pkd(srcPtrTemp, srcPtrRow, dstPtrTemp, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                    dstPtrTemp += padLength * 3;
+#if __loongarch_asx
+                    // process remaining columns in each row
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount += 16)
+                    {
+                        __m256 pRow[3], pTemp[3], pDst[2];
+                        rpp_load_box_filter_float_3x3_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+                        add_rows_3x3(pRow, &pTemp[0]);
+
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 8);
+                        rpp_load_box_filter_float_3x3_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+                        add_rows_3x3(pRow, &pTemp[1]);
+
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 8);
+                        rpp_load_box_filter_float_3x3_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+                        add_rows_3x3(pRow, &pTemp[2]);
+
+                        blend_permute_add_mul_3x3_host<7, 63>(&pTemp[0], &pDst[0], pConvolutionFactor, pxMaskPkd, blendRegisterOrder);
+                        blend_permute_add_mul_3x3_host<7, 63>(&pTemp[1], &pDst[1], pConvolutionFactor, pxMaskPkd, blendRegisterOrder);
+
+                        rpp_store16_float(dstPtrTemp, pDst);
+                        dstPtrTemp += 16;
+                    }
+#endif
+                    vectorLoopCount += padLength * 3;
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                    {
+                        box_filter_generic_tensor(srcPtrTemp, dstPtrTemp, vectorLoopCount / 3, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare, 3);
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 1);
+                        dstPtrTemp++;
+                    }
+                    // for the first padLength rows, we need not increment the src row pointers to next rows
+                    increment_row_ptrs(srcPtrRow, kernelSize, (!padLengthRows) ? srcDescPtr->strides.hStride : 0);
+                    dstPtrRow += dstDescPtr->strides.hStride;
+                }
+            }
+            else if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
+            {
+                /* exclude ((2 * padLength) * 3) number of columns from alignedLength calculation
+                   since (padLength * 3) number of columns from the beginning and end of each row will be computed using raw c code */
+                Rpp32u alignedLength = ((bufferLength - (2 * padLength) * 3) / 24) * 24;
+                T *dstPtrChannels[3];
+                for (int i = 0; i < 3; i++)
+                    dstPtrChannels[i] = dstPtrChannel + i * dstDescPtr->strides.cStride;
+                for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+                {
+                    int vectorLoopCount = 0;
+                    bool padLengthRows = (i < padLength) ? 1: 0;
+                    T *srcPtrTemp[3] = {srcPtrRow[0], srcPtrRow[1], srcPtrRow[2]};
+                    T *dstPtrTempChannels[3] = {dstPtrChannels[0], dstPtrChannels[1], dstPtrChannels[2]};
+
+                    Rpp32s rowKernelLoopLimit = kernelSize;
+                    get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
+                    process_left_border_columns_pkd_pln(srcPtrTemp, srcPtrRow, dstPtrTempChannels, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+#if __loongarch_asx
+                    // process remaining columns in each row
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount += 12)
+                    {
+                        __m256 pRow[3], pTemp[3], pDst[2];
+                        rpp_load_box_filter_float_3x3_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+                        add_rows_3x3(pRow, &pTemp[0]);
+
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 8);
+                        rpp_load_box_filter_float_3x3_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+                        add_rows_3x3(pRow, &pTemp[1]);
+
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 8);
+                        rpp_load_box_filter_float_3x3_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+                        add_rows_3x3(pRow, &pTemp[2]);
+
+                        blend_permute_add_mul_3x3_host<7, 63>(&pTemp[0], &pDst[0], pConvolutionFactor, pxMaskPkd, blendRegisterOrder);
+                        blend_permute_add_mul_3x3_host<7, 63>(&pTemp[1], &pDst[1], pConvolutionFactor, pxMaskPkd, blendRegisterOrder);
+
+                        __m128 pDstPln[3];
+                        rpp_convert12_f32pkd3_to_f32pln3(pDst, pDstPln);
+                        rpp_store12_float_pkd_pln(dstPtrTempChannels, pDstPln);
+
+                        increment_row_ptrs(srcPtrTemp, kernelSize, -4);
+                        increment_row_ptrs(dstPtrTempChannels, kernelSize, 4);
+                    }
+#endif
+                    vectorLoopCount += padLength * 3;
+                    for (int c = 0; vectorLoopCount < bufferLength; vectorLoopCount++, c++)
+                    {
+                        int channel = c % 3;
+                        box_filter_generic_tensor(srcPtrTemp, dstPtrTempChannels[channel], vectorLoopCount / 3, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare, 3);
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 1);
+                        dstPtrTempChannels[channel]++;
+                    }
+                    // for the first padLength rows, we need not increment the src row pointers to next rows
+                    increment_row_ptrs(srcPtrRow, kernelSize, (!padLengthRows) ? srcDescPtr->strides.hStride : 0);
+                    increment_row_ptrs(dstPtrChannels, kernelSize, dstDescPtr->strides.hStride);
+                }
+            }
+            else if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NHWC))
+            {
+                /* exclude (2 * padLength) number of columns from alignedLength calculation
+                   since padLength number of columns from the beginning and end of each row will be computed using raw c code */
+                Rpp32u alignedLength = ((bufferLength - (2 * padLength)) / 16) * 16;
+                for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+                {
+                    int vectorLoopCount = 0;
+                    bool padLengthRows = (i < padLength) ? 1: 0;
+                    T *srcPtrTemp[3][3] = {
+                                                {srcPtrRow[0], srcPtrRow[1], srcPtrRow[2]},
+                                                {srcPtrRow[0] + srcDescPtr->strides.cStride, srcPtrRow[1] + srcDescPtr->strides.cStride, srcPtrRow[2] + srcDescPtr->strides.cStride},
+                                                {srcPtrRow[0] + 2 * srcDescPtr->strides.cStride, srcPtrRow[1] + 2 * srcDescPtr->strides.cStride, srcPtrRow[2] + 2 * srcDescPtr->strides.cStride}
+                                              };
+
+                    T *dstPtrTemp = dstPtrRow;
+                    // get the number of rows needs to be loaded for the corresponding row
+                    Rpp32s rowKernelLoopLimit = kernelSize;
+                    get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
+
+                    // process padLength number of columns in each row
+                    // left border pixels in image which does not have required pixels in 3x3 box, process them separately
+                    for (int k = 0; k < padLength; k++)
+                    {
+                        for (int c = 0; c < 3; c++)
+                        {
+                            box_filter_generic_tensor(srcPtrTemp[c], dstPtrTemp, k, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                            dstPtrTemp++;
+                        }
+                    }
+#if __loongarch_asx
+                    // process alignedLength number of columns in each row
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount += 14)
+                    {
+                        __m256 pResult[6];
+                        for (int c = 0; c < 3; c++)
+                        {
+                            int channelStride = c * 2;
+                            __m256 pRow[3], pTemp[3];
+                            rpp_load_box_filter_float_3x3_host(pRow, srcPtrTemp[c], rowKernelLoopLimit);
+                            add_rows_3x3(pRow, &pTemp[0]);
+
+                            increment_row_ptrs(srcPtrTemp[c], kernelSize, 8);
+                            rpp_load_box_filter_float_3x3_host(pRow, srcPtrTemp[c], rowKernelLoopLimit);
+                            add_rows_3x3(pRow, &pTemp[1]);
+                            pTemp[2] = avx_p0;
+
+                            blend_permute_add_mul_3x3_host<1, 3>(&pTemp[0], &pResult[channelStride], pConvolutionFactor, pxMaskPln, blendRegisterOrder);
+                            blend_permute_add_mul_3x3_host<1, 3>(&pTemp[1], &pResult[channelStride + 1], pConvolutionFactor, pxMaskPln, blendRegisterOrder);
+                            increment_row_ptrs(srcPtrTemp[c], kernelSize, 6);
+                        }
+
+                        // convert result from pln to pkd format and store in output buffer
+                        if constexpr (std::is_same<T, Rpp32f>::value)
+                            rpp_simd_store(rpp_store48_f32pln3_to_f32pkd3_avx, dstPtrTemp, pResult);
+                        else if constexpr (std::is_same<T, Rpp16f>::value)
+                            rpp_simd_store(rpp_store48_f32pln3_to_f16pkd3_avx, dstPtrTemp, pResult);
+
+                        dstPtrTemp += 42;
+                    }
+#endif
+                    vectorLoopCount += padLength;
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                    {
+                        for (int c = 0; c < 3; c++)
+                        {
+                            box_filter_generic_tensor(srcPtrTemp[c], dstPtrTemp, vectorLoopCount, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                            increment_row_ptrs(srcPtrTemp[c], kernelSize, 1);
+                            dstPtrTemp++;
+                        }
+                    }
+                    // for the first padLength rows, we need not increment the src row pointers to next rows
+                    increment_row_ptrs(srcPtrRow, kernelSize, (!padLengthRows) ? srcDescPtr->strides.hStride : 0);
+                    dstPtrRow += dstDescPtr->strides.hStride;
+                }
+            }
+        }
+        else if (kernelSize == 5)
+        {
+            T *srcPtrRow[5], *dstPtrRow;
+            for (int i = 0; i < 5; i++)
+                srcPtrRow[i] = srcPtrChannel + i * srcDescPtr->strides.hStride;
+            dstPtrRow = dstPtrChannel;
+
+            // box filter without fused output-layout toggle (NCHW -> NCHW)
+            if ((srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NCHW))
+            {
+                /* exclude (2 * padLength) number of columns from alignedLength calculation
+                   since padLength number of columns from the beginning and end of each row will be computed using raw c code */
+                Rpp32u alignedLength = ((bufferLength - (2 * padLength)) / 16) * 16;
+                for (int c = 0; c < srcDescPtr->c; c++)
+                {
+                    srcPtrRow[0] = srcPtrChannel;
+                    for (int k = 1; k < 5; k++)
+                        srcPtrRow[k] = srcPtrRow[k - 1] + srcDescPtr->strides.hStride;
+
+                    dstPtrRow = dstPtrChannel;
+                    for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+                    {
+                        int vectorLoopCount = 0;
+                        bool padLengthRows = (i < padLength) ? 1: 0;
+                        T *srcPtrTemp[5] = {srcPtrRow[0], srcPtrRow[1], srcPtrRow[2], srcPtrRow[3], srcPtrRow[4]};
+                        T *dstPtrTemp = dstPtrRow;
+
+                        // get the number of rows needs to be loaded for the corresponding row
+                        Rpp32s rowKernelLoopLimit = kernelSize;
+                        get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
+                        process_left_border_columns_pln_pln(srcPtrTemp, dstPtrTemp, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                        dstPtrTemp += padLength;
+#if __loongarch_asx
+                        // process alignedLength number of columns in each row
+                        for (; vectorLoopCount < alignedLength; vectorLoopCount += 12)
+                        {
+                            __m256 pRow[5], pDst[2], pTemp[3];
+                            rpp_load_box_filter_float_5x5_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+                            add_rows_5x5(pRow, &pTemp[0]);
+
+                            increment_row_ptrs(srcPtrTemp, kernelSize, 8);
+                            rpp_load_box_filter_float_5x5_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+                            add_rows_5x5(pRow, &pTemp[1]);
+                            pTemp[2] = avx_p0;
+
+                            blend_permute_add_mul_5x5_host<1, 3, 7, 15>(&pTemp[0], &pDst[0], pConvolutionFactor, pxMaskPln, blendRegisterOrder);
+                            blend_permute_add_mul_5x5_host<1, 3, 7, 15>(&pTemp[1], &pDst[1], pConvolutionFactor, pxMaskPln, blendRegisterOrder);
+
+                            rpp_store16_float(dstPtrTemp, pDst);
+                            increment_row_ptrs(srcPtrTemp, kernelSize, 4);
+                            dstPtrTemp += 12;
+                        }
+#endif
+                        vectorLoopCount += padLength;
+                        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                        {
+                            box_filter_generic_tensor(srcPtrTemp, dstPtrTemp, vectorLoopCount, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                            increment_row_ptrs(srcPtrTemp, kernelSize, 1);
+                            dstPtrTemp++;
+                        }
+                        // for the first padLength rows, we need not increment the src row pointers to next rows
+                        increment_row_ptrs(srcPtrRow, kernelSize, (!padLengthRows) ? srcDescPtr->strides.hStride : 0);
+                        dstPtrRow += dstDescPtr->strides.hStride;
+                    }
+                    srcPtrChannel += srcDescPtr->strides.cStride;
+                    dstPtrChannel += dstDescPtr->strides.cStride;
+                }
+            }
+            else if ((srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NHWC))
+            {
+                /* exclude ((2 * padLength) * 3) number of columns from alignedLength calculation
+                   since (padLength * 3) number of columns from the beginning and end of each row will be computed using raw c code */
+                Rpp32u alignedLength = ((bufferLength - (2 * padLength * 3)) / 24) * 24;
+                for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+                {
+                    int vectorLoopCount = 0;
+                    bool padLengthRows = (i < padLength) ? 1: 0;
+                    T *srcPtrTemp[5];
+                    for (int k = 0; k < 5; k++)
+                        srcPtrTemp[k] = srcPtrRow[k];
+                    T *dstPtrTemp = dstPtrRow;
+
+                    Rpp32s rowKernelLoopLimit = kernelSize;
+                    get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
+                    process_left_border_columns_pkd_pkd(srcPtrTemp, srcPtrRow, dstPtrTemp, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                    dstPtrTemp += padLength * 3;
+#if __loongarch_asx
+                    // process remaining columns in each row
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount += 12)
+                    {
+                        // add loaded values from 9 rows
+                        __m256 pRow[5], pDst[2], pTemp[4];
+                        rpp_load_box_filter_float_5x5_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+                        add_rows_5x5(pRow, &pTemp[0]);
+
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 8);
+                        rpp_load_box_filter_float_5x5_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+                        add_rows_5x5(pRow, &pTemp[1]);
+
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 8);
+                        rpp_load_box_filter_float_5x5_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+                        add_rows_5x5(pRow, &pTemp[2]);
+                        pTemp[3] = avx_p0;
+
+                        blend_permute_add_mul_5x5_host<7, 63, 1, 15>(&pTemp[0], &pDst[0], pConvolutionFactor, pxMaskPkd, blendRegisterOrder);
+                        blend_permute_add_mul_5x5_host<7, 63, 1, 15>(&pTemp[1], &pDst[1], pConvolutionFactor, pxMaskPkd, blendRegisterOrder);
+
+                        rpp_store16_float(dstPtrTemp, pDst);
+                        increment_row_ptrs(srcPtrTemp, kernelSize, -4);
+                        dstPtrTemp += 12;
+                    }
+#endif
+                    vectorLoopCount += padLength * 3;
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                    {
+                        box_filter_generic_tensor(srcPtrTemp, dstPtrTemp, vectorLoopCount / 3, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare, 3);
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 1);
+                        dstPtrTemp++;
+                    }
+                    // for the first padLength rows, we need not increment the src row pointers to next rows
+                    increment_row_ptrs(srcPtrRow, kernelSize, (!padLengthRows) ? srcDescPtr->strides.hStride : 0);
+                    dstPtrRow += dstDescPtr->strides.hStride;
+                }
+            }
+            else if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NHWC))
+            {
+                /* exclude (2 * padLength) number of columns from alignedLength calculation
+                   since padLength number of columns from the beginning and end of each row will be computed using raw c code */
+                Rpp32u alignedLength = ((bufferLength - (2 * padLength)) / 16) * 16;
+                for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+                {
+                    int vectorLoopCount = 0;
+                    bool padLengthRows = (i < padLength) ? 1: 0;
+                    T *srcPtrTemp[3][5];
+                    for (int c = 0; c < 3; c++)
+                    {
+                        Rpp32u channelStride = c * srcDescPtr->strides.cStride;
+                        for (int k = 0; k < 5; k++)
+                            srcPtrTemp[c][k] = srcPtrRow[k] + channelStride;
+                    }
+                    T *dstPtrTemp = dstPtrRow;
+
+                    // get the number of rows needs to be loaded for the corresponding row
+                    Rpp32s rowKernelLoopLimit = kernelSize;
+                    get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
+
+                    // process padLength number of columns in each row
+                    for (int k = 0; k < padLength; k++)
+                    {
+                        for (int c = 0; c < 3; c++)
+                        {
+                            box_filter_generic_tensor(srcPtrTemp[c], dstPtrTemp, k, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                            dstPtrTemp++;
+                        }
+                    }
+#if __loongarch_asx
+                    // process alignedLength number of columns in each row
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
+                    {
+                        __m256 pResultPln[3];
+                        for (int c = 0; c < 3; c++)
+                        {
+                            __m256 pRow[5], pTemp[2];
+                            rpp_load_box_filter_float_5x5_host(pRow, srcPtrTemp[c], rowKernelLoopLimit);
+                            add_rows_5x5(pRow, &pTemp[0]);
+
+                            increment_row_ptrs(srcPtrTemp[c], kernelSize, 8);
+                            rpp_load_box_filter_float_5x5_host(pRow, srcPtrTemp[c], rowKernelLoopLimit);
+                            add_rows_5x5(pRow, &pTemp[1]);
+                            blend_permute_add_mul_5x5_host<1, 3, 7, 15>(pTemp, &pResultPln[c], pConvolutionFactor, pxMaskPln, blendRegisterOrder);
+                        }
+
+                        // convert result from pln to pkd format and store in output buffer
+                        if constexpr (std::is_same<T, Rpp32f>::value)
+                            rpp_simd_store(rpp_store24_f32pln3_to_f32pkd3_avx, dstPtrTemp, pResultPln);
+                        else if constexpr (std::is_same<T, Rpp16f>::value)
+                            rpp_simd_store(rpp_store24_f32pln3_to_f16pkd3_avx, dstPtrTemp, pResultPln);
+
+                        dstPtrTemp += 24;
+                    }
+#endif
+                    vectorLoopCount += padLength;
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                    {
+                        for (int c = 0; c < srcDescPtr->c; c++)
+                        {
+                            box_filter_generic_tensor(srcPtrTemp[c], dstPtrTemp, vectorLoopCount, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                            increment_row_ptrs(srcPtrTemp[c], kernelSize, 1);
+                            dstPtrTemp++;
+                        }
+                    }
+                    // for the first padLength rows, we need not increment the src row pointers to next rows
+                    increment_row_ptrs(srcPtrRow, kernelSize, (!padLengthRows) ? srcDescPtr->strides.hStride : 0);
+                    dstPtrRow += dstDescPtr->strides.hStride;
+                }
+            }
+            else if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
+            {
+                /* exclude ((2 * padLength) * 3) number of columns from alignedLength calculation
+                   since (padLength * 3) number of columns from the beginning and end of each row will be computed using raw c code */
+                Rpp32u alignedLength = ((bufferLength - (2 * padLength) * 3) / 24) * 24;
+                T *dstPtrChannels[3];
+                for (int i = 0; i < 3; i++)
+                    dstPtrChannels[i] = dstPtrChannel + i * dstDescPtr->strides.cStride;
+                for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+                {
+                    int vectorLoopCount = 0;
+                    bool padLengthRows = (i < padLength) ? 1: 0;
+                    T *srcPtrTemp[5];
+                    for (int k = 0; k < 5; k++)
+                        srcPtrTemp[k] = srcPtrRow[k];
+                    T *dstPtrTempChannels[3] = {dstPtrChannels[0], dstPtrChannels[1], dstPtrChannels[2]};
+
+                    Rpp32s rowKernelLoopLimit = kernelSize;
+                    get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
+                    process_left_border_columns_pkd_pln(srcPtrTemp, srcPtrRow, dstPtrTempChannels, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+#if __loongarch_asx
+                    // process remaining columns in each row
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount += 12)
+                    {
+                        // add loaded values from 9 rows
+                        __m256 pRow[5], pDst[2], pTemp[4];
+                        rpp_load_box_filter_float_5x5_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+                        add_rows_5x5(pRow, &pTemp[0]);
+
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 8);
+                        rpp_load_box_filter_float_5x5_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+                        add_rows_5x5(pRow, &pTemp[1]);
+
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 8);
+                        rpp_load_box_filter_float_5x5_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+                        add_rows_5x5(pRow, &pTemp[2]);
+                        pTemp[3] = avx_p0;
+
+                        blend_permute_add_mul_5x5_host<7, 63, 1, 15>(&pTemp[0], &pDst[0], pConvolutionFactor, pxMaskPkd, blendRegisterOrder);
+                        blend_permute_add_mul_5x5_host<7, 63, 1, 15>(&pTemp[1], &pDst[1], pConvolutionFactor, pxMaskPkd, blendRegisterOrder);
+
+                        __m128 pDstPln[3];
+                        rpp_convert12_f32pkd3_to_f32pln3(pDst, pDstPln);
+                        rpp_store12_float_pkd_pln(dstPtrTempChannels, pDstPln);
+
+                        increment_row_ptrs(srcPtrTemp, kernelSize, -4);
+                        increment_row_ptrs(dstPtrTempChannels, 3, 4);
+                    }
+#endif
+                    vectorLoopCount += padLength * 3;
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                    {
+                        int channel = vectorLoopCount % 3;
+                        box_filter_generic_tensor(srcPtrTemp, dstPtrTempChannels[channel], vectorLoopCount / 3, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare, 3);
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 1);
+                        dstPtrTempChannels[channel]++;
+                    }
+                    // for the first padLength rows, we need not increment the src row pointers to next rows
+                    increment_row_ptrs(srcPtrRow, kernelSize, (!padLengthRows) ? srcDescPtr->strides.hStride : 0);
+                    increment_row_ptrs(dstPtrChannels, 3, dstDescPtr->strides.hStride);
+                }
+            }
+        }
+        else if (kernelSize == 7)
+        {
+            T *srcPtrRow[7], *dstPtrRow;
+            for (int i = 0; i < 7; i++)
+                srcPtrRow[i] = srcPtrChannel + i * srcDescPtr->strides.hStride;
+            dstPtrRow = dstPtrChannel;
+
+            // box filter without fused output-layout toggle (NCHW -> NCHW)
+            if ((srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NCHW))
+            {
+                /* exclude (2 * padLength) number of columns from alignedLength calculation
+                   since padLength number of columns from the beginning and end of each row will be computed using raw c code */
+                Rpp32u alignedLength = ((bufferLength - (2 * padLength)) / 16) * 16;
+                for (int c = 0; c < srcDescPtr->c; c++)
+                {
+                    srcPtrRow[0] = srcPtrChannel;
+                    for (int k = 1; k < 7; k++)
+                        srcPtrRow[k] = srcPtrRow[k - 1] + srcDescPtr->strides.hStride;
+
+                    dstPtrRow = dstPtrChannel;
+                    for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+                    {
+                        int vectorLoopCount = 0;
+                        bool padLengthRows = (i < padLength) ? 1: 0;
+                        T *srcPtrTemp[7];
+                        for (int k = 0; k < 7; k++)
+                            srcPtrTemp[k] = srcPtrRow[k];
+                        T *dstPtrTemp = dstPtrRow;
+
+                        // get the number of rows needs to be loaded for the corresponding row
+                        Rpp32s rowKernelLoopLimit = kernelSize;
+                        get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
+                        process_left_border_columns_pln_pln(srcPtrTemp, dstPtrTemp, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                        dstPtrTemp += padLength;
+#if __loongarch_asx
+                        // process alignedLength number of columns in each row
+                        for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
+                        {
+                            __m256 pRow[7], pTemp[2], pDst;
+                            rpp_load_box_filter_float_7x7_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+                            add_rows_7x7(pRow, &pTemp[0]);
+
+                            increment_row_ptrs(srcPtrTemp, kernelSize, 8);
+                            rpp_load_box_filter_float_7x7_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+                            add_rows_7x7(pRow, &pTemp[1]);
+                            blend_permute_add_mul_7x7_host<1, 3, 7, 15, 31, 63>(&pTemp[0], &pDst, pConvolutionFactor, pxMaskPln, blendRegisterOrder);
+
+                            // convert result from pln to pkd format and store in output buffer
+                            if constexpr (std::is_same<T, Rpp32f>::value)
+                                __lasx_xvst((__m256i)pDst, dstPtrTemp, 0);
+                            else if constexpr (std::is_same<T, Rpp16f>::value)
+                                __lsx_vst(lasx_cvtf32_ph(pDst, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC), (__m128i *)dstPtrTemp, 0);
+
+                            dstPtrTemp += 8;
+                        }
+#endif
+                        vectorLoopCount += padLength;
+                        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                        {
+                            box_filter_generic_tensor(srcPtrTemp, dstPtrTemp, vectorLoopCount, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                            increment_row_ptrs(srcPtrTemp, kernelSize, 1);
+                            dstPtrTemp++;
+                        }
+                        // for the first padLength rows, we need not increment the src row pointers to next rows
+                        increment_row_ptrs(srcPtrRow, kernelSize, (!padLengthRows) ? srcDescPtr->strides.hStride : 0);
+                        dstPtrRow += dstDescPtr->strides.hStride;
+                    }
+                    srcPtrChannel += srcDescPtr->strides.cStride;
+                    dstPtrChannel += dstDescPtr->strides.cStride;
+                }
+            }
+            else if ((srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NHWC))
+            {
+                /* exclude ((2 * padLength) * 3) number of columns from alignedLength calculation
+                   since (padLength * 3) number of columns from the beginning and end of each row will be computed using raw c code */
+                Rpp32u alignedLength = ((bufferLength - (2 * padLength) * 3) / 32) * 32;
+                for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+                {
+                    int vectorLoopCount = 0;
+                    bool padLengthRows = (i < padLength) ? 1: 0;
+                    T *srcPtrTemp[7];
+                    for (int k = 0; k < 7; k++)
+                        srcPtrTemp[k] = srcPtrRow[k];
+                    T *dstPtrTemp = dstPtrRow;
+
+                    Rpp32s rowKernelLoopLimit = kernelSize;
+                    get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
+                    process_left_border_columns_pkd_pkd(srcPtrTemp, srcPtrRow, dstPtrTemp, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                    dstPtrTemp += padLength * 3;
+#if __loongarch_asx
+                    __m256 pRow[7], pTemp[4];
+                    if (alignedLength)
+                    {
+                        rpp_load_box_filter_float_7x7_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+                        add_rows_7x7(pRow, &pTemp[0]);
+
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 8);
+                        rpp_load_box_filter_float_7x7_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+                        add_rows_7x7(pRow, &pTemp[1]);
+
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 8);
+                        rpp_load_box_filter_float_7x7_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+                        add_rows_7x7(pRow, &pTemp[2]);
+                    }
+
+                    // process remaining columns in each row
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
+                    {
+                        // add loaded values from 7 rows
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 8);
+                        rpp_load_box_filter_float_7x7_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+                        add_rows_7x7(pRow, &pTemp[3]);
+
+                        __m256 pDst;
+                        blend_permute_add_mul_7x7_host<7, 63, 1, 15, 127, 3>(pTemp, &pDst, pConvolutionFactor, pxMaskPkd, blendRegisterOrder);
+
+                        // convert result from pln to pkd format and store in output buffer
+                        if constexpr (std::is_same<T, Rpp32f>::value)
+                            __lasx_xvst((__m256i)pDst, dstPtrTemp, 0);
+                        else if constexpr (std::is_same<T, Rpp16f>::value)
+                            __lsx_vst(lasx_cvtf32_ph(pDst, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC), (__m128i *)dstPtrTemp, 0);
+
+                        dstPtrTemp += 8;
+                        pTemp[0] = pTemp[1];
+                        pTemp[1] = pTemp[2];
+                        pTemp[2] = pTemp[3];
+                    }
+                    increment_row_ptrs(srcPtrTemp, kernelSize, -16);
+#endif
+                    vectorLoopCount += padLength * 3;
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                    {
+                        box_filter_generic_tensor(srcPtrTemp, dstPtrTemp, vectorLoopCount / 3, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare, 3);
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 1);
+                        dstPtrTemp++;
+                    }
+                    // for the first padLength rows, we need not increment the src row pointers to next rows
+                    increment_row_ptrs(srcPtrRow, kernelSize, (!padLengthRows) ? srcDescPtr->strides.hStride : 0);
+                    dstPtrRow += dstDescPtr->strides.hStride;
+                }
+            }
+            else if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NHWC))
+            {
+                /* exclude (2 * padLength) number of columns from alignedLength calculation
+                   since padLength number of columns from the beginning and end of each row will be computed using raw c code */
+                Rpp32u alignedLength = ((bufferLength - (2 * padLength)) / 16) * 16;
+                for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+                {
+                    int vectorLoopCount = 0;
+                    bool padLengthRows = (i < padLength) ? 1: 0;
+                    T *srcPtrTemp[3][7];
+                    for (int c = 0; c < 3; c++)
+                    {
+                        Rpp32u channelStride = c * srcDescPtr->strides.cStride;
+                        for (int k = 0; k < 7; k++)
+                            srcPtrTemp[c][k] = srcPtrRow[k] + channelStride;
+                    }
+                    T *dstPtrTemp = dstPtrRow;
+
+                    // get the number of rows needs to be loaded for the corresponding row
+                    Rpp32s rowKernelLoopLimit = kernelSize;
+                    get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
+
+                    // process padLength number of columns in each row
+                    for (int k = 0; k < padLength; k++)
+                    {
+                        for (int c = 0; c < 3; c++)
+                        {
+                            box_filter_generic_tensor(srcPtrTemp[c], dstPtrTemp, k, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                            dstPtrTemp++;
+                        }
+                    }
+#if __loongarch_asx
+                    // process alignedLength number of columns in each row
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
+                    {
+                        __m256 pResultPln[3];
+                        for (int c = 0; c < 3; c++)
+                        {
+                            __m256 pRow[7], pTemp[2];
+                            rpp_load_box_filter_float_7x7_host(pRow, srcPtrTemp[c], rowKernelLoopLimit);
+                            add_rows_7x7(pRow, &pTemp[0]);
+
+                            increment_row_ptrs(srcPtrTemp[c], kernelSize, 8);
+                            rpp_load_box_filter_float_7x7_host(pRow, srcPtrTemp[c], rowKernelLoopLimit);
+                            add_rows_7x7(pRow, &pTemp[1]);
+                            blend_permute_add_mul_7x7_host<1, 3, 7, 15, 31, 63>(pTemp, &pResultPln[c], pConvolutionFactor, pxMaskPln, blendRegisterOrder);
+                        }
+                        // convert result from pln to pkd format and store in output buffer
+                        if constexpr (std::is_same<T, Rpp32f>::value)
+                            rpp_store24_f32pln3_to_f32pkd3_avx(dstPtrTemp, pResultPln);
+                        else if constexpr (std::is_same<T, Rpp16f>::value)
+                            rpp_store24_f32pln3_to_f16pkd3_avx(dstPtrTemp, pResultPln);
+
+                        dstPtrTemp += 24;
+                    }
+#endif
+                    vectorLoopCount += padLength;
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                    {
+                        for (int c = 0; c < srcDescPtr->c; c++)
+                        {
+                            box_filter_generic_tensor(srcPtrTemp[c], dstPtrTemp, vectorLoopCount, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                            increment_row_ptrs(srcPtrTemp[c], kernelSize, 1);
+                            dstPtrTemp++;
+                        }
+                    }
+                    // for the first padLength rows, we need not increment the src row pointers to next rows
+                    increment_row_ptrs(srcPtrRow, kernelSize, (!padLengthRows) ? srcDescPtr->strides.hStride : 0);
+                    dstPtrRow += dstDescPtr->strides.hStride;
+                }
+            }
+            else if ((srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
+            {
+                /* exclude ((2 * padLength) * 3) number of columns from alignedLength calculation
+                   since (padLength * 3) number of columns from the beginning and end of each row will be computed using raw c code */
+                Rpp32u alignedLength = ((bufferLength - (2 * padLength) * 3) / 32) * 32;
+                T *dstPtrChannels[3];
+                for (int i = 0; i < 3; i++)
+                    dstPtrChannels[i] = dstPtrChannel + i * dstDescPtr->strides.cStride;
+                for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+                {
+                    int vectorLoopCount = 0;
+                    bool padLengthRows = (i < padLength) ? 1: 0;
+                    T *srcPtrTemp[7];
+                    for (int k = 0; k < 7; k++)
+                        srcPtrTemp[k] = srcPtrRow[k];
+                    T *dstPtrTempChannels[3] = {dstPtrChannels[0], dstPtrChannels[1], dstPtrChannels[2]};
+
+                    Rpp32s rowKernelLoopLimit = kernelSize;
+                    get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
+                    process_left_border_columns_pkd_pln(srcPtrTemp, srcPtrRow, dstPtrTempChannels, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+#if __loongarch_asx
+                    // process remaining columns in each row
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount += 12)
+                    {
+                        __m256 pRow[7], pTemp[5];
+                        rpp_load_box_filter_float_7x7_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+                        add_rows_7x7(pRow, &pTemp[0]);
+
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 8);
+                        rpp_load_box_filter_float_7x7_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+                        add_rows_7x7(pRow, &pTemp[1]);
+
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 8);
+                        rpp_load_box_filter_float_7x7_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+                        add_rows_7x7(pRow, &pTemp[2]);
+
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 8);
+                        rpp_load_box_filter_float_7x7_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+                        add_rows_7x7(pRow, &pTemp[3]);
+                        pTemp[4] = avx_p0;
+
+                        __m256 pDst[2];
+                        blend_permute_add_mul_7x7_host<7, 63, 1, 15, 127, 3>(&pTemp[0], &pDst[0], pConvolutionFactor, pxMaskPkd, blendRegisterOrder);
+                        blend_permute_add_mul_7x7_host<7, 63, 1, 15, 127, 3>(&pTemp[1], &pDst[1], pConvolutionFactor, pxMaskPkd, blendRegisterOrder);
+
+                        __m128 pDstPln[3];
+                        rpp_convert12_f32pkd3_to_f32pln3(pDst, pDstPln);
+                        rpp_store12_float_pkd_pln(dstPtrTempChannels, pDstPln);
+
+                        increment_row_ptrs(srcPtrTemp, kernelSize, -12);
+                        increment_row_ptrs(dstPtrTempChannels, 3, 4);
+                    }
+#endif
+                    vectorLoopCount += padLength * 3;
+
+                    // process remaining columns in each row
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                    {
+                        int channel = vectorLoopCount % 3;
+                        box_filter_generic_tensor(srcPtrTemp, dstPtrTempChannels[channel], vectorLoopCount / 3, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare, 3);
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 1);
+                        dstPtrTempChannels[channel]++;
+                    }
+                    // for the first padLength rows, we need not increment the src row pointers to next rows
+                    increment_row_ptrs(srcPtrRow, kernelSize, (!padLengthRows) ? srcDescPtr->strides.hStride : 0);
+                    increment_row_ptrs(dstPtrChannels, 3, dstDescPtr->strides.hStride);
+                }
+            }
+        }
+        else if (kernelSize == 9)
+        {
+            T *srcPtrRow[9], *dstPtrRow;
+            for (int i = 0; i < 9; i++)
+                srcPtrRow[i] = srcPtrChannel + i * srcDescPtr->strides.hStride;
+            dstPtrRow = dstPtrChannel;
+
+            // box filter without fused output-layout toggle (NCHW -> NCHW)
+            if ((srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NCHW))
+            {
+                /* exclude (2 * padLength) number of columns from alignedLength calculation
+                   since padLength number of columns from the beginning and end of each row will be computed using raw c code */
+                Rpp32u alignedLength = ((bufferLength - (2 * padLength)) / 16) * 16;
+                for (int c = 0; c < srcDescPtr->c; c++)
+                {
+                    srcPtrRow[0] = srcPtrChannel;
+                    for (int k = 1; k < 9; k++)
+                        srcPtrRow[k] = srcPtrRow[k - 1] + srcDescPtr->strides.hStride;
+                    dstPtrRow = dstPtrChannel;
+                    for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+                    {
+                        int vectorLoopCount = 0;
+                        bool padLengthRows = (i < padLength) ? 1: 0;
+                        T *srcPtrTemp[9];
+                        for (int k = 0; k < 9; k++)
+                            srcPtrTemp[k] = srcPtrRow[k];
+                        T *dstPtrTemp = dstPtrRow;
+
+                        // get the number of rows needs to be loaded for the corresponding row
+                        Rpp32s rowKernelLoopLimit = kernelSize;
+                        get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
+                        process_left_border_columns_pln_pln(srcPtrTemp, dstPtrTemp, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                        dstPtrTemp += padLength;
+#if __loongarch_asx
+                        __m256 pRow[9];
+                        if (alignedLength)
+                            rpp_load_box_filter_float_9x9_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+
+                        // process alignedLength number of columns in each row
+                        for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
+                        {
+                            // add loaded values from 9 rows
+                            __m256 pTemp[2], pDst;
+                            add_rows_9x9(pRow, &pTemp[0]);
+                            increment_row_ptrs(srcPtrTemp, kernelSize, 8);
+
+                            rpp_load_box_filter_float_9x9_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+                            add_rows_9x9(pRow, &pTemp[1]);
+                            blend_permute_add_mul_9x9_host<1, 3, 7, 15, 31, 63, 127>(pTemp, &pDst, pConvolutionFactor, pxMaskPln, blendRegisterOrder);
+
+                            if constexpr (std::is_same<T, Rpp32f>::value)
+                                __lasx_xvst((__m256i)pDst, dstPtrTemp, 0);
+                            else if constexpr (std::is_same<T, Rpp16f>::value)
+                                __lsx_vst(lasx_cvtf32_ph(pDst, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC), (__m128i *)dstPtrTemp, 0);
+
+                            dstPtrTemp += 8;
+                        }
+#endif
+                        vectorLoopCount += padLength;
+                        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                        {
+                            box_filter_generic_tensor(srcPtrTemp, dstPtrTemp, vectorLoopCount, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                            increment_row_ptrs(srcPtrTemp, kernelSize, 1);
+                            dstPtrTemp++;
+                        }
+                        // for the first padLength rows, we need not increment the src row pointers to next rows
+                        increment_row_ptrs(srcPtrRow, kernelSize, (!padLengthRows) ? srcDescPtr->strides.hStride : 0);
+                        dstPtrRow += dstDescPtr->strides.hStride;
+                    }
+                    srcPtrChannel += srcDescPtr->strides.cStride;
+                    dstPtrChannel += dstDescPtr->strides.cStride;
+                }
+            }
+            else if ((srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NHWC))
+            {
+                /* exclude ((2 * padLength) * 3) number of columns from alignedLength calculation
+                   since (padLength * 3) number of columns from the beginning and end of each row will be computed using raw c code */
+                Rpp32u alignedLength = ((bufferLength - (2 * padLength) * 3) / 32) * 32;
+                for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+                {
+                    int vectorLoopCount = 0;
+                    bool padLengthRows = (i < padLength) ? 1: 0;
+                    T *srcPtrTemp[9];
+                    for (int k = 0; k < 9; k++)
+                        srcPtrTemp[k] = srcPtrRow[k];
+                    T *dstPtrTemp = dstPtrRow;
+
+                    Rpp32s rowKernelLoopLimit = kernelSize;
+                    get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
+                    process_left_border_columns_pkd_pkd(srcPtrTemp, srcPtrRow, dstPtrTemp, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                    dstPtrTemp += padLength * 3;
+#if __loongarch_asx
+                    __m256 pRow[9], pTemp[4];
+                    if (alignedLength)
+                    {
+                        rpp_load_box_filter_float_9x9_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+                        add_rows_9x9(pRow, &pTemp[0]);
+
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 8);
+                        rpp_load_box_filter_float_9x9_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+                        add_rows_9x9(pRow, &pTemp[1]);
+
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 8);
+                        rpp_load_box_filter_float_9x9_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+                        add_rows_9x9(pRow, &pTemp[2]);
+                    }
+
+                    // process remaining columns in each row
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
+                    {
+                        // add loaded values from 9 rows
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 8);
+                        rpp_load_box_filter_float_9x9_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+                        add_rows_9x9(pRow, &pTemp[3]);
+
+                        __m256 pDst;
+                        blend_permute_add_mul_9x9_host<7, 63, 1, 15, 127, 3, 31>(pTemp, &pDst, pConvolutionFactor, pxMaskPkd, blendRegisterOrder);
+                        if constexpr (std::is_same<T, Rpp32f>::value)
+                            __lasx_xvst((__m256i)pDst, dstPtrTemp, 0);
+                        else if constexpr (std::is_same<T, Rpp16f>::value)
+                            __lsx_vst(lasx_cvtf32_ph(pDst, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC), (__m128i *)dstPtrTemp, 0);
+
+                        dstPtrTemp += 8;
+                        pTemp[0] = pTemp[1];
+                        pTemp[1] = pTemp[2];
+                        pTemp[2] = pTemp[3];
+                    }
+                    increment_row_ptrs(srcPtrTemp, kernelSize, -16);
+#endif
+                    vectorLoopCount += padLength * 3;
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                    {
+                        box_filter_generic_tensor(srcPtrTemp, dstPtrTemp, vectorLoopCount / 3, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare, 3);
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 1);
+                        dstPtrTemp++;
+                    }
+                    // for the first padLength rows, we need not increment the src row pointers to next rows
+                    increment_row_ptrs(srcPtrRow, kernelSize, (!padLengthRows) ? srcDescPtr->strides.hStride : 0);
+                    dstPtrRow += dstDescPtr->strides.hStride;
+                }
+            }
+            // box filter with fused output-layout toggle (NCHW -> NHWC)
+            else if ((srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NHWC))
+            {
+                /* exclude (2 * padLength) number of columns from alignedLength calculation
+                   since padLength number of columns from the beginning and end of each row will be computed using raw c code */
+                Rpp32u alignedLength = ((bufferLength - (2 * padLength)) / 16) * 16;
+                for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+                {
+                    int vectorLoopCount = 0;
+                    bool padLengthRows = (i < padLength) ? 1: 0;
+                    T *srcPtrTemp[3][9];
+                    for (int c = 0; c < 3; c++)
+                    {
+                        Rpp32u channelStride = c * srcDescPtr->strides.cStride;
+                        for (int k = 0; k < 9; k++)
+                            srcPtrTemp[c][k] = srcPtrRow[k] + channelStride;
+                    }
+                    T *dstPtrTemp = dstPtrRow;
+
+                    // get the number of rows needs to be loaded for the corresponding row
+                    Rpp32s rowKernelLoopLimit = kernelSize;
+                    get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
+                    for (int k = 0; k < padLength; k++)
+                    {
+                        for (int c = 0; c < 3; c++)
+                        {
+                            box_filter_generic_tensor(srcPtrTemp[c], dstPtrTemp, k, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                            dstPtrTemp++;
+                        }
+                    }
+#if __loongarch_asx
+                    // process alignedLength number of columns in each row
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
+                    {
+                        __m256 pResultPln[3];
+                        for (int c = 0; c < 3; c++)
+                        {
+                            // add loaded values from 9 rows
+                            __m256 pRow[9], pTemp[2];
+                            rpp_load_box_filter_float_9x9_host(pRow, srcPtrTemp[c], rowKernelLoopLimit);
+                            add_rows_9x9(pRow, &pTemp[0]);
+
+                            increment_row_ptrs(srcPtrTemp[c], kernelSize, 8);
+                            rpp_load_box_filter_float_9x9_host(pRow, srcPtrTemp[c], rowKernelLoopLimit);
+                            add_rows_9x9(pRow, &pTemp[1]);
+
+                            blend_permute_add_mul_9x9_host<1, 3, 7, 15, 31, 63, 127>(pTemp, &pResultPln[c], pConvolutionFactor, pxMaskPln, blendRegisterOrder);
+                        }
+
+                        if constexpr (std::is_same<T, Rpp32f>::value)
+                           rpp_store24_f32pln3_to_f32pkd3_avx(dstPtrTemp, pResultPln);
+                        else if constexpr (std::is_same<T, Rpp16f>::value)
+                           rpp_store24_f32pln3_to_f16pkd3_avx(dstPtrTemp, pResultPln);
+                        dstPtrTemp += 24;
+                    }
+#endif
+                    vectorLoopCount += padLength;
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                    {
+                        for (int c = 0; c < srcDescPtr->c; c++)
+                        {
+                            box_filter_generic_tensor(srcPtrTemp[c], dstPtrTemp, vectorLoopCount, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                            increment_row_ptrs(srcPtrTemp[c], kernelSize, 1);
+                            dstPtrTemp++;
+                        }
+                    }
+                    // for the first padLength rows, we need not increment the src row pointers to next rows
+                    increment_row_ptrs(srcPtrRow, kernelSize, (!padLengthRows) ? srcDescPtr->strides.hStride : 0);
+                    dstPtrRow += dstDescPtr->strides.hStride;
+                }
+            }
+            else if ((srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
+            {
+                /* exclude ((2 * padLength) * 3) number of columns from alignedLength calculation
+                   since (padLength * 3) number of columns from the beginning and end of each row will be computed using raw c code */
+                Rpp32u alignedLength = ((bufferLength - (2 * padLength) * 3) / 40) * 40;
+                T *dstPtrChannels[3];
+                for (int i = 0; i < 3; i++)
+                    dstPtrChannels[i] = dstPtrChannel + i * dstDescPtr->strides.cStride;
+                for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+                {
+                    int vectorLoopCount = 0;
+                    bool padLengthRows = (i < padLength) ? 1: 0;
+                    T *srcPtrTemp[9];
+                    for (int k = 0; k < 9; k++)
+                        srcPtrTemp[k] = srcPtrRow[k];
+                    T *dstPtrTempChannels[3] = {dstPtrChannels[0], dstPtrChannels[1], dstPtrChannels[2]};
+
+                    Rpp32s rowKernelLoopLimit = kernelSize;
+                    get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
+                    process_left_border_columns_pkd_pln(srcPtrTemp, srcPtrRow, dstPtrTempChannels, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+#if __loongarch_asx
+                    // process remaining columns in each row
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount += 12)
+                    {
+                        __m256 pRow[9], pTemp[5];
+                        rpp_load_box_filter_float_9x9_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+                        add_rows_9x9(pRow, &pTemp[0]);
+
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 8);
+                        rpp_load_box_filter_float_9x9_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+                        add_rows_9x9(pRow, &pTemp[1]);
+
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 8);
+                        rpp_load_box_filter_float_9x9_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+                        add_rows_9x9(pRow, &pTemp[2]);
+
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 8);
+                        rpp_load_box_filter_float_9x9_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+                        add_rows_9x9(pRow, &pTemp[3]);
+
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 8);
+                        rpp_load_box_filter_float_9x9_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+                        add_rows_9x9(pRow, &pTemp[4]);
+
+                        __m256 pDst[2];
+                        blend_permute_add_mul_9x9_host<7, 63, 1, 15, 127, 3, 31>(&pTemp[0], &pDst[0], pConvolutionFactor, pxMaskPkd, blendRegisterOrder);
+                        blend_permute_add_mul_9x9_host<7, 63, 1, 15, 127, 3, 31>(&pTemp[1], &pDst[1], pConvolutionFactor, pxMaskPkd, blendRegisterOrder);
+
+                        __m128 pDstPln[3];
+                        rpp_convert12_f32pkd3_to_f32pln3(pDst, pDstPln);
+                        rpp_store12_float_pkd_pln(dstPtrTempChannels, pDstPln);
+
+                        increment_row_ptrs(srcPtrTemp, kernelSize, -20);
+                        increment_row_ptrs(dstPtrTempChannels, 3, 4);
+                    }
+#endif
+                    vectorLoopCount += padLength * 3;
+
+                    // process remaining columns in each row
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                    {
+                        int channel = vectorLoopCount % 3;
+                        box_filter_generic_tensor(srcPtrTemp, dstPtrTempChannels[channel], vectorLoopCount / 3, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare, 3);
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 1);
+                        dstPtrTempChannels[channel]++;
+                    }
+                    // for the first padLength rows, we need not increment the src row pointers to next rows
+                    increment_row_ptrs(srcPtrRow, kernelSize, (!padLengthRows) ? srcDescPtr->strides.hStride : 0);
+                    increment_row_ptrs(dstPtrChannels, 3, dstDescPtr->strides.hStride);
+                }
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template<typename T>
+RppStatus box_filter_generic_host_tensor(T *srcPtr,
+                                         RpptDescPtr srcDescPtr,
+                                         T *dstPtr,
+                                         RpptDescPtr dstDescPtr,
+                                         Rpp32u kernelSize,
+                                         RpptROIPtr roiTensorPtrSrc,
+                                         RpptRoiType roiType,
+                                         RppLayoutParams layoutParams,
+                                         rpp::Handle& handle)
+{
+    RpptROI roiDefault = {0, 0, (Rpp32s)srcDescPtr->w, (Rpp32s)srcDescPtr->h};
+    Rpp32u numThreads = handle.GetNumThreads();
+
+    omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+    for(int batchCount = 0; batchCount < dstDescPtr->n; batchCount++)
+    {
+        RpptROI roi;
+        RpptROIPtr roiPtrInput = &roiTensorPtrSrc[batchCount];
+        compute_roi_validation_host(roiPtrInput, &roi, &roiDefault, roiType);
+
+        T *srcPtrImage, *dstPtrImage;
+        srcPtrImage = srcPtr + batchCount * srcDescPtr->strides.nStride;
+        dstPtrImage = dstPtr + batchCount * dstDescPtr->strides.nStride;
+
+        Rpp32u padLength = kernelSize / 2;
+        Rpp32u bufferLength = roi.xywhROI.roiWidth * layoutParams.bufferMultiplier;
+        Rpp32f kernelSizeInverseSquare = 1.0 / (kernelSize * kernelSize);
+        Rpp32u unpaddedHeight = roi.xywhROI.roiHeight - padLength;
+        Rpp32u unpaddedWidth = roi.xywhROI.roiWidth - padLength;
+
+        T *srcPtrChannel, *dstPtrChannel;
+        srcPtrChannel = srcPtrImage + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
+        dstPtrChannel = dstPtrImage;
+
+        T *srcPtrRow[kernelSize], *dstPtrRow;
+        for (int k = 0; k < kernelSize; k++)
+            srcPtrRow[k] = srcPtrChannel + k * srcDescPtr->strides.hStride;
+        dstPtrRow = dstPtrChannel;
+        if ((srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NCHW))
+        {
+            for (int c = 0; c < srcDescPtr->c; c++)
+            {
+                srcPtrRow[0] = srcPtrChannel;
+                for (int k = 1; k < kernelSize; k++)
+                    srcPtrRow[k] = srcPtrRow[k - 1] + srcDescPtr->strides.hStride;
+                dstPtrRow = dstPtrChannel;
+                for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+                {
+                    int vectorLoopCount = 0;
+                    bool padLengthRows = (i < padLength) ? 1: 0;
+                    T *srcPtrTemp[kernelSize];
+                    for (int k = 0; k < kernelSize; k++)
+                        srcPtrTemp[k] = srcPtrRow[k];
+                    T *dstPtrTemp = dstPtrRow;
+
+                    Rpp32s rowKernelLoopLimit = kernelSize;
+                    get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
+                    process_left_border_columns_pln_pln(srcPtrTemp, dstPtrTemp, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                    dstPtrTemp += padLength;
+                    vectorLoopCount += padLength;
+
+                    // process remaining columns in each row
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                    {
+                        box_filter_generic_tensor(srcPtrTemp, dstPtrTemp, vectorLoopCount, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 1);
+                        dstPtrTemp++;
+                    }
+                    // for the first padLength rows, we need not increment the src row pointers to next rows
+                    increment_row_ptrs(srcPtrRow, kernelSize, (!padLengthRows) ? srcDescPtr->strides.hStride : 0);
+                    dstPtrRow += dstDescPtr->strides.hStride;
+                }
+                srcPtrChannel += srcDescPtr->strides.cStride;
+                dstPtrChannel += dstDescPtr->strides.cStride;
+            }
+        }
+        else if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NHWC))
+        {
+            for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+            {
+                int vectorLoopCount = 0;
+                bool padLengthRows = (i < padLength) ? 1: 0;
+                T *srcPtrTemp[kernelSize];
+                for (int k = 0; k < kernelSize; k++)
+                    srcPtrTemp[k] = srcPtrRow[k];
+                T *dstPtrTemp = dstPtrRow;
+
+                Rpp32s rowKernelLoopLimit = kernelSize;
+                get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
+                process_left_border_columns_pkd_pkd(srcPtrTemp, srcPtrRow, dstPtrTemp, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                dstPtrTemp += padLength * 3;
+                vectorLoopCount += padLength * 3;
+
+                // process remaining columns in each row
+                for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                {
+                    box_filter_generic_tensor(srcPtrTemp, dstPtrTemp, vectorLoopCount / 3, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare, 3);
+                    increment_row_ptrs(srcPtrTemp, kernelSize, 1);
+                    dstPtrTemp++;
+                }
+                // for the first padLength rows, we need not increment the src row pointers to next rows
+                increment_row_ptrs(srcPtrRow, kernelSize, (!padLengthRows) ? srcDescPtr->strides.hStride : 0);
+                dstPtrRow += dstDescPtr->strides.hStride;
+            }
+        }
+        else if ((srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NHWC))
+        {
+            for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+            {
+                int vectorLoopCount = 0;
+                bool padLengthRows = (i < padLength) ? 1: 0;
+                T *srcPtrTemp[3][kernelSize];
+                for (int c = 0; c < 3; c++)
+                {
+                    Rpp32u channelStride = c * srcDescPtr->strides.cStride;
+                    for (int k = 0; k < kernelSize; k++)
+                        srcPtrTemp[c][k] = srcPtrRow[k] + channelStride;
+                }
+                T *dstPtrTemp = dstPtrRow;
+
+                Rpp32s rowKernelLoopLimit = kernelSize;
+                get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
+
+                // process padLength number of columns in each row
+                for (int k = 0; k < padLength; k++)
+                {
+                    for (int c = 0; c < 3; c++)
+                    {
+                        box_filter_generic_tensor(srcPtrTemp[c], dstPtrTemp, k, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                        dstPtrTemp++;
+                    }
+                }
+                vectorLoopCount += padLength;
+
+                // process remaining columns in each row
+                for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                {
+                    for (int c = 0; c < srcDescPtr->c; c++)
+                    {
+                        box_filter_generic_tensor(srcPtrTemp[c], dstPtrTemp, vectorLoopCount, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                        increment_row_ptrs(srcPtrTemp[c], kernelSize, 1);
+                        dstPtrTemp++;
+                    }
+                }
+                // for the first padLength rows, we need not increment the src row pointers to next rows
+                increment_row_ptrs(srcPtrRow, kernelSize, (!padLengthRows) ? srcDescPtr->strides.hStride : 0);
+                dstPtrRow += dstDescPtr->strides.hStride;
+            }
+        }
+        else if ((srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
+        {
+            T *dstPtrChannels[3];
+            for (int c = 0; c < 3; c++)
+                dstPtrChannels[c] = dstPtrChannel + c * dstDescPtr->strides.cStride;
+            for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+            {
+                int vectorLoopCount = 0;
+                bool padLengthRows = (i < padLength) ? 1: 0;
+                T *srcPtrTemp[kernelSize];
+                for (int k = 0; k < kernelSize; k++)
+                    srcPtrTemp[k] = srcPtrRow[k];
+                T *dstPtrTempChannels[3] = {dstPtrChannels[0], dstPtrChannels[1], dstPtrChannels[2]};
+
+                Rpp32s rowKernelLoopLimit = kernelSize;
+                get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
+                process_left_border_columns_pkd_pln(srcPtrTemp, srcPtrRow, dstPtrTempChannels, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare);
+                vectorLoopCount += padLength * 3;
+
+                // process remaining columns in each row
+                for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                {
+                    int channel = vectorLoopCount % 3;
+                    box_filter_generic_tensor(srcPtrTemp, dstPtrTempChannels[channel], vectorLoopCount / 3, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, kernelSizeInverseSquare, 3);
+                    increment_row_ptrs(srcPtrTemp, kernelSize, 1);
+                    dstPtrTempChannels[channel]++;
+                }
+                // for the first padLength rows, we need not increment the src row pointers to next rows
+                increment_row_ptrs(srcPtrRow, kernelSize, (!padLengthRows) ? srcDescPtr->strides.hStride : 0);
+                increment_row_ptrs(dstPtrChannels, 3, dstDescPtr->strides.hStride);
+            }
+        }
+    }
+    return RPP_SUCCESS;
+}
diff --git a/src/modules/cpu/kernel/brightness.hpp b/src/modules/cpu/kernel/brightness.hpp
index cec8d039..6a274b86 100644
--- a/src/modules/cpu/kernel/brightness.hpp
+++ b/src/modules/cpu/kernel/brightness.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 RppStatus brightness_u8_u8_host_tensor(Rpp8u *srcPtr,
                                        RpptDescPtr srcDescPtr,
@@ -65,10 +70,18 @@ RppStatus brightness_u8_u8_host_tensor(Rpp8u *srcPtr,
         Rpp32u vectorIncrement = 48;
         Rpp32u vectorIncrementPerChannel = 16;
 
-#if __AVX2__
+#if defined(__AVX2__)
         __m256 pBrightnessParams[2];
         pBrightnessParams[0] = _mm256_set1_ps(alpha);
         pBrightnessParams[1] = _mm256_set1_ps(beta);
+#elif defined(__loongarch_asx)
+        __m256 pBrightnessParams[2];
+        pBrightnessParams[0] = lasx_set1_f32(alpha);
+        pBrightnessParams[1] = lasx_set1_f32(beta);
+#elif defined(__loongarch_sx)
+        __m128 pBrightnessParams[2];
+        pBrightnessParams[0] = lsx_set1_f32(alpha);
+        pBrightnessParams[1] = lsx_set1_f32(beta);
 #else
         __m128 pBrightnessParams[2];
         pBrightnessParams[0] = _mm_set1_ps(alpha);
@@ -95,7 +108,7 @@ RppStatus brightness_u8_u8_host_tensor(Rpp8u *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[6];
                     rpp_simd_load(rpp_load48_u8pkd3_to_f32pln3_avx, srcPtrTemp, p);
                     compute_brightness_48_host(p, pBrightnessParams);  // brightness adjustment
@@ -151,7 +164,7 @@ RppStatus brightness_u8_u8_host_tensor(Rpp8u *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[6];
                     rpp_simd_load(rpp_load48_u8pln3_to_f32pln3_avx, srcPtrTempR, srcPtrTempG, srcPtrTempB, p);    // simd loads
                     compute_brightness_48_host(p, pBrightnessParams);  // brightness adjustment
@@ -205,7 +218,7 @@ RppStatus brightness_u8_u8_host_tensor(Rpp8u *srcPtr,
                     int vectorLoopCount = 0;
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += 16)
                     {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                         __m256 p[2];
 
                         rpp_simd_load(rpp_load16_u8_to_f32_avx, srcPtrTemp, p);    // simd loads
@@ -275,7 +288,7 @@ RppStatus brightness_f32_f32_host_tensor(Rpp32f *srcPtr,
         srcPtrChannel = srcPtrImage + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
         dstPtrChannel = dstPtrImage;
 
-#if __AVX2__
+#if defined(__AVX2__)
         Rpp32u alignedLength = (bufferLength / 24) * 24;
         Rpp32u vectorIncrement = 24;
         Rpp32u vectorIncrementPerChannel = 8;
@@ -283,6 +296,22 @@ RppStatus brightness_f32_f32_host_tensor(Rpp32f *srcPtr,
         __m256 pBrightnessParams[2];
         pBrightnessParams[0] = _mm256_set1_ps(alpha);
         pBrightnessParams[1] = _mm256_set1_ps(beta);
+#elif defined(__loongarch_asx)
+        Rpp32u alignedLength = (bufferLength / 24) * 24;
+        Rpp32u vectorIncrement = 24;
+        Rpp32u vectorIncrementPerChannel = 8;
+
+        __m256 pBrightnessParams[2];
+        pBrightnessParams[0] = lasx_set1_f32(alpha);
+        pBrightnessParams[1] = lasx_set1_f32(beta);
+#elif defined(__loongarch_sx)
+        Rpp32u alignedLength = (bufferLength / 12) * 12;
+        Rpp32u vectorIncrement = 12;
+        Rpp32u vectorIncrementPerChannel = 4;
+
+        __m128 pBrightnessParams[2];
+        pBrightnessParams[0] = lsx_set1_f32(alpha);
+        pBrightnessParams[1] = lsx_set1_f32(beta);
 #else
         Rpp32u alignedLength = (bufferLength / 12) * 12;
         Rpp32u vectorIncrement = 12;
@@ -313,7 +342,7 @@ RppStatus brightness_f32_f32_host_tensor(Rpp32f *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[3];
                     rpp_simd_load(rpp_load24_f32pkd3_to_f32pln3_avx, srcPtrTemp, p);    // simd loads
                     compute_brightness_24_host(p, pBrightnessParams);  // brightness adjustment
@@ -368,7 +397,7 @@ RppStatus brightness_f32_f32_host_tensor(Rpp32f *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[3];
                     rpp_simd_load(rpp_load24_f32pln3_to_f32pln3_avx, srcPtrTempR, srcPtrTempG, srcPtrTempB, p);    // simd loads
                     compute_brightness_24_host(p, pBrightnessParams);  // brightness adjustment
@@ -423,7 +452,7 @@ RppStatus brightness_f32_f32_host_tensor(Rpp32f *srcPtr,
                     int vectorLoopCount = 0;
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                     {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                         __m256 p[1];
 
                         rpp_simd_load(rpp_load8_f32_to_f32_avx, srcPtrTemp, p);    // simd loads
@@ -495,7 +524,7 @@ RppStatus brightness_f16_f16_host_tensor(Rpp16f *srcPtr,
         srcPtrChannel = srcPtrImage + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
         dstPtrChannel = dstPtrImage;
 
-#if __AVX2__
+#if defined(__AVX2__)
         Rpp32u alignedLength = (bufferLength / 24) * 24;
         Rpp32u vectorIncrement = 24;
         Rpp32u vectorIncrementPerChannel = 8;
@@ -503,6 +532,22 @@ RppStatus brightness_f16_f16_host_tensor(Rpp16f *srcPtr,
         __m256 pBrightnessParams[2];
         pBrightnessParams[0] = _mm256_set1_ps(alpha);
         pBrightnessParams[1] = _mm256_set1_ps(beta);
+#elif defined(__loongarch_asx)
+        Rpp32u alignedLength = (bufferLength / 24) * 24;
+        Rpp32u vectorIncrement = 24;
+        Rpp32u vectorIncrementPerChannel = 8;
+
+        __m256 pBrightnessParams[2];
+        pBrightnessParams[0] = lasx_set1_f32(alpha);
+        pBrightnessParams[1] = lasx_set1_f32(beta);
+#elif defined(__loongarch_sx)
+        Rpp32u alignedLength = (bufferLength / 12) * 12;
+        Rpp32u vectorIncrement = 12;
+        Rpp32u vectorIncrementPerChannel = 4;
+
+        __m128 pBrightnessParams[2];
+        pBrightnessParams[0] = lsx_set1_f32(alpha);
+        pBrightnessParams[1] = lsx_set1_f32(beta);
 #else
         Rpp32u alignedLength = (bufferLength / 12) * 12;
         Rpp32u vectorIncrement = 12;
@@ -539,7 +584,7 @@ RppStatus brightness_f16_f16_host_tensor(Rpp16f *srcPtr,
                     for(int cnt = 0; cnt < vectorIncrement; cnt++)
                         srcPtrTemp_ps[cnt] = (Rpp32f) srcPtrTemp[cnt];
 
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[3];
                     rpp_simd_load(rpp_load24_f32pkd3_to_f32pln3_avx, srcPtrTemp_ps, p);    // simd loads
                     compute_brightness_24_host(p, pBrightnessParams);  // brightness adjustment
@@ -610,7 +655,7 @@ RppStatus brightness_f16_f16_host_tensor(Rpp16f *srcPtr,
                         srcPtrTempG_ps[cnt] = (Rpp32f) srcPtrTempG[cnt];
                         srcPtrTempB_ps[cnt] = (Rpp32f) srcPtrTempB[cnt];
                     }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[3];
                     rpp_simd_load(rpp_load24_f32pln3_to_f32pln3_avx, srcPtrTempR_ps, srcPtrTempG_ps, srcPtrTempB_ps, p);    // simd loads
                     compute_brightness_24_host(p, pBrightnessParams);  // brightness adjustment
@@ -673,7 +718,7 @@ RppStatus brightness_f16_f16_host_tensor(Rpp16f *srcPtr,
                         {
                             srcPtrTemp_ps[cnt] = (Rpp16f) srcPtrTemp[cnt];
                         }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                         __m256 p[1];
 
                         rpp_simd_load(rpp_load8_f32_to_f32_avx, srcPtrTemp_ps, p);    // simd loads
@@ -755,10 +800,18 @@ RppStatus brightness_i8_i8_host_tensor(Rpp8s *srcPtr,
         Rpp32u vectorIncrement = 48;
         Rpp32u vectorIncrementPerChannel = 16;
 
-#if __AVX2__
+#if defined(__AVX2__)
         __m256 pBrightnessParams[2];
         pBrightnessParams[0] = _mm256_set1_ps(alpha);
         pBrightnessParams[1] = _mm256_set1_ps(beta);
+#elif defined(__loongarch_asx)
+        __m256 pBrightnessParams[2];
+        pBrightnessParams[0] = lasx_set1_f32(alpha);
+        pBrightnessParams[1] = lasx_set1_f32(beta);
+#elif defined(__loongarch_sx)
+        __m128 pBrightnessParams[2];
+        pBrightnessParams[0] = lsx_set1_f32(alpha);
+        pBrightnessParams[1] = lsx_set1_f32(beta);
 #else
         __m128 pBrightnessParams[2];
         pBrightnessParams[0] = _mm_set1_ps(alpha);
@@ -785,7 +838,7 @@ RppStatus brightness_i8_i8_host_tensor(Rpp8s *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[6];
                     rpp_simd_load(rpp_load48_i8pkd3_to_f32pln3_avx, srcPtrTemp, p);
                     compute_brightness_48_host(p, pBrightnessParams);  // brightness adjustment
@@ -841,7 +894,7 @@ RppStatus brightness_i8_i8_host_tensor(Rpp8s *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[6];
                     rpp_simd_load(rpp_load48_i8pln3_to_f32pln3_avx, srcPtrTempR, srcPtrTempG, srcPtrTempB, p);    // simd loads
                     compute_brightness_48_host(p, pBrightnessParams);  // brightness adjustment
@@ -896,7 +949,7 @@ RppStatus brightness_i8_i8_host_tensor(Rpp8s *srcPtr,
                     int vectorLoopCount = 0;
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += 16)
                     {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                         __m256 p[2];
 
                         rpp_simd_load(rpp_load16_i8_to_f32_avx, srcPtrTemp, p);    // simd loads
diff --git a/src/modules/cpu/kernel/color_cast.hpp b/src/modules/cpu/kernel/color_cast.hpp
index d8fd2097..af7eb182 100644
--- a/src/modules/cpu/kernel/color_cast.hpp
+++ b/src/modules/cpu/kernel/color_cast.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 RppStatus color_cast_u8_u8_host_tensor(Rpp8u *srcPtr,
                                        RpptDescPtr srcDescPtr,
@@ -59,11 +64,19 @@ RppStatus color_cast_u8_u8_host_tensor(Rpp8u *srcPtr,
 
         Rpp32u bufferLength = roi.xywhROI.roiWidth * layoutParams.bufferMultiplier;
 
+#ifdef __loongarch_sx
+        __m128 pMul = lsx_set1_f32(alphaParam);
+        __m128 pAdd[3];
+        pAdd[0] = lsx_set1_f32(bParam);
+        pAdd[1] = lsx_set1_f32(gParam);
+        pAdd[2] = lsx_set1_f32(rParam);
+#else
         __m128 pMul = _mm_set1_ps(alphaParam);
         __m128 pAdd[3];
         pAdd[0] = _mm_set1_ps(bParam);
         pAdd[1] = _mm_set1_ps(gParam);
         pAdd[2] = _mm_set1_ps(rParam);
+#endif
 
         Rpp8u *srcPtrChannel, *dstPtrChannel;
         srcPtrChannel = srcPtrImage + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
@@ -314,11 +327,19 @@ RppStatus color_cast_f32_f32_host_tensor(Rpp32f *srcPtr,
 
         Rpp32u bufferLength = roi.xywhROI.roiWidth * layoutParams.bufferMultiplier;
 
+#ifdef __loongarch_sx
+        __m128 pMul = lsx_set1_f32(alphaParam);
+        __m128 pAdd[3];
+        pAdd[0] = lsx_set1_f32(bParam);
+        pAdd[1] = lsx_set1_f32(gParam);
+        pAdd[2] = lsx_set1_f32(rParam);
+#else
         __m128 pMul = _mm_set1_ps(alphaParam);
         __m128 pAdd[3];
         pAdd[0] = _mm_set1_ps(bParam);
         pAdd[1] = _mm_set1_ps(gParam);
         pAdd[2] = _mm_set1_ps(rParam);
+#endif
 
         Rpp32f *srcPtrChannel, *dstPtrChannel;
         srcPtrChannel = srcPtrImage + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
@@ -569,11 +590,19 @@ RppStatus color_cast_f16_f16_host_tensor(Rpp16f *srcPtr,
 
         Rpp32u bufferLength = roi.xywhROI.roiWidth * layoutParams.bufferMultiplier;
 
+#ifdef __loongarch_sx
+        __m128 pMul = lsx_set1_f32(alphaParam);
+        __m128 pAdd[3];
+        pAdd[0] = lsx_set1_f32(bParam);
+        pAdd[1] = lsx_set1_f32(gParam);
+        pAdd[2] = lsx_set1_f32(rParam);
+#else
         __m128 pMul = _mm_set1_ps(alphaParam);
         __m128 pAdd[3];
         pAdd[0] = _mm_set1_ps(bParam);
         pAdd[1] = _mm_set1_ps(gParam);
         pAdd[2] = _mm_set1_ps(rParam);
+#endif
 
         Rpp16f *srcPtrChannel, *dstPtrChannel;
         srcPtrChannel = srcPtrImage + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
@@ -880,11 +909,19 @@ RppStatus color_cast_i8_i8_host_tensor(Rpp8s *srcPtr,
 
         Rpp32u bufferLength = roi.xywhROI.roiWidth * layoutParams.bufferMultiplier;
 
+#ifdef __loongarch_sx
+        __m128 pMul = lsx_set1_f32(alphaParam);
+        __m128 pAdd[3];
+        pAdd[0] = lsx_set1_f32(bParam);
+        pAdd[1] = lsx_set1_f32(gParam);
+        pAdd[2] = lsx_set1_f32(rParam);
+#else
         __m128 pMul = _mm_set1_ps(alphaParam);
         __m128 pAdd[3];
         pAdd[0] = _mm_set1_ps(bParam);
         pAdd[1] = _mm_set1_ps(gParam);
         pAdd[2] = _mm_set1_ps(rParam);
+#endif
 
         Rpp8s *srcPtrChannel, *dstPtrChannel;
         srcPtrChannel = srcPtrImage + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
diff --git a/src/modules/cpu/kernel/color_jitter.hpp b/src/modules/cpu/kernel/color_jitter.hpp
index cb3a9156..137ddb08 100644
--- a/src/modules/cpu/kernel/color_jitter.hpp
+++ b/src/modules/cpu/kernel/color_jitter.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 RppStatus color_jitter_u8_u8_host_tensor(Rpp8u *srcPtr,
                                          RpptDescPtr srcDescPtr,
@@ -67,7 +72,11 @@ RppStatus color_jitter_u8_u8_host_tensor(Rpp8u *srcPtr,
         __m128 pCtm[12];
         for(int i = 0; i < 12; i++)
         {
+#ifdef __loongarch_sx
+            pCtm[i] = lsx_set1_f32(ctm[i]);
+#else
             pCtm[i] = _mm_set1_ps(ctm[i]);
+#endif
         }
 
         Rpp8u *srcPtrChannel, *dstPtrChannel;
@@ -327,7 +336,11 @@ RppStatus color_jitter_f32_f32_host_tensor(Rpp32f *srcPtr,
         __m128 pCtm[12];
         for(int i = 0; i < 12; i++)
         {
+#ifdef __loongarch_sx
+            pCtm[i] = lsx_set1_f32(ctm[i]);
+#else
             pCtm[i] = _mm_set1_ps(ctm[i]);
+#endif
         }
 
         Rpp32f *srcPtrChannel, *dstPtrChannel;
@@ -587,7 +600,11 @@ RppStatus color_jitter_f16_f16_host_tensor(Rpp16f *srcPtr,
         __m128 pCtm[12];
         for(int i = 0; i < 12; i++)
         {
+#ifdef __loongarch_sx
+            pCtm[i] = lsx_set1_f32(ctm[i]);
+#else
             pCtm[i] = _mm_set1_ps(ctm[i]);
+#endif
         }
 
         Rpp16f *srcPtrChannel, *dstPtrChannel;
@@ -903,7 +920,11 @@ RppStatus color_jitter_i8_i8_host_tensor(Rpp8s *srcPtr,
         __m128 pCtm[12];
         for(int i = 0; i < 12; i++)
         {
+#ifdef __loongarch_sx
+            pCtm[i] = lsx_set1_f32(ctm[i]);
+#else
             pCtm[i] = _mm_set1_ps(ctm[i]);
+#endif
         }
 
         Rpp8s *srcPtrChannel, *dstPtrChannel;
diff --git a/src/modules/cpu/kernel/color_temperature.hpp b/src/modules/cpu/kernel/color_temperature.hpp
index dbe33a51..5a4a8fd8 100644
--- a/src/modules/cpu/kernel/color_temperature.hpp
+++ b/src/modules/cpu/kernel/color_temperature.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 RppStatus color_temperature_u8_u8_host_tensor(Rpp8u *srcPtr,
                                               RpptDescPtr srcDescPtr,
@@ -53,7 +58,11 @@ RppStatus color_temperature_u8_u8_host_tensor(Rpp8u *srcPtr,
 
         Rpp32u bufferLength = roi.xywhROI.roiWidth * layoutParams.bufferMultiplier;
 
+#ifdef __loongarch_asx
+        __m256 pAdj = lasx_set1_f32(adjustmentValue);
+#else
         __m256 pAdj = _mm256_set1_ps(adjustmentValue);
+#endif
 
         Rpp8u *srcPtrChannel, *dstPtrChannel;
         srcPtrChannel = srcPtrImage + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
@@ -292,7 +301,11 @@ RppStatus color_temperature_f32_f32_host_tensor(Rpp32f *srcPtr,
 
         Rpp32u bufferLength = roi.xywhROI.roiWidth * layoutParams.bufferMultiplier;
 
+#ifdef __loongarch_asx
+        __m256 pAdj = lasx_set1_f32(adjustmentValue);
+#else
         __m256 pAdj = _mm256_set1_ps(adjustmentValue);
+#endif
 
         Rpp32f *srcPtrChannel, *dstPtrChannel;
         srcPtrChannel = srcPtrImage + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
@@ -531,7 +544,11 @@ RppStatus color_temperature_f16_f16_host_tensor(Rpp16f *srcPtr,
 
         Rpp32u bufferLength = roi.xywhROI.roiWidth * layoutParams.bufferMultiplier;
 
+#ifdef __loongarch_asx
+        __m256 pAdj = lasx_set1_f32(adjustmentValue);
+#else
         __m256 pAdj = _mm256_set1_ps(adjustmentValue);
+#endif
 
         Rpp16f *srcPtrChannel, *dstPtrChannel;
         srcPtrChannel = srcPtrImage + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
@@ -822,7 +839,11 @@ RppStatus color_temperature_i8_i8_host_tensor(Rpp8s *srcPtr,
 
         Rpp32u bufferLength = roi.xywhROI.roiWidth * layoutParams.bufferMultiplier;
 
+#ifdef __loongarch_asx
+        __m256 pAdj = lasx_set1_f32(adjustmentValue);
+#else
         __m256 pAdj = _mm256_set1_ps(adjustmentValue);
+#endif
 
         Rpp8s *srcPtrChannel, *dstPtrChannel;
         srcPtrChannel = srcPtrImage + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
diff --git a/src/modules/cpu/kernel/color_to_greyscale.hpp b/src/modules/cpu/kernel/color_to_greyscale.hpp
index 3df62b1c..ee8a5291 100644
--- a/src/modules/cpu/kernel/color_to_greyscale.hpp
+++ b/src/modules/cpu/kernel/color_to_greyscale.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 RppStatus color_to_greyscale_u8_u8_host_tensor(Rpp8u *srcPtr,
                                                RpptDescPtr srcDescPtr,
@@ -48,11 +53,21 @@ RppStatus color_to_greyscale_u8_u8_host_tensor(Rpp8u *srcPtr,
         Rpp32u vectorIncrement = 48;
         Rpp32u vectorIncrementPerChannel = 16;
 
-#if __AVX2__
+#if defined(__AVX2__)
         __m256 pChannelWeights[3];
         pChannelWeights[0] = _mm256_set1_ps(channelWeights[0]);
         pChannelWeights[1] = _mm256_set1_ps(channelWeights[1]);
         pChannelWeights[2] = _mm256_set1_ps(channelWeights[2]);
+#elif defined(__loongarch_asx)
+        __m256 pChannelWeights[3];
+        pChannelWeights[0] = lasx_set1_f32(channelWeights[0]);
+        pChannelWeights[1] = lasx_set1_f32(channelWeights[1]);
+        pChannelWeights[2] = lasx_set1_f32(channelWeights[2]);
+#elif defined(__loongarch_sx)
+        __m128 pChannelWeights[3];
+        pChannelWeights[0] = lsx_set1_f32(channelWeights[0]);
+        pChannelWeights[1] = lsx_set1_f32(channelWeights[1]);
+        pChannelWeights[2] = lsx_set1_f32(channelWeights[2]);
 #else
         __m128 pChannelWeights[3];
         pChannelWeights[0] = _mm_set1_ps(channelWeights[0]);
@@ -76,7 +91,7 @@ RppStatus color_to_greyscale_u8_u8_host_tensor(Rpp8u *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[6];
                     rpp_simd_load(rpp_load48_u8pkd3_to_f32pln3_avx, srcPtrTemp, p); // simd loads
                     compute_color_48_to_greyscale_16_host(p, pChannelWeights);       // color_to_greyscale adjustment
@@ -123,7 +138,7 @@ RppStatus color_to_greyscale_u8_u8_host_tensor(Rpp8u *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[6];
                     rpp_simd_load(rpp_load48_u8pln3_to_f32pln3_avx, srcPtrTempR, srcPtrTempG, srcPtrTempB, p);  // simd loads
                     compute_color_48_to_greyscale_16_host(p, pChannelWeights);                                   // color_to_greyscale adjustment
@@ -178,7 +193,7 @@ RppStatus color_to_greyscale_f32_f32_host_tensor(Rpp32f *srcPtr,
 
         Rpp32u bufferLength = srcDescPtr->w * layoutParams.bufferMultiplier;
 
-#if __AVX2__
+#if defined(__AVX2__)
         Rpp32u alignedLength = (bufferLength / 24) * 24;
         Rpp32u vectorIncrement = 24;
         Rpp32u vectorIncrementPerChannel = 8;
@@ -187,6 +202,24 @@ RppStatus color_to_greyscale_f32_f32_host_tensor(Rpp32f *srcPtr,
         pChannelWeights[0] = _mm256_set1_ps(channelWeights[0]);
         pChannelWeights[1] = _mm256_set1_ps(channelWeights[1]);
         pChannelWeights[2] = _mm256_set1_ps(channelWeights[2]);
+#elif defined(__loongarch_asx)
+        Rpp32u alignedLength = (bufferLength / 24) * 24;
+        Rpp32u vectorIncrement = 24;
+        Rpp32u vectorIncrementPerChannel = 8;
+
+        __m256 pChannelWeights[3];
+        pChannelWeights[0] = lasx_set1_f32(channelWeights[0]);
+        pChannelWeights[1] = lasx_set1_f32(channelWeights[1]);
+        pChannelWeights[2] = lasx_set1_f32(channelWeights[2]);
+#elif defined(__loongarch_sx)
+        Rpp32u alignedLength = (bufferLength / 12) * 12;
+        Rpp32u vectorIncrement = 12;
+        Rpp32u vectorIncrementPerChannel = 4;
+
+        __m128 pChannelWeights[3];
+        pChannelWeights[0] = lsx_set1_f32(channelWeights[0]);
+        pChannelWeights[1] = lsx_set1_f32(channelWeights[1]);
+        pChannelWeights[2] = lsx_set1_f32(channelWeights[2]);
 #else
         Rpp32u alignedLength = (bufferLength / 12) * 12;
         Rpp32u vectorIncrement = 12;
@@ -214,7 +247,7 @@ RppStatus color_to_greyscale_f32_f32_host_tensor(Rpp32f *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[3];
                     rpp_simd_load(rpp_load24_f32pkd3_to_f32pln3_avx, srcPtrTemp, p);    // simd loads
                     compute_color_24_to_greyscale_8_host(p, pChannelWeights);            // color_to_greyscale adjustment
@@ -260,7 +293,7 @@ RppStatus color_to_greyscale_f32_f32_host_tensor(Rpp32f *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[3];
                     rpp_simd_load(rpp_load24_f32pln3_to_f32pln3_avx, srcPtrTempR, srcPtrTempG, srcPtrTempB, p); // simd loads
                     compute_color_24_to_greyscale_8_host(p, pChannelWeights);                                    // color_to_greyscale adjustment
@@ -315,7 +348,7 @@ RppStatus color_to_greyscale_f16_f16_host_tensor(Rpp16f *srcPtr,
 
         Rpp32u bufferLength = srcDescPtr->w * layoutParams.bufferMultiplier;
 
-#if __AVX2__
+#if defined(__AVX2__)
         Rpp32u alignedLength = (bufferLength / 24) * 24;
         Rpp32u vectorIncrement = 24;
         Rpp32u vectorIncrementPerChannel = 8;
@@ -324,6 +357,24 @@ RppStatus color_to_greyscale_f16_f16_host_tensor(Rpp16f *srcPtr,
         pChannelWeights[0] = _mm256_set1_ps(channelWeights[0]);
         pChannelWeights[1] = _mm256_set1_ps(channelWeights[1]);
         pChannelWeights[2] = _mm256_set1_ps(channelWeights[2]);
+#elif defined(__loongarch_asx)
+        Rpp32u alignedLength = (bufferLength / 24) * 24;
+        Rpp32u vectorIncrement = 24;
+        Rpp32u vectorIncrementPerChannel = 8;
+
+        __m256 pChannelWeights[3];
+        pChannelWeights[0] = lasx_set1_f32(channelWeights[0]);
+        pChannelWeights[1] = lasx_set1_f32(channelWeights[1]);
+        pChannelWeights[2] = lasx_set1_f32(channelWeights[2]);
+#elif defined(__loongarch_asx)
+        Rpp32u alignedLength = (bufferLength / 12) * 12;
+        Rpp32u vectorIncrement = 12;
+        Rpp32u vectorIncrementPerChannel = 4;
+
+        __m128 pChannelWeights[3];
+        pChannelWeights[0] = lsx_set1_f32(channelWeights[0]);
+        pChannelWeights[1] = lsx_set1_f32(channelWeights[1]);
+        pChannelWeights[2] = lsx_set1_f32(channelWeights[2]);
 #else
         Rpp32u alignedLength = (bufferLength / 12) * 12;
         Rpp32u vectorIncrement = 12;
@@ -354,7 +405,7 @@ RppStatus color_to_greyscale_f16_f16_host_tensor(Rpp16f *srcPtr,
                     Rpp32f srcPtrTemp_ps[24], dstPtrTemp_ps[8];
                     for(int cnt = 0; cnt < vectorIncrement; cnt++)
                         srcPtrTemp_ps[cnt] = (Rpp32f) srcPtrTemp[cnt];
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[3];
                     rpp_simd_load(rpp_load24_f32pkd3_to_f32pln3_avx, srcPtrTemp_ps, p); // simd loads
                     compute_color_24_to_greyscale_8_host(p, pChannelWeights);            // color_to_greyscale adjustment
@@ -410,7 +461,7 @@ RppStatus color_to_greyscale_f16_f16_host_tensor(Rpp16f *srcPtr,
                         srcPtrTempG_ps[cnt] = (Rpp32f) srcPtrTempG[cnt];
                         srcPtrTempB_ps[cnt] = (Rpp32f) srcPtrTempB[cnt];
                     }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[3];
                     rpp_simd_load(rpp_load24_f32pln3_to_f32pln3_avx, srcPtrTempR_ps, srcPtrTempG_ps, srcPtrTempB_ps, p);    // simd loads
                     compute_color_24_to_greyscale_8_host(p, pChannelWeights);                                                // color_to_greyscale adjustment
@@ -470,11 +521,21 @@ RppStatus color_to_greyscale_i8_i8_host_tensor(Rpp8s *srcPtr,
         Rpp32u vectorIncrement = 48;
         Rpp32u vectorIncrementPerChannel = 16;
 
-#if __AVX2__
+#if defined(__AVX2__)
         __m256 pChannelWeights[3];
         pChannelWeights[0] = _mm256_set1_ps(channelWeights[0]);
         pChannelWeights[1] = _mm256_set1_ps(channelWeights[1]);
         pChannelWeights[2] = _mm256_set1_ps(channelWeights[2]);
+#elif defined(__loongarch_asx)
+        __m256 pChannelWeights[3];
+        pChannelWeights[0] = lasx_set1_f32(channelWeights[0]);
+        pChannelWeights[1] = lasx_set1_f32(channelWeights[1]);
+        pChannelWeights[2] = lasx_set1_f32(channelWeights[2]);
+#elif defined(__loongarch_asx)
+        __m128 pChannelWeights[3];
+        pChannelWeights[0] = lsx_set1_f32(channelWeights[0]);
+        pChannelWeights[1] = lsx_set1_f32(channelWeights[1]);
+        pChannelWeights[2] = lsx_set1_f32(channelWeights[2]);
 #else
         __m128 pChannelWeights[3];
         pChannelWeights[0] = _mm_set1_ps(channelWeights[0]);
@@ -498,7 +559,7 @@ RppStatus color_to_greyscale_i8_i8_host_tensor(Rpp8s *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[6];
                     rpp_simd_load(rpp_load48_i8pkd3_to_f32pln3_avx, srcPtrTemp, p); // simd loads
                     compute_color_48_to_greyscale_16_host(p, pChannelWeights);       // color_to_greyscale adjustment
@@ -545,7 +606,7 @@ RppStatus color_to_greyscale_i8_i8_host_tensor(Rpp8s *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[6];
                     rpp_simd_load(rpp_load48_i8pln3_to_f32pln3_avx, srcPtrTempR, srcPtrTempG, srcPtrTempB, p);  // simd loads
                     compute_color_48_to_greyscale_16_host(p, pChannelWeights);                                   // color_to_greyscale adjustment
diff --git a/src/modules/cpu/kernel/color_twist.hpp b/src/modules/cpu/kernel/color_twist.hpp
index 2fc94e11..3008073e 100644
--- a/src/modules/cpu/kernel/color_twist.hpp
+++ b/src/modules/cpu/kernel/color_twist.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 RppStatus color_twist_u8_u8_host_tensor(Rpp8u *srcPtr,
                                         RpptDescPtr srcDescPtr,
@@ -69,12 +74,24 @@ RppStatus color_twist_u8_u8_host_tensor(Rpp8u *srcPtr,
         Rpp32u vectorIncrement = 48;
         Rpp32u vectorIncrementPerChannel = 16;
 
-#if __AVX2__
+#if defined(__AVX2__)
         __m256 pColorTwistParams[4];
         pColorTwistParams[0] = _mm256_set1_ps(brightnessParam);
         pColorTwistParams[1] = _mm256_set1_ps(contrastParam);
         pColorTwistParams[2] = _mm256_set1_ps(hueParam);
         pColorTwistParams[3] = _mm256_set1_ps(saturationParam);
+#elif defined(__loongarch_asx)
+        __m256 pColorTwistParams[4];
+        pColorTwistParams[0] = lasx_set1_f32(brightnessParam);
+        pColorTwistParams[1] = lasx_set1_f32(contrastParam);
+        pColorTwistParams[2] = lasx_set1_f32(hueParam);
+        pColorTwistParams[3] = lasx_set1_f32(saturationParam);
+#elif defined(__loongarch_sx)
+        __m128 pColorTwistParams[4];
+        pColorTwistParams[0] = lsx_set1_f32(brightnessParam);
+        pColorTwistParams[1] = lsx_set1_f32(contrastParam);
+        pColorTwistParams[2] = lsx_set1_f32(hueParam);
+        pColorTwistParams[3] = lsx_set1_f32(saturationParam);
 #else
         __m128 pColorTwistParams[4];
         pColorTwistParams[0] = _mm_set1_ps(brightnessParam);
@@ -103,7 +120,7 @@ RppStatus color_twist_u8_u8_host_tensor(Rpp8u *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[6];
                     rpp_simd_load(rpp_load48_u8pkd3_to_f32pln3_avx, srcPtrTemp, p);    // simd loads
                     rpp_simd_load(rpp_normalize48_avx, p);    // simd normalize
@@ -169,7 +186,7 @@ RppStatus color_twist_u8_u8_host_tensor(Rpp8u *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[6];
                     rpp_simd_load(rpp_load48_u8pln3_to_f32pln3_avx, srcPtrTempR, srcPtrTempG, srcPtrTempB, p);    // simd loads
                     rpp_simd_load(rpp_normalize48_avx, p);    // simd normalize
@@ -231,7 +248,7 @@ RppStatus color_twist_u8_u8_host_tensor(Rpp8u *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[6];
                     rpp_simd_load(rpp_load48_u8pkd3_to_f32pln3_avx, srcPtrTemp, p);    // simd loads
                     rpp_simd_load(rpp_normalize48_avx, p);    // simd normalize
@@ -295,7 +312,7 @@ RppStatus color_twist_u8_u8_host_tensor(Rpp8u *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[6];
                     rpp_simd_load(rpp_load48_u8pln3_to_f32pln3_avx, srcPtrTempR, srcPtrTempG, srcPtrTempB, p);    // simd loads
                     rpp_simd_load(rpp_normalize48_avx, p);    // simd normalize
@@ -390,7 +407,7 @@ RppStatus color_twist_f32_f32_host_tensor(Rpp32f *srcPtr,
         srcPtrChannel = srcPtrImage + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
         dstPtrChannel = dstPtrImage;
 
-#if __AVX2__
+#if defined(__AVX2__)
         Rpp32u alignedLength = (bufferLength / 24) * 24;
         Rpp32u vectorIncrement = 24;
         Rpp32u vectorIncrementPerChannel = 8;
@@ -400,6 +417,26 @@ RppStatus color_twist_f32_f32_host_tensor(Rpp32f *srcPtr,
         pColorTwistParams[1] = _mm256_set1_ps(contrastParam);
         pColorTwistParams[2] = _mm256_set1_ps(hueParam);
         pColorTwistParams[3] = _mm256_set1_ps(saturationParam);
+#elif defined(__loongarch_asx)
+        Rpp32u alignedLength = (bufferLength / 24) * 24;
+        Rpp32u vectorIncrement = 24;
+        Rpp32u vectorIncrementPerChannel = 8;
+
+        __m256 pColorTwistParams[4];
+        pColorTwistParams[0] = lasx_set1_f32(brightnessParam);
+        pColorTwistParams[1] = lasx_set1_f32(contrastParam);
+        pColorTwistParams[2] = lasx_set1_f32(hueParam);
+        pColorTwistParams[3] = lasx_set1_f32(saturationParam);
+#elif defined(__loongarch_sx)
+        Rpp32u alignedLength = (bufferLength / 12) * 12;
+        Rpp32u vectorIncrement = 12;
+        Rpp32u vectorIncrementPerChannel = 4;
+
+        __m128 pColorTwistParams[4];
+        pColorTwistParams[0] = lsx_set1_f32(brightnessParam);
+        pColorTwistParams[1] = lsx_set1_f32(contrastParam);
+        pColorTwistParams[2] = lsx_set1_f32(hueParam);
+        pColorTwistParams[3] = lsx_set1_f32(saturationParam);
 #else
         Rpp32u alignedLength = (bufferLength / 12) * 12;
         Rpp32u vectorIncrement = 12;
@@ -432,7 +469,7 @@ RppStatus color_twist_f32_f32_host_tensor(Rpp32f *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[3];
                     rpp_simd_load(rpp_load24_f32pkd3_to_f32pln3_avx, srcPtrTemp, p);    // simd loads
                     compute_color_twist_24_host(p[0], p[1], p[2], pColorTwistParams);    // color_twist adjustment
@@ -492,7 +529,7 @@ RppStatus color_twist_f32_f32_host_tensor(Rpp32f *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[3];
                     rpp_simd_load(rpp_load24_f32pln3_to_f32pln3_avx, srcPtrTempR, srcPtrTempG, srcPtrTempB, p);    // simd loads
                     compute_color_twist_24_host(p[0], p[1], p[2], pColorTwistParams);    // color_twist adjustment
@@ -548,7 +585,7 @@ RppStatus color_twist_f32_f32_host_tensor(Rpp32f *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[3];
                     rpp_simd_load(rpp_load24_f32pkd3_to_f32pln3_avx, srcPtrTemp, p);    // simd loads
                     compute_color_twist_24_host(p[0], p[1], p[2], pColorTwistParams);    // color_twist adjustment
@@ -606,7 +643,7 @@ RppStatus color_twist_f32_f32_host_tensor(Rpp32f *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[3];
                     rpp_simd_load(rpp_load24_f32pln3_to_f32pln3_avx, srcPtrTempR, srcPtrTempG, srcPtrTempB, p);    // simd loads
                     compute_color_twist_24_host(p[0], p[1], p[2], pColorTwistParams);    // color_twist adjustment
@@ -695,7 +732,7 @@ RppStatus color_twist_f16_f16_host_tensor(Rpp16f *srcPtr,
         srcPtrChannel = srcPtrImage + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
         dstPtrChannel = dstPtrImage;
 
-#if __AVX2__
+#if defined(__AVX2__)
         Rpp32u alignedLength = (bufferLength / 24) * 24;
         Rpp32u vectorIncrement = 24;
         Rpp32u vectorIncrementPerChannel = 8;
@@ -705,6 +742,26 @@ RppStatus color_twist_f16_f16_host_tensor(Rpp16f *srcPtr,
         pColorTwistParams[1] = _mm256_set1_ps(contrastParam);
         pColorTwistParams[2] = _mm256_set1_ps(hueParam);
         pColorTwistParams[3] = _mm256_set1_ps(saturationParam);
+#elif defined(__loongarch_asx)
+        Rpp32u alignedLength = (bufferLength / 24) * 24;
+        Rpp32u vectorIncrement = 24;
+        Rpp32u vectorIncrementPerChannel = 8;
+
+        __m256 pColorTwistParams[4];
+        pColorTwistParams[0] = lasx_set1_f32(brightnessParam);
+        pColorTwistParams[1] = lasx_set1_f32(contrastParam);
+        pColorTwistParams[2] = lasx_set1_f32(hueParam);
+        pColorTwistParams[3] = lasx_set1_f32(saturationParam);
+#elif defined(__loongarch_sx)
+        Rpp32u alignedLength = (bufferLength / 12) * 12;
+        Rpp32u vectorIncrement = 12;
+        Rpp32u vectorIncrementPerChannel = 4;
+
+        __m128 pColorTwistParams[4];
+        pColorTwistParams[0] = lsx_set1_f32(brightnessParam);
+        pColorTwistParams[1] = lsx_set1_f32(contrastParam);
+        pColorTwistParams[2] = lsx_set1_f32(hueParam);
+        pColorTwistParams[3] = lsx_set1_f32(saturationParam);
 #else
         Rpp32u alignedLength = (bufferLength / 12) * 12;
         Rpp32u vectorIncrement = 12;
@@ -741,7 +798,7 @@ RppStatus color_twist_f16_f16_host_tensor(Rpp16f *srcPtr,
                     Rpp32f dstPtrTempR_ps[8], dstPtrTempG_ps[8], dstPtrTempB_ps[8];
                     for(int cnt = 0; cnt < vectorIncrement; cnt++)
                         srcPtrTemp_ps[cnt] = (Rpp32f) srcPtrTemp[cnt];
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[3];
                     rpp_simd_load(rpp_load24_f32pkd3_to_f32pln3_avx, srcPtrTemp_ps, p);    // simd loads
                     compute_color_twist_24_host(p[0], p[1], p[2], pColorTwistParams);    // color_twist adjustment
@@ -815,7 +872,7 @@ RppStatus color_twist_f16_f16_host_tensor(Rpp16f *srcPtr,
                         srcPtrTempG_ps[cnt] = (Rpp32f) srcPtrTempG[cnt];
                         srcPtrTempB_ps[cnt] = (Rpp32f) srcPtrTempB[cnt];
                     }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[3];
                     rpp_simd_load(rpp_load24_f32pln3_to_f32pln3_avx, srcPtrTempR_ps, srcPtrTempG_ps, srcPtrTempB_ps, p);    // simd loads
                     compute_color_twist_24_host(p[0], p[1], p[2], pColorTwistParams);    // color_twist adjustment
@@ -877,7 +934,7 @@ RppStatus color_twist_f16_f16_host_tensor(Rpp16f *srcPtr,
                     Rpp32f dstPtrTemp_ps[25];
                     for(int cnt = 0; cnt < vectorIncrement; cnt++)
                         srcPtrTemp_ps[cnt] = (Rpp32f) srcPtrTemp[cnt];
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[3];
                     rpp_simd_load(rpp_load24_f32pkd3_to_f32pln3_avx, srcPtrTemp_ps, p);    // simd loads
                     compute_color_twist_24_host(p[0], p[1], p[2], pColorTwistParams);    // color_twist adjustment
@@ -945,7 +1002,7 @@ RppStatus color_twist_f16_f16_host_tensor(Rpp16f *srcPtr,
                         srcPtrTempG_ps[cnt] = (Rpp32f) srcPtrTempG[cnt];
                         srcPtrTempB_ps[cnt] = (Rpp32f) srcPtrTempB[cnt];
                     }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[3];
                     rpp_simd_load(rpp_load24_f32pln3_to_f32pln3_avx, srcPtrTempR_ps, srcPtrTempG_ps, srcPtrTempB_ps, p);    // simd loads
                     compute_color_twist_24_host(p[0], p[1], p[2], pColorTwistParams);    // color_twist adjustment
@@ -1044,12 +1101,24 @@ RppStatus color_twist_i8_i8_host_tensor(Rpp8s *srcPtr,
         Rpp32u vectorIncrement = 48;
         Rpp32u vectorIncrementPerChannel = 16;
 
-#if __AVX2__
+#if defined(__AVX2__)
         __m256 pColorTwistParams[4];
         pColorTwistParams[0] = _mm256_set1_ps(brightnessParam);
         pColorTwistParams[1] = _mm256_set1_ps(contrastParam);
         pColorTwistParams[2] = _mm256_set1_ps(hueParam);
         pColorTwistParams[3] = _mm256_set1_ps(saturationParam);
+#elif defined(__loongarch_asx)
+        __m256 pColorTwistParams[4];
+        pColorTwistParams[0] = lasx_set1_f32(brightnessParam);
+        pColorTwistParams[1] = lasx_set1_f32(contrastParam);
+        pColorTwistParams[2] = lasx_set1_f32(hueParam);
+        pColorTwistParams[3] = lasx_set1_f32(saturationParam);
+#elif defined(__loongarch_sx)
+        __m128 pColorTwistParams[4];
+        pColorTwistParams[0] = lsx_set1_f32(brightnessParam);
+        pColorTwistParams[1] = lsx_set1_f32(contrastParam);
+        pColorTwistParams[2] = lsx_set1_f32(hueParam);
+        pColorTwistParams[3] = lsx_set1_f32(saturationParam);
 #else
         __m128 pColorTwistParams[4];
         pColorTwistParams[0] = _mm_set1_ps(brightnessParam);
@@ -1078,7 +1147,7 @@ RppStatus color_twist_i8_i8_host_tensor(Rpp8s *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[6];
                     rpp_simd_load(rpp_load48_i8pkd3_to_f32pln3_avx, srcPtrTemp, p);    // simd loads
                     rpp_simd_load(rpp_normalize48_avx, p);    // simd normalize
@@ -1144,7 +1213,7 @@ RppStatus color_twist_i8_i8_host_tensor(Rpp8s *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[6];
                     rpp_simd_load(rpp_load48_i8pln3_to_f32pln3_avx, srcPtrTempR, srcPtrTempG, srcPtrTempB, p);    // simd loads
                     rpp_simd_load(rpp_normalize48_avx, p);    // simd normalize
@@ -1206,7 +1275,7 @@ RppStatus color_twist_i8_i8_host_tensor(Rpp8s *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[6];
                     rpp_simd_load(rpp_load48_i8pkd3_to_f32pln3_avx, srcPtrTemp, p);    // simd loads
                     rpp_simd_load(rpp_normalize48_avx, p);    // simd normalize
@@ -1270,7 +1339,7 @@ RppStatus color_twist_i8_i8_host_tensor(Rpp8s *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[6];
                     rpp_simd_load(rpp_load48_i8pln3_to_f32pln3_avx, srcPtrTempR, srcPtrTempG, srcPtrTempB, p);    // simd loads
                     rpp_simd_load(rpp_normalize48_avx, p);    // simd normalize
diff --git a/src/modules/cpu/kernel/contrast.hpp b/src/modules/cpu/kernel/contrast.hpp
index 311835f7..21ae2b5c 100644
--- a/src/modules/cpu/kernel/contrast.hpp
+++ b/src/modules/cpu/kernel/contrast.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 RppStatus contrast_u8_u8_host_tensor(Rpp8u *srcPtr,
                                      RpptDescPtr srcDescPtr,
@@ -65,9 +70,15 @@ RppStatus contrast_u8_u8_host_tensor(Rpp8u *srcPtr,
         Rpp32u vectorIncrement = 48;
         Rpp32u vectorIncrementPerChannel = 16;
 
+#ifdef __loongarch_asx
+        __m256 pContrastParams[2];
+        pContrastParams[0] = lasx_set1_f32(contrastFactor);
+        pContrastParams[1] = lasx_set1_f32(contrastCenter);
+#else
         __m256 pContrastParams[2];
         pContrastParams[0] = _mm256_set1_ps(contrastFactor);
         pContrastParams[1] = _mm256_set1_ps(contrastCenter);
+#endif
 
         // contrast with fused output-layout toggle (NHWC -> NCHW)
         if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
@@ -252,9 +263,15 @@ RppStatus contrast_f32_f32_host_tensor(Rpp32f *srcPtr,
         Rpp32u vectorIncrement = 24;
         Rpp32u vectorIncrementPerChannel = 8;
 
+#ifdef __loongarch_asx
+        __m256 pContrastParams[2];
+        pContrastParams[0] = lasx_set1_f32(contrastFactor);
+        pContrastParams[1] = lasx_set1_f32(contrastCenter);
+#else
         __m256 pContrastParams[2];
         pContrastParams[0] = _mm256_set1_ps(contrastFactor);
         pContrastParams[1] = _mm256_set1_ps(contrastCenter);
+#endif
 
 
         // contrast with fused output-layout toggle (NHWC -> NCHW)
@@ -440,9 +457,15 @@ RppStatus contrast_f16_f16_host_tensor(Rpp16f *srcPtr,
         Rpp32u vectorIncrement = 24;
         Rpp32u vectorIncrementPerChannel = 8;
 
+#ifdef __loongarch_asx
+        __m256 pContrastParams[2];
+        pContrastParams[0] = lasx_set1_f32(contrastFactor);
+        pContrastParams[1] = lasx_set1_f32(contrastCenter);
+#else
         __m256 pContrastParams[2];
         pContrastParams[0] = _mm256_set1_ps(contrastFactor);
         pContrastParams[1] = _mm256_set1_ps(contrastCenter);
+#endif
 
         // contrast with fused output-layout toggle (NHWC -> NCHW)
         if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
@@ -660,9 +683,15 @@ RppStatus contrast_i8_i8_host_tensor(Rpp8s *srcPtr,
         Rpp32u vectorIncrement = 48;
         Rpp32u vectorIncrementPerChannel = 16;
 
+#ifdef __loongarch_asx
+        __m256 pContrastParams[2];
+        pContrastParams[0] = lasx_set1_f32(contrastFactor);
+        pContrastParams[1] = lasx_set1_f32(contrastCenter);
+#else
         __m256 pContrastParams[2];
         pContrastParams[0] = _mm256_set1_ps(contrastFactor);
         pContrastParams[1] = _mm256_set1_ps(contrastCenter);
+#endif
 
         // contrast with fused output-layout toggle (NHWC -> NCHW)
         if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
diff --git a/src/modules/cpu/kernel/copy.hpp b/src/modules/cpu/kernel/copy.hpp
index c3e31a62..1503fe4d 100644
--- a/src/modules/cpu/kernel/copy.hpp
+++ b/src/modules/cpu/kernel/copy.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 RppStatus copy_u8_u8_host_tensor(Rpp8u *srcPtr,
                                  RpptDescPtr srcDescPtr,
diff --git a/src/modules/cpu/kernel/crop.hpp b/src/modules/cpu/kernel/crop.hpp
index 5c4f0f37..188c558a 100644
--- a/src/modules/cpu/kernel/crop.hpp
+++ b/src/modules/cpu/kernel/crop.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 RppStatus crop_u8_u8_host_tensor(Rpp8u *srcPtr,
                                  RpptDescPtr srcDescPtr,
diff --git a/src/modules/cpu/kernel/crop_and_patch.hpp b/src/modules/cpu/kernel/crop_and_patch.hpp
index 72f31eeb..e5e614db 100644
--- a/src/modules/cpu/kernel/crop_and_patch.hpp
+++ b/src/modules/cpu/kernel/crop_and_patch.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 RppStatus crop_and_patch_u8_u8_host_tensor(Rpp8u *srcPtr1,
                                            Rpp8u *srcPtr2,
@@ -90,7 +95,7 @@ RppStatus crop_and_patch_u8_u8_host_tensor(Rpp8u *srcPtr1,
                 int vectorLoopCount = 0;
                 if(i >= patchRoi->xywhROI.xy.y && i < (patchRoi->xywhROI.xy.y + cropRoi->xywhROI.roiHeight))
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for(; vectorLoopCount < patchAlignedLength1; vectorLoopCount += vectorIncrement)
                     {
                         __m128i px[3];
@@ -112,7 +117,7 @@ RppStatus crop_and_patch_u8_u8_host_tensor(Rpp8u *srcPtr1,
 
                     vectorLoopCount = 0;
                     srcPtr1Temp = srcPtr1Row;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for (; vectorLoopCount < cropAlignedLength; vectorLoopCount += vectorIncrement)
                     {
                         __m128i px[3];
@@ -134,7 +139,7 @@ RppStatus crop_and_patch_u8_u8_host_tensor(Rpp8u *srcPtr1,
 
                     vectorLoopCount = 0;
                     srcPtr2Temp += cropBufferLength;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for(; vectorLoopCount < patchAlignedLength2; vectorLoopCount += vectorIncrement)
                     {
                         __m128i px[3];
@@ -157,7 +162,7 @@ RppStatus crop_and_patch_u8_u8_host_tensor(Rpp8u *srcPtr1,
                 }
                 else
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for(; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                     {
                         __m128i px[3];
@@ -209,7 +214,7 @@ RppStatus crop_and_patch_u8_u8_host_tensor(Rpp8u *srcPtr1,
                 int vectorLoopCount = 0;
                 if(i >= patchRoi->xywhROI.xy.y && i < (patchRoi->xywhROI.xy.y + cropRoi->xywhROI.roiHeight))
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for(; vectorLoopCount < patchAlignedLength1; vectorLoopCount += vectorIncrementPerChannel)
                     {
                         __m128i px[3];
@@ -233,7 +238,7 @@ RppStatus crop_and_patch_u8_u8_host_tensor(Rpp8u *srcPtr1,
                     srcPtr1TempR = srcPtr1RowR;
                     srcPtr1TempG = srcPtr1RowG;
                     srcPtr1TempB = srcPtr1RowB;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for (; vectorLoopCount < cropAlignedLength; vectorLoopCount += vectorIncrementPerChannel)
                     {
                         __m128i px[3];
@@ -257,7 +262,7 @@ RppStatus crop_and_patch_u8_u8_host_tensor(Rpp8u *srcPtr1,
                     srcPtr2TempR += cropBufferLength;
                     srcPtr2TempG += cropBufferLength;
                     srcPtr2TempB += cropBufferLength;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for(; vectorLoopCount < patchAlignedLength2; vectorLoopCount += vectorIncrementPerChannel)
                     {
                         __m128i px[3];
@@ -282,7 +287,7 @@ RppStatus crop_and_patch_u8_u8_host_tensor(Rpp8u *srcPtr1,
                 }
                 else
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for(; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                     {
                         __m128i px[3];
@@ -443,7 +448,7 @@ RppStatus crop_and_patch_f32_f32_host_tensor(Rpp32f *srcPtr1,
                 int vectorLoopCount = 0;
                 if(i >= patchRoi->xywhROI.xy.y && i < (patchRoi->xywhROI.xy.y + cropRoi->xywhROI.roiHeight))
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for(; vectorLoopCount < patchAlignedLength1; vectorLoopCount += vectorIncrement)
                     {
                         __m128 p[4];
@@ -465,7 +470,7 @@ RppStatus crop_and_patch_f32_f32_host_tensor(Rpp32f *srcPtr1,
 
                     vectorLoopCount = 0;
                     srcPtr1Temp = srcPtr1Row;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for (; vectorLoopCount < cropAlignedLength; vectorLoopCount += vectorIncrement)
                     {
                         __m128 p[4];
@@ -487,7 +492,7 @@ RppStatus crop_and_patch_f32_f32_host_tensor(Rpp32f *srcPtr1,
 
                     vectorLoopCount = 0;
                     srcPtr2Temp += cropBufferLength;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for(; vectorLoopCount < patchAlignedLength2; vectorLoopCount += vectorIncrement)
                     {
                         __m128 p[4];
@@ -510,7 +515,7 @@ RppStatus crop_and_patch_f32_f32_host_tensor(Rpp32f *srcPtr1,
                 }
                 else
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for(; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                     {
                         __m128 p[4];
@@ -561,7 +566,7 @@ RppStatus crop_and_patch_f32_f32_host_tensor(Rpp32f *srcPtr1,
                 int vectorLoopCount = 0;
                 if(i >= patchRoi->xywhROI.xy.y && i < (patchRoi->xywhROI.xy.y + cropRoi->xywhROI.roiHeight))
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for(; vectorLoopCount < patchAlignedLength1; vectorLoopCount+=4)
                     {
                         __m128 p[4];
@@ -585,7 +590,7 @@ RppStatus crop_and_patch_f32_f32_host_tensor(Rpp32f *srcPtr1,
                     srcPtr1TempR = srcPtr1RowR;
                     srcPtr1TempG = srcPtr1RowG;
                     srcPtr1TempB = srcPtr1RowB;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for (; vectorLoopCount < cropAlignedLength; vectorLoopCount+=4)
                     {
                         __m128 p[4];
@@ -609,7 +614,7 @@ RppStatus crop_and_patch_f32_f32_host_tensor(Rpp32f *srcPtr1,
                     srcPtr2TempR += cropBufferLength;
                     srcPtr2TempG += cropBufferLength;
                     srcPtr2TempB += cropBufferLength;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for(; vectorLoopCount < patchAlignedLength2; vectorLoopCount+=4)
                     {
                         __m128 p[4];
@@ -634,7 +639,7 @@ RppStatus crop_and_patch_f32_f32_host_tensor(Rpp32f *srcPtr1,
                 }
                 else
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for(; vectorLoopCount < alignedLength; vectorLoopCount+=4)
                     {
                         __m128 p[4];
@@ -804,7 +809,7 @@ RppStatus crop_and_patch_f16_f16_host_tensor(Rpp16f *srcPtr1,
                 int vectorLoopCount = 0;
                 if(i >= patchRoi->xywhROI.xy.y && i < (patchRoi->xywhROI.xy.y + cropRoi->xywhROI.roiHeight))
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for(; vectorLoopCount < patchAlignedLength1; vectorLoopCount += vectorIncrement)
                     {
                         Rpp32f srcPtrTemp_ps[12], dstPtrTemp_ps[12];
@@ -835,7 +840,7 @@ RppStatus crop_and_patch_f16_f16_host_tensor(Rpp16f *srcPtr1,
                     }
                     vectorLoopCount = 0;
                     srcPtr1Temp = srcPtr1Row;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for (; vectorLoopCount < cropAlignedLength; vectorLoopCount += vectorIncrement)
                     {
                         Rpp32f srcPtrTemp_ps[12], dstPtrTemp_ps[12];
@@ -867,7 +872,7 @@ RppStatus crop_and_patch_f16_f16_host_tensor(Rpp16f *srcPtr1,
 
                     vectorLoopCount = 0;
                     srcPtr2Temp += cropBufferLength;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for(; vectorLoopCount < patchAlignedLength2; vectorLoopCount += vectorIncrement)
                     {
                         Rpp32f srcPtrTemp_ps[12], dstPtrTemp_ps[12];
@@ -900,7 +905,7 @@ RppStatus crop_and_patch_f16_f16_host_tensor(Rpp16f *srcPtr1,
                 }
                 else
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for(; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                     {
                         Rpp32f srcPtrTemp_ps[12], dstPtrTemp_ps[12];
@@ -963,7 +968,7 @@ RppStatus crop_and_patch_f16_f16_host_tensor(Rpp16f *srcPtr1,
 
                 if(i >= patchRoi->xywhROI.xy.y && i < (patchRoi->xywhROI.xy.y + cropRoi->xywhROI.roiHeight))
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for(; vectorLoopCount < patchAlignedLength1; vectorLoopCount+=4)
                     {
                         Rpp32f srcPtrTemp_ps[12], dstPtrTemp_ps[13];
@@ -996,7 +1001,7 @@ RppStatus crop_and_patch_f16_f16_host_tensor(Rpp16f *srcPtr1,
                     srcPtr1TempR = srcPtr1RowR;
                     srcPtr1TempG = srcPtr1RowG;
                     srcPtr1TempB = srcPtr1RowB;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for (; vectorLoopCount < cropAlignedLength; vectorLoopCount+=4)
                     {
                         Rpp32f srcPtrTemp_ps[12], dstPtrTemp_ps[13];
@@ -1030,7 +1035,7 @@ RppStatus crop_and_patch_f16_f16_host_tensor(Rpp16f *srcPtr1,
                     srcPtr2TempR += cropBufferLength;
                     srcPtr2TempG += cropBufferLength;
                     srcPtr2TempB += cropBufferLength;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for(; vectorLoopCount < patchAlignedLength2; vectorLoopCount+=4)
                     {
                         Rpp32f srcPtrTemp_ps[12], dstPtrTemp_ps[13];
@@ -1065,7 +1070,7 @@ RppStatus crop_and_patch_f16_f16_host_tensor(Rpp16f *srcPtr1,
                 }
                 else
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for(; vectorLoopCount < alignedLength; vectorLoopCount+=4)
                     {
                         Rpp32f srcPtrTemp_ps[12], dstPtrTemp_ps[13];
@@ -1247,7 +1252,7 @@ RppStatus crop_and_patch_i8_i8_host_tensor(Rpp8s *srcPtr1,
                 int vectorLoopCount = 0;
                 if(i >= patchRoi->xywhROI.xy.y && i < (patchRoi->xywhROI.xy.y + cropRoi->xywhROI.roiHeight))
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for(; vectorLoopCount < patchAlignedLength1; vectorLoopCount += vectorIncrement)
                     {
                         __m128i px[3];
@@ -1270,7 +1275,7 @@ RppStatus crop_and_patch_i8_i8_host_tensor(Rpp8s *srcPtr1,
 
                     vectorLoopCount = 0;
                     srcPtr1Temp = srcPtr1Row;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for (; vectorLoopCount < cropAlignedLength; vectorLoopCount += vectorIncrement)
                     {
                         __m128i px[3];
@@ -1292,7 +1297,7 @@ RppStatus crop_and_patch_i8_i8_host_tensor(Rpp8s *srcPtr1,
 
                     vectorLoopCount = 0;
                     srcPtr2Temp += cropBufferLength;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for(; vectorLoopCount < patchAlignedLength2; vectorLoopCount += vectorIncrement)
                     {
                         __m128i px[3];
@@ -1315,7 +1320,7 @@ RppStatus crop_and_patch_i8_i8_host_tensor(Rpp8s *srcPtr1,
                 }
                 else
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for(; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                     {
                         __m128i px[3];
@@ -1368,7 +1373,7 @@ RppStatus crop_and_patch_i8_i8_host_tensor(Rpp8s *srcPtr1,
                 int vectorLoopCount = 0;
                 if(i >= patchRoi->xywhROI.xy.y && i < (patchRoi->xywhROI.xy.y + cropRoi->xywhROI.roiHeight))
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for(; vectorLoopCount < patchAlignedLength1; vectorLoopCount += vectorIncrementPerChannel)
                     {
                         __m128i px[3];
@@ -1392,7 +1397,7 @@ RppStatus crop_and_patch_i8_i8_host_tensor(Rpp8s *srcPtr1,
                     srcPtr1TempR = srcPtr1RowR;
                     srcPtr1TempG = srcPtr1RowG;
                     srcPtr1TempB = srcPtr1RowB;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for (; vectorLoopCount < cropAlignedLength; vectorLoopCount += vectorIncrementPerChannel)
                     {
                         __m128i px[3];
@@ -1416,7 +1421,7 @@ RppStatus crop_and_patch_i8_i8_host_tensor(Rpp8s *srcPtr1,
                     srcPtr2TempR += cropBufferLength;
                     srcPtr2TempG += cropBufferLength;
                     srcPtr2TempB += cropBufferLength;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for(; vectorLoopCount < patchAlignedLength2; vectorLoopCount += vectorIncrementPerChannel)
                     {
                         __m128i px[3];
@@ -1441,7 +1446,7 @@ RppStatus crop_and_patch_i8_i8_host_tensor(Rpp8s *srcPtr1,
                 }
                 else
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for(; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                     {
                         __m128i px[3];
diff --git a/src/modules/cpu/kernel/crop_mirror_normalize.hpp b/src/modules/cpu/kernel/crop_mirror_normalize.hpp
index a2d5ee8a..025e43b1 100644
--- a/src/modules/cpu/kernel/crop_mirror_normalize.hpp
+++ b/src/modules/cpu/kernel/crop_mirror_normalize.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 RppStatus crop_mirror_normalize_u8_u8_host_tensor(Rpp8u *srcPtr,
                                                   RpptDescPtr srcDescPtr,
@@ -52,6 +57,15 @@ RppStatus crop_mirror_normalize_u8_u8_host_tensor(Rpp8u *srcPtr,
         Rpp32u cmnParamLoc = srcDescPtr->c * batchCount;
         Rpp32u cmnParamLocs[3] = {cmnParamLoc, cmnParamLoc + 1, cmnParamLoc + 2};
         Rpp32s numRegs = 2 * srcDescPtr->c;
+#ifdef __loongarch_asx
+        __m256 pCMNParams[numRegs];
+        for(int pos = 0; pos < numRegs; pos += 2)
+        {
+            pCMNParams[pos] = lasx_set1_f32(multiplierTensor[cmnParamLoc]);
+            pCMNParams[pos + 1] = lasx_set1_f32(offsetTensor[cmnParamLoc]);
+            cmnParamLoc++;
+        }
+#else
         __m256 pCMNParams[numRegs];
         for(int pos = 0; pos < numRegs; pos += 2)
         {
@@ -59,6 +73,7 @@ RppStatus crop_mirror_normalize_u8_u8_host_tensor(Rpp8u *srcPtr,
             pCMNParams[pos + 1] = _mm256_set1_ps(offsetTensor[cmnParamLoc]);
             cmnParamLoc++;
         }
+#endif
         Rpp32u mirrorFlag = mirrorTensor[batchCount];
 
         Rpp8u *srcPtrImage, *dstPtrImage;
@@ -472,6 +487,15 @@ RppStatus crop_mirror_normalize_f32_f32_host_tensor(Rpp32f *srcPtr,
         Rpp32u cmnParamLoc = srcDescPtr->c * batchCount;
         Rpp32u cmnParamLocs[3] = {cmnParamLoc, cmnParamLoc + 1, cmnParamLoc + 2};
         Rpp32s numRegs = 2 * srcDescPtr->c;
+#ifdef __loongarch_asx
+        __m256 pCMNParams[numRegs];
+        for(int pos = 0; pos < numRegs; pos += 2)
+        {
+            pCMNParams[pos] = lasx_set1_f32(multiplierTensor[cmnParamLoc]);
+            pCMNParams[pos + 1] = lasx_set1_f32(offsetTensor[cmnParamLoc]);
+            cmnParamLoc++;
+        }
+#else
         __m256 pCMNParams[numRegs];
         for(int pos = 0; pos < numRegs; pos += 2)
         {
@@ -479,6 +503,7 @@ RppStatus crop_mirror_normalize_f32_f32_host_tensor(Rpp32f *srcPtr,
             pCMNParams[pos + 1] = _mm256_set1_ps(offsetTensor[cmnParamLoc]);
             cmnParamLoc++;
         }
+#endif
         Rpp32u mirrorFlag = mirrorTensor[batchCount];
 
         Rpp32f *srcPtrImage, *dstPtrImage;
@@ -892,6 +917,15 @@ RppStatus crop_mirror_normalize_f16_f16_host_tensor(Rpp16f *srcPtr,
         Rpp32u cmnParamLoc = srcDescPtr->c * batchCount;
         Rpp32u cmnParamLocs[3] = {cmnParamLoc, cmnParamLoc + 1, cmnParamLoc + 2};
         Rpp32s numRegs = 2 * srcDescPtr->c;
+#ifdef __loongarch_asx
+        __m256 pCMNParams[numRegs];
+        for(int pos = 0; pos < numRegs; pos += 2)
+        {
+            pCMNParams[pos] = lasx_set1_f32(multiplierTensor[cmnParamLoc]);
+            pCMNParams[pos + 1] = lasx_set1_f32(offsetTensor[cmnParamLoc]);
+            cmnParamLoc++;
+        }
+#else
         __m256 pCMNParams[numRegs];
         for(int pos = 0; pos < numRegs; pos += 2)
         {
@@ -899,6 +933,7 @@ RppStatus crop_mirror_normalize_f16_f16_host_tensor(Rpp16f *srcPtr,
             pCMNParams[pos + 1] = _mm256_set1_ps(offsetTensor[cmnParamLoc]);
             cmnParamLoc++;
         }
+#endif
         Rpp32u mirrorFlag = mirrorTensor[batchCount];
 
         Rpp16f *srcPtrImage, *dstPtrImage;
@@ -1359,6 +1394,15 @@ RppStatus crop_mirror_normalize_i8_i8_host_tensor(Rpp8s *srcPtr,
         Rpp32u cmnParamLoc = srcDescPtr->c * batchCount;
         Rpp32u cmnParamLocs[3] = {cmnParamLoc, cmnParamLoc + 1, cmnParamLoc + 2};
         Rpp32s numRegs = 2 * srcDescPtr->c;
+#ifdef __loongarch_asx
+        __m256 pCMNParams[numRegs];
+        for(int pos = 0; pos < numRegs; pos += 2)
+        {
+            pCMNParams[pos] = lasx_set1_f32(multiplierTensor[cmnParamLoc]);
+            pCMNParams[pos + 1] = lasx_set1_f32(offsetTensor[cmnParamLoc]);
+            cmnParamLoc++;
+        }
+#else
         __m256 pCMNParams[numRegs];
         for(int pos = 0; pos < numRegs; pos += 2)
         {
@@ -1366,6 +1410,7 @@ RppStatus crop_mirror_normalize_i8_i8_host_tensor(Rpp8s *srcPtr,
             pCMNParams[pos + 1] = _mm256_set1_ps(offsetTensor[cmnParamLoc]);
             cmnParamLoc++;
         }
+#endif
         Rpp32u mirrorFlag = mirrorTensor[batchCount];
 
         Rpp8s *srcPtrImage, *dstPtrImage;
@@ -1787,16 +1832,27 @@ RppStatus crop_mirror_normalize_u8_f32_host_tensor(Rpp8u *srcPtr,
         // set invStdDev as offsetTensor[cmnParamLocs[0]] offsetTensor[cmnParamLocs[1]] offsetTensor[cmnParamLocs[2]] 1 offsetTensor[cmnParamLocs[0]] offsetTensor[cmnParamLocs[1]] offsetTensor[cmnParamLocs[2]]
         if((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NHWC))
         {
+#ifdef __loongarch_asx
+            pCMNParams[0] = lasx_setr_f32(multiplierTensor[cmnParamLocs[0]], multiplierTensor[cmnParamLocs[1]], multiplierTensor[cmnParamLocs[2]], 0.0f, multiplierTensor[cmnParamLocs[0]], multiplierTensor[cmnParamLocs[1]], multiplierTensor[cmnParamLocs[2]], 0.0f);
+            pCMNParams[1] = lasx_setr_f32(offsetTensor[cmnParamLocs[0]], offsetTensor[cmnParamLocs[1]], offsetTensor[cmnParamLocs[2]], 1.0f, offsetTensor[cmnParamLocs[0]], offsetTensor[cmnParamLocs[1]], offsetTensor[cmnParamLocs[2]], 1.0f);
+#else
             pCMNParams[0] = _mm256_setr_ps(multiplierTensor[cmnParamLocs[0]], multiplierTensor[cmnParamLocs[1]], multiplierTensor[cmnParamLocs[2]], 0.0f, multiplierTensor[cmnParamLocs[0]], multiplierTensor[cmnParamLocs[1]], multiplierTensor[cmnParamLocs[2]], 0.0f);
             pCMNParams[1] = _mm256_setr_ps(offsetTensor[cmnParamLocs[0]], offsetTensor[cmnParamLocs[1]], offsetTensor[cmnParamLocs[2]], 1.0f, offsetTensor[cmnParamLocs[0]], offsetTensor[cmnParamLocs[1]], offsetTensor[cmnParamLocs[2]], 1.0f);
+#endif
         }
         else
         {
             for(int pos = 0; pos < numRegs; pos += 2)
             {
+#ifdef __loongarch_asx
+                pCMNParams[pos] = lasx_set1_f32(multiplierTensor[cmnParamLoc]);
+                pCMNParams[pos + 1] = lasx_set1_f32(offsetTensor[cmnParamLoc]);
+                cmnParamLoc++;
+#else
                 pCMNParams[pos] = _mm256_set1_ps(multiplierTensor[cmnParamLoc]);
                 pCMNParams[pos + 1] = _mm256_set1_ps(offsetTensor[cmnParamLoc]);
                 cmnParamLoc++;
+#endif
             }
         }
 
@@ -2235,16 +2291,27 @@ RppStatus crop_mirror_normalize_u8_f16_host_tensor(Rpp8u *srcPtr,
         // set invStdDev as offsetTensor[cmnParamLocs[0]] offsetTensor[cmnParamLocs[1]] offsetTensor[cmnParamLocs[2]] 1 offsetTensor[cmnParamLocs[0]] offsetTensor[cmnParamLocs[1]] offsetTensor[cmnParamLocs[2]]
         if((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NHWC))
         {
+#ifdef __loongarch_asx
+            pCMNParams[0] = lasx_setr_f32(multiplierTensor[cmnParamLocs[0]], multiplierTensor[cmnParamLocs[1]], multiplierTensor[cmnParamLocs[2]], 0.0f, multiplierTensor[cmnParamLocs[0]], multiplierTensor[cmnParamLocs[1]], multiplierTensor[cmnParamLocs[2]], 0.0f);
+            pCMNParams[1] = lasx_setr_f32(offsetTensor[cmnParamLocs[0]], offsetTensor[cmnParamLocs[1]], offsetTensor[cmnParamLocs[2]], 1.0f, offsetTensor[cmnParamLocs[0]], offsetTensor[cmnParamLocs[1]], offsetTensor[cmnParamLocs[2]], 1.0f);
+#else
             pCMNParams[0] = _mm256_setr_ps(multiplierTensor[cmnParamLocs[0]], multiplierTensor[cmnParamLocs[1]], multiplierTensor[cmnParamLocs[2]], 0.0f, multiplierTensor[cmnParamLocs[0]], multiplierTensor[cmnParamLocs[1]], multiplierTensor[cmnParamLocs[2]], 0.0f);
             pCMNParams[1] = _mm256_setr_ps(offsetTensor[cmnParamLocs[0]], offsetTensor[cmnParamLocs[1]], offsetTensor[cmnParamLocs[2]], 1.0f, offsetTensor[cmnParamLocs[0]], offsetTensor[cmnParamLocs[1]], offsetTensor[cmnParamLocs[2]], 1.0f);
+#endif
         }
         else
         {
             for(int pos = 0; pos < numRegs; pos += 2)
             {
+#ifdef __loongarch_asx
+                pCMNParams[pos] = lasx_set1_f32(multiplierTensor[cmnParamLoc]);
+                pCMNParams[pos + 1] = lasx_set1_f32(offsetTensor[cmnParamLoc]);
+                cmnParamLoc++;
+#else
                 pCMNParams[pos] = _mm256_set1_ps(multiplierTensor[cmnParamLoc]);
                 pCMNParams[pos + 1] = _mm256_set1_ps(offsetTensor[cmnParamLoc]);
                 cmnParamLoc++;
+#endif
             }
         }
 
diff --git a/src/modules/cpu/kernel/down_mixing.hpp b/src/modules/cpu/kernel/down_mixing.hpp
index c9edb319..d497878e 100644
--- a/src/modules/cpu/kernel/down_mixing.hpp
+++ b/src/modules/cpu/kernel/down_mixing.hpp
@@ -85,6 +85,20 @@ RppStatus down_mixing_host_tensor(Rpp32f *srcPtr,
                 // if number of channels are greater than or equal to 8, use AVX implementation
                 if(flagAVX)
                 {
+#ifdef __loongarch_asx
+                    __m256 pDst = avx_p0;
+                    for(; channelLoopCount < alignedChannels; channelLoopCount += channelIncrement)
+                    {
+                        __m256 pSrc, pWeights;
+                        pWeights = lasx_setr_f32(weights[channelLoopCount], weights[channelLoopCount + 1], weights[channelLoopCount + 2], weights[channelLoopCount + 3],
+                                weights[channelLoopCount + 4], weights[channelLoopCount + 5], weights[channelLoopCount + 6], weights[channelLoopCount + 7]);
+                        pSrc = (__m256)__lasx_xvld(srcPtrTemp, 0);
+                        pSrc = __lasx_xvfmul_s(pSrc, pWeights);
+                        pDst = __lasx_xvfadd_s(pDst, pSrc);
+                        srcPtrTemp += channelIncrement;
+                    }
+                    dstPtrTemp[dstIdx] = rpp_hsum_ps(pDst);
+#else
                     __m256 pDst = avx_p0;
                     for(; channelLoopCount < alignedChannels; channelLoopCount += channelIncrement)
                     {
@@ -97,9 +111,23 @@ RppStatus down_mixing_host_tensor(Rpp32f *srcPtr,
                         srcPtrTemp += channelIncrement;
                     }
                     dstPtrTemp[dstIdx] = rpp_hsum_ps(pDst);
+#endif
                 }
                 else
                 {
+#ifdef __loongarch_sx
+                    __m128 pDst = xmm_p0;
+                    for(; channelLoopCount < alignedChannels; channelLoopCount += channelIncrement)
+                    {
+                        __m128 pSrc, pWeights;
+                        pWeights = lsx_setr_f32(weights[channelLoopCount], weights[channelLoopCount + 1], weights[channelLoopCount + 2], weights[channelLoopCount + 3]);
+                        pSrc = (__m128)__lsx_vld(srcPtrTemp, 0);
+                        pSrc = __lsx_vfmul_s(pSrc, pWeights);
+                        pDst = __lsx_vfadd_s(pDst, pSrc);
+                        srcPtrTemp += channelIncrement;
+                    }
+                    dstPtrTemp[dstIdx] = rpp_hsum_ps(pDst);
+#else
                     __m128 pDst = xmm_p0;
                     for(; channelLoopCount < alignedChannels; channelLoopCount += channelIncrement)
                     {
@@ -111,6 +139,7 @@ RppStatus down_mixing_host_tensor(Rpp32f *srcPtr,
                         srcPtrTemp += channelIncrement;
                     }
                     dstPtrTemp[dstIdx] = rpp_hsum_ps(pDst);
+#endif
                 }
                 for(; channelLoopCount < channels; channelLoopCount++)
                     dstPtrTemp[dstIdx] += ((*srcPtrTemp++) * weights[channelLoopCount]);
diff --git a/src/modules/cpu/kernel/erase.hpp b/src/modules/cpu/kernel/erase.hpp
index 487b0b56..d0582718 100644
--- a/src/modules/cpu/kernel/erase.hpp
+++ b/src/modules/cpu/kernel/erase.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 template <typename T>
 RppStatus erase_host_tensor(T *srcPtr,
diff --git a/src/modules/cpu/kernel/exposure.hpp b/src/modules/cpu/kernel/exposure.hpp
index adcd9b5d..3cd76bb0 100644
--- a/src/modules/cpu/kernel/exposure.hpp
+++ b/src/modules/cpu/kernel/exposure.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 RppStatus exposure_u8_u8_host_tensor(Rpp8u *srcPtr,
                                      RpptDescPtr srcDescPtr,
@@ -64,8 +69,13 @@ RppStatus exposure_u8_u8_host_tensor(Rpp8u *srcPtr,
         Rpp32u vectorIncrement = 48;
         Rpp32u vectorIncrementPerChannel = 16;
 
+#ifdef __loongarch_asx
+        __m256 pExposureParam;
+        pExposureParam = lasx_set1_f32(multiplyingFactor);
+#else
         __m256 pExposureParam;
         pExposureParam = _mm256_set1_ps(multiplyingFactor);
+#endif
 
         // Exposure with fused output-layout toggle (NHWC -> NCHW)
         if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
@@ -250,8 +260,13 @@ RppStatus exposure_f32_f32_host_tensor(Rpp32f *srcPtr,
         Rpp32u vectorIncrement = 24;
         Rpp32u vectorIncrementPerChannel = 8;
 
+#ifdef __loongarch_asx
+        __m256 pExposureParam;
+        pExposureParam = lasx_set1_f32(multiplyingFactor);
+#else
         __m256 pExposureParam;
         pExposureParam = _mm256_set1_ps(multiplyingFactor);
+#endif
 
         // Exposure with fused output-layout toggle (NHWC -> NCHW)
         if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
@@ -436,8 +451,13 @@ RppStatus exposure_f16_f16_host_tensor(Rpp16f *srcPtr,
         Rpp32u vectorIncrement = 24;
         Rpp32u vectorIncrementPerChannel = 8;
 
+#ifdef __loongarch_asx
+        __m256 pExposureParam;
+        pExposureParam = lasx_set1_f32(multiplyingFactor);
+#else
         __m256 pExposureParam;
         pExposureParam = _mm256_set1_ps(multiplyingFactor);
+#endif
 
         // Exposure with fused output-layout toggle (NHWC -> NCHW)
         if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
@@ -656,8 +676,13 @@ RppStatus exposure_i8_i8_host_tensor(Rpp8s *srcPtr,
         Rpp32u vectorIncrement = 48;
         Rpp32u vectorIncrementPerChannel = 16;
 
+#ifdef __loongarch_asx
+        __m256 pExposureParam;
+        pExposureParam = lasx_set1_f32(multiplyingFactor);
+#else
         __m256 pExposureParam;
         pExposureParam = _mm256_set1_ps(multiplyingFactor);
+#endif
 
         // Exposure with fused output-layout toggle (NHWC -> NCHW)
         if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
diff --git a/src/modules/cpu/kernel/flip.hpp b/src/modules/cpu/kernel/flip.hpp
index 0eb5693a..aa154c1d 100644
--- a/src/modules/cpu/kernel/flip.hpp
+++ b/src/modules/cpu/kernel/flip.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 RppStatus flip_u8_u8_host_tensor(Rpp8u *srcPtr,
                                  RpptDescPtr srcDescPtr,
diff --git a/src/modules/cpu/kernel/flip_voxel.hpp b/src/modules/cpu/kernel/flip_voxel.hpp
index 0a8ab043..b2c62dd1 100644
--- a/src/modules/cpu/kernel/flip_voxel.hpp
+++ b/src/modules/cpu/kernel/flip_voxel.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 RppStatus flip_voxel_f32_f32_host_tensor(Rpp32f *srcPtr,
                                          RpptGenericDescPtr srcGenericDescPtr,
@@ -128,7 +133,7 @@ RppStatus flip_voxel_f32_f32_host_tensor(Rpp32f *srcPtr,
                         srcPtrTemp = srcPtrRow;
                         dstPtrTemp = dstPtrRow;
                         int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                         for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                         {
                             __m256 p[1];
@@ -191,7 +196,7 @@ RppStatus flip_voxel_f32_f32_host_tensor(Rpp32f *srcPtr,
                     srcPtrTemp = srcPtrRow;
                     dstPtrTemp = dstPtrRow;
                     int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                     {
                         __m256 p[6];
@@ -324,7 +329,7 @@ RppStatus flip_voxel_u8_u8_host_tensor(Rpp8u *srcPtr,
                         srcPtrTemp = srcPtrRow;
                         dstPtrTemp = dstPtrRow;
                         int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                         for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                         {
                             __m256 p[2];
@@ -387,7 +392,7 @@ RppStatus flip_voxel_u8_u8_host_tensor(Rpp8u *srcPtr,
                     srcPtrTemp = srcPtrRow;
                     dstPtrTemp = dstPtrRow;
                     int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                     {
 
diff --git a/src/modules/cpu/kernel/fog.hpp b/src/modules/cpu/kernel/fog.hpp
index bfc48c21..7002cbe7 100644
--- a/src/modules/cpu/kernel/fog.hpp
+++ b/src/modules/cpu/kernel/fog.hpp
@@ -23,9 +23,14 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#include "fog_mask.hpp"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
-#include "fog_mask.hpp"
+#endif
 
 RppStatus fog_u8_u8_host_tensor(Rpp8u *srcPtr,
                                 RpptDescPtr srcDescPtr,
@@ -79,13 +84,20 @@ RppStatus fog_u8_u8_host_tensor(Rpp8u *srcPtr,
         Rpp32u alignedLength = (bufferLength / 48) * 48;
         Rpp32u vectorIncrement = 48;
         Rpp32u vectorIncrementPerChannel = 16;
-#if __AVX2__
+#if defined(__AVX2__)
         __m256 pIntensity = _mm256_set1_ps(intensityValue);
         __m256 pGrayFactor = _mm256_set1_ps(grayValue);
         __m256 pConversionFactor[3];
         pConversionFactor[0] = _mm256_set1_ps(RGB_TO_GREY_WEIGHT_RED);
         pConversionFactor[1] = _mm256_set1_ps(RGB_TO_GREY_WEIGHT_GREEN);
         pConversionFactor[2] = _mm256_set1_ps(RGB_TO_GREY_WEIGHT_BLUE);
+#elif defined(__loongarch_asx)
+        __m256 pIntensity = lasx_set1_f32(intensityValue);
+        __m256 pGrayFactor = lasx_set1_f32(grayValue);
+        __m256 pConversionFactor[3];
+        pConversionFactor[0] = lasx_set1_f32(RGB_TO_GREY_WEIGHT_RED);
+        pConversionFactor[1] = lasx_set1_f32(RGB_TO_GREY_WEIGHT_GREEN);
+        pConversionFactor[2] = lasx_set1_f32(RGB_TO_GREY_WEIGHT_BLUE);
 #endif
         // Fog without fused output-layout toggle (NHWC -> NCHW)
         if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
@@ -110,7 +122,7 @@ RppStatus fog_u8_u8_host_tensor(Rpp8u *srcPtr,
                 fogIntensityMaskPtrTemp = fogIntensityMaskPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256 pFogAlphaMask[2], pFogIntensityMask[2], p[6];
@@ -175,7 +187,7 @@ RppStatus fog_u8_u8_host_tensor(Rpp8u *srcPtr,
                 fogAlphaMaskPtrTemp = fogAlphaMaskPtrRow;
                 fogIntensityMaskPtrTemp = fogIntensityMaskPtrRow;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pFogAlphaMask[2], pFogIntensityMask[2], p[6];
@@ -238,7 +250,7 @@ RppStatus fog_u8_u8_host_tensor(Rpp8u *srcPtr,
                 fogAlphaMaskPtrTemp = fogAlphaMaskPtrRow;
                 fogIntensityMaskPtrTemp = fogIntensityMaskPtrRow;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256 pFogAlphaMask[2], pFogIntensityMask[2], p[6];
@@ -304,7 +316,7 @@ RppStatus fog_u8_u8_host_tensor(Rpp8u *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 pFogAlphaMask[2], pFogIntensityMask[2], p[6];
                     rpp_simd_load(rpp_load48_u8pln3_to_f32pln3_avx, srcPtrTempR, srcPtrTempG, srcPtrTempB, p);              // simd loads
                     rpp_simd_load(rpp_load16_f32_to_f32_avx, fogAlphaMaskPtrTemp, pFogAlphaMask);                           // simd loads
@@ -373,7 +385,7 @@ RppStatus fog_u8_u8_host_tensor(Rpp8u *srcPtr,
                 fogAlphaMaskPtrTemp = fogAlphaMaskPtrRow;
                 fogIntensityMaskPtrTemp = fogIntensityMaskPtrRow;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pFogAlphaMask[2], pFogIntensityMask[2],p[2];
@@ -460,13 +472,20 @@ RppStatus fog_f16_f16_host_tensor(Rpp16f *srcPtr,
         Rpp32u alignedLength = (bufferLength / 24) * 24;
         Rpp32u vectorIncrement = 24;
         Rpp32u vectorIncrementPerChannel = 8;
-#if __AVX2__
+#if defined(__AVX2__)
         __m256 pIntensity = _mm256_set1_ps(intensityValue);
         __m256 pGrayFactor = _mm256_set1_ps(grayValue);
         __m256 pConversionFactor[3];
         pConversionFactor[0] = _mm256_set1_ps(RGB_TO_GREY_WEIGHT_RED);
         pConversionFactor[1] = _mm256_set1_ps(RGB_TO_GREY_WEIGHT_GREEN);
         pConversionFactor[2] = _mm256_set1_ps(RGB_TO_GREY_WEIGHT_BLUE);
+#elif defined(__loongarch_asx)
+        __m256 pIntensity = lasx_set1_f32(intensityValue);
+        __m256 pGrayFactor = lasx_set1_f32(grayValue);
+        __m256 pConversionFactor[3];
+        pConversionFactor[0] = lasx_set1_f32(RGB_TO_GREY_WEIGHT_RED);
+        pConversionFactor[1] = lasx_set1_f32(RGB_TO_GREY_WEIGHT_GREEN);
+        pConversionFactor[2] = lasx_set1_f32(RGB_TO_GREY_WEIGHT_BLUE);
 #endif
         // Fog without fused output-layout toggle (NHWC -> NCHW)
         if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
@@ -490,7 +509,7 @@ RppStatus fog_f16_f16_host_tensor(Rpp16f *srcPtr,
                 fogAlphaMaskPtrTemp = fogAlphaMaskPtrRow;
                 fogIntensityMaskPtrTemp = fogIntensityMaskPtrRow;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256 pFogAlphaMask, pFogIntensityMask, p[3];
@@ -557,7 +576,7 @@ RppStatus fog_f16_f16_host_tensor(Rpp16f *srcPtr,
                 fogAlphaMaskPtrTemp = fogAlphaMaskPtrRow;
                 fogIntensityMaskPtrTemp = fogIntensityMaskPtrRow;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pFogAlphaMask, pFogIntensityMask, p[3];
@@ -623,7 +642,7 @@ RppStatus fog_f16_f16_host_tensor(Rpp16f *srcPtr,
                 fogAlphaMaskPtrTemp = fogAlphaMaskPtrRow;
                 fogIntensityMaskPtrTemp = fogIntensityMaskPtrRow;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256 pFogAlphaMask, pFogIntensityMask, p[3];
@@ -694,7 +713,7 @@ RppStatus fog_f16_f16_host_tensor(Rpp16f *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 pFogAlphaMask, pFogIntensityMask, p[3];
                     rpp_simd_load(rpp_load8_f32_to_f32_avx, fogAlphaMaskPtrTemp, &pFogAlphaMask);                           // simd loads
                     rpp_simd_load(rpp_load8_f32_to_f32_avx, fogIntensityMaskPtrTemp, &pFogIntensityMask);                   // simd loads
@@ -747,7 +766,7 @@ RppStatus fog_f16_f16_host_tensor(Rpp16f *srcPtr,
         // Fog without fused output-layout toggle (NCHW -> NCHW)
         else if ((srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NCHW))
         {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             alignedLength = bufferLength & ~7;
 #endif
 
@@ -766,7 +785,7 @@ RppStatus fog_f16_f16_host_tensor(Rpp16f *srcPtr,
                 fogAlphaMaskPtrTemp = fogAlphaMaskPtrRow;
                 fogIntensityMaskPtrTemp = fogIntensityMaskPtrRow;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pFogAlphaMask, pFogIntensityMask, p;
@@ -781,6 +800,21 @@ RppStatus fog_f16_f16_host_tensor(Rpp16f *srcPtr,
                     fogAlphaMaskPtrTemp += vectorIncrementPerChannel;
                     fogIntensityMaskPtrTemp += vectorIncrementPerChannel;
                 }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256 pFogAlphaMask, pFogIntensityMask, p;
+                    rpp_simd_load(rpp_load8_f32_to_f32_avx, fogAlphaMaskPtrTemp, &pFogAlphaMask);                   // simd loads
+                    rpp_simd_load(rpp_load8_f32_to_f32_avx, fogIntensityMaskPtrTemp, &pFogIntensityMask);           // simd loads
+                    pFogIntensityMask = __lasx_xvfmul_s(pFogIntensityMask, avx_p1op255);                              // u8 normalization to range[0,1];
+                    rpp_simd_load(rpp_load8_f16_to_f32_avx, srcPtrTemp, &p);                                        // simd loads
+                    compute_fog_8_host(&p, &pFogAlphaMask, &pFogIntensityMask, pIntensity);                         // fog adjustment
+                    rpp_simd_store(rpp_store8_f32_to_f16_avx, dstPtrTemp, &p);                                      // simd stores
+                    srcPtrTemp += vectorIncrementPerChannel;
+                    dstPtrTemp += vectorIncrementPerChannel;
+                    fogAlphaMaskPtrTemp += vectorIncrementPerChannel;
+                    fogIntensityMaskPtrTemp += vectorIncrementPerChannel;
+                }
 #endif
                 for (; vectorLoopCount < bufferLength; vectorLoopCount++)
                 {
@@ -852,13 +886,20 @@ RppStatus fog_f32_f32_host_tensor(Rpp32f *srcPtr,
         Rpp32u alignedLength = (bufferLength / 24) * 24;
         Rpp32u vectorIncrement = 24;
         Rpp32u vectorIncrementPerChannel = 8;
-#if __AVX2__
+#if defined(__AVX2__)
         __m256 pIntensity = _mm256_set1_ps(intensityValue);
         __m256 pGrayFactor = _mm256_set1_ps(grayValue);
         __m256 pConversionFactor[3];
         pConversionFactor[0] = _mm256_set1_ps(RGB_TO_GREY_WEIGHT_RED);
         pConversionFactor[1] = _mm256_set1_ps(RGB_TO_GREY_WEIGHT_GREEN);
         pConversionFactor[2] = _mm256_set1_ps(RGB_TO_GREY_WEIGHT_BLUE);
+#elif defined(__loongarch_asx)
+        __m256 pIntensity = lasx_set1_f32(intensityValue);
+        __m256 pGrayFactor = lasx_set1_f32(grayValue);
+        __m256 pConversionFactor[3];
+        pConversionFactor[0] = lasx_set1_f32(RGB_TO_GREY_WEIGHT_RED);
+        pConversionFactor[1] = lasx_set1_f32(RGB_TO_GREY_WEIGHT_GREEN);
+        pConversionFactor[2] = lasx_set1_f32(RGB_TO_GREY_WEIGHT_BLUE);
 #endif
         // Fog without fused output-layout toggle (NHWC -> NCHW)
         if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
@@ -882,7 +923,7 @@ RppStatus fog_f32_f32_host_tensor(Rpp32f *srcPtr,
                 fogAlphaMaskPtrTemp = fogAlphaMaskPtrRow;
                 fogIntensityMaskPtrTemp = fogIntensityMaskPtrRow;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {                    
                     __m256 pFogAlphaMask, pFogIntensityMask, p[3];
@@ -949,7 +990,7 @@ RppStatus fog_f32_f32_host_tensor(Rpp32f *srcPtr,
                 fogAlphaMaskPtrTemp = fogAlphaMaskPtrRow;
                 fogIntensityMaskPtrTemp = fogIntensityMaskPtrRow;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pFogAlphaMask, pFogIntensityMask, p[3];
@@ -1015,7 +1056,7 @@ RppStatus fog_f32_f32_host_tensor(Rpp32f *srcPtr,
                 fogAlphaMaskPtrTemp = fogAlphaMaskPtrRow;
                 fogIntensityMaskPtrTemp = fogIntensityMaskPtrRow;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256 pFogAlphaMask, pFogIntensityMask, p[3];
@@ -1086,7 +1127,7 @@ RppStatus fog_f32_f32_host_tensor(Rpp32f *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 pFogAlphaMask, pFogIntensityMask, p[3];
                     rpp_simd_load(rpp_load8_f32_to_f32_avx, fogAlphaMaskPtrTemp, &pFogAlphaMask);                           // simd loads
                     rpp_simd_load(rpp_load8_f32_to_f32_avx, fogIntensityMaskPtrTemp, &pFogIntensityMask);                   // simd loads
@@ -1157,7 +1198,7 @@ RppStatus fog_f32_f32_host_tensor(Rpp32f *srcPtr,
                 fogAlphaMaskPtrTemp = fogAlphaMaskPtrRow;
                 fogIntensityMaskPtrTemp = fogIntensityMaskPtrRow;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pFogAlphaMask, pFogIntensityMask, p;
@@ -1168,6 +1209,22 @@ RppStatus fog_f32_f32_host_tensor(Rpp32f *srcPtr,
                     compute_fog_8_host(&p, &pFogAlphaMask, &pFogIntensityMask, pIntensity);                         // fog adjustment
                     rpp_simd_store(rpp_store8_f32_to_f32_avx, dstPtrTemp, &p);                                      // simd stores
                     
+                    srcPtrTemp += vectorIncrementPerChannel;
+                    dstPtrTemp += vectorIncrementPerChannel;
+                    fogAlphaMaskPtrTemp += vectorIncrementPerChannel;
+                    fogIntensityMaskPtrTemp += vectorIncrementPerChannel;
+                }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256 pFogAlphaMask, pFogIntensityMask, p;
+                    rpp_simd_load(rpp_load8_f32_to_f32_avx, fogAlphaMaskPtrTemp, &pFogAlphaMask);                   // simd loads
+                    rpp_simd_load(rpp_load8_f32_to_f32_avx, fogIntensityMaskPtrTemp, &pFogIntensityMask);           // simd loads
+                    pFogIntensityMask = __lasx_xvfmul_s(pFogIntensityMask, avx_p1op255);                              // u8 normalization to range[0,1]
+                    rpp_simd_load(rpp_load8_f32_to_f32_avx, srcPtrTemp, &p);                                        // simd loads
+                    compute_fog_8_host(&p, &pFogAlphaMask, &pFogIntensityMask, pIntensity);                         // fog adjustment
+                    rpp_simd_store(rpp_store8_f32_to_f32_avx, dstPtrTemp, &p);                                      // simd stores
+                    
                     srcPtrTemp += vectorIncrementPerChannel;
                     dstPtrTemp += vectorIncrementPerChannel;
                     fogAlphaMaskPtrTemp += vectorIncrementPerChannel;
@@ -1245,13 +1302,20 @@ RppStatus fog_i8_i8_host_tensor(Rpp8s *srcPtr,
         Rpp32u alignedLength = (bufferLength / 48) * 48;
         Rpp32u vectorIncrement = 48;
         Rpp32u vectorIncrementPerChannel = 16;
-#if __AVX2__
+#if defined(__AVX2__)
         __m256 pIntensity = _mm256_set1_ps(intensityValue);
         __m256 pGrayFactor = _mm256_set1_ps(grayValue);
         __m256 pConversionFactor[3];
         pConversionFactor[0] = _mm256_set1_ps(RGB_TO_GREY_WEIGHT_RED);
         pConversionFactor[1] = _mm256_set1_ps(RGB_TO_GREY_WEIGHT_GREEN);
         pConversionFactor[2] = _mm256_set1_ps(RGB_TO_GREY_WEIGHT_BLUE);
+#elif defined(__loongarch_asx)
+        __m256 pIntensity = lasx_set1_f32(intensityValue);
+        __m256 pGrayFactor = lasx_set1_f32(grayValue);
+        __m256 pConversionFactor[3];
+        pConversionFactor[0] = lasx_set1_f32(RGB_TO_GREY_WEIGHT_RED);
+        pConversionFactor[1] = lasx_set1_f32(RGB_TO_GREY_WEIGHT_GREEN);
+        pConversionFactor[2] = lasx_set1_f32(RGB_TO_GREY_WEIGHT_BLUE);
 #endif
         // Fog without fused output-layout toggle (NHWC -> NCHW)
         if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
@@ -1275,7 +1339,7 @@ RppStatus fog_i8_i8_host_tensor(Rpp8s *srcPtr,
                 fogAlphaMaskPtrTemp = fogAlphaMaskPtrRow;
                 fogIntensityMaskPtrTemp = fogIntensityMaskPtrRow;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256 pFogAlphaMask[2], pFogIntensityMask[2], p[6];
@@ -1340,7 +1404,7 @@ RppStatus fog_i8_i8_host_tensor(Rpp8s *srcPtr,
                 fogAlphaMaskPtrTemp = fogAlphaMaskPtrRow;
                 fogIntensityMaskPtrTemp = fogIntensityMaskPtrRow;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pFogAlphaMask[2], pFogIntensityMask[2], p[6];
@@ -1404,7 +1468,7 @@ RppStatus fog_i8_i8_host_tensor(Rpp8s *srcPtr,
                 fogAlphaMaskPtrTemp = fogAlphaMaskPtrRow;
                 fogIntensityMaskPtrTemp = fogIntensityMaskPtrRow;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256 pFogAlphaMask[2], pFogIntensityMask[2], p[6];
@@ -1473,7 +1537,7 @@ RppStatus fog_i8_i8_host_tensor(Rpp8s *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[6], pFogAlphaMask[2], pFogIntensityMask[2];
                     rpp_simd_load(rpp_load16_f32_to_f32_avx, fogAlphaMaskPtrTemp, pFogAlphaMask);                           // simd loads
                     rpp_simd_load(rpp_load16_f32_to_f32_avx, fogIntensityMaskPtrTemp, pFogIntensityMask);                   // simd loads
@@ -1542,7 +1606,7 @@ RppStatus fog_i8_i8_host_tensor(Rpp8s *srcPtr,
                 fogAlphaMaskPtrTemp = fogAlphaMaskPtrRow;
                 fogIntensityMaskPtrTemp = fogIntensityMaskPtrRow;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pFogAlphaMask[2], pFogIntensityMask[2], p[2];
diff --git a/src/modules/cpu/kernel/fused_multiply_add_scalar.hpp b/src/modules/cpu/kernel/fused_multiply_add_scalar.hpp
index 609120f3..8c5ab71c 100644
--- a/src/modules/cpu/kernel/fused_multiply_add_scalar.hpp
+++ b/src/modules/cpu/kernel/fused_multiply_add_scalar.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 RppStatus fused_multiply_add_scalar_f32_f32_host_tensor(Rpp32f *srcPtr,
                                            RpptGenericDescPtr srcGenericDescPtr,
@@ -64,13 +69,21 @@ RppStatus fused_multiply_add_scalar_f32_f32_host_tensor(Rpp32f *srcPtr,
         Rpp32f *srcPtrChannel, *dstPtrChannel;
         dstPtrChannel = dstPtrImage;
 
-#if __AVX2__
+#if defined(__AVX2__)
         Rpp32u vectorIncrement = 8;
 
         __m256 pFmaddParams[2];
         pFmaddParams[0] = _mm256_set1_ps(mulParam);
         pFmaddParams[1] = _mm256_set1_ps(addParam);
 
+        Rpp32u alignedLength = bufferLength & ~(vectorIncrement-1);
+#elif defined(__loongarch_asx)
+        Rpp32u vectorIncrement = 8;
+
+        __m256 pFmaddParams[2];
+        pFmaddParams[0] = lasx_set1_f32(mulParam);
+        pFmaddParams[1] = lasx_set1_f32(addParam);
+
         Rpp32u alignedLength = bufferLength & ~(vectorIncrement-1);
 #endif
         // Fmadd without fused output-layout toggle (NCDHW -> NCDHW)
@@ -97,7 +110,7 @@ RppStatus fused_multiply_add_scalar_f32_f32_host_tensor(Rpp32f *srcPtr,
                         dstPtrTemp = dstPtrRow;
 
                         int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                         for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                         {
                             __m256 p[1];
@@ -142,7 +155,7 @@ RppStatus fused_multiply_add_scalar_f32_f32_host_tensor(Rpp32f *srcPtr,
                     dstPtrTemp = dstPtrRow;
 
                     int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                     {
                         __m256 p[1];
diff --git a/src/modules/cpu/kernel/gamma_correction.hpp b/src/modules/cpu/kernel/gamma_correction.hpp
index b676f11a..fef293a0 100644
--- a/src/modules/cpu/kernel/gamma_correction.hpp
+++ b/src/modules/cpu/kernel/gamma_correction.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 RppStatus gamma_correction_u8_u8_host_tensor(Rpp8u *srcPtr,
                                              RpptDescPtr srcDescPtr,
diff --git a/src/modules/cpu/kernel/gaussian_filter.hpp b/src/modules/cpu/kernel/gaussian_filter.hpp
index 703d2674..6efb875f 100644
--- a/src/modules/cpu/kernel/gaussian_filter.hpp
+++ b/src/modules/cpu/kernel/gaussian_filter.hpp
@@ -19,9 +19,15 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#include "rpp_loongarch_filter.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
 #include "rpp_cpu_filter.hpp"
+#endif
 
 inline void rpp_store_filter_3x3_host(Rpp8u *dstPtrTemp, __m256 *pDst)
 {
@@ -97,7 +103,7 @@ RppStatus gaussian_filter_host_tensor(T *srcPtr,
         return gaussian_filter_generic_host_tensor(srcPtr, srcDescPtr, dstPtr, dstDescPtr, stdDevTensor, kernelSize, roiTensorPtrSrc, roiType, layoutParams, handle);
 
     // set the required masks array needed for shuffle operations 
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
     __m256i pxMaskPln[7] = {avx_pxMaskRotate0To1, avx_pxMaskRotate0To2, avx_pxMaskRotate0To3, avx_pxMaskRotate0To4, avx_pxMaskRotate0To5, avx_pxMaskRotate0To6, avx_pxMaskRotate0To7};
     __m256i pxMaskPkd[7] = {avx_pxMaskRotate0To3, avx_pxMaskRotate0To6, avx_pxMaskRotate0To1, avx_pxMaskRotate0To4, avx_pxMaskRotate0To7, avx_pxMaskRotate0To2, avx_pxMaskRotate0To5};
 #endif
@@ -123,11 +129,16 @@ RppStatus gaussian_filter_host_tensor(T *srcPtr,
         srcPtrChannel = srcPtrImage + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
         dstPtrChannel = dstPtrImage;
         create_gaussian_kernel_host(filterTensor, stdDevTensor[batchCount], kernelSize);
-#if __AVX2__
+#if defined(__AVX2__)
         int size = kernelSize * kernelSize;
         __m256 pFilter[size];
         for (int i = 0; i < size; i++)
             pFilter[i] = _mm256_set1_ps(filterTensor[i]);
+#elif defined(__loongarch_asx)
+        int size = kernelSize * kernelSize;
+        __m256 pFilter[size];
+        for (int i = 0; i < size; i++)
+            pFilter[i] = lasx_set1_f32(filterTensor[i]);
 #endif
         if (kernelSize == 3)
         {
@@ -160,7 +171,7 @@ RppStatus gaussian_filter_host_tensor(T *srcPtr,
                         get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
                         process_left_border_columns_pln_pln(srcPtrTemp, dstPtrTemp, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, filterTensor);
                         dstPtrTemp += padLength;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                         // process alignedLength number of columns in each row
                         for (; vectorLoopCount < alignedLength; vectorLoopCount += 14)
                         {
@@ -211,7 +222,7 @@ RppStatus gaussian_filter_host_tensor(T *srcPtr,
                     get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
                     process_left_border_columns_pkd_pkd(srcPtrTemp, srcPtrRow, dstPtrTemp, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, filterTensor);
                     dstPtrTemp += padLength * 3;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     // process remaining columns in each row
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += 16)
                     {
@@ -261,7 +272,7 @@ RppStatus gaussian_filter_host_tensor(T *srcPtr,
                     Rpp32s rowKernelLoopLimit = kernelSize;
                     get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
                     process_left_border_columns_pkd_pln(srcPtrTemp, srcPtrRow, dstPtrTempChannels, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, filterTensor);
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     // process remaining columns in each row
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += 12)
                     {
@@ -325,7 +336,7 @@ RppStatus gaussian_filter_host_tensor(T *srcPtr,
                             dstPtrTemp++;
                         }
                     }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     // process alignedLength number of columns in each row
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += 14)
                     {
@@ -404,7 +415,7 @@ RppStatus gaussian_filter_host_tensor(T *srcPtr,
                         get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
                         process_left_border_columns_pln_pln(srcPtrTemp, dstPtrTemp, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, filterTensor);
                         dstPtrTemp += padLength;
-    #if __AVX2__
+    #if defined(__AVX2__) || defined(__loongarch_asx)
                         // process alignedLength number of columns in each row
                         for (; vectorLoopCount < alignedLength; vectorLoopCount += 12)
                         {
@@ -456,7 +467,7 @@ RppStatus gaussian_filter_host_tensor(T *srcPtr,
                     get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
                     process_left_border_columns_pkd_pkd(srcPtrTemp, srcPtrRow, dstPtrTemp, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, filterTensor);
                     dstPtrTemp += padLength * 3;
-    #if __AVX2__
+    #if defined(__AVX2__) || defined(__loongarch_asx)
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += 16)
                     {
                         __m256 pRow[20], pDst[2];
@@ -517,7 +528,7 @@ RppStatus gaussian_filter_host_tensor(T *srcPtr,
                             dstPtrTemp++;
                         }
                     }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     // process alignedLength number of columns in each row
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
                     {
@@ -580,7 +591,7 @@ RppStatus gaussian_filter_host_tensor(T *srcPtr,
                     Rpp32s rowKernelLoopLimit = kernelSize;
                     get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
                     process_left_border_columns_pkd_pln(srcPtrTemp, srcPtrRow, dstPtrTempChannels, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, filterTensor);
-    #if __AVX2__
+    #if defined(__AVX2__) || defined(__loongarch_asx)
                     // process remaining columns in each row
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += 12)
                     {
@@ -650,7 +661,7 @@ RppStatus gaussian_filter_host_tensor(T *srcPtr,
                         get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
                         process_left_border_columns_pln_pln(srcPtrTemp, dstPtrTemp, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, filterTensor);
                         dstPtrTemp += padLength;
-#if __AVX2__
+#if defined(__AVX2__)
                         // process alignedLength number of columns in each row
                         for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
                         {
@@ -670,6 +681,29 @@ RppStatus gaussian_filter_host_tensor(T *srcPtr,
                             else if constexpr (std::is_same<T, Rpp8u>::value)
                                 rpp_store8_f32_to_u8_avx(dstPtrTemp, pDst);
 
+                            increment_row_ptrs(srcPtrTemp, kernelSize, 8);
+                            dstPtrTemp += 8;
+                        }
+#elif defined(__loongarch_asx)
+                        // process alignedLength number of columns in each row
+                        for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
+                        {
+                            __m256 pRow[14], pDst;
+                            rpp_load_filter_7x7_pln_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+                            pDst = avx_p0;
+                            for (int k = 0, filterIndex = 0, rowIndex = 0; k < 7; k++, filterIndex += 7, rowIndex += 2)
+                                permute_blend_add_7x7_pln(pDst, &pRow[rowIndex], &pFilter[filterIndex]);
+
+                            // convert result from pln to pkd format and store in output buffer
+                            if constexpr (std::is_same<T, Rpp32f>::value)
+                                __lasx_xvst((__m256i)pDst, dstPtrTemp, 0);
+                            else if constexpr (std::is_same<T, Rpp16f>::value)
+                                __lsx_vst(lasx_cvtf32_ph(pDst, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC), (__m128i *)dstPtrTemp, 0);
+                            else if constexpr (std::is_same<T, Rpp8s>::value)
+                                rpp_store8_f32_to_i8_avx(dstPtrTemp, pDst);
+                            else if constexpr (std::is_same<T, Rpp8u>::value)
+                                rpp_store8_f32_to_u8_avx(dstPtrTemp, pDst);
+
                             increment_row_ptrs(srcPtrTemp, kernelSize, 8);
                             dstPtrTemp += 8;
                         }
@@ -707,7 +741,7 @@ RppStatus gaussian_filter_host_tensor(T *srcPtr,
                     get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
                     process_left_border_columns_pkd_pkd(srcPtrTemp, srcPtrRow, dstPtrTemp, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, filterTensor);
                     dstPtrTemp += padLength * 3;
-#if __AVX2__
+#if defined(__AVX2__)
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
                     {
                         __m256 pRow[28], pDst;
@@ -725,6 +759,27 @@ RppStatus gaussian_filter_host_tensor(T *srcPtr,
                         else if constexpr (std::is_same<T, Rpp8u>::value)
                             rpp_store8_f32_to_u8_avx(dstPtrTemp, pDst);
 
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 8);
+                        dstPtrTemp += 8;
+                    }
+#elif defined(__loongarch_asx)
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
+                    {
+                        __m256 pRow[28], pDst;
+                        rpp_load_filter_7x7_pkd_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+                        pDst = avx_p0;
+                        for (int k = 0, filterIndex = 0, rowIndex = 0; k < 7; k++, filterIndex += 7, rowIndex += 4)
+                            permute_blend_add_7x7_pkd(pDst, &pRow[rowIndex], pRow[rowIndex + 3], &pFilter[filterIndex]);
+
+                        if constexpr (std::is_same<T, Rpp32f>::value)
+                            __lasx_xvst((__m256i)pDst, dstPtrTemp, 0);
+                        else if constexpr (std::is_same<T, Rpp16f>::value)
+                            __lsx_vst(lasx_cvtf32_ph(pDst, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC), (__m128i *)dstPtrTemp, 0);
+                        else if constexpr (std::is_same<T, Rpp8s>::value)
+                            rpp_store8_f32_to_i8_avx(dstPtrTemp, pDst);
+                        else if constexpr (std::is_same<T, Rpp8u>::value)
+                            rpp_store8_f32_to_u8_avx(dstPtrTemp, pDst);
+
                         increment_row_ptrs(srcPtrTemp, kernelSize, 8);
                         dstPtrTemp += 8;
                     }
@@ -772,7 +827,7 @@ RppStatus gaussian_filter_host_tensor(T *srcPtr,
                             dstPtrTemp++;
                         }
                     }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     // process alignedLength number of columns in each row
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
                     {
@@ -834,7 +889,7 @@ RppStatus gaussian_filter_host_tensor(T *srcPtr,
                     Rpp32s rowKernelLoopLimit = kernelSize;
                     get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
                     process_left_border_columns_pkd_pln(srcPtrTemp, srcPtrRow, dstPtrTempChannels, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, filterTensor);
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += 12)
                     {
                         __m256 pRow[28], pDst[2];
@@ -904,7 +959,7 @@ RppStatus gaussian_filter_host_tensor(T *srcPtr,
                         get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
                         process_left_border_columns_pln_pln(srcPtrTemp, dstPtrTemp, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, filterTensor);
                         dstPtrTemp += padLength;
-#if __AVX2__
+#if defined(__AVX2__)
                         // process alignedLength number of columns in each row
                         for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
                         {
@@ -923,6 +978,28 @@ RppStatus gaussian_filter_host_tensor(T *srcPtr,
                             else if constexpr (std::is_same<T, Rpp8u>::value)
                                 rpp_store8_f32_to_u8_avx(dstPtrTemp, pDst);
 
+                            increment_row_ptrs(srcPtrTemp, kernelSize, 8);
+                            dstPtrTemp += 8;
+                        }
+#elif defined(__loongarch_asx)
+                        // process alignedLength number of columns in each row
+                        for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
+                        {
+                            __m256 pRow[18], pDst;
+                            rpp_load_filter_9x9_pln_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+                            pDst = avx_p0;
+                            for (int k = 0, filterIndex = 0, rowIndex = 0; k < 9; k++, filterIndex += 9, rowIndex += 2)
+                                permute_blend_add_9x9_pln(pDst, &pRow[rowIndex], &pFilter[filterIndex]);
+
+                            if constexpr (std::is_same<T, Rpp32f>::value)
+                                __lasx_xvst((__m256i)pDst, dstPtrTemp, 0);
+                            else if constexpr (std::is_same<T, Rpp16f>::value)
+                                __lsx_vst(lasx_cvtf32_ph(pDst, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC), (__m128i *)dstPtrTemp, 0);
+                            else if constexpr (std::is_same<T, Rpp8s>::value)
+                                rpp_store8_f32_to_i8_avx(dstPtrTemp, pDst);
+                            else if constexpr (std::is_same<T, Rpp8u>::value)
+                                rpp_store8_f32_to_u8_avx(dstPtrTemp, pDst);
+
                             increment_row_ptrs(srcPtrTemp, kernelSize, 8);
                             dstPtrTemp += 8;
                         }
@@ -960,7 +1037,7 @@ RppStatus gaussian_filter_host_tensor(T *srcPtr,
                     get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
                     process_left_border_columns_pkd_pkd(srcPtrTemp, srcPtrRow, dstPtrTemp, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, filterTensor);
                     dstPtrTemp += padLength * 3;
-#if __AVX2__
+#if defined(__AVX2__)
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
                     {
                         __m256 pRow[36], pDst;
@@ -978,6 +1055,27 @@ RppStatus gaussian_filter_host_tensor(T *srcPtr,
                         else if constexpr (std::is_same<T, Rpp8u>::value)
                             rpp_store8_f32_to_u8_avx(dstPtrTemp, pDst);
 
+                        increment_row_ptrs(srcPtrTemp, kernelSize, 8);
+                        dstPtrTemp += 8;
+                    }
+#elif defined(__loongarch_asx)
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
+                    {
+                        __m256 pRow[36], pDst;
+                        rpp_load_filter_9x9_pkd_host(pRow, srcPtrTemp, rowKernelLoopLimit);
+                        pDst = avx_p0;
+                        for (int k = 0, filterIndex = 0, rowIndex = 0; k < 9; k++, filterIndex += 9, rowIndex += 4)
+                            permute_blend_add_9x9_pkd(pDst, &pRow[rowIndex], &pFilter[filterIndex]);
+
+                        if constexpr (std::is_same<T, Rpp32f>::value)
+                            __lasx_xvst((__m256i)pDst, dstPtrTemp, 0);
+                        else if constexpr (std::is_same<T, Rpp16f>::value)
+                            __lsx_vst(lasx_cvtf32_ph(pDst, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC), (__m128i *)dstPtrTemp, 0);
+                        else if constexpr (std::is_same<T, Rpp8s>::value)
+                            rpp_store8_f32_to_i8_avx(dstPtrTemp, pDst);
+                        else if constexpr (std::is_same<T, Rpp8u>::value)
+                            rpp_store8_f32_to_u8_avx(dstPtrTemp, pDst);
+
                         increment_row_ptrs(srcPtrTemp, kernelSize, 8);
                         dstPtrTemp += 8;
                     }
@@ -1024,7 +1122,7 @@ RppStatus gaussian_filter_host_tensor(T *srcPtr,
                             dstPtrTemp++;
                         }
                     }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     // process alignedLength number of columns in each row
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
                     {
@@ -1086,7 +1184,7 @@ RppStatus gaussian_filter_host_tensor(T *srcPtr,
                     Rpp32s rowKernelLoopLimit = kernelSize;
                     get_kernel_loop_limit(i, rowKernelLoopLimit, padLength, unpaddedHeight);
                     process_left_border_columns_pkd_pln(srcPtrTemp, srcPtrRow, dstPtrTempChannels, kernelSize, padLength, unpaddedWidth, rowKernelLoopLimit, filterTensor);
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += 12)
                     {
                         __m256 pRow[45], pDst[2];
diff --git a/src/modules/cpu/kernel/glitch.hpp b/src/modules/cpu/kernel/glitch.hpp
index 4138e3f9..fef387da 100644
--- a/src/modules/cpu/kernel/glitch.hpp
+++ b/src/modules/cpu/kernel/glitch.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 inline void compute_src_loc(int row , int col, Rpp32s *locArray, RpptDescPtr srcDescPtr, RpptChannelOffsets *rgbOffsets, RpptROI roi, int batchCount, int channelValue)
 {
@@ -102,7 +107,7 @@ RppStatus glitch_u8_u8_host_tensor(Rpp8u *srcPtr,
                 Rpp8u* dstRowPtrTempG = dstPtrRow + dstDescPtr->strides.cStride;
                 Rpp8u* dstRowPtrTempB = dstPtrRow + 2 * dstDescPtr->strides.cStride;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
                 {
                     __m256 p[3];
@@ -136,7 +141,7 @@ RppStatus glitch_u8_u8_host_tensor(Rpp8u *srcPtr,
             {
                 Rpp8u* dstPtrTemp = dstPtrRow;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 16)
                 {
                     __m256 p[6];
@@ -170,7 +175,7 @@ RppStatus glitch_u8_u8_host_tensor(Rpp8u *srcPtr,
             {
                 Rpp8u* dstPtrTemp = dstPtrRow;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     compute_src_loc(dstLocRow, vectorLoopCount, glitchSrcLocArray, srcDescPtr, rgbOffsets, roi, batchCount, 1);
@@ -182,6 +187,18 @@ RppStatus glitch_u8_u8_host_tensor(Rpp8u *srcPtr,
                     }
                     dstPtrTemp += 32;
                 }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
+                {
+                    compute_src_loc(dstLocRow, vectorLoopCount, glitchSrcLocArray, srcDescPtr, rgbOffsets, roi, batchCount, 1);
+                    for (int c = 0; c < 3; c++)
+                    {
+                        __m256i p;
+                        p = __lasx_xvld((__m256i *)(srcPtrChannel + (glitchSrcLocArray[c] + (c * srcDescPtr->strides.cStride))), 0);
+                        __lasx_xvst(p, (__m256i *)(dstPtrTemp + (c * srcDescPtr->strides.cStride)), 0);
+                    }
+                    dstPtrTemp += 32;
+                }
 #endif
                 for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
                 {
@@ -206,7 +223,7 @@ RppStatus glitch_u8_u8_host_tensor(Rpp8u *srcPtr,
             {
                 Rpp8u* dstPtrTemp = dstPtrRow;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 10)
                 {
                     __m256i p;
@@ -215,6 +232,15 @@ RppStatus glitch_u8_u8_host_tensor(Rpp8u *srcPtr,
                     _mm256_storeu_si256((__m256i *)(dstPtrTemp), p);
                     dstPtrTemp += 30;
                 }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += 10)
+                {
+                    __m256i p;
+                    compute_src_loc(dstLocRow, vectorLoopCount, glitchSrcLocArray, srcDescPtr, rgbOffsets, roi, batchCount, 3);
+                    rpp_simd_load(rpp_glitch_load30_u8pkd3_to_u8pkd3_avx, srcPtrChannel, glitchSrcLocArray, p);
+                    __lasx_xvst(p, (__m256i *)(dstPtrTemp), 0);
+                    dstPtrTemp += 30;
+                }
 #endif
                 for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
                 {
@@ -274,7 +300,7 @@ RppStatus glitch_f32_f32_host_tensor(Rpp32f *srcPtr,
                 Rpp32f* dstRowPtrTempG = dstPtrRow + dstDescPtr->strides.cStride;
                 Rpp32f* dstRowPtrTempB = dstPtrRow + 2 * dstDescPtr->strides.cStride;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
                 {
                     __m256 p[3];
@@ -309,7 +335,7 @@ RppStatus glitch_f32_f32_host_tensor(Rpp32f *srcPtr,
             {
                 Rpp32f* dstPtrTemp = dstPtrRow;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
                 {
                     __m256 p[3];
@@ -320,6 +346,17 @@ RppStatus glitch_f32_f32_host_tensor(Rpp32f *srcPtr,
                     rpp_simd_store(rpp_store24_f32pln3_to_f32pkd3_avx, dstPtrTemp, p);    // simd stores
                     dstPtrTemp += 24;
                 }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
+                {
+                    __m256 p[3];
+                    compute_src_loc(dstLocRow, vectorLoopCount, glitchSrcLocArray, srcDescPtr, rgbOffsets, roi, batchCount, 1);
+                    p[0] = (__m256)__lasx_xvld(srcPtrChannel + glitchSrcLocArray[0], 0);
+                    p[1] = (__m256)__lasx_xvld(srcPtrChannel + srcDescPtr->strides.cStride + glitchSrcLocArray[1], 0);
+                    p[2] = (__m256)__lasx_xvld(srcPtrChannel + 2 * srcDescPtr->strides.cStride + glitchSrcLocArray[2], 0);
+                    rpp_simd_store(rpp_store24_f32pln3_to_f32pkd3_avx, dstPtrTemp, p);    // simd stores
+                    dstPtrTemp += 24;
+                }
 #endif
                 for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
                 {
@@ -343,7 +380,7 @@ RppStatus glitch_f32_f32_host_tensor(Rpp32f *srcPtr,
             {
                 Rpp32f* dstPtrTemp = dstPtrRow;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     compute_src_loc(dstLocRow, vectorLoopCount, glitchSrcLocArray, srcDescPtr, rgbOffsets, roi, batchCount, 1);
@@ -355,6 +392,18 @@ RppStatus glitch_f32_f32_host_tensor(Rpp32f *srcPtr,
                     }
                     dstPtrTemp += 8;
                 }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
+                {
+                    compute_src_loc(dstLocRow, vectorLoopCount, glitchSrcLocArray, srcDescPtr, rgbOffsets, roi, batchCount, 1);
+                    for (int c = 0; c < 3; c++)
+                    {
+                        __m256 p;
+                        p = (__m256)__lasx_xvld(srcPtrChannel + (glitchSrcLocArray[c] + c * srcDescPtr->strides.cStride), 0);
+                        __lasx_xvst((__m256i)p, (dstPtrTemp + c * srcDescPtr->strides.cStride), 0);
+                    }
+                    dstPtrTemp += 8;
+                }
 #endif
                 for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
                 {
@@ -379,7 +428,7 @@ RppStatus glitch_f32_f32_host_tensor(Rpp32f *srcPtr,
             {
                 Rpp32f* dstPtrTemp = dstPtrRow;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 2)
                 {
                     __m256 p;
@@ -388,6 +437,15 @@ RppStatus glitch_f32_f32_host_tensor(Rpp32f *srcPtr,
                     _mm256_storeu_ps(dstPtrTemp, p);
                     dstPtrTemp += 6;
                 }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += 2)
+                {
+                    __m256 p;
+                    compute_src_loc(dstLocRow, vectorLoopCount, glitchSrcLocArray, srcDescPtr, rgbOffsets, roi, batchCount, 3);
+                    rpp_simd_load(rpp_glitch_load6_f32pkd3_to_f32pkd3_avx, srcPtrChannel, glitchSrcLocArray, p);
+                    __lasx_xvst((__m256i)p, dstPtrTemp, 0);
+                    dstPtrTemp += 6;
+                }
 #endif
                 for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
                 {
@@ -559,7 +617,7 @@ RppStatus glitch_i8_i8_host_tensor(Rpp8s *srcPtr,
                 Rpp8s* dstRowPtrTempG = dstPtrRow + dstDescPtr->strides.cStride;
                 Rpp8s* dstRowPtrTempB = dstPtrRow + 2 * dstDescPtr->strides.cStride;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
                 {
                     __m256 p[3];
@@ -593,7 +651,7 @@ RppStatus glitch_i8_i8_host_tensor(Rpp8s *srcPtr,
             {
                 Rpp8s* dstPtrTemp = dstPtrRow;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 16)
                 {
                     __m256 p[6];
@@ -628,7 +686,7 @@ RppStatus glitch_i8_i8_host_tensor(Rpp8s *srcPtr,
             {
                 Rpp8s* dstPtrTemp = dstPtrRow;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     compute_src_loc(dstLocRow, vectorLoopCount, glitchSrcLocArray, srcDescPtr, rgbOffsets, roi, batchCount, 1);
@@ -640,6 +698,18 @@ RppStatus glitch_i8_i8_host_tensor(Rpp8s *srcPtr,
                     }
                     dstPtrTemp += 32;
                 }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
+                {
+                    compute_src_loc(dstLocRow, vectorLoopCount, glitchSrcLocArray, srcDescPtr, rgbOffsets, roi, batchCount, 1);
+                    for (int c = 0; c < 3; c++)
+                    {
+                        __m256i p;
+                        p = __lasx_xvld((__m256i *)(srcPtrChannel + (glitchSrcLocArray[c] + (c * srcDescPtr->strides.cStride))), 0);
+                        __lasx_xvst(p, (__m256i *)(dstPtrTemp + (c * srcDescPtr->strides.cStride)), 0);
+                    }
+                    dstPtrTemp += 32;
+                }
 #endif
                 for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
                 {
@@ -664,7 +734,7 @@ RppStatus glitch_i8_i8_host_tensor(Rpp8s *srcPtr,
             {
                 Rpp8s* dstPtrTemp = dstPtrRow;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 10)
                 {
                     __m256i p;
@@ -673,6 +743,15 @@ RppStatus glitch_i8_i8_host_tensor(Rpp8s *srcPtr,
                     _mm256_storeu_si256((__m256i *)(dstPtrTemp), p);
                     dstPtrTemp += 30;
                 }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += 10)
+                {
+                    __m256i p;
+                    compute_src_loc(dstLocRow, vectorLoopCount, glitchSrcLocArray, srcDescPtr, rgbOffsets, roi, batchCount, 3);
+                    rpp_simd_load(rpp_glitch_load30_i8pkd3_to_i8pkd3_avx, srcPtrChannel, glitchSrcLocArray, p);
+                    __lasx_xvst(p, (__m256i *)(dstPtrTemp), 0);
+                    dstPtrTemp += 30;
+                }
 #endif
                 for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
                 {
diff --git a/src/modules/cpu/kernel/gridmask.hpp b/src/modules/cpu/kernel/gridmask.hpp
index 87336c68..bcdec3bf 100644
--- a/src/modules/cpu/kernel/gridmask.hpp
+++ b/src/modules/cpu/kernel/gridmask.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 RppStatus gridmask_u8_u8_host_tensor(Rpp8u *srcPtr,
                                      RpptDescPtr srcDescPtr,
@@ -69,6 +74,15 @@ RppStatus gridmask_u8_u8_host_tensor(Rpp8u *srcPtr,
         translateVectorRatio.y = translateVector.y * tileWidthInv;
 
         __m128 pCosRatio, pSinRatio, pGridRatio, pColInit[4];
+#ifdef __loongarch_sx
+        pCosRatio = lsx_set1_f32(cosRatio);
+        pSinRatio = lsx_set1_f32(sinRatio);
+        pGridRatio = lsx_set1_f32(gridRatio);
+        pColInit[0] = lsx_setr_f32(0, 1, 2, 3);
+        pColInit[1] = lsx_setr_f32(4, 5, 6, 7);
+        pColInit[2] = lsx_setr_f32(8, 9, 10, 11);
+        pColInit[3] = lsx_setr_f32(12, 13, 14, 15);
+#else
         pCosRatio = _mm_set1_ps(cosRatio);
         pSinRatio = _mm_set1_ps(sinRatio);
         pGridRatio = _mm_set1_ps(gridRatio);
@@ -76,6 +90,7 @@ RppStatus gridmask_u8_u8_host_tensor(Rpp8u *srcPtr,
         pColInit[1] = _mm_setr_ps(4, 5, 6, 7);
         pColInit[2] = _mm_setr_ps(8, 9, 10, 11);
         pColInit[3] = _mm_setr_ps(12, 13, 14, 15);
+#endif
 
         // Gridmask with fused output-layout toggle (NHWC -> NCHW)
         if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
@@ -101,8 +116,13 @@ RppStatus gridmask_u8_u8_host_tensor(Rpp8u *srcPtr,
                 gridRowRatio.y = -translateVectorRatio.y + i * cosRatio;
 
                 __m128 pGridRowRatio[2], pCol[4];
+#ifdef __loongarch_sx
+                pGridRowRatio[0] = lsx_set1_f32(gridRowRatio.x);
+                pGridRowRatio[1] = lsx_set1_f32(gridRowRatio.y);
+#else
                 pGridRowRatio[0] = _mm_set1_ps(gridRowRatio.x);
                 pGridRowRatio[1] = _mm_set1_ps(gridRowRatio.y);
+#endif
                 pCol[0] = pColInit[0];
                 pCol[1] = pColInit[1];
                 pCol[2] = pColInit[2];
@@ -175,8 +195,13 @@ RppStatus gridmask_u8_u8_host_tensor(Rpp8u *srcPtr,
                 gridRowRatio.y = -translateVectorRatio.y + i * cosRatio;
 
                 __m128 pGridRowRatio[2], pCol[4];
+#ifdef __loongarch_sx
+                pGridRowRatio[0] = lsx_set1_f32(gridRowRatio.x);
+                pGridRowRatio[1] = lsx_set1_f32(gridRowRatio.y);
+#else
                 pGridRowRatio[0] = _mm_set1_ps(gridRowRatio.x);
                 pGridRowRatio[1] = _mm_set1_ps(gridRowRatio.y);
+#endif
                 pCol[0] = pColInit[0];
                 pCol[1] = pColInit[1];
                 pCol[2] = pColInit[2];
@@ -245,8 +270,13 @@ RppStatus gridmask_u8_u8_host_tensor(Rpp8u *srcPtr,
                 gridRowRatio.y = -translateVectorRatio.y + i * cosRatio;
 
                 __m128 pGridRowRatio[2], pCol[4];
+#ifdef __loongarch_sx
+                pGridRowRatio[0] = lsx_set1_f32(gridRowRatio.x);
+                pGridRowRatio[1] = lsx_set1_f32(gridRowRatio.y);
+#else
                 pGridRowRatio[0] = _mm_set1_ps(gridRowRatio.x);
                 pGridRowRatio[1] = _mm_set1_ps(gridRowRatio.y);
+#endif
                 pCol[0] = pColInit[0];
                 pCol[1] = pColInit[1];
                 pCol[2] = pColInit[2];
@@ -314,8 +344,13 @@ RppStatus gridmask_u8_u8_host_tensor(Rpp8u *srcPtr,
                 gridRowRatio.y = -translateVectorRatio.y + i * cosRatio;
 
                 __m128 pGridRowRatio[2], pCol[4];
+#ifdef __loongarch_sx
+                pGridRowRatio[0] = lsx_set1_f32(gridRowRatio.x);
+                pGridRowRatio[1] = lsx_set1_f32(gridRowRatio.y);
+#else
                 pGridRowRatio[0] = _mm_set1_ps(gridRowRatio.x);
                 pGridRowRatio[1] = _mm_set1_ps(gridRowRatio.y);
+#endif
                 pCol[0] = pColInit[0];
                 pCol[1] = pColInit[1];
                 pCol[2] = pColInit[2];
@@ -390,8 +425,13 @@ RppStatus gridmask_u8_u8_host_tensor(Rpp8u *srcPtr,
                 gridRowRatio.y = -translateVectorRatio.y + i * cosRatio;
 
                 __m128 pGridRowRatio[2], pCol[4];
+#ifdef __loongarch_sx
+                pGridRowRatio[0] = lsx_set1_f32(gridRowRatio.x);
+                pGridRowRatio[1] = lsx_set1_f32(gridRowRatio.y);
+#else
                 pGridRowRatio[0] = _mm_set1_ps(gridRowRatio.x);
                 pGridRowRatio[1] = _mm_set1_ps(gridRowRatio.y);
+#endif
                 pCol[0] = pColInit[0];
                 pCol[1] = pColInit[1];
                 pCol[2] = pColInit[2];
@@ -475,10 +515,17 @@ RppStatus gridmask_f32_f32_host_tensor(Rpp32f *srcPtr,
         translateVectorRatio.y = translateVector.y * tileWidthInv;
 
         __m128 pCosRatio, pSinRatio, pGridRatio, pColInit;
+#ifdef __loongarch_sx
+        pCosRatio = lsx_set1_f32(cosRatio);
+        pSinRatio = lsx_set1_f32(sinRatio);
+        pGridRatio = lsx_set1_f32(gridRatio);
+        pColInit = lsx_setr_f32(0, 1, 2, 3);
+#else
         pCosRatio = _mm_set1_ps(cosRatio);
         pSinRatio = _mm_set1_ps(sinRatio);
         pGridRatio = _mm_set1_ps(gridRatio);
         pColInit = _mm_setr_ps(0, 1, 2, 3);
+#endif
 
         // Gridmask with fused output-layout toggle (NHWC -> NCHW)
         if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
@@ -504,8 +551,13 @@ RppStatus gridmask_f32_f32_host_tensor(Rpp32f *srcPtr,
                 gridRowRatio.y = -translateVectorRatio.y + i * cosRatio;
 
                 __m128 pGridRowRatio[2], pCol;
+#ifdef __loongarch_sx
+                pGridRowRatio[0] = lsx_set1_f32(gridRowRatio.x);
+                pGridRowRatio[1] = lsx_set1_f32(gridRowRatio.y);
+#else
                 pGridRowRatio[0] = _mm_set1_ps(gridRowRatio.x);
                 pGridRowRatio[1] = _mm_set1_ps(gridRowRatio.y);
+#endif
                 pCol = pColInit;
 
                 int vectorLoopCount = 0;
@@ -575,8 +627,13 @@ RppStatus gridmask_f32_f32_host_tensor(Rpp32f *srcPtr,
                 gridRowRatio.y = -translateVectorRatio.y + i * cosRatio;
 
                 __m128 pGridRowRatio[2], pCol;
+#ifdef __loongarch_sx
+                pGridRowRatio[0] = lsx_set1_f32(gridRowRatio.x);
+                pGridRowRatio[1] = lsx_set1_f32(gridRowRatio.y);
+#else
                 pGridRowRatio[0] = _mm_set1_ps(gridRowRatio.x);
                 pGridRowRatio[1] = _mm_set1_ps(gridRowRatio.y);
+#endif
                 pCol = pColInit;
 
                 int vectorLoopCount = 0;
@@ -642,8 +699,13 @@ RppStatus gridmask_f32_f32_host_tensor(Rpp32f *srcPtr,
                 gridRowRatio.y = -translateVectorRatio.y + i * cosRatio;
 
                 __m128 pGridRowRatio[2], pCol;
+#ifdef __loongarch_sx
+                pGridRowRatio[0] = lsx_set1_f32(gridRowRatio.x);
+                pGridRowRatio[1] = lsx_set1_f32(gridRowRatio.y);
+#else
                 pGridRowRatio[0] = _mm_set1_ps(gridRowRatio.x);
                 pGridRowRatio[1] = _mm_set1_ps(gridRowRatio.y);
+#endif
                 pCol = pColInit;
 
                 int vectorLoopCount = 0;
@@ -708,8 +770,13 @@ RppStatus gridmask_f32_f32_host_tensor(Rpp32f *srcPtr,
                 gridRowRatio.y = -translateVectorRatio.y + i * cosRatio;
 
                 __m128 pGridRowRatio[2], pCol;
+#ifdef __loongarch_sx
+                pGridRowRatio[0] = lsx_set1_f32(gridRowRatio.x);
+                pGridRowRatio[1] = lsx_set1_f32(gridRowRatio.y);
+#else
                 pGridRowRatio[0] = _mm_set1_ps(gridRowRatio.x);
                 pGridRowRatio[1] = _mm_set1_ps(gridRowRatio.y);
+#endif
                 pCol = pColInit;
 
                 int vectorLoopCount = 0;
@@ -781,8 +848,13 @@ RppStatus gridmask_f32_f32_host_tensor(Rpp32f *srcPtr,
                 gridRowRatio.y = -translateVectorRatio.y + i * cosRatio;
 
                 __m128 pGridRowRatio[2], pCol;
+#ifdef __loongarch_sx
+                pGridRowRatio[0] = lsx_set1_f32(gridRowRatio.x);
+                pGridRowRatio[1] = lsx_set1_f32(gridRowRatio.y);
+#else
                 pGridRowRatio[0] = _mm_set1_ps(gridRowRatio.x);
                 pGridRowRatio[1] = _mm_set1_ps(gridRowRatio.y);
+#endif
                 pCol = pColInit;
 
                 int vectorLoopCount = 0;
@@ -863,10 +935,17 @@ RppStatus gridmask_f16_f16_host_tensor(Rpp16f *srcPtr,
         translateVectorRatio.y = translateVector.y * tileWidthInv;
 
         __m128 pCosRatio, pSinRatio, pGridRatio, pColInit;
+#ifdef __loongarch_sx
+        pCosRatio = lsx_set1_f32(cosRatio);
+        pSinRatio = lsx_set1_f32(sinRatio);
+        pGridRatio = lsx_set1_f32(gridRatio);
+        pColInit = lsx_setr_f32(0, 1, 2, 3);
+#else
         pCosRatio = _mm_set1_ps(cosRatio);
         pSinRatio = _mm_set1_ps(sinRatio);
         pGridRatio = _mm_set1_ps(gridRatio);
         pColInit = _mm_setr_ps(0, 1, 2, 3);
+#endif
 
         // Gridmask with fused output-layout toggle (NHWC -> NCHW)
         if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
@@ -892,8 +971,13 @@ RppStatus gridmask_f16_f16_host_tensor(Rpp16f *srcPtr,
                 gridRowRatio.y = -translateVectorRatio.y + i * cosRatio;
 
                 __m128 pGridRowRatio[2], pCol;
+#ifdef __loongarch_sx
+                pGridRowRatio[0] = lsx_set1_f32(gridRowRatio.x);
+                pGridRowRatio[1] = lsx_set1_f32(gridRowRatio.y);
+#else
                 pGridRowRatio[0] = _mm_set1_ps(gridRowRatio.x);
                 pGridRowRatio[1] = _mm_set1_ps(gridRowRatio.y);
+#endif
                 pCol = pColInit;
 
                 int vectorLoopCount = 0;
@@ -974,8 +1058,13 @@ RppStatus gridmask_f16_f16_host_tensor(Rpp16f *srcPtr,
                 gridRowRatio.y = -translateVectorRatio.y + i * cosRatio;
 
                 __m128 pGridRowRatio[2], pCol;
+#ifdef __loongarch_sx
+                pGridRowRatio[0] = lsx_set1_f32(gridRowRatio.x);
+                pGridRowRatio[1] = lsx_set1_f32(gridRowRatio.y);
+#else
                 pGridRowRatio[0] = _mm_set1_ps(gridRowRatio.x);
                 pGridRowRatio[1] = _mm_set1_ps(gridRowRatio.y);
+#endif
                 pCol = pColInit;
 
                 int vectorLoopCount = 0;
@@ -1052,8 +1141,13 @@ RppStatus gridmask_f16_f16_host_tensor(Rpp16f *srcPtr,
                 gridRowRatio.y = -translateVectorRatio.y + i * cosRatio;
 
                 __m128 pGridRowRatio[2], pCol;
+#ifdef __loongarch_sx
+                pGridRowRatio[0] = lsx_set1_f32(gridRowRatio.x);
+                pGridRowRatio[1] = lsx_set1_f32(gridRowRatio.y);
+#else
                 pGridRowRatio[0] = _mm_set1_ps(gridRowRatio.x);
                 pGridRowRatio[1] = _mm_set1_ps(gridRowRatio.y);
+#endif
                 pCol = pColInit;
 
                 int vectorLoopCount = 0;
@@ -1127,8 +1221,13 @@ RppStatus gridmask_f16_f16_host_tensor(Rpp16f *srcPtr,
                 gridRowRatio.y = -translateVectorRatio.y + i * cosRatio;
 
                 __m128 pGridRowRatio[2], pCol;
+#ifdef __loongarch_sx
+                pGridRowRatio[0] = lsx_set1_f32(gridRowRatio.x);
+                pGridRowRatio[1] = lsx_set1_f32(gridRowRatio.y);
+#else
                 pGridRowRatio[0] = _mm_set1_ps(gridRowRatio.x);
                 pGridRowRatio[1] = _mm_set1_ps(gridRowRatio.y);
+#endif
                 pCol = pColInit;
 
                 int vectorLoopCount = 0;
@@ -1213,8 +1312,13 @@ RppStatus gridmask_f16_f16_host_tensor(Rpp16f *srcPtr,
                 gridRowRatio.y = -translateVectorRatio.y + i * cosRatio;
 
                 __m128 pGridRowRatio[2], pCol;
+#ifdef __loongarch_sx
+                pGridRowRatio[0] = lsx_set1_f32(gridRowRatio.x);
+                pGridRowRatio[1] = lsx_set1_f32(gridRowRatio.y);
+#else
                 pGridRowRatio[0] = _mm_set1_ps(gridRowRatio.x);
                 pGridRowRatio[1] = _mm_set1_ps(gridRowRatio.y);
+#endif
                 pCol = pColInit;
 
                 int vectorLoopCount = 0;
@@ -1304,6 +1408,15 @@ RppStatus gridmask_i8_i8_host_tensor(Rpp8s *srcPtr,
         translateVectorRatio.y = translateVector.y * tileWidthInv;
 
         __m128 pCosRatio, pSinRatio, pGridRatio, pColInit[4];
+#ifdef __loongarch_sx
+        pCosRatio = lsx_set1_f32(cosRatio);
+        pSinRatio = lsx_set1_f32(sinRatio);
+        pGridRatio = lsx_set1_f32(gridRatio);
+        pColInit[0] = lsx_setr_f32(0, 1, 2, 3);
+        pColInit[1] = lsx_setr_f32(4, 5, 6, 7);
+        pColInit[2] = lsx_setr_f32(8, 9, 10, 11);
+        pColInit[3] = lsx_setr_f32(12, 13, 14, 15);
+#else
         pCosRatio = _mm_set1_ps(cosRatio);
         pSinRatio = _mm_set1_ps(sinRatio);
         pGridRatio = _mm_set1_ps(gridRatio);
@@ -1311,6 +1424,7 @@ RppStatus gridmask_i8_i8_host_tensor(Rpp8s *srcPtr,
         pColInit[1] = _mm_setr_ps(4, 5, 6, 7);
         pColInit[2] = _mm_setr_ps(8, 9, 10, 11);
         pColInit[3] = _mm_setr_ps(12, 13, 14, 15);
+#endif
 
         // Gridmask with fused output-layout toggle (NHWC -> NCHW)
         if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
@@ -1336,8 +1450,13 @@ RppStatus gridmask_i8_i8_host_tensor(Rpp8s *srcPtr,
                 gridRowRatio.y = -translateVectorRatio.y + i * cosRatio;
 
                 __m128 pGridRowRatio[2], pCol[4];
+#ifdef __loongarch_sx
+                pGridRowRatio[0] = lsx_set1_f32(gridRowRatio.x);
+                pGridRowRatio[1] = lsx_set1_f32(gridRowRatio.y);
+#else
                 pGridRowRatio[0] = _mm_set1_ps(gridRowRatio.x);
                 pGridRowRatio[1] = _mm_set1_ps(gridRowRatio.y);
+#endif
                 pCol[0] = pColInit[0];
                 pCol[1] = pColInit[1];
                 pCol[2] = pColInit[2];
@@ -1410,8 +1529,13 @@ RppStatus gridmask_i8_i8_host_tensor(Rpp8s *srcPtr,
                 gridRowRatio.y = -translateVectorRatio.y + i * cosRatio;
 
                 __m128 pGridRowRatio[2], pCol[4];
+#ifdef __loongarch_sx
+                pGridRowRatio[0] = lsx_set1_f32(gridRowRatio.x);
+                pGridRowRatio[1] = lsx_set1_f32(gridRowRatio.y);
+#else
                 pGridRowRatio[0] = _mm_set1_ps(gridRowRatio.x);
                 pGridRowRatio[1] = _mm_set1_ps(gridRowRatio.y);
+#endif
                 pCol[0] = pColInit[0];
                 pCol[1] = pColInit[1];
                 pCol[2] = pColInit[2];
@@ -1480,8 +1604,13 @@ RppStatus gridmask_i8_i8_host_tensor(Rpp8s *srcPtr,
                 gridRowRatio.y = -translateVectorRatio.y + i * cosRatio;
 
                 __m128 pGridRowRatio[2], pCol[4];
+#ifdef __loongarch_sx
+                pGridRowRatio[0] = lsx_set1_f32(gridRowRatio.x);
+                pGridRowRatio[1] = lsx_set1_f32(gridRowRatio.y);
+#else
                 pGridRowRatio[0] = _mm_set1_ps(gridRowRatio.x);
                 pGridRowRatio[1] = _mm_set1_ps(gridRowRatio.y);
+#endif
                 pCol[0] = pColInit[0];
                 pCol[1] = pColInit[1];
                 pCol[2] = pColInit[2];
@@ -1549,8 +1678,13 @@ RppStatus gridmask_i8_i8_host_tensor(Rpp8s *srcPtr,
                 gridRowRatio.y = -translateVectorRatio.y + i * cosRatio;
 
                 __m128 pGridRowRatio[2], pCol[4];
+#ifdef __loongarch_sx
+                pGridRowRatio[0] = lsx_set1_f32(gridRowRatio.x);
+                pGridRowRatio[1] = lsx_set1_f32(gridRowRatio.y);
+#else
                 pGridRowRatio[0] = _mm_set1_ps(gridRowRatio.x);
                 pGridRowRatio[1] = _mm_set1_ps(gridRowRatio.y);
+#endif
                 pCol[0] = pColInit[0];
                 pCol[1] = pColInit[1];
                 pCol[2] = pColInit[2];
@@ -1625,8 +1759,13 @@ RppStatus gridmask_i8_i8_host_tensor(Rpp8s *srcPtr,
                 gridRowRatio.y = -translateVectorRatio.y + i * cosRatio;
 
                 __m128 pGridRowRatio[2], pCol[4];
+#ifdef __loongarch_sx
+                pGridRowRatio[0] = lsx_set1_f32(gridRowRatio.x);
+                pGridRowRatio[1] = lsx_set1_f32(gridRowRatio.y);
+#else
                 pGridRowRatio[0] = _mm_set1_ps(gridRowRatio.x);
                 pGridRowRatio[1] = _mm_set1_ps(gridRowRatio.y);
+#endif
                 pCol[0] = pColInit[0];
                 pCol[1] = pColInit[1];
                 pCol[2] = pColInit[2];
diff --git a/src/modules/cpu/kernel/jitter.hpp b/src/modules/cpu/kernel/jitter.hpp
index ec717150..79b9e7bc 100644
--- a/src/modules/cpu/kernel/jitter.hpp
+++ b/src/modules/cpu/kernel/jitter.hpp
@@ -1,6 +1,11 @@
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 RppStatus jitter_u8_u8_host_tensor(Rpp8u *srcPtr,
                                    RpptDescPtr srcDescPtr,
@@ -43,6 +48,16 @@ RppStatus jitter_u8_u8_host_tensor(Rpp8u *srcPtr,
         RpptXorwowStateBoxMuller xorwowState;
         Rpp32s srcLocArray[8] = {0};
 
+#if defined(__loongarch_asx)
+        __m256i pxXorwowStateX[5], pxXorwowStateCounter;
+        rpp_host_rng_xorwow_state_offsetted_avx(xorwowInitialStatePtr, xorwowState, offset, pxXorwowStateX, &pxXorwowStateCounter);
+        __m256 pKernelSize = lasx_set1_f32(kernelSize);
+        __m256 pChannel = lasx_set1_f32(layoutParams.bufferMultiplier);
+        __m256 pHStride = lasx_set1_f32(srcDescPtr->strides.hStride);
+        __m256 pHeightLimit = lasx_set1_f32(heightLimit);
+        __m256 pWidthLimit = lasx_set1_f32(roi.xywhROI.roiWidth - 1);
+        __m256 pBound = lasx_set1_f32(bound);
+#else
         __m256i pxXorwowStateX[5], pxXorwowStateCounter;
         rpp_host_rng_xorwow_state_offsetted_avx(xorwowInitialStatePtr, xorwowState, offset, pxXorwowStateX, &pxXorwowStateCounter);
         __m256 pKernelSize = _mm256_set1_ps(kernelSize);
@@ -51,6 +66,7 @@ RppStatus jitter_u8_u8_host_tensor(Rpp8u *srcPtr,
         __m256 pHeightLimit = _mm256_set1_ps(heightLimit);
         __m256 pWidthLimit = _mm256_set1_ps(roi.xywhROI.roiWidth - 1);
         __m256 pBound = _mm256_set1_ps(bound);
+#endif
 
         // Jitter with fused output-layout toggle (NHWC -> NCHW)
         if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
@@ -67,11 +83,16 @@ RppStatus jitter_u8_u8_host_tensor(Rpp8u *srcPtr,
                 dstPtrTempG = dstPtrRowG;
                 dstPtrTempB = dstPtrRowB;
 
+#if defined(__loongarch_asx)
+                __m256 pRow = lasx_set1_f32(dstLocRow);
+                __m256 pCol = avx_pDstLocInit;
+#else
                 __m256 pRow = _mm256_set1_ps(dstLocRow);
                 __m256 pCol = avx_pDstLocInit;
+#endif
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256i pxRow;
@@ -83,6 +104,18 @@ RppStatus jitter_u8_u8_host_tensor(Rpp8u *srcPtr,
                     dstPtrTempB += vectorIncrementPerChannel;
                     pCol = _mm256_add_ps(avx_p8, pCol);
                 }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256i pxRow;
+                    compute_jitter_src_loc_avx(pxXorwowStateX, &pxXorwowStateCounter, pRow, pCol, pKernelSize, pBound, pHeightLimit, pWidthLimit, pHStride, pChannel, srcLocArray);
+                    rpp_resize_nn_extract_pkd3_avx(srcPtrChannel, srcLocArray, pxRow);
+                    rpp_simd_store(rpp_store24_u8pkd3_to_u8pln3_avx, dstPtrTempR, dstPtrTempG, dstPtrTempB, pxRow);
+                    dstPtrTempR += vectorIncrementPerChannel;
+                    dstPtrTempG += vectorIncrementPerChannel;
+                    dstPtrTempB += vectorIncrementPerChannel;
+                    pCol = __lasx_xvfadd_s(avx_p8, pCol);
+                }
 #endif
                 for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
                 {
@@ -113,11 +146,16 @@ RppStatus jitter_u8_u8_host_tensor(Rpp8u *srcPtr,
                 Rpp8u *dstPtrTemp;
                 dstPtrTemp = dstPtrRow;
 
+#if defined(__loongarch_asx)
+                __m256 pRow = lasx_set1_f32(dstLocRow);
+                __m256 pCol = avx_pDstLocInit;
+#else
                 __m256 pRow = _mm256_set1_ps(dstLocRow);
                 __m256 pCol = avx_pDstLocInit;
+#endif
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256i pxRow[3];
@@ -129,6 +167,18 @@ RppStatus jitter_u8_u8_host_tensor(Rpp8u *srcPtr,
                     dstPtrTemp += vectorIncrement;
                     pCol = _mm256_add_ps(avx_p8, pCol);
                 }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256i pxRow[3];
+                    compute_jitter_src_loc_avx(pxXorwowStateX, &pxXorwowStateCounter, pRow, pCol, pKernelSize, pBound, pHeightLimit, pWidthLimit, pHStride, pChannel, srcLocArray);
+                    rpp_resize_nn_extract_pln1_avx(srcPtrRowR, srcLocArray, pxRow[0]);
+                    rpp_resize_nn_extract_pln1_avx(srcPtrRowG, srcLocArray, pxRow[1]);
+                    rpp_resize_nn_extract_pln1_avx(srcPtrRowB, srcLocArray, pxRow[2]);
+                    rpp_simd_store(rpp_store24_u8pln3_to_u8pkd3_avx, dstPtrTemp, pxRow);
+                    dstPtrTemp += vectorIncrement;
+                    pCol = __lasx_xvfadd_s(avx_p8, pCol);
+                }
 #endif
                 for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
                 {
@@ -153,10 +203,15 @@ RppStatus jitter_u8_u8_host_tensor(Rpp8u *srcPtr,
             {
                 Rpp8u *dstPtrTemp;
                 dstPtrTemp = dstPtrRow;
+#if defined(__loongarch_asx)
+                __m256 pRow = lasx_set1_f32(dstLocRow);
+                __m256 pCol = avx_pDstLocInit;
+#else
                 __m256 pRow = _mm256_set1_ps(dstLocRow);
                 __m256 pCol = avx_pDstLocInit;
+#endif
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256i pxRow;
@@ -166,6 +221,16 @@ RppStatus jitter_u8_u8_host_tensor(Rpp8u *srcPtr,
                     dstPtrTemp += vectorIncrement;
                     pCol = _mm256_add_ps(avx_p8, pCol);
                 }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256i pxRow;
+                    compute_jitter_src_loc_avx(pxXorwowStateX, &pxXorwowStateCounter, pRow, pCol, pKernelSize, pBound, pHeightLimit, pWidthLimit, pHStride, pChannel, srcLocArray);
+                    rpp_resize_nn_extract_pkd3_avx(srcPtrRow, srcLocArray, pxRow);
+                    rpp_simd_store(rpp_store24_u8_to_u8_avx, dstPtrTemp, pxRow);
+                    dstPtrTemp += vectorIncrement;
+                    pCol = __lasx_xvfadd_s(avx_p8, pCol);
+                }
 #endif
                 for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
                 {
@@ -189,11 +254,16 @@ RppStatus jitter_u8_u8_host_tensor(Rpp8u *srcPtr,
                 Rpp8u *dstPtrTemp;
                 dstPtrTemp = dstPtrRow;
 
+#if defined(__loongarch_asx)
+                __m256 pRow = lasx_set1_f32(dstLocRow);
+                __m256 pCol = avx_pDstLocInit;
+#else
                 __m256 pRow = _mm256_set1_ps(dstLocRow);
                 __m256 pCol = avx_pDstLocInit;
+#endif
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     Rpp8u *dstPtrTempChn, *srcPtrTempChn;
@@ -211,6 +281,24 @@ RppStatus jitter_u8_u8_host_tensor(Rpp8u *srcPtr,
                     dstPtrTemp += vectorIncrementPerChannel;
                     pCol = _mm256_add_ps(avx_p8, pCol);
                 }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    Rpp8u *dstPtrTempChn, *srcPtrTempChn;
+                    srcPtrTempChn = srcPtrChannel;
+                    dstPtrTempChn = dstPtrTemp;
+                    compute_jitter_src_loc_avx(pxXorwowStateX, &pxXorwowStateCounter, pRow, pCol, pKernelSize, pBound, pHeightLimit, pWidthLimit, pHStride, pChannel, srcLocArray);
+                    for(int c = 0; c < srcDescPtr->c; c++)
+                    {
+                        __m256i pxRow;
+                        rpp_resize_nn_extract_pln1_avx(srcPtrTempChn, srcLocArray, pxRow);
+                        rpp_storeu_si64((__m128i *)(dstPtrTempChn), __lasx_cvt_256_128(pxRow));
+                        srcPtrTempChn += srcDescPtr->strides.cStride;
+                        dstPtrTempChn += dstDescPtr->strides.cStride;
+                    }
+                    dstPtrTemp += vectorIncrementPerChannel;
+                    pCol = __lasx_xvfadd_s(avx_p8, pCol);
+                }
 #endif
                 for (;vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
                 {
@@ -275,6 +363,16 @@ RppStatus jitter_f32_f32_host_tensor(Rpp32f *srcPtr,
         RpptXorwowStateBoxMuller xorwowState;
         Rpp32s srcLocArray[8] = {0}; 
 
+#if defined(__loongarch_asx)
+        __m256i pxXorwowStateX[5], pxXorwowStateCounter;
+        rpp_host_rng_xorwow_state_offsetted_avx(xorwowInitialStatePtr, xorwowState, offset, pxXorwowStateX, &pxXorwowStateCounter);
+        __m256 pKernelSize = lasx_set1_f32(kernelSize);
+        __m256 pChannel = lasx_set1_f32(layoutParams.bufferMultiplier);
+        __m256 pHStride = lasx_set1_f32(srcDescPtr->strides.hStride);
+        __m256 pHeightLimit = lasx_set1_f32(heightLimit);
+        __m256 pWidthLimit = lasx_set1_f32(roi.xywhROI.roiWidth-1);
+        __m256 pBound = lasx_set1_f32(bound);
+#else
         __m256i pxXorwowStateX[5], pxXorwowStateCounter;
         rpp_host_rng_xorwow_state_offsetted_avx(xorwowInitialStatePtr, xorwowState, offset, pxXorwowStateX, &pxXorwowStateCounter);
         __m256 pKernelSize = _mm256_set1_ps(kernelSize);
@@ -283,6 +381,7 @@ RppStatus jitter_f32_f32_host_tensor(Rpp32f *srcPtr,
         __m256 pHeightLimit = _mm256_set1_ps(heightLimit);
         __m256 pWidthLimit = _mm256_set1_ps(roi.xywhROI.roiWidth-1);
         __m256 pBound = _mm256_set1_ps(bound);
+#endif
 
 
         // Jitter with fused output-layout toggle (NHWC -> NCHW)
@@ -300,10 +399,15 @@ RppStatus jitter_f32_f32_host_tensor(Rpp32f *srcPtr,
                 dstPtrTempG = dstPtrRowG;
                 dstPtrTempB = dstPtrRowB;
                 
+#if defined(__loongarch_asx)
+                __m256 pRow = lasx_set1_f32(dstLocRow);
+                __m256 pCol = avx_pDstLocInit;
+#else
                 __m256 pRow = _mm256_set1_ps(dstLocRow);
                 __m256 pCol = avx_pDstLocInit;
+#endif
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pxRow[3];
@@ -315,6 +419,18 @@ RppStatus jitter_f32_f32_host_tensor(Rpp32f *srcPtr,
                     dstPtrTempB += vectorIncrementPerChannel;
                     pCol = _mm256_add_ps(avx_p8, pCol);
                 }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256 pxRow[3];
+                    compute_jitter_src_loc_avx(pxXorwowStateX, &pxXorwowStateCounter, pRow, pCol, pKernelSize, pBound, pHeightLimit, pWidthLimit, pHStride, pChannel, srcLocArray);
+                    rpp_simd_load(rpp_resize_nn_load_f32pkd3_to_f32pln3_avx, srcPtrChannel, srcLocArray, pxRow);
+                    rpp_simd_store(rpp_store24_f32pln3_to_f32pln3_avx, dstPtrTempR, dstPtrTempG, dstPtrTempB, pxRow);
+                    dstPtrTempR += vectorIncrementPerChannel;
+                    dstPtrTempG += vectorIncrementPerChannel;
+                    dstPtrTempB += vectorIncrementPerChannel;
+                    pCol = __lasx_xvfadd_s(avx_p8, pCol);
+                }
 #endif
                 for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
                 {
@@ -344,10 +460,15 @@ RppStatus jitter_f32_f32_host_tensor(Rpp32f *srcPtr,
             {
                 Rpp32f *dstPtrTemp;
                 dstPtrTemp = dstPtrRow;
+#if defined(__loongarch_asx)
+                __m256 pRow = lasx_set1_f32(dstLocRow);
+                __m256 pCol = avx_pDstLocInit;
+#else
                 __m256 pRow = _mm256_set1_ps(dstLocRow);
                 __m256 pCol = avx_pDstLocInit;
+#endif
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pxRow[4];
@@ -359,6 +480,18 @@ RppStatus jitter_f32_f32_host_tensor(Rpp32f *srcPtr,
                     dstPtrTemp += vectorIncrement;
                     pCol = _mm256_add_ps(avx_p8, pCol);
                 }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256 pxRow[4];
+                    compute_jitter_src_loc_avx(pxXorwowStateX, &pxXorwowStateCounter, pRow, pCol, pKernelSize, pBound, pHeightLimit, pWidthLimit, pHStride, pChannel, srcLocArray);
+                    rpp_simd_load(rpp_resize_nn_load_f32pln1_avx, srcPtrRowR, srcLocArray, pxRow[0]);
+                    rpp_simd_load(rpp_resize_nn_load_f32pln1_avx, srcPtrRowG, srcLocArray, pxRow[1]);
+                    rpp_simd_load(rpp_resize_nn_load_f32pln1_avx, srcPtrRowB, srcLocArray, pxRow[2]);
+                    rpp_simd_store(rpp_store24_f32pln3_to_f32pkd3_avx, dstPtrTemp, pxRow);
+                    dstPtrTemp += vectorIncrement;
+                    pCol = __lasx_xvfadd_s(avx_p8, pCol);
+                }
 #endif
                 for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
                 {
@@ -384,7 +517,7 @@ RppStatus jitter_f32_f32_host_tensor(Rpp32f *srcPtr,
                 Rpp32f *dstPtrTemp;
                 dstPtrTemp = dstPtrRow;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
                 {
                     Rpp32s loc;
@@ -407,11 +540,16 @@ RppStatus jitter_f32_f32_host_tensor(Rpp32f *srcPtr,
             {
                 Rpp32f *dstPtrTemp;
                 dstPtrTemp = dstPtrRow;
+#if defined(__loongarch_asx)
+                __m256 pRow = lasx_set1_f32(dstLocRow);
+                __m256 pCol = avx_pDstLocInit;
+#else
                 __m256 pRow = _mm256_set1_ps(dstLocRow);
                 __m256 pCol = avx_pDstLocInit;
+#endif
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     Rpp32f *srcPtrTempChn, *dstPtrTempChn;
@@ -430,6 +568,25 @@ RppStatus jitter_f32_f32_host_tensor(Rpp32f *srcPtr,
                     dstPtrTemp += vectorIncrementPerChannel;
                     pCol = _mm256_add_ps(avx_p8, pCol);
                 }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    Rpp32f *srcPtrTempChn, *dstPtrTempChn;
+                    srcPtrTempChn = srcPtrChannel;
+                    dstPtrTempChn = dstPtrTemp;
+                    compute_jitter_src_loc_avx(pxXorwowStateX, &pxXorwowStateCounter, pRow, pCol, pKernelSize, pBound, pHeightLimit, pWidthLimit, pHStride, pChannel, srcLocArray);
+
+                    for (int c = 0; c < dstDescPtr->c; c++)
+                    {
+                        __m256 pxRow;
+                        rpp_simd_load(rpp_resize_nn_load_f32pln1_avx, srcPtrTempChn, srcLocArray, pxRow);
+                        rpp_simd_store(rpp_store8_f32_to_f32_avx, dstPtrTempChn, &pxRow);
+                        srcPtrTempChn += srcDescPtr->strides.cStride;
+                        dstPtrTempChn += dstDescPtr->strides.cStride;
+                    }
+                    dstPtrTemp += vectorIncrementPerChannel;
+                    pCol = __lasx_xvfadd_s(avx_p8, pCol);
+                }
 #endif
                 for (;vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
                 {
@@ -494,6 +651,16 @@ RppStatus jitter_f16_f16_host_tensor(Rpp16f *srcPtr,
         RpptXorwowStateBoxMuller xorwowState;
         Rpp32s srcLocArray[8] = {0}; 
 
+#if defined(__loongarch_asx)
+        __m256i pxXorwowStateX[5], pxXorwowStateCounter;
+        rpp_host_rng_xorwow_state_offsetted_avx(xorwowInitialStatePtr, xorwowState, offset, pxXorwowStateX, &pxXorwowStateCounter);
+        __m256 pKernelSize = lasx_set1_f32(kernelSize);
+        __m256 pChannel = lasx_set1_f32(layoutParams.bufferMultiplier);
+        __m256 pHStride = lasx_set1_f32(srcDescPtr->strides.hStride);
+        __m256 pHeightLimit = lasx_set1_f32(heightLimit);
+        __m256 pWidthLimit = lasx_set1_f32(roi.xywhROI.roiWidth-1);
+        __m256 pBound = lasx_set1_f32(bound);
+#else
         __m256i pxXorwowStateX[5], pxXorwowStateCounter;
         rpp_host_rng_xorwow_state_offsetted_avx(xorwowInitialStatePtr, xorwowState, offset, pxXorwowStateX, &pxXorwowStateCounter);
         __m256 pKernelSize = _mm256_set1_ps(kernelSize);
@@ -502,7 +669,7 @@ RppStatus jitter_f16_f16_host_tensor(Rpp16f *srcPtr,
         __m256 pHeightLimit = _mm256_set1_ps(heightLimit);
         __m256 pWidthLimit = _mm256_set1_ps(roi.xywhROI.roiWidth-1);
         __m256 pBound = _mm256_set1_ps(bound);
-
+#endif
 
         // Jitter with fused output-layout toggle (NHWC -> NCHW)
         if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
@@ -519,10 +686,15 @@ RppStatus jitter_f16_f16_host_tensor(Rpp16f *srcPtr,
                 dstPtrTempG = dstPtrRowG;
                 dstPtrTempB = dstPtrRowB;
 
+#if defined(__loongarch_asx)
+                __m256 pRow = lasx_set1_f32(dstLocRow);
+                __m256 pCol = avx_pDstLocInit;
+#else
                 __m256 pRow = _mm256_set1_ps(dstLocRow);
                 __m256 pCol = avx_pDstLocInit;
+#endif
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     Rpp32f dstPtrTempR_ps[8], dstPtrTempG_ps[8], dstPtrTempB_ps[8];
@@ -541,6 +713,25 @@ RppStatus jitter_f16_f16_host_tensor(Rpp16f *srcPtr,
                     dstPtrTempB += vectorIncrementPerChannel;
                     pCol = _mm256_add_ps(avx_p8, pCol);
                 }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    Rpp32f dstPtrTempR_ps[8], dstPtrTempG_ps[8], dstPtrTempB_ps[8];
+                    __m256 pxRow[3];
+                    compute_jitter_src_loc_avx(pxXorwowStateX, &pxXorwowStateCounter, pRow, pCol, pKernelSize, pBound, pHeightLimit, pWidthLimit, pHStride, pChannel, srcLocArray);
+                    rpp_simd_load(rpp_resize_nn_load_f16pkd3_to_f32pln3_avx, srcPtrChannel, srcLocArray, pxRow);
+                    rpp_simd_store(rpp_store24_f32pln3_to_f32pln3_avx, dstPtrTempR_ps, dstPtrTempG_ps, dstPtrTempB_ps, pxRow);
+                    for(int cnt = 0; cnt < vectorIncrementPerChannel; cnt++)
+                    {
+                        dstPtrTempR[cnt] = (Rpp16f) dstPtrTempR_ps[cnt];
+                        dstPtrTempG[cnt] = (Rpp16f) dstPtrTempG_ps[cnt];
+                        dstPtrTempB[cnt] = (Rpp16f) dstPtrTempB_ps[cnt];
+                    }
+                    dstPtrTempR += vectorIncrementPerChannel;
+                    dstPtrTempG += vectorIncrementPerChannel;
+                    dstPtrTempB += vectorIncrementPerChannel;
+                    pCol = __lasx_xvfadd_s(avx_p8, pCol);
+                }
 #endif
                 for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
                 {
@@ -570,10 +761,15 @@ RppStatus jitter_f16_f16_host_tensor(Rpp16f *srcPtr,
             {
                 Rpp16f *dstPtrTemp;
                 dstPtrTemp = dstPtrRow;
+#if defined(__loongarch_asx)
+                __m256 pRow = lasx_set1_f32(dstLocRow);
+                __m256 pCol = avx_pDstLocInit;
+#else
                 __m256 pRow = _mm256_set1_ps(dstLocRow);
                 __m256 pCol = avx_pDstLocInit;
+#endif
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     Rpp32f dstPtrTemp_ps[25];
@@ -588,6 +784,21 @@ RppStatus jitter_f16_f16_host_tensor(Rpp16f *srcPtr,
                     dstPtrTemp += vectorIncrement;
                     pCol = _mm256_add_ps(avx_p8, pCol);
                 }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    Rpp32f dstPtrTemp_ps[25];
+                    __m256 pxRow[4];
+                    compute_jitter_src_loc_avx(pxXorwowStateX, &pxXorwowStateCounter, pRow, pCol, pKernelSize, pBound, pHeightLimit, pWidthLimit, pHStride, pChannel, srcLocArray);
+                    rpp_simd_load(rpp_resize_nn_load_f16pln1_avx, srcPtrRowR, srcLocArray, pxRow[0]);
+                    rpp_simd_load(rpp_resize_nn_load_f16pln1_avx, srcPtrRowG, srcLocArray, pxRow[1]);
+                    rpp_simd_load(rpp_resize_nn_load_f16pln1_avx, srcPtrRowB, srcLocArray, pxRow[2]);
+                    rpp_simd_store(rpp_store24_f32pln3_to_f32pkd3_avx, dstPtrTemp_ps, pxRow);
+                    for(int cnt = 0; cnt < vectorIncrement; cnt++)
+                        dstPtrTemp[cnt] = (Rpp16f) dstPtrTemp_ps[cnt];
+                    dstPtrTemp += vectorIncrement;
+                    pCol = __lasx_xvfadd_s(avx_p8, pCol);
+                }
 #endif
                 for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
                 {
@@ -613,7 +824,7 @@ RppStatus jitter_f16_f16_host_tensor(Rpp16f *srcPtr,
                 Rpp16f *dstPtrTemp;
                 dstPtrTemp = dstPtrRow;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
                 {
                     Rpp32f srcPtrTemp_ps[8], dstPtrTemp_ps[8];
@@ -648,11 +859,16 @@ RppStatus jitter_f16_f16_host_tensor(Rpp16f *srcPtr,
             {
                 Rpp16f *dstPtrTemp;
                 dstPtrTemp = dstPtrRow;
+#if defined(__loongarch_asx)
+                __m256 pRow = lasx_set1_f32(dstLocRow);
+                __m256 pCol = avx_pDstLocInit;
+#else
                 __m256 pRow = _mm256_set1_ps(dstLocRow);
                 __m256 pCol = avx_pDstLocInit;
+#endif
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     Rpp16f *srcPtrTempChn, *dstPtrTempChn;
@@ -676,6 +892,30 @@ RppStatus jitter_f16_f16_host_tensor(Rpp16f *srcPtr,
                     dstPtrTemp += vectorIncrementPerChannel;
                     pCol = _mm256_add_ps(avx_p8, pCol);
                 }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    Rpp16f *srcPtrTempChn, *dstPtrTempChn;
+                    srcPtrTempChn = srcPtrChannel;
+                    dstPtrTempChn = dstPtrTemp;
+                    compute_jitter_src_loc_avx(pxXorwowStateX, &pxXorwowStateCounter, pRow, pCol, pKernelSize, pBound, pHeightLimit, pWidthLimit, pHStride, pChannel, srcLocArray);
+
+                    for (int c = 0; c < dstDescPtr->c; c++)
+                    {
+                        Rpp32f dstPtrTemp_ps[8];
+                        __m256 pxRow;
+                        rpp_simd_load(rpp_resize_nn_load_f16pln1_avx, srcPtrTempChn, srcLocArray, pxRow);
+                        rpp_simd_store(rpp_store8_f32_to_f32_avx, dstPtrTemp_ps, &pxRow);
+                        for(int cnt = 0; cnt < vectorIncrementPerChannel; cnt++)
+                        {
+                            dstPtrTempChn[cnt] = (Rpp16f) dstPtrTemp_ps[cnt];
+                        }
+                        srcPtrTempChn += srcDescPtr->strides.cStride;
+                        dstPtrTempChn += dstDescPtr->strides.cStride;
+                    }
+                    dstPtrTemp += vectorIncrementPerChannel;
+                    pCol = __lasx_xvfadd_s(avx_p8, pCol);
+                }
 #endif
                 for (;vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
                 {
@@ -740,6 +980,16 @@ RppStatus jitter_i8_i8_host_tensor(Rpp8s *srcPtr,
         RpptXorwowStateBoxMuller xorwowState;
         Rpp32s srcLocArray[8] = {0};
 
+#if defined(__loongarch_asx)
+        __m256i pxXorwowStateX[5], pxXorwowStateCounter;
+        rpp_host_rng_xorwow_state_offsetted_avx(xorwowInitialStatePtr, xorwowState, offset, pxXorwowStateX, &pxXorwowStateCounter);
+        __m256 pKernelSize = lasx_set1_f32(kernelSize);
+        __m256 pChannel = lasx_set1_f32(layoutParams.bufferMultiplier);
+        __m256 pHStride = lasx_set1_f32(srcDescPtr->strides.hStride);
+        __m256 pHeightLimit = lasx_set1_f32(heightLimit);
+        __m256 pWidthLimit = lasx_set1_f32(roi.xywhROI.roiWidth-1);
+        __m256 pBound = lasx_set1_f32(bound);
+#else
         __m256i pxXorwowStateX[5], pxXorwowStateCounter;
         rpp_host_rng_xorwow_state_offsetted_avx(xorwowInitialStatePtr, xorwowState, offset, pxXorwowStateX, &pxXorwowStateCounter);
         __m256 pKernelSize = _mm256_set1_ps(kernelSize);
@@ -748,6 +998,7 @@ RppStatus jitter_i8_i8_host_tensor(Rpp8s *srcPtr,
         __m256 pHeightLimit = _mm256_set1_ps(heightLimit);
         __m256 pWidthLimit = _mm256_set1_ps(roi.xywhROI.roiWidth-1);
         __m256 pBound = _mm256_set1_ps(bound);
+#endif
 
         // Jitter with fused output-layout toggle (NHWC -> NCHW)
         if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
@@ -764,10 +1015,15 @@ RppStatus jitter_i8_i8_host_tensor(Rpp8s *srcPtr,
                 dstPtrTempG = dstPtrRowG;
                 dstPtrTempB = dstPtrRowB;
                 
+#if defined(__loongarch_asx)
+                __m256 pRow = lasx_set1_f32(dstLocRow);
+                __m256 pCol = avx_pDstLocInit;
+#else
                 __m256 pRow = _mm256_set1_ps(dstLocRow);
                 __m256 pCol = avx_pDstLocInit;
+#endif
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256i pxRow;
@@ -779,6 +1035,18 @@ RppStatus jitter_i8_i8_host_tensor(Rpp8s *srcPtr,
                     dstPtrTempB += vectorIncrementPerChannel;
                     pCol = _mm256_add_ps(avx_p8, pCol);
                 }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256i pxRow;
+                    compute_jitter_src_loc_avx(pxXorwowStateX, &pxXorwowStateCounter, pRow, pCol, pKernelSize, pBound, pHeightLimit, pWidthLimit, pHStride, pChannel, srcLocArray);
+                    rpp_resize_nn_extract_pkd3_avx(srcPtrChannel, srcLocArray, pxRow);
+                    rpp_simd_store(rpp_store24_i8pkd3_to_i8pln3_avx, dstPtrTempR, dstPtrTempG, dstPtrTempB, pxRow);
+                    dstPtrTempR += vectorIncrementPerChannel;
+                    dstPtrTempG += vectorIncrementPerChannel;
+                    dstPtrTempB += vectorIncrementPerChannel;
+                    pCol = __lasx_xvfadd_s(avx_p8, pCol);
+                }
 #endif
                 for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
                 {
@@ -809,10 +1077,15 @@ RppStatus jitter_i8_i8_host_tensor(Rpp8s *srcPtr,
                 Rpp8s *dstPtrTemp;
                 dstPtrTemp = dstPtrRow;
 
+#if defined(__loongarch_asx)
+                __m256 pRow = lasx_set1_f32(dstLocRow);
+                __m256 pCol = avx_pDstLocInit;
+#else
                 __m256 pRow = _mm256_set1_ps(dstLocRow);
                 __m256 pCol = avx_pDstLocInit;
+#endif
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256i pxRow[3];
@@ -824,6 +1097,18 @@ RppStatus jitter_i8_i8_host_tensor(Rpp8s *srcPtr,
                     dstPtrTemp += vectorIncrement;
                     pCol = _mm256_add_ps(avx_p8, pCol);
                 }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256i pxRow[3];
+                    compute_jitter_src_loc_avx(pxXorwowStateX, &pxXorwowStateCounter, pRow, pCol, pKernelSize, pBound, pHeightLimit, pWidthLimit, pHStride, pChannel, srcLocArray);
+                    rpp_resize_nn_extract_pln1_avx(srcPtrRowR, srcLocArray, pxRow[0]);
+                    rpp_resize_nn_extract_pln1_avx(srcPtrRowG, srcLocArray, pxRow[1]);
+                    rpp_resize_nn_extract_pln1_avx(srcPtrRowB, srcLocArray, pxRow[2]);
+                    rpp_simd_store(rpp_store24_i8pln3_to_i8pkd3_avx, dstPtrTemp, pxRow);
+                    dstPtrTemp += vectorIncrement;
+                    pCol = __lasx_xvfadd_s(avx_p8, pCol);
+                }
 #endif
                 for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
                 {
@@ -849,10 +1134,15 @@ RppStatus jitter_i8_i8_host_tensor(Rpp8s *srcPtr,
                 Rpp8s *dstPtrTemp;
                 dstPtrTemp = dstPtrRow;
 
+#if defined(__loongarch_asx)
+                __m256 pRow = lasx_set1_f32(dstLocRow);
+                __m256 pCol = avx_pDstLocInit;
+#else
                 __m256 pRow = _mm256_set1_ps(dstLocRow);
                 __m256 pCol = avx_pDstLocInit;
+#endif
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256i pxRow;
@@ -862,6 +1152,16 @@ RppStatus jitter_i8_i8_host_tensor(Rpp8s *srcPtr,
                     dstPtrTemp += vectorIncrement;
                     pCol = _mm256_add_ps(avx_p8, pCol);
                 }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256i pxRow;
+                    compute_jitter_src_loc_avx(pxXorwowStateX, &pxXorwowStateCounter, pRow, pCol, pKernelSize, pBound, pHeightLimit, pWidthLimit, pHStride, pChannel, srcLocArray);
+                    rpp_resize_nn_extract_pkd3_avx(srcPtrRow, srcLocArray, pxRow);
+                    rpp_simd_store(rpp_store24_i8_to_i8_avx, dstPtrTemp, pxRow);
+                    dstPtrTemp += vectorIncrement;
+                    pCol = __lasx_xvfadd_s(avx_p8, pCol);
+                }
 #endif
                 for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
                 {
@@ -884,10 +1184,15 @@ RppStatus jitter_i8_i8_host_tensor(Rpp8s *srcPtr,
                 Rpp8s *dstPtrTemp;
                 dstPtrTemp = dstPtrRow;
 
+#if defined(__loongarch_asx)
+                __m256 pRow = lasx_set1_f32(dstLocRow);
+                __m256 pCol = avx_pDstLocInit;
+#else
                 __m256 pRow = _mm256_set1_ps(dstLocRow);
                 __m256 pCol = avx_pDstLocInit;
+#endif
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     Rpp8s *dstPtrTempChn, *srcPtrTempChn;
@@ -905,6 +1210,24 @@ RppStatus jitter_i8_i8_host_tensor(Rpp8s *srcPtr,
                     dstPtrTemp += vectorIncrementPerChannel;
                     pCol = _mm256_add_ps(avx_p8, pCol);
                 }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    Rpp8s *dstPtrTempChn, *srcPtrTempChn;
+                    srcPtrTempChn = srcPtrChannel;
+                    dstPtrTempChn = dstPtrTemp;
+                    compute_jitter_src_loc_avx(pxXorwowStateX, &pxXorwowStateCounter, pRow, pCol, pKernelSize, pBound, pHeightLimit, pWidthLimit, pHStride, pChannel, srcLocArray);
+                    for(int c = 0; c < srcDescPtr->c; c++)
+                    {
+                        __m256i pxRow;
+                        rpp_resize_nn_extract_pln1_avx(srcPtrTempChn, srcLocArray, pxRow);
+                        rpp_storeu_si64((__m128i *)(dstPtrTempChn), __lasx_cvt_256_128(pxRow));
+                        srcPtrTempChn += srcDescPtr->strides.cStride;
+                        dstPtrTempChn += dstDescPtr->strides.cStride;
+                    }
+                    dstPtrTemp += vectorIncrementPerChannel;
+                    pCol = __lasx_xvfadd_s(avx_p8, pCol);
+                }
 #endif
                 for (;vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
                 {
diff --git a/src/modules/cpu/kernel/lens_correction.hpp b/src/modules/cpu/kernel/lens_correction.hpp
index 1632568a..9cdcbcaa 100644
--- a/src/modules/cpu/kernel/lens_correction.hpp
+++ b/src/modules/cpu/kernel/lens_correction.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 #include <omp.h>
 
 // Compute Inverse matrix (3x3)
@@ -81,6 +86,86 @@ inline void compute_lens_correction_remap_tables_host_tensor(RpptDescPtr srcDesc
         Rpp32f rCoeff[6] = { distortionCoeffs[0], distortionCoeffs[1], distortionCoeffs[4], distortionCoeffs[5], distortionCoeffs[6], distortionCoeffs[7] };
         Rpp32f tCoeff[2] = { distortionCoeffs[2], distortionCoeffs[3] };
 
+#ifdef __loongarch_asx
+        __m256 pRCoeff[6], pTCoeff[2];
+        pRCoeff[0] = lasx_set1_f32(rCoeff[0]);
+        pRCoeff[1] = lasx_set1_f32(rCoeff[1]);
+        pRCoeff[2] = lasx_set1_f32(rCoeff[2]);
+        pRCoeff[3] = lasx_set1_f32(rCoeff[3]);
+        pRCoeff[4] = lasx_set1_f32(rCoeff[4]);
+        pRCoeff[5] = lasx_set1_f32(rCoeff[5]);
+        pTCoeff[0] = lasx_set1_f32(tCoeff[0]);
+        pTCoeff[1] = lasx_set1_f32(tCoeff[1]);
+
+        Rpp32f u0 = cameraMatrix[2],  v0 = cameraMatrix[5];
+        Rpp32f fx = cameraMatrix[0],  fy = cameraMatrix[4];
+        __m256 pFx, pFy, pU0, pV0;
+        pFx = lasx_set1_f32(fx);
+        pFy = lasx_set1_f32(fy);
+        pU0 = lasx_set1_f32(u0);
+        pV0 = lasx_set1_f32(v0);
+
+        __m256 pInvMat0, pInvMat3, pInvMat6;
+        pInvMat0 = lasx_set1_f32(invMat[0]);
+        pInvMat3 = lasx_set1_f32(invMat[3]);
+        pInvMat6 = lasx_set1_f32(invMat[6]);
+
+        __m256 pXCameraInit, pYCameraInit, pZCameraInit;
+        __m256 pXCameraIncrement, pYCameraIncrement, pZCameraIncrement;
+        pXCameraInit = __lasx_xvfmul_s(avx_pDstLocInit, pInvMat0);
+        pYCameraInit = __lasx_xvfmul_s(avx_pDstLocInit, pInvMat3);
+        pZCameraInit = __lasx_xvfmul_s(avx_pDstLocInit, pInvMat6);
+        pXCameraIncrement = __lasx_xvfmul_s(pInvMat0, avx_p8);
+        pYCameraIncrement = __lasx_xvfmul_s(pInvMat3, avx_p8);
+        pZCameraIncrement = __lasx_xvfmul_s(pInvMat6, avx_p8);
+        for(int i = 0; i < height; i++)
+        {
+            Rpp32f *rowRemapTableRow = rowRemapTableTemp + i * tableDescPtr->strides.hStride;
+            Rpp32f *colRemapTableRow = colRemapTableTemp + i * tableDescPtr->strides.hStride;
+            Rpp32f xCamera = i * invMat[1] + invMat[2];
+            Rpp32f yCamera = i * invMat[4] + invMat[5];
+            Rpp32f zCamera = i * invMat[7] + invMat[8];
+            __m256 pXCamera = __lasx_xvfadd_s(lasx_set1_f32(xCamera), pXCameraInit);
+            __m256 pYCamera = __lasx_xvfadd_s(lasx_set1_f32(yCamera), pYCameraInit);
+            __m256 pZCamera = __lasx_xvfadd_s(lasx_set1_f32(zCamera), pZCameraInit);
+            int vectorLoopCount = 0;
+            for(; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
+            {
+                // float z = 1./zCamera, x = xCamera*z, y = yCamera*z;
+                __m256 pZ = __lasx_xvfdiv_s(avx_p1, pZCamera);
+                __m256 pX = __lasx_xvfmul_s(pXCamera, pZ);
+                __m256 pY = __lasx_xvfmul_s(pYCamera, pZ);
+
+                // float xSquare = x*x, ySquare = y*y, r2 = xSquare + ySquare;
+                __m256 pXSquare = __lasx_xvfmul_s(pX, pX);
+                __m256 pYSquare = __lasx_xvfmul_s(pY, pY);
+                __m256 pR2 = __lasx_xvfadd_s(pXSquare, pYSquare);
+
+                // float xyMul2 = 2*x*y;
+                __m256 p2xy = __lasx_xvfmul_s(avx_p2, __lasx_xvfmul_s(pX, pY));
+
+                // float kr = std::fmaf(std::fmaf(std::fmaf(rCoeff[2], r2, rCoeff[1]), r2, rCoeff[0]), r2, 1) / std::fmaf(std::fmaf(std::fmaf(rCoeff[5], r2, rCoeff[4]), r2, rCoeff[3]), r2, 1);
+                __m256 pNum = __lasx_xvfmadd_s(__lasx_xvfmadd_s(__lasx_xvfmadd_s(pRCoeff[2], pR2, pRCoeff[1]), pR2, pRCoeff[0]), pR2, avx_p1);
+                __m256 pDen = __lasx_xvfmadd_s(__lasx_xvfmadd_s(__lasx_xvfmadd_s(pRCoeff[5], pR2, pRCoeff[4]), pR2, pRCoeff[3]), pR2, avx_p1);
+                __m256 pKR = __lasx_xvfdiv_s(pNum, pDen);
+
+                // float colLoc = std::fmaf(fx, (std::fmaf(tCoeff[1], (std::fmaf(2, xSquare, r2)), std::fmaf(x, kr, (tCoeff[0] * xyMul2)))), u0);
+                __m256 pColLoc = __lasx_xvfmadd_s(pFx, __lasx_xvfmadd_s(pTCoeff[1], __lasx_xvfmadd_s(avx_p2, pXSquare, pR2), __lasx_xvfmadd_s(pX, pKR,  __lasx_xvfmul_s(pTCoeff[0], p2xy))), pU0);
+
+                // float rowLoc = std::fmaf(fy, (std::fmaf(tCoeff[0], (std::fmaf(2, ySquare, r2)), std::fmaf(y, kr, (tCoeff[1] * xyMul2)))), v0);
+                __m256 pRowLoc = __lasx_xvfmadd_s(pFy, __lasx_xvfmadd_s(pTCoeff[0], __lasx_xvfmadd_s(avx_p2, pYSquare, pR2), __lasx_xvfmadd_s(pY, pKR,  __lasx_xvfmul_s(pTCoeff[1], p2xy))), pV0);
+
+                __lasx_xvst((__m256i)pRowLoc, rowRemapTableRow, 0);
+                __lasx_xvst((__m256i)pColLoc, colRemapTableRow, 0);
+                rowRemapTableRow += vectorIncrement;
+                colRemapTableRow += vectorIncrement;
+
+                // xCamera += invMat[0], yCamera += invMat[3], zCamera += invMat[6]
+                pXCamera = __lasx_xvfadd_s(pXCamera, pXCameraIncrement);
+                pYCamera = __lasx_xvfadd_s(pYCamera, pYCameraIncrement);
+                pZCamera = __lasx_xvfadd_s(pZCamera, pZCameraIncrement);
+            }
+#else
         __m256 pRCoeff[6], pTCoeff[2];
         pRCoeff[0] = _mm256_set1_ps(rCoeff[0]);
         pRCoeff[1] = _mm256_set1_ps(rCoeff[1]);
@@ -159,6 +244,7 @@ inline void compute_lens_correction_remap_tables_host_tensor(RpptDescPtr srcDesc
                 pYCamera = _mm256_add_ps(pYCamera, pYCameraIncrement);
                 pZCamera = _mm256_add_ps(pZCamera, pZCameraIncrement);
             }
+#endif
             for(; vectorLoopCount < width; vectorLoopCount++)
             {
                 Rpp32f z = 1./zCamera, x = xCamera * z, y = yCamera * z;
diff --git a/src/modules/cpu/kernel/log.hpp b/src/modules/cpu/kernel/log.hpp
index 75153394..7eec78e1 100644
--- a/src/modules/cpu/kernel/log.hpp
+++ b/src/modules/cpu/kernel/log.hpp
@@ -23,7 +23,11 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_common.hpp"
+#endif
 
 // 1 pixel log helper functions
 // NOTE: log(0) leads to undefined thus using nextafter() to avoid this result
@@ -80,7 +84,7 @@ RppStatus log_generic_host_tensor(Rpp8u *srcPtr,
         {
             alignedLength = length[0] & ~15;
             int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
             {
                 __m256 p[2];
@@ -108,7 +112,7 @@ RppStatus log_generic_host_tensor(Rpp8u *srcPtr,
                 Rpp32f *dstPtrTemp = dstPtr1;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256 p[2];
@@ -144,7 +148,7 @@ RppStatus log_generic_host_tensor(Rpp8u *srcPtr,
                     Rpp32f *dstPtrTemp = dstPtrRow;
 
                     int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                     {
                         __m256 p[2];
@@ -206,7 +210,7 @@ RppStatus log_generic_host_tensor(Rpp8s *srcPtr,
         {
             alignedLength = length[0] & ~15;
             int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
             {
                 __m256 p[2];
@@ -234,7 +238,7 @@ RppStatus log_generic_host_tensor(Rpp8s *srcPtr,
                 Rpp32f *dstPtrTemp = dstPtr1;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256 p[2];
@@ -270,7 +274,7 @@ RppStatus log_generic_host_tensor(Rpp8s *srcPtr,
                     Rpp32f *dstPtrTemp = dstPtrRow;
 
                     int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                     {
                         __m256 p[2];
@@ -332,7 +336,7 @@ RppStatus log_generic_host_tensor(Rpp32f *srcPtr,
         {
             alignedLength = length[0] & ~15;
             int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
             {
                 __m256 p[2];
@@ -360,7 +364,7 @@ RppStatus log_generic_host_tensor(Rpp32f *srcPtr,
                 Rpp32f *dstPtrTemp = dstPtr1;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256 p[2];
@@ -396,7 +400,7 @@ RppStatus log_generic_host_tensor(Rpp32f *srcPtr,
                     Rpp32f *dstPtrTemp = dstPtrRow;
 
                     int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                     {
                         __m256 p[2];
@@ -457,7 +461,7 @@ RppStatus log_generic_host_tensor(Rpp16f *srcPtr,
         if (nDim == 1)
         {
             int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
             {
                 Rpp32f srcPtrTemp_ps[16];
@@ -488,7 +492,7 @@ RppStatus log_generic_host_tensor(Rpp16f *srcPtr,
                 Rpp16f *dstPtrTemp = dstPtr1;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     Rpp32f srcPtrTemp_ps[16];
@@ -527,7 +531,7 @@ RppStatus log_generic_host_tensor(Rpp16f *srcPtr,
                     Rpp16f *dstPtrTemp = dstPtrRow;
 
                     int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                     {
                         Rpp32f srcPtrTemp_ps[16];
diff --git a/src/modules/cpu/kernel/lut.hpp b/src/modules/cpu/kernel/lut.hpp
index 352a1e02..7606b845 100644
--- a/src/modules/cpu/kernel/lut.hpp
+++ b/src/modules/cpu/kernel/lut.hpp
@@ -23,7 +23,11 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_common.hpp"
+#endif
 
 RppStatus lut_u8_u8_host_tensor(Rpp8u *srcPtr,
                                 RpptDescPtr srcDescPtr,
diff --git a/src/modules/cpu/kernel/magnitude.hpp b/src/modules/cpu/kernel/magnitude.hpp
index 6eaf4f23..81ebdda9 100644
--- a/src/modules/cpu/kernel/magnitude.hpp
+++ b/src/modules/cpu/kernel/magnitude.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 RppStatus magnitude_u8_u8_host_tensor(Rpp8u *srcPtr1,
                                       Rpp8u *srcPtr2,
@@ -59,7 +64,7 @@ RppStatus magnitude_u8_u8_host_tensor(Rpp8u *srcPtr1,
         srcPtr2Channel = srcPtr2Image + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
         dstPtrChannel = dstPtrImage;
 
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
         Rpp32u alignedLength = (bufferLength / 48) * 48;
         Rpp32u vectorIncrement = 48;
         Rpp32u vectorIncrementPerChannel = 16;
@@ -85,7 +90,7 @@ RppStatus magnitude_u8_u8_host_tensor(Rpp8u *srcPtr1,
                 dstPtrTempB = dstPtrRowB;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256 p1[6], p2[6];
@@ -100,6 +105,27 @@ RppStatus magnitude_u8_u8_host_tensor(Rpp8u *srcPtr1,
                     p1[5] = _mm256_sqrt_ps(_mm256_fmadd_ps(p1[5], p1[5], _mm256_mul_ps(p2[5], p2[5])));    // magnitude computation
                     rpp_simd_store(rpp_store48_f32pln3_to_u8pln3_avx, dstPtrTempR, dstPtrTempG, dstPtrTempB, p1);    // simd stores
 
+                    srcPtr1Temp += vectorIncrement;
+                    srcPtr2Temp += vectorIncrement;
+                    dstPtrTempR += vectorIncrementPerChannel;
+                    dstPtrTempG += vectorIncrementPerChannel;
+                    dstPtrTempB += vectorIncrementPerChannel;
+                }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
+                {
+                    __m256 p1[6], p2[6];
+
+                    rpp_simd_load(rpp_load48_u8pkd3_to_f32pln3_avx, srcPtr1Temp, p1);    // simd loads
+                    rpp_simd_load(rpp_load48_u8pkd3_to_f32pln3_avx, srcPtr2Temp, p2);    // simd loads
+                    p1[0] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[0], p1[0], __lasx_xvfmul_s(p2[0], p2[0])));    // magnitude computation
+                    p1[1] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[1], p1[1], __lasx_xvfmul_s(p2[1], p2[1])));    // magnitude computation
+                    p1[2] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[2], p1[2], __lasx_xvfmul_s(p2[2], p2[2])));    // magnitude computation
+                    p1[3] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[3], p1[3], __lasx_xvfmul_s(p2[3], p2[3])));    // magnitude computation
+                    p1[4] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[4], p1[4], __lasx_xvfmul_s(p2[4], p2[4])));    // magnitude computation
+                    p1[5] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[5], p1[5], __lasx_xvfmul_s(p2[5], p2[5])));    // magnitude computation
+                    rpp_simd_store(rpp_store48_f32pln3_to_u8pln3_avx, dstPtrTempR, dstPtrTempG, dstPtrTempB, p1);    // simd stores
+
                     srcPtr1Temp += vectorIncrement;
                     srcPtr2Temp += vectorIncrement;
                     dstPtrTempR += vectorIncrementPerChannel;
@@ -155,7 +181,7 @@ RppStatus magnitude_u8_u8_host_tensor(Rpp8u *srcPtr1,
                 dstPtrTemp = dstPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 p1[6], p2[6];
@@ -170,6 +196,29 @@ RppStatus magnitude_u8_u8_host_tensor(Rpp8u *srcPtr1,
                     p1[5] = _mm256_sqrt_ps(_mm256_fmadd_ps(p1[5], p1[5], _mm256_mul_ps(p2[5], p2[5])));    // magnitude computation
                     rpp_simd_store(rpp_store48_f32pln3_to_u8pkd3_avx, dstPtrTemp, p1);    // simd stores
 
+                    srcPtr1TempR += vectorIncrementPerChannel;
+                    srcPtr1TempG += vectorIncrementPerChannel;
+                    srcPtr1TempB += vectorIncrementPerChannel;
+                    srcPtr2TempR += vectorIncrementPerChannel;
+                    srcPtr2TempG += vectorIncrementPerChannel;
+                    srcPtr2TempB += vectorIncrementPerChannel;
+                    dstPtrTemp += vectorIncrement;
+                }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256 p1[6], p2[6];
+
+                    rpp_simd_load(rpp_load48_u8pln3_to_f32pln3_avx, srcPtr1TempR, srcPtr1TempG, srcPtr1TempB, p1);    // simd loads
+                    rpp_simd_load(rpp_load48_u8pln3_to_f32pln3_avx, srcPtr2TempR, srcPtr2TempG, srcPtr2TempB, p2);    // simd loads
+                    p1[0] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[0], p1[0], __lasx_xvfmul_s(p2[0], p2[0])));    // magnitude computation
+                    p1[1] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[1], p1[1], __lasx_xvfmul_s(p2[1], p2[1])));    // magnitude computation
+                    p1[2] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[2], p1[2], __lasx_xvfmul_s(p2[2], p2[2])));    // magnitude computation
+                    p1[3] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[3], p1[3], __lasx_xvfmul_s(p2[3], p2[3])));    // magnitude computation
+                    p1[4] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[4], p1[4], __lasx_xvfmul_s(p2[4], p2[4])));    // magnitude computation
+                    p1[5] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[5], p1[5], __lasx_xvfmul_s(p2[5], p2[5])));    // magnitude computation
+                    rpp_simd_store(rpp_store48_f32pln3_to_u8pkd3_avx, dstPtrTemp, p1);    // simd stores
+
                     srcPtr1TempR += vectorIncrementPerChannel;
                     srcPtr1TempG += vectorIncrementPerChannel;
                     srcPtr1TempB += vectorIncrementPerChannel;
@@ -213,7 +262,7 @@ RppStatus magnitude_u8_u8_host_tensor(Rpp8u *srcPtr1,
         // Magnitude without fused output-layout toggle (NHWC -> NHWC or NCHW -> NCHW)
         else
         {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             alignedLength = bufferLength & ~15;
 #endif
 
@@ -232,7 +281,7 @@ RppStatus magnitude_u8_u8_host_tensor(Rpp8u *srcPtr1,
                     dstPtrTemp = dstPtrRow;
 
                     int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                     {
                         __m256 p1[2], p2[2];
@@ -243,6 +292,21 @@ RppStatus magnitude_u8_u8_host_tensor(Rpp8u *srcPtr1,
                         p1[1] = _mm256_sqrt_ps(_mm256_fmadd_ps(p1[1], p1[1], _mm256_mul_ps(p2[1], p2[1])));    // magnitude computation
                         rpp_simd_store(rpp_store16_f32_to_u8_avx, dstPtrTemp, p1);    // simd stores
 
+                        srcPtr1Temp += vectorIncrementPerChannel;
+                        srcPtr2Temp += vectorIncrementPerChannel;
+                        dstPtrTemp += vectorIncrementPerChannel;
+                    }
+#elif defined(__loongarch_asx)
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                    {
+                        __m256 p1[2], p2[2];
+
+                        rpp_simd_load(rpp_load16_u8_to_f32_avx, srcPtr1Temp, p1);    // simd loads
+                        rpp_simd_load(rpp_load16_u8_to_f32_avx, srcPtr2Temp, p2);    // simd loads
+                        p1[0] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[0], p1[0], __lasx_xvfmul_s(p2[0], p2[0])));    // magnitude computation
+                        p1[1] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[1], p1[1], __lasx_xvfmul_s(p2[1], p2[1])));    // magnitude computation
+                        rpp_simd_store(rpp_store16_f32_to_u8_avx, dstPtrTemp, p1);    // simd stores
+
                         srcPtr1Temp += vectorIncrementPerChannel;
                         srcPtr2Temp += vectorIncrementPerChannel;
                         dstPtrTemp += vectorIncrementPerChannel;
@@ -306,7 +370,7 @@ RppStatus magnitude_f32_f32_host_tensor(Rpp32f *srcPtr1,
         srcPtr2Channel = srcPtr2Image + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
         dstPtrChannel = dstPtrImage;
 
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
         Rpp32u alignedLength = (bufferLength / 24) * 24;
         Rpp32u vectorIncrement = 24;
         Rpp32u vectorIncrementPerChannel = 8;
@@ -332,7 +396,7 @@ RppStatus magnitude_f32_f32_host_tensor(Rpp32f *srcPtr1,
                 dstPtrTempB = dstPtrRowB;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256 p1[3], p2[3];
@@ -344,6 +408,24 @@ RppStatus magnitude_f32_f32_host_tensor(Rpp32f *srcPtr1,
                     p1[2] = _mm256_sqrt_ps(_mm256_fmadd_ps(p1[2], p1[2], _mm256_mul_ps(p2[2], p2[2])));    // magnitude computation
                     rpp_simd_store(rpp_store24_f32pln3_to_f32pln3_avx, dstPtrTempR, dstPtrTempG, dstPtrTempB, p1);    // simd stores
 
+                    srcPtr1Temp += vectorIncrement;
+                    srcPtr2Temp += vectorIncrement;
+                    dstPtrTempR += vectorIncrementPerChannel;
+                    dstPtrTempG += vectorIncrementPerChannel;
+                    dstPtrTempB += vectorIncrementPerChannel;
+                }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
+                {
+                    __m256 p1[3], p2[3];
+
+                    rpp_simd_load(rpp_load24_f32pkd3_to_f32pln3_avx, srcPtr1Temp, p1);    // simd loads
+                    rpp_simd_load(rpp_load24_f32pkd3_to_f32pln3_avx, srcPtr2Temp, p2);    // simd loads
+                    p1[0] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[0], p1[0], __lasx_xvfmul_s(p2[0], p2[0])));    // magnitude computation
+                    p1[1] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[1], p1[1], __lasx_xvfmul_s(p2[1], p2[1])));    // magnitude computation
+                    p1[2] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[2], p1[2], __lasx_xvfmul_s(p2[2], p2[2])));    // magnitude computation
+                    rpp_simd_store(rpp_store24_f32pln3_to_f32pln3_avx, dstPtrTempR, dstPtrTempG, dstPtrTempB, p1);    // simd stores
+
                     srcPtr1Temp += vectorIncrement;
                     srcPtr2Temp += vectorIncrement;
                     dstPtrTempR += vectorIncrementPerChannel;
@@ -393,7 +475,7 @@ RppStatus magnitude_f32_f32_host_tensor(Rpp32f *srcPtr1,
                 dstPtrTemp = dstPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 p1[3], p2[3];
@@ -405,6 +487,26 @@ RppStatus magnitude_f32_f32_host_tensor(Rpp32f *srcPtr1,
                     p1[2] = _mm256_sqrt_ps(_mm256_fmadd_ps(p1[2], p1[2], _mm256_mul_ps(p2[2], p2[2])));    // magnitude computation
                     rpp_simd_store(rpp_store24_f32pln3_to_f32pkd3_avx, dstPtrTemp, p1);    // simd stores
 
+                    srcPtr1TempR += vectorIncrementPerChannel;
+                    srcPtr1TempG += vectorIncrementPerChannel;
+                    srcPtr1TempB += vectorIncrementPerChannel;
+                    srcPtr2TempR += vectorIncrementPerChannel;
+                    srcPtr2TempG += vectorIncrementPerChannel;
+                    srcPtr2TempB += vectorIncrementPerChannel;
+                    dstPtrTemp += vectorIncrement;
+                }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256 p1[3], p2[3];
+
+                    rpp_simd_load(rpp_load24_f32pln3_to_f32pln3_avx, srcPtr1TempR, srcPtr1TempG, srcPtr1TempB, p1);    // simd loads
+                    rpp_simd_load(rpp_load24_f32pln3_to_f32pln3_avx, srcPtr2TempR, srcPtr2TempG, srcPtr2TempB, p2);    // simd loads
+                    p1[0] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[0], p1[0], __lasx_xvfmul_s(p2[0], p2[0])));    // magnitude computation
+                    p1[1] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[1], p1[1], __lasx_xvfmul_s(p2[1], p2[1])));    // magnitude computation
+                    p1[2] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[2], p1[2], __lasx_xvfmul_s(p2[2], p2[2])));    // magnitude computation
+                    rpp_simd_store(rpp_store24_f32pln3_to_f32pkd3_avx, dstPtrTemp, p1);    // simd stores
+
                     srcPtr1TempR += vectorIncrementPerChannel;
                     srcPtr1TempG += vectorIncrementPerChannel;
                     srcPtr1TempB += vectorIncrementPerChannel;
@@ -442,7 +544,7 @@ RppStatus magnitude_f32_f32_host_tensor(Rpp32f *srcPtr1,
         // Magnitude without fused output-layout toggle (NHWC -> NHWC or NCHW -> NCHW)
         else
         {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             alignedLength = bufferLength & ~7;
 #endif
 
@@ -461,7 +563,7 @@ RppStatus magnitude_f32_f32_host_tensor(Rpp32f *srcPtr1,
                     dstPtrTemp = dstPtrRow;
 
                     int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                     {
                         __m256 p1[1], p2[1];
@@ -471,6 +573,20 @@ RppStatus magnitude_f32_f32_host_tensor(Rpp32f *srcPtr1,
                         p1[0] = _mm256_sqrt_ps(_mm256_fmadd_ps(p1[0], p1[0], _mm256_mul_ps(p2[0], p2[0])));    // magnitude computation
                         rpp_simd_store(rpp_store8_f32_to_f32_avx, dstPtrTemp, p1);    // simd stores
 
+                        srcPtr1Temp += vectorIncrementPerChannel;
+                        srcPtr2Temp += vectorIncrementPerChannel;
+                        dstPtrTemp += vectorIncrementPerChannel;
+                    }
+#elif defined(__loongarch_asx)
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                    {
+                        __m256 p1[1], p2[1];
+
+                        rpp_simd_load(rpp_load8_f32_to_f32_avx, srcPtr1Temp, p1);    // simd loads
+                        rpp_simd_load(rpp_load8_f32_to_f32_avx, srcPtr2Temp, p2);    // simd loads
+                        p1[0] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[0], p1[0], __lasx_xvfmul_s(p2[0], p2[0])));    // magnitude computation
+                        rpp_simd_store(rpp_store8_f32_to_f32_avx, dstPtrTemp, p1);    // simd stores
+
                         srcPtr1Temp += vectorIncrementPerChannel;
                         srcPtr2Temp += vectorIncrementPerChannel;
                         dstPtrTemp += vectorIncrementPerChannel;
@@ -532,7 +648,7 @@ RppStatus magnitude_f16_f16_host_tensor(Rpp16f *srcPtr1,
         srcPtr2Channel = srcPtr2Image + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
         dstPtrChannel = dstPtrImage;
 
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
         Rpp32u alignedLength = (bufferLength / 24) * 24;
         Rpp32u vectorIncrement = 24;
         Rpp32u vectorIncrementPerChannel = 8;
@@ -558,7 +674,7 @@ RppStatus magnitude_f16_f16_host_tensor(Rpp16f *srcPtr1,
                 dstPtrTempB = dstPtrRowB;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     Rpp32f srcPtr1Temp_ps[24], srcPtr2Temp_ps[24];
@@ -578,6 +694,32 @@ RppStatus magnitude_f16_f16_host_tensor(Rpp16f *srcPtr1,
                     p1[2] = _mm256_sqrt_ps(_mm256_fmadd_ps(p1[2], p1[2], _mm256_mul_ps(p2[2], p2[2])));    // magnitude computation
                     rpp_simd_store(rpp_store24_f32pln3_to_f16pln3_avx, dstPtrTempR, dstPtrTempG, dstPtrTempB, p1);    // simd stores
 
+                    srcPtr1Temp += vectorIncrement;
+                    srcPtr2Temp += vectorIncrement;
+                    dstPtrTempR += vectorIncrementPerChannel;
+                    dstPtrTempG += vectorIncrementPerChannel;
+                    dstPtrTempB += vectorIncrementPerChannel;
+                }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
+                {
+                    Rpp32f srcPtr1Temp_ps[24], srcPtr2Temp_ps[24];
+
+                    for(int cnt = 0; cnt < vectorIncrement; cnt++)
+                    {
+                        srcPtr1Temp_ps[cnt] = static_cast<Rpp32f>(srcPtr1Temp[cnt]);
+                        srcPtr2Temp_ps[cnt] = static_cast<Rpp32f>(srcPtr2Temp[cnt]);
+                    }
+
+                    __m256 p1[3], p2[3];
+
+                    rpp_simd_load(rpp_load24_f32pkd3_to_f32pln3_avx, srcPtr1Temp_ps, p1);    // simd loads
+                    rpp_simd_load(rpp_load24_f32pkd3_to_f32pln3_avx, srcPtr2Temp_ps, p2);    // simd loads
+                    p1[0] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[0], p1[0], __lasx_xvfmul_s(p2[0], p2[0])));    // magnitude computation
+                    p1[1] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[1], p1[1], __lasx_xvfmul_s(p2[1], p2[1])));    // magnitude computation
+                    p1[2] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[2], p1[2], __lasx_xvfmul_s(p2[2], p2[2])));    // magnitude computation
+                    rpp_simd_store(rpp_store24_f32pln3_to_f16pln3_avx, dstPtrTempR, dstPtrTempG, dstPtrTempB, p1);    // simd stores
+
                     srcPtr1Temp += vectorIncrement;
                     srcPtr2Temp += vectorIncrement;
                     dstPtrTempR += vectorIncrementPerChannel;
@@ -627,7 +769,7 @@ RppStatus magnitude_f16_f16_host_tensor(Rpp16f *srcPtr1,
                 dstPtrTemp = dstPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     Rpp32f srcPtr1Temp_ps[24], srcPtr2Temp_ps[24];
@@ -652,6 +794,39 @@ RppStatus magnitude_f16_f16_host_tensor(Rpp16f *srcPtr1,
                     p1[2] = _mm256_sqrt_ps(_mm256_fmadd_ps(p1[2], p1[2], _mm256_mul_ps(p2[2], p2[2])));    // magnitude computation
                     rpp_simd_store(rpp_store24_f32pln3_to_f16pkd3_avx, dstPtrTemp, p1);    // simd stores
 
+                    srcPtr1TempR += vectorIncrementPerChannel;
+                    srcPtr1TempG += vectorIncrementPerChannel;
+                    srcPtr1TempB += vectorIncrementPerChannel;
+                    srcPtr2TempR += vectorIncrementPerChannel;
+                    srcPtr2TempG += vectorIncrementPerChannel;
+                    srcPtr2TempB += vectorIncrementPerChannel;
+                    dstPtrTemp += vectorIncrement;
+                }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    Rpp32f srcPtr1Temp_ps[24], srcPtr2Temp_ps[24];
+
+                    for(int cnt = 0; cnt < vectorIncrementPerChannel; cnt++)
+                    {
+                        srcPtr1Temp_ps[cnt] = static_cast<Rpp32f>(srcPtr1TempR[cnt]);
+                        srcPtr1Temp_ps[cnt + 8] = static_cast<Rpp32f>(srcPtr1TempG[cnt]);
+                        srcPtr1Temp_ps[cnt + 16] = static_cast<Rpp32f>(srcPtr1TempB[cnt]);
+
+                        srcPtr2Temp_ps[cnt] = static_cast<Rpp32f>(srcPtr2TempR[cnt]);
+                        srcPtr2Temp_ps[cnt + 8] = static_cast<Rpp32f>(srcPtr2TempG[cnt]);
+                        srcPtr2Temp_ps[cnt + 16] = static_cast<Rpp32f>(srcPtr2TempB[cnt]);
+                    }
+
+                    __m256 p1[4], p2[4];
+
+                    rpp_simd_load(rpp_load24_f32pln3_to_f32pln3_avx, srcPtr1Temp_ps, srcPtr1Temp_ps + 8, srcPtr1Temp_ps + 16, p1);    // simd loads
+                    rpp_simd_load(rpp_load24_f32pln3_to_f32pln3_avx, srcPtr2Temp_ps, srcPtr2Temp_ps + 8, srcPtr2Temp_ps + 16, p2);    // simd loads
+                    p1[0] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[0], p1[0], __lasx_xvfmul_s(p2[0], p2[0])));    // magnitude computation
+                    p1[1] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[1], p1[1], __lasx_xvfmul_s(p2[1], p2[1])));    // magnitude computation
+                    p1[2] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[2], p1[2], __lasx_xvfmul_s(p2[2], p2[2])));    // magnitude computation
+                    rpp_simd_store(rpp_store24_f32pln3_to_f16pkd3_avx, dstPtrTemp, p1);    // simd stores
+
                     srcPtr1TempR += vectorIncrementPerChannel;
                     srcPtr1TempG += vectorIncrementPerChannel;
                     srcPtr1TempB += vectorIncrementPerChannel;
@@ -689,7 +864,7 @@ RppStatus magnitude_f16_f16_host_tensor(Rpp16f *srcPtr1,
         // Magnitude without fused output-layout toggle (NHWC -> NHWC or NCHW -> NCHW)
         else
         {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             alignedLength = bufferLength & ~7;
 #endif
 
@@ -708,7 +883,7 @@ RppStatus magnitude_f16_f16_host_tensor(Rpp16f *srcPtr1,
                     dstPtrTemp = dstPtrRow;
 
                     int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                     {
                         Rpp32f srcPtr1Temp_ps[8], srcPtr2Temp_ps[8];
@@ -726,6 +901,28 @@ RppStatus magnitude_f16_f16_host_tensor(Rpp16f *srcPtr1,
                         p1[0] = _mm256_sqrt_ps(_mm256_fmadd_ps(p1[0], p1[0], _mm256_mul_ps(p2[0], p2[0])));    // magnitude computation
                         rpp_simd_store(rpp_store8_f32_to_f16_avx, dstPtrTemp, p1);    // simd stores
 
+                        srcPtr1Temp += vectorIncrementPerChannel;
+                        srcPtr2Temp += vectorIncrementPerChannel;
+                        dstPtrTemp += vectorIncrementPerChannel;
+                    }
+#elif defined(__loongarch_asx)
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                    {
+                        Rpp32f srcPtr1Temp_ps[8], srcPtr2Temp_ps[8];
+
+                        for(int cnt = 0; cnt < vectorIncrementPerChannel; cnt++)
+                        {
+                            srcPtr1Temp_ps[cnt] = static_cast<Rpp32f>(srcPtr1Temp[cnt]);
+                            srcPtr2Temp_ps[cnt] = static_cast<Rpp32f>(srcPtr2Temp[cnt]);
+                        }
+
+                        __m256 p1[1], p2[1];
+
+                        rpp_simd_load(rpp_load8_f32_to_f32_avx, srcPtr1Temp_ps, p1);    // simd loads
+                        rpp_simd_load(rpp_load8_f32_to_f32_avx, srcPtr2Temp_ps, p2);    // simd loads
+                        p1[0] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[0], p1[0], __lasx_xvfmul_s(p2[0], p2[0])));    // magnitude computation
+                        rpp_simd_store(rpp_store8_f32_to_f16_avx, dstPtrTemp, p1);    // simd stores
+
                         srcPtr1Temp += vectorIncrementPerChannel;
                         srcPtr2Temp += vectorIncrementPerChannel;
                         dstPtrTemp += vectorIncrementPerChannel;
@@ -786,7 +983,7 @@ RppStatus magnitude_i8_i8_host_tensor(Rpp8s *srcPtr1,
         srcPtr2Channel = srcPtr2Image + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
         dstPtrChannel = dstPtrImage;
 
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
         Rpp32u alignedLength = (bufferLength / 48) * 48;
         Rpp32u vectorIncrement = 48;
         Rpp32u vectorIncrementPerChannel = 16;
@@ -812,7 +1009,7 @@ RppStatus magnitude_i8_i8_host_tensor(Rpp8s *srcPtr1,
                 dstPtrTempB = dstPtrRowB;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256 p1[6], p2[6];
@@ -827,6 +1024,27 @@ RppStatus magnitude_i8_i8_host_tensor(Rpp8s *srcPtr1,
                     p1[5] = _mm256_sqrt_ps(_mm256_fmadd_ps(p1[5], p1[5], _mm256_mul_ps(p2[5], p2[5])));    // magnitude computation
                     rpp_simd_store(rpp_store48_f32pln3_to_i8pln3_avx, dstPtrTempR, dstPtrTempG, dstPtrTempB, p1);    // simd stores
 
+                    srcPtr1Temp += vectorIncrement;
+                    srcPtr2Temp += vectorIncrement;
+                    dstPtrTempR += vectorIncrementPerChannel;
+                    dstPtrTempG += vectorIncrementPerChannel;
+                    dstPtrTempB += vectorIncrementPerChannel;
+                }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
+                {
+                    __m256 p1[6], p2[6];
+
+                    rpp_simd_load(rpp_load48_i8pkd3_to_f32pln3_avx, srcPtr1Temp, p1);    // simd loads
+                    rpp_simd_load(rpp_load48_i8pkd3_to_f32pln3_avx, srcPtr2Temp, p2);    // simd loads
+                    p1[0] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[0], p1[0], __lasx_xvfmul_s(p2[0], p2[0])));    // magnitude computation
+                    p1[1] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[1], p1[1], __lasx_xvfmul_s(p2[1], p2[1])));    // magnitude computation
+                    p1[2] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[2], p1[2], __lasx_xvfmul_s(p2[2], p2[2])));    // magnitude computation
+                    p1[3] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[3], p1[3], __lasx_xvfmul_s(p2[3], p2[3])));    // magnitude computation
+                    p1[4] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[4], p1[4], __lasx_xvfmul_s(p2[4], p2[4])));    // magnitude computation
+                    p1[5] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[5], p1[5], __lasx_xvfmul_s(p2[5], p2[5])));    // magnitude computation
+                    rpp_simd_store(rpp_store48_f32pln3_to_i8pln3_avx, dstPtrTempR, dstPtrTempG, dstPtrTempB, p1);    // simd stores
+
                     srcPtr1Temp += vectorIncrement;
                     srcPtr2Temp += vectorIncrement;
                     dstPtrTempR += vectorIncrementPerChannel;
@@ -882,7 +1100,7 @@ RppStatus magnitude_i8_i8_host_tensor(Rpp8s *srcPtr1,
                 dstPtrTemp = dstPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 p1[6], p2[6];
@@ -897,6 +1115,29 @@ RppStatus magnitude_i8_i8_host_tensor(Rpp8s *srcPtr1,
                     p1[5] = _mm256_sqrt_ps(_mm256_fmadd_ps(p1[5], p1[5], _mm256_mul_ps(p2[5], p2[5])));    // magnitude computation
                     rpp_simd_store(rpp_store48_f32pln3_to_i8pkd3_avx, dstPtrTemp, p1);    // simd stores
 
+                    srcPtr1TempR += vectorIncrementPerChannel;
+                    srcPtr1TempG += vectorIncrementPerChannel;
+                    srcPtr1TempB += vectorIncrementPerChannel;
+                    srcPtr2TempR += vectorIncrementPerChannel;
+                    srcPtr2TempG += vectorIncrementPerChannel;
+                    srcPtr2TempB += vectorIncrementPerChannel;
+                    dstPtrTemp += vectorIncrement;
+                }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256 p1[6], p2[6];
+
+                    rpp_simd_load(rpp_load48_i8pln3_to_f32pln3_avx, srcPtr1TempR, srcPtr1TempG, srcPtr1TempB, p1);    // simd loads
+                    rpp_simd_load(rpp_load48_i8pln3_to_f32pln3_avx, srcPtr2TempR, srcPtr2TempG, srcPtr2TempB, p2);    // simd loads
+                    p1[0] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[0], p1[0], __lasx_xvfmul_s(p2[0], p2[0])));    // magnitude computation
+                    p1[1] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[1], p1[1], __lasx_xvfmul_s(p2[1], p2[1])));    // magnitude computation
+                    p1[2] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[2], p1[2], __lasx_xvfmul_s(p2[2], p2[2])));    // magnitude computation
+                    p1[3] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[3], p1[3], __lasx_xvfmul_s(p2[3], p2[3])));    // magnitude computation
+                    p1[4] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[4], p1[4], __lasx_xvfmul_s(p2[4], p2[4])));    // magnitude computation
+                    p1[5] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[5], p1[5], __lasx_xvfmul_s(p2[5], p2[5])));    // magnitude computation
+                    rpp_simd_store(rpp_store48_f32pln3_to_i8pkd3_avx, dstPtrTemp, p1);    // simd stores
+
                     srcPtr1TempR += vectorIncrementPerChannel;
                     srcPtr1TempG += vectorIncrementPerChannel;
                     srcPtr1TempB += vectorIncrementPerChannel;
@@ -940,7 +1181,7 @@ RppStatus magnitude_i8_i8_host_tensor(Rpp8s *srcPtr1,
         // Magnitude without fused output-layout toggle (NHWC -> NHWC or NCHW -> NCHW)
         else
         {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             alignedLength = bufferLength & ~15;
 #endif
 
@@ -959,7 +1200,7 @@ RppStatus magnitude_i8_i8_host_tensor(Rpp8s *srcPtr1,
                     dstPtrTemp = dstPtrRow;
 
                     int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                     {
                         __m256 p1[2], p2[2];
@@ -970,6 +1211,21 @@ RppStatus magnitude_i8_i8_host_tensor(Rpp8s *srcPtr1,
                         p1[1] = _mm256_sqrt_ps(_mm256_fmadd_ps(p1[1], p1[1], _mm256_mul_ps(p2[1], p2[1])));    // magnitude computation
                         rpp_simd_store(rpp_store16_f32_to_i8_avx, dstPtrTemp, p1);    // simd stores
 
+                        srcPtr1Temp += vectorIncrementPerChannel;
+                        srcPtr2Temp += vectorIncrementPerChannel;
+                        dstPtrTemp += vectorIncrementPerChannel;
+                    }
+#elif defined(__loongarch_asx)
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                    {
+                        __m256 p1[2], p2[2];
+
+                        rpp_simd_load(rpp_load16_i8_to_f32_avx, srcPtr1Temp, p1);    // simd loads
+                        rpp_simd_load(rpp_load16_i8_to_f32_avx, srcPtr2Temp, p2);    // simd loads
+                        p1[0] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[0], p1[0], __lasx_xvfmul_s(p2[0], p2[0])));    // magnitude computation
+                        p1[1] = __lasx_xvfsqrt_s(__lasx_xvfmadd_s(p1[1], p1[1], __lasx_xvfmul_s(p2[1], p2[1])));    // magnitude computation
+                        rpp_simd_store(rpp_store16_f32_to_i8_avx, dstPtrTemp, p1);    // simd stores
+
                         srcPtr1Temp += vectorIncrementPerChannel;
                         srcPtr2Temp += vectorIncrementPerChannel;
                         dstPtrTemp += vectorIncrementPerChannel;
diff --git a/src/modules/cpu/kernel/mel_filter_bank.hpp b/src/modules/cpu/kernel/mel_filter_bank.hpp
index a8d430d0..2f00d1d4 100644
--- a/src/modules/cpu/kernel/mel_filter_bank.hpp
+++ b/src/modules/cpu/kernel/mel_filter_bank.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 RppStatus mel_filter_bank_host_tensor(Rpp32f *srcPtr,
                                       RpptDescPtr srcDescPtr,
@@ -124,7 +129,11 @@ RppStatus mel_filter_bank_host_tensor(Rpp32f *srcPtr,
             Rpp32u vectorLoopCount = 0;
             for(; vectorLoopCount < maxAlignedLength; vectorLoopCount += 8)
             {
+#ifdef __loongarch_asx
+                __lasx_xvst((__m256i)avx_p0, dstPtrRow, 0);
+#else
                 _mm256_storeu_ps(dstPtrRow, avx_p0);
+#endif
                 dstPtrRow += 8;
             }
             for(; vectorLoopCount < maxFrames; vectorLoopCount++)
@@ -148,6 +157,21 @@ RppStatus mel_filter_bank_host_tensor(Rpp32f *srcPtr,
 
                 if (normalize)
                     weightDown *= normFactors[filterDown];
+#ifdef __loongarch_asx
+                __m256 pWeightDown = lasx_set1_f32(weightDown);
+
+                int vectorLoopCount = 0;
+                for(; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
+                {
+                    pSrc = (__m256)__lasx_xvld(srcRowPtrTemp, 0);
+                    pSrc = __lasx_xvfmul_s(pSrc, pWeightDown);
+                    pDst = (__m256)__lasx_xvld(dstRowPtrTemp, 0);
+                    pDst = __lasx_xvfadd_s(pDst, pSrc);
+                    __lasx_xvst((__m256i)pDst, dstRowPtrTemp, 0);
+                    dstRowPtrTemp += vectorIncrement;
+                    srcRowPtrTemp += vectorIncrement;
+                }
+#else
                 __m256 pWeightDown = _mm256_set1_ps(weightDown);
 
                 int vectorLoopCount = 0;
@@ -161,6 +185,7 @@ RppStatus mel_filter_bank_host_tensor(Rpp32f *srcPtr,
                     dstRowPtrTemp += vectorIncrement;
                     srcRowPtrTemp += vectorIncrement;
                 }
+#endif
 
                 for (; vectorLoopCount < numFrames; vectorLoopCount++)
                     (*dstRowPtrTemp++) += weightDown * (*srcRowPtrTemp++);
@@ -173,6 +198,21 @@ RppStatus mel_filter_bank_host_tensor(Rpp32f *srcPtr,
 
                 if (normalize)
                     weightUp *= normFactors[filterUp];
+#ifdef __loongarch_asx
+                __m256 pWeightUp = lasx_set1_f32(weightUp);
+
+                int vectorLoopCount = 0;
+                for(; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
+                {
+                    pSrc = (__m256)__lasx_xvld(srcRowPtrTemp, 0);
+                    pSrc = __lasx_xvfmul_s(pSrc, pWeightUp);
+                    pDst = (__m256)__lasx_xvld(dstRowPtrTemp, 0);
+                    pDst = __lasx_xvfadd_s(pDst, pSrc);
+                    __lasx_xvst((__m256i)pDst, dstRowPtrTemp, 0);
+                    dstRowPtrTemp += vectorIncrement;
+                    srcRowPtrTemp += vectorIncrement;
+                }
+#else
                 __m256 pWeightUp = _mm256_set1_ps(weightUp);
 
                 int vectorLoopCount = 0;
@@ -186,6 +226,7 @@ RppStatus mel_filter_bank_host_tensor(Rpp32f *srcPtr,
                     dstRowPtrTemp += vectorIncrement;
                     srcRowPtrTemp += vectorIncrement;
                 }
+#endif
 
                 for (; vectorLoopCount < numFrames; vectorLoopCount++)
                     (*dstRowPtrTemp++) += weightUp * (*srcRowPtrTemp++);
diff --git a/src/modules/cpu/kernel/multiply_scalar.hpp b/src/modules/cpu/kernel/multiply_scalar.hpp
index a27782bc..058700b9 100644
--- a/src/modules/cpu/kernel/multiply_scalar.hpp
+++ b/src/modules/cpu/kernel/multiply_scalar.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 RppStatus multiply_scalar_f32_f32_host_tensor(Rpp32f *srcPtr,
                                               RpptGenericDescPtr srcGenericDescPtr,
@@ -59,10 +64,14 @@ RppStatus multiply_scalar_f32_f32_host_tensor(Rpp32f *srcPtr,
 
         Rpp32f *srcPtrChannel, *dstPtrChannel;
         dstPtrChannel = dstPtrImage;
-#if __AVX2__
+#if defined(__AVX2__)
         Rpp32u vectorIncrement = 16;
         __m256 pMulParam = _mm256_set1_ps(mulParam);
         Rpp32u alignedLength = bufferLength & ~(vectorIncrement - 1);
+#elif defined(__loongarch_asx)
+        Rpp32u vectorIncrement = 16;
+        __m256 pMulParam = lasx_set1_f32(mulParam);
+        Rpp32u alignedLength = bufferLength & ~(vectorIncrement - 1);
 #endif
         // multiply without fused output-layout toggle (NCDHW -> NCDHW)
         if((srcGenericDescPtr->layout == RpptLayout::NCDHW) && (dstGenericDescPtr->layout == RpptLayout::NCDHW))
@@ -84,7 +93,7 @@ RppStatus multiply_scalar_f32_f32_host_tensor(Rpp32f *srcPtr,
                         srcPtrTemp = srcPtrRow;
                         dstPtrTemp = dstPtrRow;
                         int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loonarch_asx)
                         for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                         {
                             __m256 p[2];
@@ -125,7 +134,7 @@ RppStatus multiply_scalar_f32_f32_host_tensor(Rpp32f *srcPtr,
                     srcPtrTemp = srcPtrRow;
                     dstPtrTemp = dstPtrRow;
                     int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loonarch_asx)
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                     {
                         __m256 p[2];
diff --git a/src/modules/cpu/kernel/noise_gaussian.hpp b/src/modules/cpu/kernel/noise_gaussian.hpp
index 7e87dfe2..37208651 100644
--- a/src/modules/cpu/kernel/noise_gaussian.hpp
+++ b/src/modules/cpu/kernel/noise_gaussian.hpp
@@ -23,19 +23,34 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 inline void compute_gaussian_noise_params_initialize_4_host_sse(Rpp32f &mean, Rpp32f &stdDev, __m128 *pGaussianNoiseParams)
 {
+#if defined(__loongarch_sx)
+    pGaussianNoiseParams[0] = lsx_set1_f32(mean);
+    pGaussianNoiseParams[1] = lsx_set1_f32(stdDev);
+#else
     pGaussianNoiseParams[0] = _mm_set1_ps(mean);
     pGaussianNoiseParams[1] = _mm_set1_ps(stdDev);
+#endif
 }
 
 inline void compute_gaussian_noise_params_initialize_8_host_avx(Rpp32f &mean, Rpp32f &stdDev, __m256 *pGaussianNoiseParams)
 {
+#if defined(__loongarch_asx)
+    pGaussianNoiseParams[0] = lasx_set1_f32(mean);
+    pGaussianNoiseParams[1] = lasx_set1_f32(stdDev);
+#else
     pGaussianNoiseParams[0] = _mm256_set1_ps(mean);
     pGaussianNoiseParams[1] = _mm256_set1_ps(stdDev);
+#endif
 }
 
 RppStatus gaussian_noise_u8_u8_host_tensor(Rpp8u *srcPtr,
@@ -1443,12 +1458,24 @@ RppStatus gaussian_noise_voxel_u8_u8_host_tensor(Rpp8u *srcPtr,
             Rpp32u vectorIncrement = 48;
             Rpp32u vectorIncrementPerChannel = 16;
             RpptXorwowStateBoxMuller xorwowState;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256i pxXorwowStateX[5], pxXorwowStateCounter;
             rpp_host_rng_xorwow_state_offsetted_avx(xorwowInitialStatePtr, xorwowState, offset, pxXorwowStateX, &pxXorwowStateCounter);
             __m256 pGaussianNoiseParams[2];
             pGaussianNoiseParams[0] = _mm256_set1_ps(mean);
             pGaussianNoiseParams[1] = _mm256_set1_ps(stdDev);
+#elif defined(__loongarch_asx)
+            __m256i pxXorwowStateX[5], pxXorwowStateCounter;
+            rpp_host_rng_xorwow_state_offsetted_avx(xorwowInitialStatePtr, xorwowState, offset, pxXorwowStateX, &pxXorwowStateCounter);
+            __m256 pGaussianNoiseParams[2];
+            pGaussianNoiseParams[0] = lasx_set1_f32(mean);
+            pGaussianNoiseParams[1] = lasx_set1_f32(stdDev);
+#elif defined(__loongarch_sx)
+            __m128i pxXorwowStateX[5], pxXorwowStateCounter;
+            rpp_host_rng_xorwow_state_offsetted_sse(xorwowInitialStatePtr, xorwowState, offset, pxXorwowStateX, &pxXorwowStateCounter);
+            __m128 pGaussianNoiseParams[2];
+            pGaussianNoiseParams[0] = lsx_set1_f32(mean);
+            pGaussianNoiseParams[1] = lsx_set1_f32(stdDev);
 #else
             __m128i pxXorwowStateX[5], pxXorwowStateCounter;
             rpp_host_rng_xorwow_state_offsetted_sse(xorwowInitialStatePtr, xorwowState, offset, pxXorwowStateX, &pxXorwowStateCounter);
@@ -1477,7 +1504,7 @@ RppStatus gaussian_noise_voxel_u8_u8_host_tensor(Rpp8u *srcPtr,
                         int vectorLoopCount = 0;
                         for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                         {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                             __m256 p[6];
                             rpp_simd_load(rpp_load48_u8pkd3_to_f32pln3_avx, srcPtrTemp, p);                                 // simd loads
                             rpp_multiply48_constant(p, avx_p1op255);                                                        // u8 normalization to range[0,1]
@@ -1543,7 +1570,7 @@ RppStatus gaussian_noise_voxel_u8_u8_host_tensor(Rpp8u *srcPtr,
                         int vectorLoopCount = 0;
                         for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                         {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                             __m256 p[6];
                             rpp_simd_load(rpp_load48_u8pln3_to_f32pln3_avx, srcPtrTempR, srcPtrTempG, srcPtrTempB, p);      // simd loads
                             rpp_multiply48_constant(p, avx_p1op255);                                                        // u8 normalization to range[0,1]
@@ -1608,7 +1635,7 @@ RppStatus gaussian_noise_voxel_u8_u8_host_tensor(Rpp8u *srcPtr,
                         int vectorLoopCount = 0;
                         for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                         {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                             __m256 p[2];
                             rpp_simd_load(rpp_load16_u8_to_f32_avx, srcPtrTemp, p);                                         // simd loads
                             rpp_multiply16_constant(p, avx_p1op255);                                                        // u8 normalization to range[0,1]
@@ -1689,7 +1716,7 @@ RppStatus gaussian_noise_voxel_f32_f32_host_tensor(Rpp32f *srcPtr,
             Rpp32f *srcPtrChannel, *dstPtrChannel;
             dstPtrChannel = dstPtrImage;
             RpptXorwowStateBoxMuller xorwowState;
-#if __AVX2__
+#if defined(__AVX2__)
             Rpp32u alignedLength = (bufferLength / 24) * 24;
             Rpp32u vectorIncrement = 24;
             Rpp32u vectorIncrementPerChannel = 8;
@@ -1699,6 +1726,26 @@ RppStatus gaussian_noise_voxel_f32_f32_host_tensor(Rpp32f *srcPtr,
             __m256 pGaussianNoiseParams[2];
             pGaussianNoiseParams[0] = _mm256_set1_ps(mean);
             pGaussianNoiseParams[1] = _mm256_set1_ps(stdDev);
+#elif defined(__loongarch_asx)
+            Rpp32u alignedLength = (bufferLength / 24) * 24;
+            Rpp32u vectorIncrement = 24;
+            Rpp32u vectorIncrementPerChannel = 8;
+            __m256i pxXorwowStateX[5], pxXorwowStateCounter;
+            rpp_host_rng_xorwow_state_offsetted_avx(xorwowInitialStatePtr, xorwowState, offset, pxXorwowStateX, &pxXorwowStateCounter);
+
+            __m256 pGaussianNoiseParams[2];
+            pGaussianNoiseParams[0] = lasx_set1_f32(mean);
+            pGaussianNoiseParams[1] = lasx_set1_f32(stdDev);
+#elif defined(__loongarch_asx)
+            Rpp32u alignedLength = (bufferLength / 12) * 12;
+            Rpp32u vectorIncrement = 12;
+            Rpp32u vectorIncrementPerChannel = 4;
+            __m128i pxXorwowStateX[5], pxXorwowStateCounter;
+            rpp_host_rng_xorwow_state_offsetted_sse(xorwowInitialStatePtr, xorwowState, offset, pxXorwowStateX, &pxXorwowStateCounter);
+
+            __m128 pGaussianNoiseParams[2];
+            pGaussianNoiseParams[0] = lsx_set1_f32(mean);
+            pGaussianNoiseParams[1] = lsx_set1_f32(stdDev);
 #else
             Rpp32u alignedLength = (bufferLength / 12) * 12;
             Rpp32u vectorIncrement = 12;
@@ -1730,7 +1777,7 @@ RppStatus gaussian_noise_voxel_f32_f32_host_tensor(Rpp32f *srcPtr,
                         int vectorLoopCount = 0;
                         for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                         {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                             __m256 p[3];
                             rpp_simd_load(rpp_load24_f32pkd3_to_f32pln3_avx, srcPtrTemp, p);                                // simd loads
                             compute_gaussian_noise_voxel_24_host(p, pxXorwowStateX, &pxXorwowStateCounter, pGaussianNoiseParams); // gaussian_noise adjustment
@@ -1792,7 +1839,7 @@ RppStatus gaussian_noise_voxel_f32_f32_host_tensor(Rpp32f *srcPtr,
                         int vectorLoopCount = 0;
                         for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                         {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                             __m256 p[3];
                             rpp_simd_load(rpp_load24_f32pln3_to_f32pln3_avx, srcPtrTempR, srcPtrTempG, srcPtrTempB, p);     // simd loads
                             compute_gaussian_noise_voxel_24_host(p, pxXorwowStateX, &pxXorwowStateCounter, pGaussianNoiseParams); // gaussian_noise adjustment
@@ -1836,7 +1883,7 @@ RppStatus gaussian_noise_voxel_f32_f32_host_tensor(Rpp32f *srcPtr,
             else if ((srcGenericDescPtr->dims[1] == 1) && (srcGenericDescPtr->layout == RpptLayout::NCDHW) && (dstGenericDescPtr->layout == RpptLayout::NCDHW))
             {
                 srcPtrChannel = srcPtrImage + (roi.xyzwhdROI.xyz.z * srcGenericDescPtr->strides[2]) + (roi.xyzwhdROI.xyz.y * srcGenericDescPtr->strides[3]) + (roi.xyzwhdROI.xyz.x * layoutParams.bufferMultiplier);
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 alignedLength = bufferLength & ~15;
 #else
                 alignedLength = bufferLength & ~7;
@@ -1858,7 +1905,7 @@ RppStatus gaussian_noise_voxel_f32_f32_host_tensor(Rpp32f *srcPtr,
                         int vectorLoopCount = 0;
                         for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannelDouble)
                         {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                             __m256 p[2];
                             rpp_simd_load(rpp_load16_f32_to_f32_avx, srcPtrTemp, p);                                        // simd loads
                             compute_gaussian_noise_voxel_16_host(p, pxXorwowStateX, &pxXorwowStateCounter, pGaussianNoiseParams); // gaussian_noise adjustment
diff --git a/src/modules/cpu/kernel/noise_salt_and_pepper.hpp b/src/modules/cpu/kernel/noise_salt_and_pepper.hpp
index 25618184..73f9b8db 100644
--- a/src/modules/cpu/kernel/noise_salt_and_pepper.hpp
+++ b/src/modules/cpu/kernel/noise_salt_and_pepper.hpp
@@ -23,23 +23,42 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 inline void compute_salt_and_pepper_noise_params_initialize_4_host_sse(Rpp32f &noiseProbability, Rpp32f &saltProbability, Rpp32f salt, Rpp32f pepper, __m128 *pSaltAndPepperNoiseParams)
 {
+#if defined(__loongarch_sx)
+    pSaltAndPepperNoiseParams[0] = lsx_set1_f32(noiseProbability);
+    pSaltAndPepperNoiseParams[1] = lsx_set1_f32(saltProbability);
+    pSaltAndPepperNoiseParams[2] = lsx_set1_f32(salt);
+    pSaltAndPepperNoiseParams[3] = lsx_set1_f32(pepper);
+#else
     pSaltAndPepperNoiseParams[0] = _mm_set1_ps(noiseProbability);
     pSaltAndPepperNoiseParams[1] = _mm_set1_ps(saltProbability);
     pSaltAndPepperNoiseParams[2] = _mm_set1_ps(salt);
     pSaltAndPepperNoiseParams[3] = _mm_set1_ps(pepper);
+#endif
 }
 
 inline void compute_salt_and_pepper_noise_params_initialize_8_host_avx(Rpp32f &noiseProbability, Rpp32f &saltProbability, Rpp32f salt, Rpp32f pepper, __m256 *pSaltAndPepperNoiseParams)
 {
+#if defined(__AVX2__)
     pSaltAndPepperNoiseParams[0] = _mm256_set1_ps(noiseProbability);
     pSaltAndPepperNoiseParams[1] = _mm256_set1_ps(saltProbability);
     pSaltAndPepperNoiseParams[2] = _mm256_set1_ps(salt);
     pSaltAndPepperNoiseParams[3] = _mm256_set1_ps(pepper);
+#elif defined(__loongarch_asx)
+    pSaltAndPepperNoiseParams[0] = lasx_set1_f32(noiseProbability);
+    pSaltAndPepperNoiseParams[1] = lasx_set1_f32(saltProbability);
+    pSaltAndPepperNoiseParams[2] = lasx_set1_f32(salt);
+    pSaltAndPepperNoiseParams[3] = lasx_set1_f32(pepper);
+#endif
 }
 
 RppStatus salt_and_pepper_noise_u8_u8_host_tensor(Rpp8u *srcPtr,
@@ -88,7 +107,7 @@ RppStatus salt_and_pepper_noise_u8_u8_host_tensor(Rpp8u *srcPtr,
         Rpp32u vectorIncrementPerChannel = 16;
 
         RpptXorwowState xorwowState;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
         __m256i pxXorwowStateX[5], pxXorwowStateCounter;
         __m256 pSaltAndPepperNoiseParams[4];
         rpp_host_rng_xorwow_state_offsetted_avx(xorwowInitialStatePtr, xorwowState, offset, pxXorwowStateX, &pxXorwowStateCounter);
@@ -120,7 +139,7 @@ RppStatus salt_and_pepper_noise_u8_u8_host_tensor(Rpp8u *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[6];
                     rpp_simd_load(rpp_load48_u8pkd3_to_f32pln3_avx, srcPtrTemp, p);    // simd loads
                     compute_salt_and_pepper_noise_48_host(p, pxXorwowStateX, &pxXorwowStateCounter, pSaltAndPepperNoiseParams);    // salt_and_pepper_noise adjustment
@@ -184,7 +203,7 @@ RppStatus salt_and_pepper_noise_u8_u8_host_tensor(Rpp8u *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[6];
                     rpp_simd_load(rpp_load48_u8pln3_to_f32pln3_avx, srcPtrTempR, srcPtrTempG, srcPtrTempB, p);    // simd loads
                     compute_salt_and_pepper_noise_48_host(p, pxXorwowStateX, &pxXorwowStateCounter, pSaltAndPepperNoiseParams);    // salt_and_pepper_noise adjustment
@@ -241,7 +260,7 @@ RppStatus salt_and_pepper_noise_u8_u8_host_tensor(Rpp8u *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[6];
                     rpp_simd_load(rpp_load48_u8pkd3_to_f32pln3_avx, srcPtrTemp, p);    // simd loads
                     compute_salt_and_pepper_noise_48_host(p, pxXorwowStateX, &pxXorwowStateCounter, pSaltAndPepperNoiseParams);    // salt_and_pepper_noise adjustment
@@ -299,7 +318,7 @@ RppStatus salt_and_pepper_noise_u8_u8_host_tensor(Rpp8u *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[6];
                     rpp_simd_load(rpp_load48_u8pln3_to_f32pln3_avx, srcPtrTempR, srcPtrTempG, srcPtrTempB, p);    // simd loads
                     compute_salt_and_pepper_noise_48_host(p, pxXorwowStateX, &pxXorwowStateCounter, pSaltAndPepperNoiseParams);    // salt_and_pepper_noise adjustment
@@ -377,7 +396,7 @@ RppStatus salt_and_pepper_noise_u8_u8_host_tensor(Rpp8u *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[2];
                     rpp_simd_load(rpp_load16_u8_to_f32_avx, srcPtrTemp, p);    // simd loads
                     compute_salt_and_pepper_noise_16_host(p, pxXorwowStateX, &pxXorwowStateCounter, pSaltAndPepperNoiseParams);    // salt_and_pepper_noise adjustment
@@ -457,7 +476,7 @@ RppStatus salt_and_pepper_noise_f32_f32_host_tensor(Rpp32f *srcPtr,
         dstPtrChannel = dstPtrImage;
 
         RpptXorwowState xorwowState;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
         Rpp32u alignedLength = (bufferLength / 24) * 24;
         Rpp32u vectorIncrement = 24;
         Rpp32u vectorIncrementPerChannel = 8;
@@ -497,7 +516,7 @@ RppStatus salt_and_pepper_noise_f32_f32_host_tensor(Rpp32f *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[3];
                     rpp_simd_load(rpp_load24_f32pkd3_to_f32pln3_avx, srcPtrTemp, p);    // simd loads
                     compute_salt_and_pepper_noise_24_host(p, p + 1, p + 2, pxXorwowStateX, &pxXorwowStateCounter, pSaltAndPepperNoiseParams);    // salt_and_pepper_noise adjustment
@@ -561,7 +580,7 @@ RppStatus salt_and_pepper_noise_f32_f32_host_tensor(Rpp32f *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[3];
                     rpp_simd_load(rpp_load24_f32pln3_to_f32pln3_avx, srcPtrTempR, srcPtrTempG, srcPtrTempB, p);    // simd loads
                     compute_salt_and_pepper_noise_24_host(p, p + 1, p + 2, pxXorwowStateX, &pxXorwowStateCounter, pSaltAndPepperNoiseParams);    // salt_and_pepper_noise adjustment
@@ -618,7 +637,7 @@ RppStatus salt_and_pepper_noise_f32_f32_host_tensor(Rpp32f *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[3];
                     rpp_simd_load(rpp_load24_f32pkd3_to_f32pln3_avx, srcPtrTemp, p);    // simd loads
                     compute_salt_and_pepper_noise_24_host(p, p + 1, p + 2, pxXorwowStateX, &pxXorwowStateCounter, pSaltAndPepperNoiseParams);    // salt_and_pepper_noise adjustment
@@ -676,7 +695,7 @@ RppStatus salt_and_pepper_noise_f32_f32_host_tensor(Rpp32f *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[3];
                     rpp_simd_load(rpp_load24_f32pln3_to_f32pln3_avx, srcPtrTempR, srcPtrTempG, srcPtrTempB, p);    // simd loads
                     compute_salt_and_pepper_noise_24_host(p, p + 1, p + 2, pxXorwowStateX, &pxXorwowStateCounter, pSaltAndPepperNoiseParams);    // salt_and_pepper_noise adjustment
@@ -739,7 +758,7 @@ RppStatus salt_and_pepper_noise_f32_f32_host_tensor(Rpp32f *srcPtr,
         // Salt and Pepper Noise without fused output-layout toggle single channel (NCHW -> NCHW)
         else if ((srcDescPtr->c == 1) && (srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NCHW))
         {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             alignedLength = bufferLength & ~7;
 #else
             alignedLength = bufferLength & ~3;
@@ -757,7 +776,7 @@ RppStatus salt_and_pepper_noise_f32_f32_host_tensor(Rpp32f *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p;
                     rpp_simd_load(rpp_load8_f32_to_f32_avx, srcPtrTemp, &p);    // simd loads
                     compute_salt_and_pepper_noise_8_host(&p, pxXorwowStateX, &pxXorwowStateCounter, pSaltAndPepperNoiseParams);    // salt_and_pepper_noise adjustment
@@ -837,7 +856,7 @@ RppStatus salt_and_pepper_noise_f16_f16_host_tensor(Rpp16f *srcPtr,
         dstPtrChannel = dstPtrImage;
 
         RpptXorwowState xorwowState;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
         Rpp32u alignedLength = (bufferLength / 24) * 24;
         Rpp32u vectorIncrement = 24;
         Rpp32u vectorIncrementPerChannel = 8;
@@ -881,7 +900,7 @@ RppStatus salt_and_pepper_noise_f16_f16_host_tensor(Rpp16f *srcPtr,
                     Rpp32f dstPtrTempR_ps[8], dstPtrTempG_ps[8], dstPtrTempB_ps[8];
                     for(int cnt = 0; cnt < vectorIncrement; cnt++)
                         srcPtrTemp_ps[cnt] = (Rpp32f) srcPtrTemp[cnt];
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[3];
                     rpp_simd_load(rpp_load24_f32pkd3_to_f32pln3_avx, srcPtrTemp_ps, p);    // simd loads
                     compute_salt_and_pepper_noise_24_host(p, p + 1, p + 2, pxXorwowStateX, &pxXorwowStateCounter, pSaltAndPepperNoiseParams);    // salt_and_pepper_noise adjustment
@@ -959,7 +978,7 @@ RppStatus salt_and_pepper_noise_f16_f16_host_tensor(Rpp16f *srcPtr,
                         srcPtrTempG_ps[cnt] = (Rpp32f) srcPtrTempG[cnt];
                         srcPtrTempB_ps[cnt] = (Rpp32f) srcPtrTempB[cnt];
                     }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[3];
                     rpp_simd_load(rpp_load24_f32pln3_to_f32pln3_avx, srcPtrTempR_ps, srcPtrTempG_ps, srcPtrTempB_ps, p);    // simd loads
                     compute_salt_and_pepper_noise_24_host(p, p + 1, p + 2, pxXorwowStateX, &pxXorwowStateCounter, pSaltAndPepperNoiseParams);    // salt_and_pepper_noise adjustment
@@ -1022,7 +1041,7 @@ RppStatus salt_and_pepper_noise_f16_f16_host_tensor(Rpp16f *srcPtr,
                     Rpp32f dstPtrTemp_ps[25];
                     for(int cnt = 0; cnt < vectorIncrement; cnt++)
                         srcPtrTemp_ps[cnt] = (Rpp32f) srcPtrTemp[cnt];
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[3];
                     rpp_simd_load(rpp_load24_f32pkd3_to_f32pln3_avx, srcPtrTemp_ps, p);    // simd loads
                     compute_salt_and_pepper_noise_24_host(p, p + 1, p + 2, pxXorwowStateX, &pxXorwowStateCounter, pSaltAndPepperNoiseParams);    // salt_and_pepper_noise adjustment
@@ -1090,7 +1109,7 @@ RppStatus salt_and_pepper_noise_f16_f16_host_tensor(Rpp16f *srcPtr,
                         srcPtrTempG_ps[cnt] = (Rpp32f) srcPtrTempG[cnt];
                         srcPtrTempB_ps[cnt] = (Rpp32f) srcPtrTempB[cnt];
                     }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[3];
                     rpp_simd_load(rpp_load24_f32pln3_to_f32pln3_avx, srcPtrTempR_ps, srcPtrTempG_ps, srcPtrTempB_ps, p);    // simd loads
                     compute_salt_and_pepper_noise_24_host(p, p + 1, p + 2, pxXorwowStateX, &pxXorwowStateCounter, pSaltAndPepperNoiseParams);    // salt_and_pepper_noise adjustment
@@ -1159,7 +1178,7 @@ RppStatus salt_and_pepper_noise_f16_f16_host_tensor(Rpp16f *srcPtr,
         // Salt and Pepper Noise without fused output-layout toggle single channel (NCHW -> NCHW)
         else if ((srcDescPtr->c == 1) && (srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NCHW))
         {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             alignedLength = bufferLength & ~7;
 #else
             alignedLength = bufferLength & ~3;
@@ -1180,7 +1199,7 @@ RppStatus salt_and_pepper_noise_f16_f16_host_tensor(Rpp16f *srcPtr,
                     Rpp32f srcPtrTemp_ps[8], dstPtrTemp_ps[8];
                     for(int cnt = 0; cnt < vectorIncrementPerChannel; cnt++)
                         srcPtrTemp_ps[cnt] = (Rpp32f) srcPtrTemp[cnt];
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p;
                     rpp_simd_load(rpp_load8_f32_to_f32_avx, srcPtrTemp_ps, &p);    // simd loads
                     compute_salt_and_pepper_noise_8_host(&p, pxXorwowStateX, &pxXorwowStateCounter, pSaltAndPepperNoiseParams);    // salt_and_pepper_noise adjustment
@@ -1266,7 +1285,7 @@ RppStatus salt_and_pepper_noise_i8_i8_host_tensor(Rpp8s *srcPtr,
         Rpp32u vectorIncrementPerChannel = 16;
 
         RpptXorwowState xorwowState;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
         __m256i pxXorwowStateX[5], pxXorwowStateCounter;
         __m256 pSaltAndPepperNoiseParams[4];
         rpp_host_rng_xorwow_state_offsetted_avx(xorwowInitialStatePtr, xorwowState, offset, pxXorwowStateX, &pxXorwowStateCounter);
@@ -1298,7 +1317,7 @@ RppStatus salt_and_pepper_noise_i8_i8_host_tensor(Rpp8s *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[6];
                     rpp_simd_load(rpp_load48_i8pkd3_to_f32pln3_avx, srcPtrTemp, p);    // simd loads
                     compute_salt_and_pepper_noise_48_host(p, pxXorwowStateX, &pxXorwowStateCounter, pSaltAndPepperNoiseParams);    // salt_and_pepper_noise adjustment
@@ -1362,7 +1381,7 @@ RppStatus salt_and_pepper_noise_i8_i8_host_tensor(Rpp8s *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[6];
                     rpp_simd_load(rpp_load48_i8pln3_to_f32pln3_avx, srcPtrTempR, srcPtrTempG, srcPtrTempB, p);    // simd loads
                     compute_salt_and_pepper_noise_48_host(p, pxXorwowStateX, &pxXorwowStateCounter, pSaltAndPepperNoiseParams);    // salt_and_pepper_noise adjustment
@@ -1419,7 +1438,7 @@ RppStatus salt_and_pepper_noise_i8_i8_host_tensor(Rpp8s *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[6];
                     rpp_simd_load(rpp_load48_i8pkd3_to_f32pln3_avx, srcPtrTemp, p);    // simd loads
                     compute_salt_and_pepper_noise_48_host(p, pxXorwowStateX, &pxXorwowStateCounter, pSaltAndPepperNoiseParams);    // salt_and_pepper_noise adjustment
@@ -1477,7 +1496,7 @@ RppStatus salt_and_pepper_noise_i8_i8_host_tensor(Rpp8s *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[6];
                     rpp_simd_load(rpp_load48_i8pln3_to_f32pln3_avx, srcPtrTempR, srcPtrTempG, srcPtrTempB, p);    // simd loads
                     compute_salt_and_pepper_noise_48_host(p, pxXorwowStateX, &pxXorwowStateCounter, pSaltAndPepperNoiseParams);    // salt_and_pepper_noise adjustment
@@ -1555,7 +1574,7 @@ RppStatus salt_and_pepper_noise_i8_i8_host_tensor(Rpp8s *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[2];
                     rpp_simd_load(rpp_load16_i8_to_f32_avx, srcPtrTemp, p);    // simd loads
                     compute_salt_and_pepper_noise_16_host(p, pxXorwowStateX, &pxXorwowStateCounter, pSaltAndPepperNoiseParams);    // salt_and_pepper_noise adjustment
diff --git a/src/modules/cpu/kernel/noise_shot.hpp b/src/modules/cpu/kernel/noise_shot.hpp
index 704f115f..10050ed8 100644
--- a/src/modules/cpu/kernel/noise_shot.hpp
+++ b/src/modules/cpu/kernel/noise_shot.hpp
@@ -23,19 +23,34 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 inline void compute_shot_noise_params_initialize_4_host_sse(Rpp32f &shotNoiseFactor, Rpp32f &shotNoiseFactorInv, __m128 &pShotNoiseFactor, __m128 &pShotNoiseFactorInv)
 {
+#if defined(__loongarch_sx)
+    pShotNoiseFactor = lsx_set1_f32(shotNoiseFactor);
+    pShotNoiseFactorInv = lsx_set1_f32(shotNoiseFactorInv);
+#else
     pShotNoiseFactor = _mm_set1_ps(shotNoiseFactor);
     pShotNoiseFactorInv = _mm_set1_ps(shotNoiseFactorInv);
+#endif
 }
 
 inline void compute_shot_noise_params_initialize_8_host_avx(Rpp32f &shotNoiseFactor, Rpp32f &shotNoiseFactorInv, __m256 &pShotNoiseFactor, __m256 &pShotNoiseFactorInv)
 {
+#if defined(__loongarch_asx)
+    pShotNoiseFactor = lasx_set1_f32(shotNoiseFactor);
+    pShotNoiseFactorInv = lasx_set1_f32(shotNoiseFactorInv);
+#elif defined(__AVX2__)
     pShotNoiseFactor = _mm256_set1_ps(shotNoiseFactor);
     pShotNoiseFactorInv = _mm256_set1_ps(shotNoiseFactorInv);
+#endif
 }
 
 RppStatus shot_noise_u8_u8_host_tensor(Rpp8u *srcPtr,
@@ -79,7 +94,7 @@ RppStatus shot_noise_u8_u8_host_tensor(Rpp8u *srcPtr,
         Rpp32u vectorIncrementPerChannel = 16;
 
         RpptXorwowState xorwowState;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
         __m256i pxXorwowStateX[5], pxXorwowStateCounter;
         __m256 pShotNoiseFactor, pShotNoiseFactorInv;
         rpp_host_rng_xorwow_state_offsetted_avx(xorwowInitialStatePtr, xorwowState, offset, pxXorwowStateX, &pxXorwowStateCounter);
@@ -111,7 +126,7 @@ RppStatus shot_noise_u8_u8_host_tensor(Rpp8u *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[6];
                     rpp_simd_load(rpp_load48_u8pkd3_to_f32pln3_avx, srcPtrTemp, p);    // simd loads
                     if (shotNoiseFactor)
@@ -181,7 +196,7 @@ RppStatus shot_noise_u8_u8_host_tensor(Rpp8u *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[6];
                     rpp_simd_load(rpp_load48_u8pln3_to_f32pln3_avx, srcPtrTempR, srcPtrTempG, srcPtrTempB, p);    // simd loads
                     if (shotNoiseFactor)
@@ -245,7 +260,7 @@ RppStatus shot_noise_u8_u8_host_tensor(Rpp8u *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[6];
                     rpp_simd_load(rpp_load48_u8pkd3_to_f32pln3_avx, srcPtrTemp, p);    // simd loads
                     if (shotNoiseFactor)
@@ -309,7 +324,7 @@ RppStatus shot_noise_u8_u8_host_tensor(Rpp8u *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[6];
                     rpp_simd_load(rpp_load48_u8pln3_to_f32pln3_avx, srcPtrTempR, srcPtrTempG, srcPtrTempB, p);    // simd loads
                     if (shotNoiseFactor)
@@ -383,7 +398,7 @@ RppStatus shot_noise_u8_u8_host_tensor(Rpp8u *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[2];
                     rpp_simd_load(rpp_load16_u8_to_f32_avx, srcPtrTemp, p);    // simd loads
                     if (shotNoiseFactor)
@@ -456,7 +471,7 @@ RppStatus shot_noise_f32_f32_host_tensor(Rpp32f *srcPtr,
         dstPtrChannel = dstPtrImage;
 
         RpptXorwowState xorwowState;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
         Rpp32u alignedLength = (bufferLength / 24) * 24;
         Rpp32u vectorIncrement = 24;
         Rpp32u vectorIncrementPerChannel = 8;
@@ -496,7 +511,7 @@ RppStatus shot_noise_f32_f32_host_tensor(Rpp32f *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[3];
                     rpp_simd_load(rpp_load24_f32pkd3_to_f32pln3_avx, srcPtrTemp, p);    // simd loads
                     if (shotNoiseFactor)
@@ -566,7 +581,7 @@ RppStatus shot_noise_f32_f32_host_tensor(Rpp32f *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[3];
                     rpp_simd_load(rpp_load24_f32pln3_to_f32pln3_avx, srcPtrTempR, srcPtrTempG, srcPtrTempB, p);    // simd loads
                     if (shotNoiseFactor)
@@ -632,7 +647,7 @@ RppStatus shot_noise_f32_f32_host_tensor(Rpp32f *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[3];
                     rpp_simd_load(rpp_load24_f32pkd3_to_f32pln3_avx, srcPtrTemp, p);    // simd loads
                     if (shotNoiseFactor)
@@ -700,7 +715,7 @@ RppStatus shot_noise_f32_f32_host_tensor(Rpp32f *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[3];
                     rpp_simd_load(rpp_load24_f32pln3_to_f32pln3_avx, srcPtrTempR, srcPtrTempG, srcPtrTempB, p);    // simd loads
                     if (shotNoiseFactor)
@@ -752,7 +767,7 @@ RppStatus shot_noise_f32_f32_host_tensor(Rpp32f *srcPtr,
         // Shot Noise without fused output-layout toggle single channel (NCHW -> NCHW)
         else if ((srcDescPtr->c == 1) && (srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NCHW))
         {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             alignedLength = bufferLength & ~7;
 #else
             alignedLength = bufferLength & ~3;
@@ -770,7 +785,7 @@ RppStatus shot_noise_f32_f32_host_tensor(Rpp32f *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p;
                     rpp_simd_load(rpp_load8_f32_to_f32_avx, srcPtrTemp, &p);    // simd loads
                     if (shotNoiseFactor)
@@ -846,7 +861,7 @@ RppStatus shot_noise_f16_f16_host_tensor(Rpp16f *srcPtr,
         dstPtrChannel = dstPtrImage;
 
         RpptXorwowState xorwowState;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
         Rpp32u alignedLength = (bufferLength / 24) * 24;
         Rpp32u vectorIncrement = 24;
         Rpp32u vectorIncrementPerChannel = 8;
@@ -890,7 +905,7 @@ RppStatus shot_noise_f16_f16_host_tensor(Rpp16f *srcPtr,
                     Rpp32f dstPtrTempR_ps[8], dstPtrTempG_ps[8], dstPtrTempB_ps[8];
                     for(int cnt = 0; cnt < vectorIncrement; cnt++)
                         srcPtrTemp_ps[cnt] = (Rpp32f) srcPtrTemp[cnt];
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[3];
                     rpp_simd_load(rpp_load24_f32pkd3_to_f32pln3_avx, srcPtrTemp_ps, p);    // simd loads
                     if (shotNoiseFactor)
@@ -974,7 +989,7 @@ RppStatus shot_noise_f16_f16_host_tensor(Rpp16f *srcPtr,
                         srcPtrTempG_ps[cnt] = (Rpp32f) srcPtrTempG[cnt];
                         srcPtrTempB_ps[cnt] = (Rpp32f) srcPtrTempB[cnt];
                     }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[3];
                     rpp_simd_load(rpp_load24_f32pln3_to_f32pln3_avx, srcPtrTempR_ps, srcPtrTempG_ps, srcPtrTempB_ps, p);    // simd loads
                     if (shotNoiseFactor)
@@ -1046,7 +1061,7 @@ RppStatus shot_noise_f16_f16_host_tensor(Rpp16f *srcPtr,
                     Rpp32f dstPtrTemp_ps[25];
                     for(int cnt = 0; cnt < vectorIncrement; cnt++)
                         srcPtrTemp_ps[cnt] = (Rpp32f) srcPtrTemp[cnt];
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[3];
                     rpp_simd_load(rpp_load24_f32pkd3_to_f32pln3_avx, srcPtrTemp_ps, p);    // simd loads
                     if (shotNoiseFactor)
@@ -1124,7 +1139,7 @@ RppStatus shot_noise_f16_f16_host_tensor(Rpp16f *srcPtr,
                         srcPtrTempG_ps[cnt] = (Rpp32f) srcPtrTempG[cnt];
                         srcPtrTempB_ps[cnt] = (Rpp32f) srcPtrTempB[cnt];
                     }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[3];
                     rpp_simd_load(rpp_load24_f32pln3_to_f32pln3_avx, srcPtrTempR_ps, srcPtrTempG_ps, srcPtrTempB_ps, p);    // simd loads
                     if (shotNoiseFactor)
@@ -1182,7 +1197,7 @@ RppStatus shot_noise_f16_f16_host_tensor(Rpp16f *srcPtr,
         // Shot Noise without fused output-layout toggle single channel (NCHW -> NCHW)
         else if ((srcDescPtr->c == 1) && (srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NCHW))
         {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             alignedLength = bufferLength & ~7;
 #else
             alignedLength = bufferLength & ~3;
@@ -1203,7 +1218,7 @@ RppStatus shot_noise_f16_f16_host_tensor(Rpp16f *srcPtr,
                     Rpp32f srcPtrTemp_ps[8], dstPtrTemp_ps[8];
                     for(int cnt = 0; cnt < vectorIncrementPerChannel; cnt++)
                         srcPtrTemp_ps[cnt] = (Rpp32f) srcPtrTemp[cnt];
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p;
                     rpp_simd_load(rpp_load8_f32_to_f32_avx, srcPtrTemp_ps, &p);    // simd loads
                     if (shotNoiseFactor)
@@ -1285,7 +1300,7 @@ RppStatus shot_noise_i8_i8_host_tensor(Rpp8s *srcPtr,
         Rpp32u vectorIncrementPerChannel = 16;
 
         RpptXorwowState xorwowState;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
         __m256i pxXorwowStateX[5], pxXorwowStateCounter;
         __m256 pShotNoiseFactor, pShotNoiseFactorInv;
         rpp_host_rng_xorwow_state_offsetted_avx(xorwowInitialStatePtr, xorwowState, offset, pxXorwowStateX, &pxXorwowStateCounter);
@@ -1317,7 +1332,7 @@ RppStatus shot_noise_i8_i8_host_tensor(Rpp8s *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[6];
                     rpp_simd_load(rpp_load48_i8pkd3_to_f32pln3_avx, srcPtrTemp, p);    // simd loads
                     if (shotNoiseFactor)
@@ -1387,7 +1402,7 @@ RppStatus shot_noise_i8_i8_host_tensor(Rpp8s *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[6];
                     rpp_simd_load(rpp_load48_i8pln3_to_f32pln3_avx, srcPtrTempR, srcPtrTempG, srcPtrTempB, p);    // simd loads
                     if (shotNoiseFactor)
@@ -1451,7 +1466,7 @@ RppStatus shot_noise_i8_i8_host_tensor(Rpp8s *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[6];
                     rpp_simd_load(rpp_load48_i8pkd3_to_f32pln3_avx, srcPtrTemp, p);    // simd loads
                     if (shotNoiseFactor)
@@ -1515,7 +1530,7 @@ RppStatus shot_noise_i8_i8_host_tensor(Rpp8s *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[6];
                     rpp_simd_load(rpp_load48_i8pln3_to_f32pln3_avx, srcPtrTempR, srcPtrTempG, srcPtrTempB, p);    // simd loads
                     if (shotNoiseFactor)
@@ -1589,7 +1604,7 @@ RppStatus shot_noise_i8_i8_host_tensor(Rpp8s *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 p[2];
                     rpp_simd_load(rpp_load16_i8_to_f32_avx, srcPtrTemp, p);    // simd loads
                     if (shotNoiseFactor)
diff --git a/src/modules/cpu/kernel/non_linear_blend.hpp b/src/modules/cpu/kernel/non_linear_blend.hpp
index 6448bfe0..2e3b03fa 100644
--- a/src/modules/cpu/kernel/non_linear_blend.hpp
+++ b/src/modules/cpu/kernel/non_linear_blend.hpp
@@ -23,13 +23,31 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 // -------------------- non_linear_blend host helpers --------------------
 
 inline void compute_non_linear_blend_48_host(__m256 *p1, __m256 *p2, __m256 &pMultiplier, __m256 &pILocComponent, __m256 &pJLocComponent)
 {
+#ifdef __loongarch_asx
+    __m256 pGaussianValue;
+    pGaussianValue = fast_exp_avx(__lasx_xvfmadd_s(__lasx_xvfmul_s(pJLocComponent, pJLocComponent), pMultiplier, pILocComponent));
+    p1[0] = __lasx_xvfmadd_s(__lasx_xvfsub_s(p1[0], p2[0]), pGaussianValue, p2[0]);    // non_linear_blend adjustment
+    p1[2] = __lasx_xvfmadd_s(__lasx_xvfsub_s(p1[2], p2[2]), pGaussianValue, p2[2]);    // non_linear_blend adjustment
+    p1[4] = __lasx_xvfmadd_s(__lasx_xvfsub_s(p1[4], p2[4]), pGaussianValue, p2[4]);    // non_linear_blend adjustment
+    pJLocComponent = __lasx_xvfadd_s(pJLocComponent, avx_p8);
+    pGaussianValue = fast_exp_avx(__lasx_xvfmadd_s(__lasx_xvfmul_s(pJLocComponent, pJLocComponent), pMultiplier, pILocComponent));
+    p1[1] = __lasx_xvfmadd_s(__lasx_xvfsub_s(p1[1], p2[1]), pGaussianValue, p2[1]);    // non_linear_blend adjustment
+    p1[3] = __lasx_xvfmadd_s(__lasx_xvfsub_s(p1[3], p2[3]), pGaussianValue, p2[3]);    // non_linear_blend adjustment
+    p1[5] = __lasx_xvfmadd_s(__lasx_xvfsub_s(p1[5], p2[5]), pGaussianValue, p2[5]);    // non_linear_blend adjustment
+    pJLocComponent = __lasx_xvfadd_s(pJLocComponent, avx_p8);
+#else
     __m256 pGaussianValue;
     pGaussianValue = fast_exp_avx(_mm256_fmadd_ps(_mm256_mul_ps(pJLocComponent, pJLocComponent), pMultiplier, pILocComponent));
     p1[0] = _mm256_fmadd_ps(_mm256_sub_ps(p1[0], p2[0]), pGaussianValue, p2[0]);    // non_linear_blend adjustment
@@ -41,20 +59,39 @@ inline void compute_non_linear_blend_48_host(__m256 *p1, __m256 *p2, __m256 &pMu
     p1[3] = _mm256_fmadd_ps(_mm256_sub_ps(p1[3], p2[3]), pGaussianValue, p2[3]);    // non_linear_blend adjustment
     p1[5] = _mm256_fmadd_ps(_mm256_sub_ps(p1[5], p2[5]), pGaussianValue, p2[5]);    // non_linear_blend adjustment
     pJLocComponent = _mm256_add_ps(pJLocComponent, avx_p8);
+#endif
 }
 
 inline void compute_non_linear_blend_24_host(__m256 *p1, __m256 *p2, __m256 &pMultiplier, __m256 &pILocComponent, __m256 &pJLocComponent)
 {
+#ifdef __loongarch_asx
+    __m256 pGaussianValue;
+    pGaussianValue = fast_exp_avx(__lasx_xvfmadd_s(__lasx_xvfmul_s(pJLocComponent, pJLocComponent), pMultiplier, pILocComponent));
+    p1[0] = __lasx_xvfmadd_s(__lasx_xvfsub_s(p1[0], p2[0]), pGaussianValue, p2[0]);    // non_linear_blend adjustment
+    p1[1] = __lasx_xvfmadd_s(__lasx_xvfsub_s(p1[1], p2[1]), pGaussianValue, p2[1]);    // non_linear_blend adjustment
+    p1[2] = __lasx_xvfmadd_s(__lasx_xvfsub_s(p1[2], p2[2]), pGaussianValue, p2[2]);    // non_linear_blend adjustment
+    pJLocComponent = __lasx_xvfadd_s(pJLocComponent, avx_p8);
+#else
     __m256 pGaussianValue;
     pGaussianValue = fast_exp_avx(_mm256_fmadd_ps(_mm256_mul_ps(pJLocComponent, pJLocComponent), pMultiplier, pILocComponent));
     p1[0] = _mm256_fmadd_ps(_mm256_sub_ps(p1[0], p2[0]), pGaussianValue, p2[0]);    // non_linear_blend adjustment
     p1[1] = _mm256_fmadd_ps(_mm256_sub_ps(p1[1], p2[1]), pGaussianValue, p2[1]);    // non_linear_blend adjustment
     p1[2] = _mm256_fmadd_ps(_mm256_sub_ps(p1[2], p2[2]), pGaussianValue, p2[2]);    // non_linear_blend adjustment
     pJLocComponent = _mm256_add_ps(pJLocComponent, avx_p8);
+#endif
 }
 
 inline void compute_non_linear_blend_16_host(__m256 *p1, __m256 *p2, __m256 &pMultiplier, __m256 &pILocComponent, __m256 &pJLocComponent)
 {
+#ifdef __loongarch_asx
+    __m256 pGaussianValue;
+    pGaussianValue = fast_exp_avx(__lasx_xvfmadd_s(__lasx_xvfmul_s(pJLocComponent, pJLocComponent), pMultiplier, pILocComponent));
+    p1[0] = __lasx_xvfmadd_s(__lasx_xvfsub_s(p1[0], p2[0]), pGaussianValue, p2[0]);    // non_linear_blend adjustment
+    pJLocComponent = __lasx_xvfadd_s(pJLocComponent, avx_p8);
+    pGaussianValue = fast_exp_avx(__lasx_xvfmadd_s(__lasx_xvfmul_s(pJLocComponent, pJLocComponent), pMultiplier, pILocComponent));
+    p1[1] = __lasx_xvfmadd_s(__lasx_xvfsub_s(p1[1], p2[1]), pGaussianValue, p2[1]);    // non_linear_blend adjustment
+    pJLocComponent = __lasx_xvfadd_s(pJLocComponent, avx_p8);
+#else
     __m256 pGaussianValue;
     pGaussianValue = fast_exp_avx(_mm256_fmadd_ps(_mm256_mul_ps(pJLocComponent, pJLocComponent), pMultiplier, pILocComponent));
     p1[0] = _mm256_fmadd_ps(_mm256_sub_ps(p1[0], p2[0]), pGaussianValue, p2[0]);    // non_linear_blend adjustment
@@ -62,14 +99,22 @@ inline void compute_non_linear_blend_16_host(__m256 *p1, __m256 *p2, __m256 &pMu
     pGaussianValue = fast_exp_avx(_mm256_fmadd_ps(_mm256_mul_ps(pJLocComponent, pJLocComponent), pMultiplier, pILocComponent));
     p1[1] = _mm256_fmadd_ps(_mm256_sub_ps(p1[1], p2[1]), pGaussianValue, p2[1]);    // non_linear_blend adjustment
     pJLocComponent = _mm256_add_ps(pJLocComponent, avx_p8);
+#endif
 }
 
 inline void compute_non_linear_blend_8_host(__m256 *p1, __m256 *p2, __m256 &pMultiplier, __m256 &pILocComponent, __m256 &pJLocComponent)
 {
+#ifdef __loongarch_asx
+    __m256 pGaussianValue;
+    pGaussianValue = fast_exp_avx(__lasx_xvfmadd_s(__lasx_xvfmul_s(pJLocComponent, pJLocComponent), pMultiplier, pILocComponent));
+    p1[0] = __lasx_xvfmadd_s(__lasx_xvfsub_s(p1[0], p2[0]), pGaussianValue, p2[0]);    // non_linear_blend adjustment
+    pJLocComponent = __lasx_xvfadd_s(pJLocComponent, avx_p8);
+#else
     __m256 pGaussianValue;
     pGaussianValue = fast_exp_avx(_mm256_fmadd_ps(_mm256_mul_ps(pJLocComponent, pJLocComponent), pMultiplier, pILocComponent));
     p1[0] = _mm256_fmadd_ps(_mm256_sub_ps(p1[0], p2[0]), pGaussianValue, p2[0]);    // non_linear_blend adjustment
     pJLocComponent = _mm256_add_ps(pJLocComponent, avx_p8);
+#endif
 }
 
 // -------------------- non_linear_blend host executors --------------------
@@ -114,8 +159,13 @@ RppStatus non_linear_blend_u8_u8_host_tensor(Rpp8u *srcPtr1,
         srcPtr2Channel = srcPtr2Image + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
         dstPtrChannel = dstPtrImage;
 
+#ifdef __loongarch_asx
+        __m256 pMultiplier = lasx_set1_f32(multiplier);
+        __m256 pHalfWidth = lasx_set1_f32(halfWidth);
+#else
         __m256 pMultiplier = _mm256_set1_ps(multiplier);
         __m256 pHalfWidth = _mm256_set1_ps(halfWidth);
+#endif
 
         // Non linear blend with fused output-layout toggle (NHWC -> NCHW)
         if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
@@ -138,8 +188,13 @@ RppStatus non_linear_blend_u8_u8_host_tensor(Rpp8u *srcPtr1,
 
                 Rpp32s iLoc = i - halfHeight;
                 Rpp32f iLocComponent = iLoc * iLoc * multiplier;
+#ifdef __loongarch_asx
+                __m256 pILocComponent = lasx_set1_f32(iLocComponent);
+                __m256 pJLocComponent = __lasx_xvfsub_s(lasx_setr_f32(0, 1, 2, 3, 4, 5, 6, 7), pHalfWidth);
+#else
                 __m256 pILocComponent = _mm256_set1_ps(iLocComponent);
                 __m256 pJLocComponent = _mm256_sub_ps(_mm256_setr_ps(0, 1, 2, 3, 4, 5, 6, 7), pHalfWidth);
+#endif
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 16)
@@ -205,8 +260,13 @@ RppStatus non_linear_blend_u8_u8_host_tensor(Rpp8u *srcPtr1,
 
                 Rpp32s iLoc = i - halfHeight;
                 Rpp32f iLocComponent = iLoc * iLoc * multiplier;
+#ifdef __loongarch_asx
+                __m256 pILocComponent = lasx_set1_f32(iLocComponent);
+                __m256 pJLocComponent = __lasx_xvfsub_s(lasx_setr_f32(0, 1, 2, 3, 4, 5, 6, 7), pHalfWidth);
+#else
                 __m256 pILocComponent = _mm256_set1_ps(iLocComponent);
                 __m256 pJLocComponent = _mm256_sub_ps(_mm256_setr_ps(0, 1, 2, 3, 4, 5, 6, 7), pHalfWidth);
+#endif
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 16)
@@ -270,8 +330,13 @@ RppStatus non_linear_blend_u8_u8_host_tensor(Rpp8u *srcPtr1,
 
                 Rpp32s iLoc = i - halfHeight;
                 Rpp32f iLocComponent = iLoc * iLoc * multiplier;
+#ifdef __loongarch_asx
+                __m256 pILocComponent = lasx_set1_f32(iLocComponent);
+                __m256 pJLocComponent = __lasx_xvfsub_s(lasx_setr_f32(0, 1, 2, 3, 4, 5, 6, 7), pHalfWidth);
+#else
                 __m256 pILocComponent = _mm256_set1_ps(iLocComponent);
                 __m256 pJLocComponent = _mm256_sub_ps(_mm256_setr_ps(0, 1, 2, 3, 4, 5, 6, 7), pHalfWidth);
+#endif
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 16)
@@ -335,8 +400,13 @@ RppStatus non_linear_blend_u8_u8_host_tensor(Rpp8u *srcPtr1,
 
                 Rpp32s iLoc = i - halfHeight;
                 Rpp32f iLocComponent = iLoc * iLoc * multiplier;
+#ifdef __loongarch_asx
+                __m256 pILocComponent = lasx_set1_f32(iLocComponent);
+                __m256 pJLocComponent = __lasx_xvfsub_s(lasx_setr_f32(0, 1, 2, 3, 4, 5, 6, 7), pHalfWidth);
+#else
                 __m256 pILocComponent = _mm256_set1_ps(iLocComponent);
                 __m256 pJLocComponent = _mm256_sub_ps(_mm256_setr_ps(0, 1, 2, 3, 4, 5, 6, 7), pHalfWidth);
+#endif
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 16)
@@ -406,8 +476,13 @@ RppStatus non_linear_blend_u8_u8_host_tensor(Rpp8u *srcPtr1,
 
                 Rpp32s iLoc = i - halfHeight;
                 Rpp32f iLocComponent = iLoc * iLoc * multiplier;
+#ifdef __loongarch_asx
+                __m256 pILocComponent = lasx_set1_f32(iLocComponent);
+                __m256 pJLocComponent = __lasx_xvfsub_s(lasx_setr_f32(0, 1, 2, 3, 4, 5, 6, 7), pHalfWidth);
+#else
                 __m256 pILocComponent = _mm256_set1_ps(iLocComponent);
                 __m256 pJLocComponent = _mm256_sub_ps(_mm256_setr_ps(0, 1, 2, 3, 4, 5, 6, 7), pHalfWidth);
+#endif
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 16)
@@ -482,8 +557,13 @@ RppStatus non_linear_blend_f32_f32_host_tensor(Rpp32f *srcPtr1,
         srcPtr2Channel = srcPtr2Image + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
         dstPtrChannel = dstPtrImage;
 
+#ifdef __loongarch_asx
+        __m256 pMultiplier = lasx_set1_f32(multiplier);
+        __m256 pHalfWidth = lasx_set1_f32(halfWidth);
+#else
         __m256 pMultiplier = _mm256_set1_ps(multiplier);
         __m256 pHalfWidth = _mm256_set1_ps(halfWidth);
+#endif
 
         // Non linear blend with fused output-layout toggle (NHWC -> NCHW)
         if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
@@ -506,8 +586,13 @@ RppStatus non_linear_blend_f32_f32_host_tensor(Rpp32f *srcPtr1,
 
                 Rpp32s iLoc = i - halfHeight;
                 Rpp32f iLocComponent = iLoc * iLoc * multiplier;
+#ifdef __loongarch_asx
+                __m256 pILocComponent = lasx_set1_f32(iLocComponent);
+                __m256 pJLocComponent = __lasx_xvfsub_s(lasx_setr_f32(0, 1, 2, 3, 4, 5, 6, 7), pHalfWidth);
+#else
                 __m256 pILocComponent = _mm256_set1_ps(iLocComponent);
                 __m256 pJLocComponent = _mm256_sub_ps(_mm256_setr_ps(0, 1, 2, 3, 4, 5, 6, 7), pHalfWidth);
+#endif
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
@@ -573,8 +658,13 @@ RppStatus non_linear_blend_f32_f32_host_tensor(Rpp32f *srcPtr1,
 
                 Rpp32s iLoc = i - halfHeight;
                 Rpp32f iLocComponent = iLoc * iLoc * multiplier;
+#ifdef __loongarch_asx
+                __m256 pILocComponent = lasx_set1_f32(iLocComponent);
+                __m256 pJLocComponent = __lasx_xvfsub_s(lasx_setr_f32(0, 1, 2, 3, 4, 5, 6, 7), pHalfWidth);
+#else
                 __m256 pILocComponent = _mm256_set1_ps(iLocComponent);
                 __m256 pJLocComponent = _mm256_sub_ps(_mm256_setr_ps(0, 1, 2, 3, 4, 5, 6, 7), pHalfWidth);
+#endif
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
@@ -638,8 +728,13 @@ RppStatus non_linear_blend_f32_f32_host_tensor(Rpp32f *srcPtr1,
 
                 Rpp32s iLoc = i - halfHeight;
                 Rpp32f iLocComponent = iLoc * iLoc * multiplier;
+#ifdef __loongarch_asx
+                __m256 pILocComponent = lasx_set1_f32(iLocComponent);
+                __m256 pJLocComponent = __lasx_xvfsub_s(lasx_setr_f32(0, 1, 2, 3, 4, 5, 6, 7), pHalfWidth);
+#else
                 __m256 pILocComponent = _mm256_set1_ps(iLocComponent);
                 __m256 pJLocComponent = _mm256_sub_ps(_mm256_setr_ps(0, 1, 2, 3, 4, 5, 6, 7), pHalfWidth);
+#endif
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
@@ -703,8 +798,13 @@ RppStatus non_linear_blend_f32_f32_host_tensor(Rpp32f *srcPtr1,
 
                 Rpp32s iLoc = i - halfHeight;
                 Rpp32f iLocComponent = iLoc * iLoc * multiplier;
+#ifdef __loongarch_asx
+                __m256 pILocComponent = lasx_set1_f32(iLocComponent);
+                __m256 pJLocComponent = __lasx_xvfsub_s(lasx_setr_f32(0, 1, 2, 3, 4, 5, 6, 7), pHalfWidth);
+#else
                 __m256 pILocComponent = _mm256_set1_ps(iLocComponent);
                 __m256 pJLocComponent = _mm256_sub_ps(_mm256_setr_ps(0, 1, 2, 3, 4, 5, 6, 7), pHalfWidth);
+#endif
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
@@ -774,8 +874,13 @@ RppStatus non_linear_blend_f32_f32_host_tensor(Rpp32f *srcPtr1,
 
                 Rpp32s iLoc = i - halfHeight;
                 Rpp32f iLocComponent = iLoc * iLoc * multiplier;
+#ifdef __loongarch_asx
+                __m256 pILocComponent = lasx_set1_f32(iLocComponent);
+                __m256 pJLocComponent = __lasx_xvfsub_s(lasx_setr_f32(0, 1, 2, 3, 4, 5, 6, 7), pHalfWidth);
+#else
                 __m256 pILocComponent = _mm256_set1_ps(iLocComponent);
                 __m256 pJLocComponent = _mm256_sub_ps(_mm256_setr_ps(0, 1, 2, 3, 4, 5, 6, 7), pHalfWidth);
+#endif
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
@@ -850,8 +955,13 @@ RppStatus non_linear_blend_i8_i8_host_tensor(Rpp8s *srcPtr1,
         srcPtr2Channel = srcPtr2Image + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
         dstPtrChannel = dstPtrImage;
 
+#ifdef __loongarch_asx
+        __m256 pMultiplier = lasx_set1_f32(multiplier);
+        __m256 pHalfWidth = lasx_set1_f32(halfWidth);
+#else
         __m256 pMultiplier = _mm256_set1_ps(multiplier);
         __m256 pHalfWidth = _mm256_set1_ps(halfWidth);
+#endif
 
         // Non linear blend with fused output-layout toggle (NHWC -> NCHW)
         if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
@@ -874,8 +984,13 @@ RppStatus non_linear_blend_i8_i8_host_tensor(Rpp8s *srcPtr1,
 
                 Rpp32s iLoc = i - halfHeight;
                 Rpp32f iLocComponent = iLoc * iLoc * multiplier;
+#ifdef __loongarch_asx
+                __m256 pILocComponent = lasx_set1_f32(iLocComponent);
+                __m256 pJLocComponent = __lasx_xvfsub_s(lasx_setr_f32(0, 1, 2, 3, 4, 5, 6, 7), pHalfWidth);
+#else
                 __m256 pILocComponent = _mm256_set1_ps(iLocComponent);
                 __m256 pJLocComponent = _mm256_sub_ps(_mm256_setr_ps(0, 1, 2, 3, 4, 5, 6, 7), pHalfWidth);
+#endif
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 16)
@@ -941,8 +1056,13 @@ RppStatus non_linear_blend_i8_i8_host_tensor(Rpp8s *srcPtr1,
 
                 Rpp32s iLoc = i - halfHeight;
                 Rpp32f iLocComponent = iLoc * iLoc * multiplier;
+#ifdef __loongarch_asx
+                __m256 pILocComponent = lasx_set1_f32(iLocComponent);
+                __m256 pJLocComponent = __lasx_xvfsub_s(lasx_setr_f32(0, 1, 2, 3, 4, 5, 6, 7), pHalfWidth);
+#else
                 __m256 pILocComponent = _mm256_set1_ps(iLocComponent);
                 __m256 pJLocComponent = _mm256_sub_ps(_mm256_setr_ps(0, 1, 2, 3, 4, 5, 6, 7), pHalfWidth);
+#endif
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 16)
@@ -1006,8 +1126,13 @@ RppStatus non_linear_blend_i8_i8_host_tensor(Rpp8s *srcPtr1,
 
                 Rpp32s iLoc = i - halfHeight;
                 Rpp32f iLocComponent = iLoc * iLoc * multiplier;
+#ifdef __loongarch_asx
+                __m256 pILocComponent = lasx_set1_f32(iLocComponent);
+                __m256 pJLocComponent = __lasx_xvfsub_s(lasx_setr_f32(0, 1, 2, 3, 4, 5, 6, 7), pHalfWidth);
+#else
                 __m256 pILocComponent = _mm256_set1_ps(iLocComponent);
                 __m256 pJLocComponent = _mm256_sub_ps(_mm256_setr_ps(0, 1, 2, 3, 4, 5, 6, 7), pHalfWidth);
+#endif
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 16)
@@ -1071,8 +1196,13 @@ RppStatus non_linear_blend_i8_i8_host_tensor(Rpp8s *srcPtr1,
 
                 Rpp32s iLoc = i - halfHeight;
                 Rpp32f iLocComponent = iLoc * iLoc * multiplier;
+#ifdef __loongarch_asx
+                __m256 pILocComponent = lasx_set1_f32(iLocComponent);
+                __m256 pJLocComponent = __lasx_xvfsub_s(lasx_setr_f32(0, 1, 2, 3, 4, 5, 6, 7), pHalfWidth);
+#else
                 __m256 pILocComponent = _mm256_set1_ps(iLocComponent);
                 __m256 pJLocComponent = _mm256_sub_ps(_mm256_setr_ps(0, 1, 2, 3, 4, 5, 6, 7), pHalfWidth);
+#endif
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 16)
@@ -1142,8 +1272,13 @@ RppStatus non_linear_blend_i8_i8_host_tensor(Rpp8s *srcPtr1,
 
                 Rpp32s iLoc = i - halfHeight;
                 Rpp32f iLocComponent = iLoc * iLoc * multiplier;
+#ifdef __loongarch_asx
+                __m256 pILocComponent = lasx_set1_f32(iLocComponent);
+                __m256 pJLocComponent = __lasx_xvfsub_s(lasx_setr_f32(0, 1, 2, 3, 4, 5, 6, 7), pHalfWidth);
+#else
                 __m256 pILocComponent = _mm256_set1_ps(iLocComponent);
                 __m256 pJLocComponent = _mm256_sub_ps(_mm256_setr_ps(0, 1, 2, 3, 4, 5, 6, 7), pHalfWidth);
+#endif
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 16)
@@ -1218,8 +1353,13 @@ RppStatus non_linear_blend_f16_f16_host_tensor(Rpp16f *srcPtr1,
         srcPtr2Channel = srcPtr2Image + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
         dstPtrChannel = dstPtrImage;
 
+#ifdef __loongarch_asx
+        __m256 pMultiplier = lasx_set1_f32(multiplier);
+        __m256 pHalfWidth = lasx_set1_f32(halfWidth);
+#else
         __m256 pMultiplier = _mm256_set1_ps(multiplier);
         __m256 pHalfWidth = _mm256_set1_ps(halfWidth);
+#endif
 
         // Non linear blend with fused output-layout toggle (NHWC -> NCHW)
         if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
@@ -1242,8 +1382,13 @@ RppStatus non_linear_blend_f16_f16_host_tensor(Rpp16f *srcPtr1,
 
                 Rpp32s iLoc = i - halfHeight;
                 Rpp32f iLocComponent = iLoc * iLoc * multiplier;
+#ifdef __loongarch_asx
+                __m256 pILocComponent = lasx_set1_f32(iLocComponent);
+                __m256 pJLocComponent = __lasx_xvfsub_s(lasx_setr_f32(0, 1, 2, 3, 4, 5, 6, 7), pHalfWidth);
+#else
                 __m256 pILocComponent = _mm256_set1_ps(iLocComponent);
                 __m256 pJLocComponent = _mm256_sub_ps(_mm256_setr_ps(0, 1, 2, 3, 4, 5, 6, 7), pHalfWidth);
+#endif
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
@@ -1322,8 +1467,13 @@ RppStatus non_linear_blend_f16_f16_host_tensor(Rpp16f *srcPtr1,
 
                 Rpp32s iLoc = i - halfHeight;
                 Rpp32f iLocComponent = iLoc * iLoc * multiplier;
+#ifdef __loongarch_asx
+                __m256 pILocComponent = lasx_set1_f32(iLocComponent);
+                __m256 pJLocComponent = __lasx_xvfsub_s(lasx_setr_f32(0, 1, 2, 3, 4, 5, 6, 7), pHalfWidth);
+#else
                 __m256 pILocComponent = _mm256_set1_ps(iLocComponent);
                 __m256 pJLocComponent = _mm256_sub_ps(_mm256_setr_ps(0, 1, 2, 3, 4, 5, 6, 7), pHalfWidth);
+#endif
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
@@ -1401,8 +1551,13 @@ RppStatus non_linear_blend_f16_f16_host_tensor(Rpp16f *srcPtr1,
 
                 Rpp32s iLoc = i - halfHeight;
                 Rpp32f iLocComponent = iLoc * iLoc * multiplier;
+#ifdef __loongarch_asx
+                __m256 pILocComponent = lasx_set1_f32(iLocComponent);
+                __m256 pJLocComponent = __lasx_xvfsub_s(lasx_setr_f32(0, 1, 2, 3, 4, 5, 6, 7), pHalfWidth);
+#else
                 __m256 pILocComponent = _mm256_set1_ps(iLocComponent);
                 __m256 pJLocComponent = _mm256_sub_ps(_mm256_setr_ps(0, 1, 2, 3, 4, 5, 6, 7), pHalfWidth);
+#endif
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
@@ -1475,8 +1630,13 @@ RppStatus non_linear_blend_f16_f16_host_tensor(Rpp16f *srcPtr1,
 
                 Rpp32s iLoc = i - halfHeight;
                 Rpp32f iLocComponent = iLoc * iLoc * multiplier;
+#ifdef __loongarch_asx
+                __m256 pILocComponent = lasx_set1_f32(iLocComponent);
+                __m256 pJLocComponent = __lasx_xvfsub_s(lasx_setr_f32(0, 1, 2, 3, 4, 5, 6, 7), pHalfWidth);
+#else
                 __m256 pILocComponent = _mm256_set1_ps(iLocComponent);
                 __m256 pJLocComponent = _mm256_sub_ps(_mm256_setr_ps(0, 1, 2, 3, 4, 5, 6, 7), pHalfWidth);
+#endif
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
@@ -1564,8 +1724,13 @@ RppStatus non_linear_blend_f16_f16_host_tensor(Rpp16f *srcPtr1,
 
                 Rpp32s iLoc = i - halfHeight;
                 Rpp32f iLocComponent = iLoc * iLoc * multiplier;
+#ifdef __loongarch_asx
+                __m256 pILocComponent = lasx_set1_f32(iLocComponent);
+                __m256 pJLocComponent = __lasx_xvfsub_s(lasx_setr_f32(0, 1, 2, 3, 4, 5, 6, 7), pHalfWidth);
+#else
                 __m256 pILocComponent = _mm256_set1_ps(iLocComponent);
                 __m256 pJLocComponent = _mm256_sub_ps(_mm256_setr_ps(0, 1, 2, 3, 4, 5, 6, 7), pHalfWidth);
+#endif
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
diff --git a/src/modules/cpu/kernel/normalize.hpp b/src/modules/cpu/kernel/normalize.hpp
index a80f6b1a..958ffbf8 100644
--- a/src/modules/cpu/kernel/normalize.hpp
+++ b/src/modules/cpu/kernel/normalize.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 // Recursive reduction helper function to compute difference of input with mean and squares them up
 template<typename T>
@@ -325,11 +330,19 @@ void normalize_3D_tensor_avx_axis3(Rpp32f *srcPtr, RpptGenericDescPtr srcGeneric
     Rpp32u outerDim = length[0];
 
     // set shift, mean and stddev
+#ifdef __loongarch_asx
+    __m256 pShift = lasx_set1_f32(shift);
+    __m256 pMean1 = (__m256)__lasx_xvld(meanPtr, 0);
+    __m256 pMean2 = (__m256)__lasx_xvld(meanPtr + 8, 0);
+    __m256 pMultiplier1 = (__m256)__lasx_xvld(multiplierPtr, 0);
+    __m256 pMultiplier2 = (__m256)__lasx_xvld(multiplierPtr + 8, 0);
+#else
     __m256 pShift = _mm256_set1_ps(shift);
     __m256 pMean1 = _mm256_loadu_ps(meanPtr);
     __m256 pMean2 = _mm256_loadu_ps(meanPtr + 8);
     __m256 pMultiplier1 = _mm256_loadu_ps(multiplierPtr);
     __m256 pMultiplier2 = _mm256_loadu_ps(multiplierPtr + 8);
+#endif
 
     for(Rpp32u i = 0; i < outerDim; i++)
     {
@@ -339,12 +352,21 @@ void normalize_3D_tensor_avx_axis3(Rpp32f *srcPtr, RpptGenericDescPtr srcGeneric
         Rpp32u vectorLoopCount = 0;
         for(; vectorLoopCount < alignedLength ; vectorLoopCount += vectorIncrement)
         {
+#ifdef __loongarch_asx
+            __m256 pSrc1 = (__m256)__lasx_xvld(srcPtrTemp, 0);
+            __m256 pSrc2 = (__m256)__lasx_xvld(srcPtrTemp + 8, 0);
+            __m256 pDst1 = __lasx_xvfadd_s(__lasx_xvfmul_s(__lasx_xvfsub_s(pSrc1, pMean1), pMultiplier1), pShift);
+            __m256 pDst2 = __lasx_xvfadd_s(__lasx_xvfmul_s(__lasx_xvfsub_s(pSrc2, pMean2), pMultiplier2), pShift);
+            __lasx_xvst((__m256i)pDst1, dstPtrTemp, 0);
+            __lasx_xvst((__m256i)pDst2, dstPtrTemp + 8, 0);
+#else
             __m256 pSrc1 = _mm256_loadu_ps(srcPtrTemp);
             __m256 pSrc2 = _mm256_loadu_ps(srcPtrTemp + 8);
             __m256 pDst1 = _mm256_add_ps(_mm256_mul_ps(_mm256_sub_ps(pSrc1, pMean1), pMultiplier1), pShift);
             __m256 pDst2 = _mm256_add_ps(_mm256_mul_ps(_mm256_sub_ps(pSrc2, pMean2), pMultiplier2), pShift);
             _mm256_storeu_ps(dstPtrTemp, pDst1);
             _mm256_storeu_ps(dstPtrTemp + 8, pDst2);
+#endif
             srcPtrTemp += vectorIncrement;
             dstPtrTemp += vectorIncrement;
         }
@@ -424,7 +446,11 @@ void normalize_2D_tensor(Rpp32f *srcPtr, RpptGenericDescPtr srcDescPtr, Rpp32f *
         Rpp32u bufferLength = dims[1];
         Rpp32u alignedLength = (bufferLength / 8) * 8;
 
+#ifdef __loongarch_asx
+        __m256 pShift = lasx_set1_f32(shift);
+#else
         __m256 pShift = _mm256_set1_ps(shift);
+#endif
         for(Rpp32u i = 0; i < dims[0]; i++)
         {
             Rpp32f *srcPtrTemp = srcPtr + i * srcDescPtr->strides[1];
@@ -433,6 +459,21 @@ void normalize_2D_tensor(Rpp32f *srcPtr, RpptGenericDescPtr srcDescPtr, Rpp32f *
             // set mean and stddev
             Rpp32f mean = meanPtr[i];
             Rpp32f invStdDev = invStdDevPtr[i];
+#ifdef __loongarch_asx
+            __m256 pMean, pInvStdDev;
+            pMean = lasx_set1_f32(mean);
+            pInvStdDev = lasx_set1_f32(invStdDev);
+
+            Rpp32u vectorLoopCount = 0;
+            for(; vectorLoopCount < alignedLength ; vectorLoopCount += vectorIncrement)
+            {
+                __m256 pSrc = (__m256)__lasx_xvld(srcPtrTemp, 0);
+                __m256 pDst = __lasx_xvfadd_s(__lasx_xvfmul_s(__lasx_xvfsub_s(pSrc, pMean), pInvStdDev), pShift);
+                __lasx_xvst((__m256i)pDst, dstPtrTemp, 0);
+                srcPtrTemp += vectorIncrement;
+                dstPtrTemp += vectorIncrement;
+            }
+#else
             __m256 pMean, pInvStdDev;
             pMean = _mm256_set1_ps(mean);
             pInvStdDev = _mm256_set1_ps(invStdDev);
@@ -446,6 +487,7 @@ void normalize_2D_tensor(Rpp32f *srcPtr, RpptGenericDescPtr srcDescPtr, Rpp32f *
                 srcPtrTemp += vectorIncrement;
                 dstPtrTemp += vectorIncrement;
             }
+#endif
             for(; vectorLoopCount < dims[1] ; vectorLoopCount ++)
                 *dstPtrTemp++ = (*srcPtrTemp++ - mean) * invStdDev + shift;
         }
diff --git a/src/modules/cpu/kernel/phase.hpp b/src/modules/cpu/kernel/phase.hpp
index 95505369..f88ccab3 100644
--- a/src/modules/cpu/kernel/phase.hpp
+++ b/src/modules/cpu/kernel/phase.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 RppStatus phase_u8_u8_host_tensor(Rpp8u *srcPtr1,
                                   Rpp8u *srcPtr2,
@@ -61,11 +66,16 @@ RppStatus phase_u8_u8_host_tensor(Rpp8u *srcPtr1,
         srcPtr2Channel = srcPtr2Image + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
         dstPtrChannel = dstPtrImage;
 
-#if __AVX2__
+#if defined(__AVX2__)
         __m256 pMul = _mm256_set1_ps(multiplier);
         Rpp32u alignedLength = (bufferLength / 48) * 48;
         Rpp32u vectorIncrement = 48;
         Rpp32u vectorIncrementPerChannel = 16;
+#elif defined(__loongarch_asx)
+        __m256 pMul = lasx_set1_f32(multiplier);
+        Rpp32u alignedLength = (bufferLength / 48) * 48;
+        Rpp32u vectorIncrement = 48;
+        Rpp32u vectorIncrementPerChannel = 16;
 #endif
 
         // Phase with fused output-layout toggle (NHWC -> NCHW)
@@ -88,7 +98,7 @@ RppStatus phase_u8_u8_host_tensor(Rpp8u *srcPtr1,
                 dstPtrTempB = dstPtrRowB;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256 p1[6], p2[6];
@@ -103,6 +113,27 @@ RppStatus phase_u8_u8_host_tensor(Rpp8u *srcPtr1,
                     p1[5] = _mm256_round_ps(_mm256_mul_ps(atan2_ps(p1[5], p2[5]), pMul), _MM_FROUND_TO_ZERO);    // phase computation
                     rpp_simd_store(rpp_store48_f32pln3_to_u8pln3_avx, dstPtrTempR, dstPtrTempG, dstPtrTempB, p1);    // simd stores
 
+                    srcPtr1Temp += vectorIncrement;
+                    srcPtr2Temp += vectorIncrement;
+                    dstPtrTempR += vectorIncrementPerChannel;
+                    dstPtrTempG += vectorIncrementPerChannel;
+                    dstPtrTempB += vectorIncrementPerChannel;
+                }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
+                {
+                    __m256 p1[6], p2[6];
+
+                    rpp_simd_load(rpp_load48_u8pkd3_to_f32pln3_avx, srcPtr1Temp, p1);    // simd loads
+                    rpp_simd_load(rpp_load48_u8pkd3_to_f32pln3_avx, srcPtr2Temp, p2);    // simd loads
+                    p1[0] = __lasx_xvfrintrxxx_s(__lasx_xvfmul_s(atan2_ps(p1[0], p2[0]), pMul), _MM_FROUND_TO_ZERO);    // phase computation
+                    p1[1] = __lasx_xvfrintrxxx_s(__lasx_xvfmul_s(atan2_ps(p1[1], p2[1]), pMul), _MM_FROUND_TO_ZERO);    // phase computation
+                    p1[2] = __lasx_xvfrintrxxx_s(__lasx_xvfmul_s(atan2_ps(p1[2], p2[2]), pMul), _MM_FROUND_TO_ZERO);    // phase computation
+                    p1[3] = __lasx_xvfrintrxxx_s(__lasx_xvfmul_s(atan2_ps(p1[3], p2[3]), pMul), _MM_FROUND_TO_ZERO);    // phase computation
+                    p1[4] = __lasx_xvfrintrxxx_s(__lasx_xvfmul_s(atan2_ps(p1[4], p2[4]), pMul), _MM_FROUND_TO_ZERO);    // phase computation
+                    p1[5] = __lasx_xvfrintrxxx_s(__lasx_xvfmul_s(atan2_ps(p1[5], p2[5]), pMul), _MM_FROUND_TO_ZERO);    // phase computation
+                    rpp_simd_store(rpp_store48_f32pln3_to_u8pln3_avx, dstPtrTempR, dstPtrTempG, dstPtrTempB, p1);    // simd stores
+
                     srcPtr1Temp += vectorIncrement;
                     srcPtr2Temp += vectorIncrement;
                     dstPtrTempR += vectorIncrementPerChannel;
@@ -152,7 +183,7 @@ RppStatus phase_u8_u8_host_tensor(Rpp8u *srcPtr1,
                 dstPtrTemp = dstPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 p1[6], p2[6];
@@ -167,6 +198,29 @@ RppStatus phase_u8_u8_host_tensor(Rpp8u *srcPtr1,
                     p1[5] = _mm256_round_ps(_mm256_mul_ps(atan2_ps(p1[5], p2[5]), pMul), _MM_FROUND_TO_ZERO);    // phase computation
                     rpp_simd_store(rpp_store48_f32pln3_to_u8pkd3_avx, dstPtrTemp, p1);    // simd stores
 
+                    srcPtr1TempR += vectorIncrementPerChannel;
+                    srcPtr1TempG += vectorIncrementPerChannel;
+                    srcPtr1TempB += vectorIncrementPerChannel;
+                    srcPtr2TempR += vectorIncrementPerChannel;
+                    srcPtr2TempG += vectorIncrementPerChannel;
+                    srcPtr2TempB += vectorIncrementPerChannel;
+                    dstPtrTemp += vectorIncrement;
+                }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256 p1[6], p2[6];
+
+                    rpp_simd_load(rpp_load48_u8pln3_to_f32pln3_avx, srcPtr1TempR, srcPtr1TempG, srcPtr1TempB, p1);    // simd loads
+                    rpp_simd_load(rpp_load48_u8pln3_to_f32pln3_avx, srcPtr2TempR, srcPtr2TempG, srcPtr2TempB, p2);    // simd loads
+                    p1[0] = __lasx_xvfrintrxxx_s(__lasx_xvfmul_s(atan2_ps(p1[0], p2[0]), pMul), _MM_FROUND_TO_ZERO);    // phase computation
+                    p1[1] = __lasx_xvfrintrxxx_s(__lasx_xvfmul_s(atan2_ps(p1[1], p2[1]), pMul), _MM_FROUND_TO_ZERO);    // phase computation
+                    p1[2] = __lasx_xvfrintrxxx_s(__lasx_xvfmul_s(atan2_ps(p1[2], p2[2]), pMul), _MM_FROUND_TO_ZERO);    // phase computation
+                    p1[3] = __lasx_xvfrintrxxx_s(__lasx_xvfmul_s(atan2_ps(p1[3], p2[3]), pMul), _MM_FROUND_TO_ZERO);    // phase computation
+                    p1[4] = __lasx_xvfrintrxxx_s(__lasx_xvfmul_s(atan2_ps(p1[4], p2[4]), pMul), _MM_FROUND_TO_ZERO);    // phase computation
+                    p1[5] = __lasx_xvfrintrxxx_s(__lasx_xvfmul_s(atan2_ps(p1[5], p2[5]), pMul), _MM_FROUND_TO_ZERO);    // phase computation
+                    rpp_simd_store(rpp_store48_f32pln3_to_u8pkd3_avx, dstPtrTemp, p1);    // simd stores
+
                     srcPtr1TempR += vectorIncrementPerChannel;
                     srcPtr1TempG += vectorIncrementPerChannel;
                     srcPtr1TempB += vectorIncrementPerChannel;
@@ -204,7 +258,7 @@ RppStatus phase_u8_u8_host_tensor(Rpp8u *srcPtr1,
         // Phase without fused output-layout toggle (NHWC -> NHWC or NCHW -> NCHW)
         else
         {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             alignedLength = bufferLength & ~15;
 #endif
 
@@ -223,7 +277,7 @@ RppStatus phase_u8_u8_host_tensor(Rpp8u *srcPtr1,
                     dstPtrTemp = dstPtrRow;
 
                     int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                     {
                         __m256 p1[2], p2[2];
@@ -234,6 +288,21 @@ RppStatus phase_u8_u8_host_tensor(Rpp8u *srcPtr1,
                         p1[1] = _mm256_round_ps(_mm256_mul_ps(atan2_ps(p1[1], p2[1]), pMul), _MM_FROUND_TO_ZERO);    // phase computation
                         rpp_simd_store(rpp_store16_f32_to_u8_avx, dstPtrTemp, p1);    // simd stores
 
+                        srcPtr1Temp += vectorIncrementPerChannel;
+                        srcPtr2Temp += vectorIncrementPerChannel;
+                        dstPtrTemp += vectorIncrementPerChannel;
+                    }
+#elif defined(__loongarch_asx)
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                    {
+                        __m256 p1[2], p2[2];
+
+                        rpp_simd_load(rpp_load16_u8_to_f32_avx, srcPtr1Temp, p1);    // simd loads
+                        rpp_simd_load(rpp_load16_u8_to_f32_avx, srcPtr2Temp, p2);    // simd loads
+                        p1[0] = __lasx_xvfrintrxxx_s(__lasx_xvfmul_s(atan2_ps(p1[0], p2[0]), pMul), _MM_FROUND_TO_ZERO);    // phase computation
+                        p1[1] = __lasx_xvfrintrxxx_s(__lasx_xvfmul_s(atan2_ps(p1[1], p2[1]), pMul), _MM_FROUND_TO_ZERO);    // phase computation
+                        rpp_simd_store(rpp_store16_f32_to_u8_avx, dstPtrTemp, p1);    // simd stores
+
                         srcPtr1Temp += vectorIncrementPerChannel;
                         srcPtr2Temp += vectorIncrementPerChannel;
                         dstPtrTemp += vectorIncrementPerChannel;
@@ -297,11 +366,16 @@ RppStatus phase_f32_f32_host_tensor(Rpp32f *srcPtr1,
         srcPtr2Channel = srcPtr2Image + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
         dstPtrChannel = dstPtrImage;
 
-#if __AVX2__
+#if defined(__AVX2__)
         __m256 pMul = _mm256_set1_ps(multiplier);
         Rpp32u alignedLength = (bufferLength / 24) * 24;
         Rpp32u vectorIncrement = 24;
         Rpp32u vectorIncrementPerChannel = 8;
+#elif defined(__loongarch_asx)
+        __m256 pMul = lasx_set1_f32(multiplier);
+        Rpp32u alignedLength = (bufferLength / 24) * 24;
+        Rpp32u vectorIncrement = 24;
+        Rpp32u vectorIncrementPerChannel = 8;
 #endif
 
         // Phase with fused output-layout toggle (NHWC -> NCHW)
@@ -324,7 +398,7 @@ RppStatus phase_f32_f32_host_tensor(Rpp32f *srcPtr1,
                 dstPtrTempB = dstPtrRowB;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256 p1[3], p2[3];
@@ -336,6 +410,24 @@ RppStatus phase_f32_f32_host_tensor(Rpp32f *srcPtr1,
                     p1[2] = _mm256_mul_ps(atan2_ps(p1[2], p2[2]), pMul);    // phase computation
                     rpp_simd_store(rpp_store24_f32pln3_to_f32pln3_avx, dstPtrTempR, dstPtrTempG, dstPtrTempB, p1);    // simd stores
 
+                    srcPtr1Temp += vectorIncrement;
+                    srcPtr2Temp += vectorIncrement;
+                    dstPtrTempR += vectorIncrementPerChannel;
+                    dstPtrTempG += vectorIncrementPerChannel;
+                    dstPtrTempB += vectorIncrementPerChannel;
+                }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
+                {
+                    __m256 p1[3], p2[3];
+
+                    rpp_simd_load(rpp_load24_f32pkd3_to_f32pln3_avx, srcPtr1Temp, p1);    // simd loads
+                    rpp_simd_load(rpp_load24_f32pkd3_to_f32pln3_avx, srcPtr2Temp, p2);    // simd loads
+                    p1[0] = __lasx_xvfmul_s(atan2_ps(p1[0], p2[0]), pMul);    // phase computation
+                    p1[1] = __lasx_xvfmul_s(atan2_ps(p1[1], p2[1]), pMul);    // phase computation
+                    p1[2] = __lasx_xvfmul_s(atan2_ps(p1[2], p2[2]), pMul);    // phase computation
+                    rpp_simd_store(rpp_store24_f32pln3_to_f32pln3_avx, dstPtrTempR, dstPtrTempG, dstPtrTempB, p1);    // simd stores
+
                     srcPtr1Temp += vectorIncrement;
                     srcPtr2Temp += vectorIncrement;
                     dstPtrTempR += vectorIncrementPerChannel;
@@ -385,7 +477,7 @@ RppStatus phase_f32_f32_host_tensor(Rpp32f *srcPtr1,
                 dstPtrTemp = dstPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 p1[3], p2[3];
@@ -397,6 +489,26 @@ RppStatus phase_f32_f32_host_tensor(Rpp32f *srcPtr1,
                     p1[2] = _mm256_mul_ps(atan2_ps(p1[2], p2[2]), pMul);    // phase computation
                     rpp_simd_store(rpp_store24_f32pln3_to_f32pkd3_avx, dstPtrTemp, p1);    // simd stores
 
+                    srcPtr1TempR += vectorIncrementPerChannel;
+                    srcPtr1TempG += vectorIncrementPerChannel;
+                    srcPtr1TempB += vectorIncrementPerChannel;
+                    srcPtr2TempR += vectorIncrementPerChannel;
+                    srcPtr2TempG += vectorIncrementPerChannel;
+                    srcPtr2TempB += vectorIncrementPerChannel;
+                    dstPtrTemp += vectorIncrement;
+                }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256 p1[3], p2[3];
+
+                    rpp_simd_load(rpp_load24_f32pln3_to_f32pln3_avx, srcPtr1TempR, srcPtr1TempG, srcPtr1TempB, p1);    // simd loads
+                    rpp_simd_load(rpp_load24_f32pln3_to_f32pln3_avx, srcPtr2TempR, srcPtr2TempG, srcPtr2TempB, p2);    // simd loads
+                    p1[0] = __lasx_xvfmul_s(atan2_ps(p1[0], p2[0]), pMul);    // phase computation
+                    p1[1] = __lasx_xvfmul_s(atan2_ps(p1[1], p2[1]), pMul);    // phase computation
+                    p1[2] = __lasx_xvfmul_s(atan2_ps(p1[2], p2[2]), pMul);    // phase computation
+                    rpp_simd_store(rpp_store24_f32pln3_to_f32pkd3_avx, dstPtrTemp, p1);    // simd stores
+
                     srcPtr1TempR += vectorIncrementPerChannel;
                     srcPtr1TempG += vectorIncrementPerChannel;
                     srcPtr1TempB += vectorIncrementPerChannel;
@@ -434,7 +546,7 @@ RppStatus phase_f32_f32_host_tensor(Rpp32f *srcPtr1,
         // Phase without fused output-layout toggle (NHWC -> NHWC or NCHW -> NCHW)
         else
         {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             alignedLength = bufferLength & ~7;
 #endif
 
@@ -453,7 +565,7 @@ RppStatus phase_f32_f32_host_tensor(Rpp32f *srcPtr1,
                     dstPtrTemp = dstPtrRow;
 
                     int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                     {
                         __m256 p1[1], p2[1];
@@ -463,6 +575,20 @@ RppStatus phase_f32_f32_host_tensor(Rpp32f *srcPtr1,
                         p1[0] = _mm256_mul_ps(atan2_ps(p1[0], p2[0]), pMul);    // phase computation
                         rpp_simd_store(rpp_store8_f32_to_f32_avx, dstPtrTemp, p1);    // simd stores
 
+                        srcPtr1Temp += vectorIncrementPerChannel;
+                        srcPtr2Temp += vectorIncrementPerChannel;
+                        dstPtrTemp += vectorIncrementPerChannel;
+                    }
+#elif defined(__loongarch_asx)
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                    {
+                        __m256 p1[1], p2[1];
+
+                        rpp_simd_load(rpp_load8_f32_to_f32_avx, srcPtr1Temp, p1);    // simd loads
+                        rpp_simd_load(rpp_load8_f32_to_f32_avx, srcPtr2Temp, p2);    // simd loads
+                        p1[0] = __lasx_xvfmul_s(atan2_ps(p1[0], p2[0]), pMul);    // phase computation
+                        rpp_simd_store(rpp_store8_f32_to_f32_avx, dstPtrTemp, p1);    // simd stores
+
                         srcPtr1Temp += vectorIncrementPerChannel;
                         srcPtr2Temp += vectorIncrementPerChannel;
                         dstPtrTemp += vectorIncrementPerChannel;
@@ -526,11 +652,16 @@ RppStatus phase_f16_f16_host_tensor(Rpp16f *srcPtr1,
         srcPtr2Channel = srcPtr2Image + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
         dstPtrChannel = dstPtrImage;
 
-#if __AVX2__
+#if defined(__AVX2__)
         __m256 pMul = _mm256_set1_ps(multiplier);
         Rpp32u alignedLength = (bufferLength / 24) * 24;
         Rpp32u vectorIncrement = 24;
         Rpp32u vectorIncrementPerChannel = 8;
+#elif defined(__loongarch_asx)
+        __m256 pMul = lasx_set1_f32(multiplier);
+        Rpp32u alignedLength = (bufferLength / 24) * 24;
+        Rpp32u vectorIncrement = 24;
+        Rpp32u vectorIncrementPerChannel = 8;
 #endif
 
         // Phase with fused output-layout toggle (NHWC -> NCHW)
@@ -553,7 +684,7 @@ RppStatus phase_f16_f16_host_tensor(Rpp16f *srcPtr1,
                 dstPtrTempB = dstPtrRowB;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     Rpp32f srcPtr1Temp_ps[24], srcPtr2Temp_ps[24];
@@ -573,6 +704,32 @@ RppStatus phase_f16_f16_host_tensor(Rpp16f *srcPtr1,
                     p1[2] = _mm256_mul_ps(atan2_ps(p1[2], p2[2]), pMul);    // phase computation
                     rpp_simd_store(rpp_store24_f32pln3_to_f16pln3_avx, dstPtrTempR, dstPtrTempG, dstPtrTempB, p1);    // simd stores
 
+                    srcPtr1Temp += vectorIncrement;
+                    srcPtr2Temp += vectorIncrement;
+                    dstPtrTempR += vectorIncrementPerChannel;
+                    dstPtrTempG += vectorIncrementPerChannel;
+                    dstPtrTempB += vectorIncrementPerChannel;
+                }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
+                {
+                    Rpp32f srcPtr1Temp_ps[24], srcPtr2Temp_ps[24];
+
+                    for(int cnt = 0; cnt < vectorIncrement; cnt++)
+                    {
+                        srcPtr1Temp_ps[cnt] = static_cast<Rpp32f>(srcPtr1Temp[cnt]);
+                        srcPtr2Temp_ps[cnt] = static_cast<Rpp32f>(srcPtr2Temp[cnt]);
+                    }
+
+                    __m256 p1[3], p2[3];
+
+                    rpp_simd_load(rpp_load24_f32pkd3_to_f32pln3_avx, srcPtr1Temp_ps, p1);    // simd loads
+                    rpp_simd_load(rpp_load24_f32pkd3_to_f32pln3_avx, srcPtr2Temp_ps, p2);    // simd loads
+                    p1[0] = __lasx_xvfmul_s(atan2_ps(p1[0], p2[0]), pMul);    // phase computation
+                    p1[1] = __lasx_xvfmul_s(atan2_ps(p1[1], p2[1]), pMul);    // phase computation
+                    p1[2] = __lasx_xvfmul_s(atan2_ps(p1[2], p2[2]), pMul);    // phase computation
+                    rpp_simd_store(rpp_store24_f32pln3_to_f16pln3_avx, dstPtrTempR, dstPtrTempG, dstPtrTempB, p1);    // simd stores
+
                     srcPtr1Temp += vectorIncrement;
                     srcPtr2Temp += vectorIncrement;
                     dstPtrTempR += vectorIncrementPerChannel;
@@ -622,7 +779,7 @@ RppStatus phase_f16_f16_host_tensor(Rpp16f *srcPtr1,
                 dstPtrTemp = dstPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     Rpp32f srcPtr1Temp_ps[24], srcPtr2Temp_ps[24];
@@ -647,6 +804,39 @@ RppStatus phase_f16_f16_host_tensor(Rpp16f *srcPtr1,
                     p1[2] = _mm256_mul_ps(atan2_ps(p1[2], p2[2]), pMul);    // phase computation
                     rpp_simd_store(rpp_store24_f32pln3_to_f16pkd3_avx, dstPtrTemp, p1);    // simd stores
 
+                    srcPtr1TempR += vectorIncrementPerChannel;
+                    srcPtr1TempG += vectorIncrementPerChannel;
+                    srcPtr1TempB += vectorIncrementPerChannel;
+                    srcPtr2TempR += vectorIncrementPerChannel;
+                    srcPtr2TempG += vectorIncrementPerChannel;
+                    srcPtr2TempB += vectorIncrementPerChannel;
+                    dstPtrTemp += vectorIncrement;
+                }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    Rpp32f srcPtr1Temp_ps[24], srcPtr2Temp_ps[24];
+
+                    for(int cnt = 0; cnt < vectorIncrementPerChannel; cnt++)
+                    {
+                        srcPtr1Temp_ps[cnt] = static_cast<Rpp32f>(srcPtr1TempR[cnt]);
+                        srcPtr1Temp_ps[cnt + 8] = static_cast<Rpp32f>(srcPtr1TempG[cnt]);
+                        srcPtr1Temp_ps[cnt + 16] = static_cast<Rpp32f>(srcPtr1TempB[cnt]);
+
+                        srcPtr2Temp_ps[cnt] = static_cast<Rpp32f>(srcPtr2TempR[cnt]);
+                        srcPtr2Temp_ps[cnt + 8] = static_cast<Rpp32f>(srcPtr2TempG[cnt]);
+                        srcPtr2Temp_ps[cnt + 16] = static_cast<Rpp32f>(srcPtr2TempB[cnt]);
+                    }
+
+                    __m256 p1[4], p2[4];
+
+                    rpp_simd_load(rpp_load24_f32pln3_to_f32pln3_avx, srcPtr1Temp_ps, srcPtr1Temp_ps + 8, srcPtr1Temp_ps + 16, p1);    // simd loads
+                    rpp_simd_load(rpp_load24_f32pln3_to_f32pln3_avx, srcPtr2Temp_ps, srcPtr2Temp_ps + 8, srcPtr2Temp_ps + 16, p2);    // simd loads
+                    p1[0] = __lasx_xvfmul_s(atan2_ps(p1[0], p2[0]), pMul);    // phase computation
+                    p1[1] = __lasx_xvfmul_s(atan2_ps(p1[1], p2[1]), pMul);    // phase computation
+                    p1[2] = __lasx_xvfmul_s(atan2_ps(p1[2], p2[2]), pMul);    // phase computation
+                    rpp_simd_store(rpp_store24_f32pln3_to_f16pkd3_avx, dstPtrTemp, p1);    // simd stores
+
                     srcPtr1TempR += vectorIncrementPerChannel;
                     srcPtr1TempG += vectorIncrementPerChannel;
                     srcPtr1TempB += vectorIncrementPerChannel;
@@ -684,7 +874,7 @@ RppStatus phase_f16_f16_host_tensor(Rpp16f *srcPtr1,
         // Phase without fused output-layout toggle (NHWC -> NHWC or NCHW -> NCHW)
         else
         {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             alignedLength = bufferLength & ~7;
 #endif
 
@@ -703,7 +893,7 @@ RppStatus phase_f16_f16_host_tensor(Rpp16f *srcPtr1,
                     dstPtrTemp = dstPtrRow;
 
                     int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                     {
                         Rpp32f srcPtr1Temp_ps[8], srcPtr2Temp_ps[8];
@@ -721,6 +911,28 @@ RppStatus phase_f16_f16_host_tensor(Rpp16f *srcPtr1,
                         p1[0] = _mm256_mul_ps(atan2_ps(p1[0], p2[0]), pMul);    // phase computation
                         rpp_simd_store(rpp_store8_f32_to_f16_avx, dstPtrTemp, p1);    // simd stores
 
+                        srcPtr1Temp += vectorIncrementPerChannel;
+                        srcPtr2Temp += vectorIncrementPerChannel;
+                        dstPtrTemp += vectorIncrementPerChannel;
+                    }
+#elif defined(__loongarch_asx)
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                    {
+                        Rpp32f srcPtr1Temp_ps[8], srcPtr2Temp_ps[8];
+
+                        for(int cnt = 0; cnt < vectorIncrementPerChannel; cnt++)
+                        {
+                            srcPtr1Temp_ps[cnt] = static_cast<Rpp32f>(srcPtr1Temp[cnt]);
+                            srcPtr2Temp_ps[cnt] = static_cast<Rpp32f>(srcPtr2Temp[cnt]);
+                        }
+
+                        __m256 p1[1], p2[1];
+
+                        rpp_simd_load(rpp_load8_f32_to_f32_avx, srcPtr1Temp_ps, p1);    // simd loads
+                        rpp_simd_load(rpp_load8_f32_to_f32_avx, srcPtr2Temp_ps, p2);    // simd loads
+                        p1[0] = __lasx_xvfmul_s(atan2_ps(p1[0], p2[0]), pMul);    // phase computation
+                        rpp_simd_store(rpp_store8_f32_to_f16_avx, dstPtrTemp, p1);    // simd stores
+
                         srcPtr1Temp += vectorIncrementPerChannel;
                         srcPtr2Temp += vectorIncrementPerChannel;
                         dstPtrTemp += vectorIncrementPerChannel;
@@ -784,11 +996,16 @@ RppStatus phase_i8_i8_host_tensor(Rpp8s *srcPtr1,
         srcPtr2Channel = srcPtr2Image + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
         dstPtrChannel = dstPtrImage;
 
-#if __AVX2__
+#if defined(__AVX2__)
         __m256 pMul = _mm256_set1_ps(multiplier);
         Rpp32u alignedLength = (bufferLength / 48) * 48;
         Rpp32u vectorIncrement = 48;
         Rpp32u vectorIncrementPerChannel = 16;
+#elif defined(__loongarch_asx)
+        __m256 pMul = lasx_set1_f32(multiplier);
+        Rpp32u alignedLength = (bufferLength / 48) * 48;
+        Rpp32u vectorIncrement = 48;
+        Rpp32u vectorIncrementPerChannel = 16;
 #endif
 
         // Phase with fused output-layout toggle (NHWC -> NCHW)
@@ -811,7 +1028,7 @@ RppStatus phase_i8_i8_host_tensor(Rpp8s *srcPtr1,
                 dstPtrTempB = dstPtrRowB;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256 p1[6], p2[6];
@@ -826,6 +1043,27 @@ RppStatus phase_i8_i8_host_tensor(Rpp8s *srcPtr1,
                     p1[5] = _mm256_round_ps(_mm256_mul_ps(atan2_ps(p1[5], p2[5]), pMul), _MM_FROUND_TO_ZERO);    // phase computation
                     rpp_simd_store(rpp_store48_f32pln3_to_i8pln3_avx, dstPtrTempR, dstPtrTempG, dstPtrTempB, p1);    // simd stores
 
+                    srcPtr1Temp += vectorIncrement;
+                    srcPtr2Temp += vectorIncrement;
+                    dstPtrTempR += vectorIncrementPerChannel;
+                    dstPtrTempG += vectorIncrementPerChannel;
+                    dstPtrTempB += vectorIncrementPerChannel;
+                }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
+                {
+                    __m256 p1[6], p2[6];
+
+                    rpp_simd_load(rpp_load48_i8pkd3_to_f32pln3_avx, srcPtr1Temp, p1);    // simd loads
+                    rpp_simd_load(rpp_load48_i8pkd3_to_f32pln3_avx, srcPtr2Temp, p2);    // simd loads
+                    p1[0] = __lasx_xvfrintrxxx_s(__lasx_xvfmul_s(atan2_ps(p1[0], p2[0]), pMul), _MM_FROUND_TO_ZERO);    // phase computation
+                    p1[1] = __lasx_xvfrintrxxx_s(__lasx_xvfmul_s(atan2_ps(p1[1], p2[1]), pMul), _MM_FROUND_TO_ZERO);    // phase computation
+                    p1[2] = __lasx_xvfrintrxxx_s(__lasx_xvfmul_s(atan2_ps(p1[2], p2[2]), pMul), _MM_FROUND_TO_ZERO);    // phase computation
+                    p1[3] = __lasx_xvfrintrxxx_s(__lasx_xvfmul_s(atan2_ps(p1[3], p2[3]), pMul), _MM_FROUND_TO_ZERO);    // phase computation
+                    p1[4] = __lasx_xvfrintrxxx_s(__lasx_xvfmul_s(atan2_ps(p1[4], p2[4]), pMul), _MM_FROUND_TO_ZERO);    // phase computation
+                    p1[5] = __lasx_xvfrintrxxx_s(__lasx_xvfmul_s(atan2_ps(p1[5], p2[5]), pMul), _MM_FROUND_TO_ZERO);    // phase computation
+                    rpp_simd_store(rpp_store48_f32pln3_to_i8pln3_avx, dstPtrTempR, dstPtrTempG, dstPtrTempB, p1);    // simd stores
+
                     srcPtr1Temp += vectorIncrement;
                     srcPtr2Temp += vectorIncrement;
                     dstPtrTempR += vectorIncrementPerChannel;
@@ -875,7 +1113,7 @@ RppStatus phase_i8_i8_host_tensor(Rpp8s *srcPtr1,
                 dstPtrTemp = dstPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256 p1[6], p2[6];
@@ -890,6 +1128,29 @@ RppStatus phase_i8_i8_host_tensor(Rpp8s *srcPtr1,
                     p1[5] = _mm256_round_ps(_mm256_mul_ps(atan2_ps(p1[5], p2[5]), pMul), _MM_FROUND_TO_ZERO);    // phase computation
                     rpp_simd_store(rpp_store48_f32pln3_to_i8pkd3_avx, dstPtrTemp, p1);    // simd stores
 
+                    srcPtr1TempR += vectorIncrementPerChannel;
+                    srcPtr1TempG += vectorIncrementPerChannel;
+                    srcPtr1TempB += vectorIncrementPerChannel;
+                    srcPtr2TempR += vectorIncrementPerChannel;
+                    srcPtr2TempG += vectorIncrementPerChannel;
+                    srcPtr2TempB += vectorIncrementPerChannel;
+                    dstPtrTemp += vectorIncrement;
+                }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
+                {
+                    __m256 p1[6], p2[6];
+
+                    rpp_simd_load(rpp_load48_i8pln3_to_f32pln3_avx, srcPtr1TempR, srcPtr1TempG, srcPtr1TempB, p1);    // simd loads
+                    rpp_simd_load(rpp_load48_i8pln3_to_f32pln3_avx, srcPtr2TempR, srcPtr2TempG, srcPtr2TempB, p2);    // simd loads
+                    p1[0] = __lasx_xvfrintrxxx_s(__lasx_xvfmul_s(atan2_ps(p1[0], p2[0]), pMul), _MM_FROUND_TO_ZERO);    // phase computation
+                    p1[1] = __lasx_xvfrintrxxx_s(__lasx_xvfmul_s(atan2_ps(p1[1], p2[1]), pMul), _MM_FROUND_TO_ZERO);    // phase computation
+                    p1[2] = __lasx_xvfrintrxxx_s(__lasx_xvfmul_s(atan2_ps(p1[2], p2[2]), pMul), _MM_FROUND_TO_ZERO);    // phase computation
+                    p1[3] = __lasx_xvfrintrxxx_s(__lasx_xvfmul_s(atan2_ps(p1[3], p2[3]), pMul), _MM_FROUND_TO_ZERO);    // phase computation
+                    p1[4] = __lasx_xvfrintrxxx_s(__lasx_xvfmul_s(atan2_ps(p1[4], p2[4]), pMul), _MM_FROUND_TO_ZERO);    // phase computation
+                    p1[5] = __lasx_xvfrintrxxx_s(__lasx_xvfmul_s(atan2_ps(p1[5], p2[5]), pMul), _MM_FROUND_TO_ZERO);    // phase computation
+                    rpp_simd_store(rpp_store48_f32pln3_to_i8pkd3_avx, dstPtrTemp, p1);    // simd stores
+
                     srcPtr1TempR += vectorIncrementPerChannel;
                     srcPtr1TempG += vectorIncrementPerChannel;
                     srcPtr1TempB += vectorIncrementPerChannel;
@@ -927,7 +1188,7 @@ RppStatus phase_i8_i8_host_tensor(Rpp8s *srcPtr1,
         // Phase without fused output-layout toggle (NHWC -> NHWC or NCHW -> NCHW)
         else
         {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             alignedLength = bufferLength & ~15;
 #endif
 
@@ -946,7 +1207,7 @@ RppStatus phase_i8_i8_host_tensor(Rpp8s *srcPtr1,
                     dstPtrTemp = dstPtrRow;
 
                     int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                     {
                         __m256 p1[2], p2[2];
@@ -957,6 +1218,21 @@ RppStatus phase_i8_i8_host_tensor(Rpp8s *srcPtr1,
                         p1[1] = _mm256_mul_ps(atan2_ps(p1[1], p2[1]), pMul);    // phase computation
                         rpp_simd_store(rpp_store16_f32_to_i8_avx, dstPtrTemp, p1);    // simd stores
 
+                        srcPtr1Temp += vectorIncrementPerChannel;
+                        srcPtr2Temp += vectorIncrementPerChannel;
+                        dstPtrTemp += vectorIncrementPerChannel;
+                    }
+#elif defined(__loongarch_asx)
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                    {
+                        __m256 p1[2], p2[2];
+
+                        rpp_simd_load(rpp_load16_i8_to_f32_avx, srcPtr1Temp, p1);    // simd loads
+                        rpp_simd_load(rpp_load16_i8_to_f32_avx, srcPtr2Temp, p2);    // simd loads
+                        p1[0] = __lasx_xvfmul_s(atan2_ps(p1[0], p2[0]), pMul);    // phase computation
+                        p1[1] = __lasx_xvfmul_s(atan2_ps(p1[1], p2[1]), pMul);    // phase computation
+                        rpp_simd_store(rpp_store16_f32_to_i8_avx, dstPtrTemp, p1);    // simd stores
+
                         srcPtr1Temp += vectorIncrementPerChannel;
                         srcPtr2Temp += vectorIncrementPerChannel;
                         dstPtrTemp += vectorIncrementPerChannel;
diff --git a/src/modules/cpu/kernel/pre_emphasis_filter.hpp b/src/modules/cpu/kernel/pre_emphasis_filter.hpp
index 889cd2de..0b8db93a 100644
--- a/src/modules/cpu/kernel/pre_emphasis_filter.hpp
+++ b/src/modules/cpu/kernel/pre_emphasis_filter.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 RppStatus pre_emphasis_filter_host_tensor(Rpp32f *srcPtr,
                                           RpptDescPtr srcDescPtr,
@@ -51,18 +56,30 @@ RppStatus pre_emphasis_filter_host_tensor(Rpp32f *srcPtr,
 
         Rpp32s vectorIncrement = 8;
         Rpp32s alignedLength = (bufferLength / 8) * 8 - 8;
+#ifdef __loongarch_asx
+        __m256 pCoeff = lasx_set1_f32(coeff);
+#else
         __m256 pCoeff = _mm256_set1_ps(coeff);
+#endif
 
         Rpp32s vectorLoopCount = 1;
         dstPtrTemp++;
         srcPtrTemp++;
         for(; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
         {
+#ifdef __loongarch_asx
+            __m256 pSrc[2];
+            pSrc[0] = (__m256)__lasx_xvld(srcPtrTemp, 0);
+            pSrc[1] = (__m256)__lasx_xvld(srcPtrTemp - 1, 0);
+            pSrc[1] = __lasx_xvfsub_s(pSrc[0], __lasx_xvfmul_s(pSrc[1], pCoeff));
+            __lasx_xvst((__m256i)pSrc[1], dstPtrTemp, 0);
+#else
             __m256 pSrc[2];
             pSrc[0] = _mm256_loadu_ps(srcPtrTemp);
             pSrc[1] = _mm256_loadu_ps(srcPtrTemp - 1);
             pSrc[1] = _mm256_sub_ps(pSrc[0], _mm256_mul_ps(pSrc[1], pCoeff));
             _mm256_storeu_ps(dstPtrTemp, pSrc[1]);
+#endif
             srcPtrTemp += vectorIncrement;
             dstPtrTemp += vectorIncrement;
         }
diff --git a/src/modules/cpu/kernel/rain.hpp b/src/modules/cpu/kernel/rain.hpp
index 45b1a4f4..2e3adf37 100644
--- a/src/modules/cpu/kernel/rain.hpp
+++ b/src/modules/cpu/kernel/rain.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 // Constants to represent the rain intensity for different data types
 #define RAIN_INTENSITY_8U 200   // Intensity value for Rpp8u
@@ -105,8 +110,10 @@ RppStatus rain_u8_u8_host_tensor(Rpp8u *srcPtr,
         dstPtrChannel = dstPtrImage;
 
         Rpp32f alpha = alphaValues[batchCount];
-#if __AVX2__
+#if defined(__AVX2__)
         __m256 pMul = _mm256_set1_ps(alpha);
+#elif defined(__loongarch_asx)
+        __m256 pMul = lasx_set1_f32(alpha);
 #endif
         Rpp32u vectorIncrement = 48;
         Rpp32u vectorIncrementPerChannel = 16;
@@ -389,8 +396,10 @@ RppStatus rain_f32_f32_host_tensor(Rpp32f *srcPtr,
         dstPtrChannel = dstPtrImage;
 
         Rpp32f alpha = alphaValues[batchCount];
-#if __AVX2__
+#if defined(__AVX2__)
         __m256 pMul = _mm256_set1_ps(alpha);
+#elif defined(__loongarch_asx)
+        __m256 pMul = lasx_set1_f32(alpha);
 #endif
         Rpp32u vectorIncrement = 24;
         Rpp32u vectorIncrementPerChannel = 8;
@@ -607,7 +616,7 @@ RppStatus rain_f32_f32_host_tensor(Rpp32f *srcPtr,
                 srcPtr2Temp = srcPtr2Row;
                 dstPtrTemp = dstPtrRow;
                 Rpp32u vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 p1, p2;
@@ -619,6 +628,18 @@ RppStatus rain_f32_f32_host_tensor(Rpp32f *srcPtr,
                     srcPtr2Temp += vectorIncrementPerChannel;
                     dstPtrTemp += vectorIncrementPerChannel;
                 }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256 p1, p2;
+                    rpp_simd_load(rpp_load8_f32_to_f32_avx, srcPtr1Temp, &p1);    // simd loads
+                    rpp_simd_load(rpp_load8_f32_to_f32_avx, srcPtr2Temp, &p2);    // simd loads
+                    p1 = __lasx_xvfmadd_s(__lasx_xvfsub_s(p2, p1), pMul, p1);    // alpha-blending adjustment
+                    rpp_simd_store(rpp_store8_f32_to_f32_avx, dstPtrTemp, &p1);    // simd stores
+                    srcPtr1Temp += vectorIncrementPerChannel;
+                    srcPtr2Temp += vectorIncrementPerChannel;
+                    dstPtrTemp += vectorIncrementPerChannel;
+                }
 #endif
                 for (; vectorLoopCount < bufferLength; vectorLoopCount++)
                 {
@@ -673,8 +694,10 @@ RppStatus rain_f16_f16_host_tensor(Rpp16f *srcPtr,
         dstPtrChannel = dstPtrImage;
 
         Rpp32f alpha = alphaValues[batchCount];
-#if __AVX2__
+#if defined(__AVX2__)
         __m256 pMul = _mm256_set1_ps(alpha);
+#elif defined(__loongarch_asx)
+        __m256 pMul = lasx_set1_f32(alpha);
 #endif
         Rpp32u vectorIncrement = 24;
         Rpp32u vectorIncrementPerChannel = 8;
@@ -891,7 +914,7 @@ RppStatus rain_f16_f16_host_tensor(Rpp16f *srcPtr,
                 srcPtr2Temp = srcPtr2Row;
                 dstPtrTemp = dstPtrRow;
                 Rpp32u vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 p1, p2;
@@ -903,6 +926,18 @@ RppStatus rain_f16_f16_host_tensor(Rpp16f *srcPtr,
                     srcPtr2Temp += vectorIncrementPerChannel;
                     dstPtrTemp += vectorIncrementPerChannel;
                 }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256 p1, p2;
+                    rpp_simd_load(rpp_load8_f16_to_f32_avx, srcPtr1Temp, &p1);    // simd loads
+                    rpp_simd_load(rpp_load8_f16_to_f32_avx, srcPtr2Temp, &p2);    // simd loads
+                    p1 = __lasx_xvfmadd_s(__lasx_xvfsub_s(p2, p1), pMul, p1);    // alpha-blending adjustment
+                    rpp_simd_store(rpp_store8_f32_to_f16_avx, dstPtrTemp, &p1);    // simd stores
+                    srcPtr1Temp += vectorIncrementPerChannel;
+                    srcPtr2Temp += vectorIncrementPerChannel;
+                    dstPtrTemp += vectorIncrementPerChannel;
+                }
 #endif
                 for (; vectorLoopCount < bufferLength; vectorLoopCount++)
                 {
@@ -957,8 +992,10 @@ RppStatus rain_i8_i8_host_tensor(Rpp8s *srcPtr,
         dstPtrChannel = dstPtrImage;
 
         Rpp32f alpha = alphaValues[batchCount];
-#if __AVX2__
+#if defined(__AVX2__)
         __m256 pMul = _mm256_set1_ps(alpha);
+#elif defined(__loongarch_asx)
+        __m256 pMul = lasx_set1_f32(alpha);
 #endif
         Rpp32u vectorIncrement = 48;
         Rpp32u vectorIncrementPerChannel = 16;
diff --git a/src/modules/cpu/kernel/remap_loongarch.hpp b/src/modules/cpu/kernel/remap_loongarch.hpp
new file mode 100644
index 00000000..6ebf4a6b
--- /dev/null
+++ b/src/modules/cpu/kernel/remap_loongarch.hpp
@@ -0,0 +1,1986 @@
+/*
+MIT License
+
+Copyright (c) 2019 - 2024 Advanced Micro Devices, Inc.
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
+*/
+
+#include "rppdefs.h"
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+
+/************* NEAREST NEIGHBOR INTERPOLATION *************/
+
+RppStatus remap_nn_u8_u8_host_tensor(Rpp8u *srcPtr,
+                                     RpptDescPtr srcDescPtr,
+                                     Rpp8u *dstPtr,
+                                     RpptDescPtr dstDescPtr,
+                                     Rpp32f *rowRemapTable,
+                                     Rpp32f *colRemapTable,
+                                     RpptDescPtr remapTableDescPtr,
+                                     RpptROIPtr roiTensorPtrSrc,
+                                     RpptRoiType roiType,
+                                     RppLayoutParams layoutParams,
+                                     rpp::Handle& handle)
+{
+    RpptROI roiDefault = {0, 0, (Rpp32s)srcDescPtr->w, (Rpp32s)srcDescPtr->h};
+    Rpp32u numThreads = handle.GetNumThreads();
+
+    __m128 pSrcChannel = lsx_set1_f32(srcDescPtr->c);
+    __m128 pSrcStride = lsx_set1_f32(srcDescPtr->strides.hStride);
+
+omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+    for(int batchCount = 0; batchCount < dstDescPtr->n; batchCount++)
+    {
+        RpptROI roi;
+        RpptROIPtr roiPtrInput = &roiTensorPtrSrc[batchCount];
+        compute_roi_validation_host(roiPtrInput, &roi, &roiDefault, roiType);
+
+        Rpp8u *srcPtrChannel, *dstPtrChannel, *srcPtrImage, *dstPtrImage;
+        srcPtrImage = srcPtr + batchCount * srcDescPtr->strides.nStride;
+        dstPtrImage = dstPtr + batchCount * dstDescPtr->strides.nStride;
+        srcPtrChannel = srcPtrImage + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
+        dstPtrChannel = dstPtrImage;
+
+        Rpp32f *rowRemapTableImage, *colRemapTableImage;
+        rowRemapTableImage = rowRemapTable + batchCount * remapTableDescPtr->strides.nStride;
+        colRemapTableImage = colRemapTable + batchCount * remapTableDescPtr->strides.nStride;
+        Rpp32u alignedLength = roi.xywhROI.roiWidth & ~3;   // Align dst width to process 4 dst pixels per iteration
+        Rpp32s vectorIncrementPerChannel = 4;
+        Rpp32s vectorIncrement = 12;
+        Rpp32s remappedSrcLoc;
+        Rpp32s remapSrcLocArray[4] = {0};     // Since 4 dst pixels are processed per iteration
+        Rpp32f widthLimit = roi.xywhROI.roiWidth - 1;
+        Rpp32f heightLimit = roi.xywhROI.roiHeight - 1;
+        __m128 pWidthLimit = lsx_set1_f32(widthLimit);
+        __m128 pHeightLimit = lsx_set1_f32(heightLimit);
+
+        // Remap with fused output-layout toggle (NHWC -> NCHW)
+        if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
+        {
+            Rpp8u *dstPtrRowR, *dstPtrRowG, *dstPtrRowB;
+            dstPtrRowR = dstPtrChannel;
+            dstPtrRowG = dstPtrRowR + dstDescPtr->strides.cStride;
+            dstPtrRowB = dstPtrRowG + dstDescPtr->strides.cStride;
+
+            for(int dstLocRow = 0; dstLocRow < roi.xywhROI.roiHeight; dstLocRow++)
+            {
+                Rpp8u *dstPtrTempR, *dstPtrTempG, *dstPtrTempB;
+                dstPtrTempR = dstPtrRowR;
+                dstPtrTempG = dstPtrRowG;
+                dstPtrTempB = dstPtrRowB;
+                Rpp32f *rowRemapTableTemp, *colRemapTableTemp;
+                rowRemapTableTemp = rowRemapTableImage;
+                colRemapTableTemp = colRemapTableImage;
+
+                int vectorLoopCount = 0;
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m128i pxRow;
+                    compute_remap_src_loc_sse(rowRemapTableTemp, colRemapTableTemp, remapSrcLocArray, pSrcStride, pWidthLimit, pHeightLimit, pSrcChannel);
+                    rpp_simd_load(rpp_resize_nn_load_u8pkd3, srcPtrChannel, remapSrcLocArray, pxRow);
+                    rpp_simd_store(rpp_store12_u8pkd3_to_u8pln3, dstPtrTempR, dstPtrTempG, dstPtrTempB, pxRow);
+                    dstPtrTempR += vectorIncrementPerChannel;
+                    dstPtrTempG += vectorIncrementPerChannel;
+                    dstPtrTempB += vectorIncrementPerChannel;
+                    rowRemapTableTemp += vectorIncrementPerChannel;
+                    colRemapTableTemp += vectorIncrementPerChannel;
+                }
+                for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
+                {
+                    compute_remap_src_loc(*rowRemapTableTemp++, *colRemapTableTemp++, remappedSrcLoc, srcDescPtr->strides.hStride, widthLimit, heightLimit, srcDescPtr->c);
+                    *dstPtrTempR++ = *(srcPtrChannel + remappedSrcLoc);
+                    *dstPtrTempG++ = *(srcPtrChannel + 1 + remappedSrcLoc);
+                    *dstPtrTempB++ = *(srcPtrChannel + 2 + remappedSrcLoc);
+                }
+                dstPtrRowR += dstDescPtr->strides.hStride;
+                dstPtrRowG += dstDescPtr->strides.hStride;
+                dstPtrRowB += dstDescPtr->strides.hStride;
+                rowRemapTableImage += remapTableDescPtr->strides.hStride;
+                colRemapTableImage += remapTableDescPtr->strides.hStride;
+            }
+        }
+
+        // Remap with fused output-layout toggle (NCHW -> NHWC)
+        else if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NHWC))
+        {
+            Rpp8u *srcPtrRowR, *srcPtrRowG, *srcPtrRowB;
+            srcPtrRowR = srcPtrChannel;
+            srcPtrRowG = srcPtrRowR + srcDescPtr->strides.cStride;
+            srcPtrRowB = srcPtrRowG + srcDescPtr->strides.cStride;
+            Rpp8u *dstPtrRow;
+            dstPtrRow = dstPtrChannel;
+
+            for(int dstLocRow = 0; dstLocRow < roi.xywhROI.roiHeight; dstLocRow++)
+            {
+                Rpp8u *dstPtrTemp;
+                dstPtrTemp = dstPtrRow;
+                Rpp32f *rowRemapTableTemp, *colRemapTableTemp;
+                rowRemapTableTemp = rowRemapTableImage;
+                colRemapTableTemp = colRemapTableImage;
+
+                int vectorLoopCount = 0;
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m128i pxRow[3];
+                    compute_remap_src_loc_sse(rowRemapTableTemp, colRemapTableTemp, remapSrcLocArray, pSrcStride, pWidthLimit, pHeightLimit);
+                    rpp_simd_load(rpp_resize_nn_load_u8pln1, srcPtrRowR, remapSrcLocArray, pxRow[0]);
+                    rpp_simd_load(rpp_resize_nn_load_u8pln1, srcPtrRowG, remapSrcLocArray, pxRow[1]);
+                    rpp_simd_load(rpp_resize_nn_load_u8pln1, srcPtrRowB, remapSrcLocArray, pxRow[2]);
+                    rpp_simd_store(rpp_store12_u8pln3_to_u8pkd3, dstPtrTemp, pxRow);
+                    dstPtrTemp += vectorIncrement;
+                    rowRemapTableTemp += vectorIncrementPerChannel;
+                    colRemapTableTemp += vectorIncrementPerChannel;
+                }
+                for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
+                {
+                    compute_remap_src_loc(*rowRemapTableTemp++, *colRemapTableTemp++, remappedSrcLoc, srcDescPtr->strides.hStride, widthLimit, heightLimit);
+                    *dstPtrTemp++ = *(srcPtrRowR + remappedSrcLoc);
+                    *dstPtrTemp++ = *(srcPtrRowG + remappedSrcLoc);
+                    *dstPtrTemp++ = *(srcPtrRowB + remappedSrcLoc);
+                }
+                dstPtrRow += dstDescPtr->strides.hStride;
+                rowRemapTableImage += remapTableDescPtr->strides.hStride;
+                colRemapTableImage += remapTableDescPtr->strides.hStride;
+            }
+        }
+
+        // Remap without fused output-layout toggle (NHWC -> NHWC)
+        else if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NHWC))
+        {
+            Rpp8u *dstPtrRow;
+            dstPtrRow = dstPtrChannel;
+
+            for(int dstLocRow = 0; dstLocRow < roi.xywhROI.roiHeight; dstLocRow++)
+            {
+                Rpp8u *dstPtrTemp;
+                dstPtrTemp = dstPtrRow;
+                Rpp32f *rowRemapTableTemp, *colRemapTableTemp;
+                rowRemapTableTemp = rowRemapTableImage;
+                colRemapTableTemp = colRemapTableImage;
+
+                int vectorLoopCount = 0;
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m128i pxRow;
+                    compute_remap_src_loc_sse(rowRemapTableTemp, colRemapTableTemp, remapSrcLocArray, pSrcStride, pWidthLimit, pHeightLimit, pSrcChannel);
+                    rpp_simd_load(rpp_resize_nn_load_u8pkd3, srcPtrChannel, remapSrcLocArray, pxRow);
+                    rpp_simd_store(rpp_store12_u8_to_u8, dstPtrTemp, pxRow);
+                    dstPtrTemp += vectorIncrement;
+                    rowRemapTableTemp += vectorIncrementPerChannel;
+                    colRemapTableTemp += vectorIncrementPerChannel;
+                }
+                for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
+                {
+                    compute_remap_src_loc(*rowRemapTableTemp++, *colRemapTableTemp++, remappedSrcLoc, srcDescPtr->strides.hStride, widthLimit, heightLimit, srcDescPtr->c);
+                    *dstPtrTemp++ = *(srcPtrChannel + remappedSrcLoc);
+                    *dstPtrTemp++ = *(srcPtrChannel + 1 + remappedSrcLoc);
+                    *dstPtrTemp++ = *(srcPtrChannel + 2 + remappedSrcLoc);
+                }
+                dstPtrRow += dstDescPtr->strides.hStride;
+                rowRemapTableImage += remapTableDescPtr->strides.hStride;
+                colRemapTableImage += remapTableDescPtr->strides.hStride;
+            }
+        }
+
+        // Remap without fused output-layout toggle (NCHW -> NCHW for 1 channel and 3 channel)
+        else if ((srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NCHW))
+        {
+            Rpp8u *dstPtrRow;
+            dstPtrRow = dstPtrChannel;
+
+            for(int dstLocRow = 0; dstLocRow < roi.xywhROI.roiHeight; dstLocRow++)
+            {
+                Rpp8u *dstPtrTemp;
+                dstPtrTemp = dstPtrRow;
+                Rpp32f *rowRemapTableTemp, *colRemapTableTemp;
+                rowRemapTableTemp = rowRemapTableImage;
+                colRemapTableTemp = colRemapTableImage;
+
+                int vectorLoopCount = 0;
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    Rpp8u *srcPtrTempChn, *dstPtrTempChn;
+                    srcPtrTempChn = srcPtrChannel;
+                    dstPtrTempChn = dstPtrTemp;
+                    compute_remap_src_loc_sse(rowRemapTableTemp, colRemapTableTemp, remapSrcLocArray, pSrcStride, pWidthLimit, pHeightLimit);
+
+                    for (int c = 0; c < dstDescPtr->c; c++)
+                    {
+                        __m128i pxRow;
+                        rpp_simd_load(rpp_resize_nn_load_u8pln1, srcPtrTempChn, remapSrcLocArray, pxRow);
+                        rpp_simd_store(rpp_storeu_si32, dstPtrTempChn, pxRow);
+                        srcPtrTempChn += srcDescPtr->strides.cStride;
+                        dstPtrTempChn += dstDescPtr->strides.cStride;
+                    }
+                    dstPtrTemp += vectorIncrementPerChannel;
+                    rowRemapTableTemp += vectorIncrementPerChannel;
+                    colRemapTableTemp += vectorIncrementPerChannel;
+                }
+                for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
+                {
+                    Rpp8u * dstPtrTempChannel = dstPtrTemp;
+                    Rpp8u * srcPtrTempChannel = srcPtrChannel;
+                    compute_remap_src_loc(*rowRemapTableTemp++, *colRemapTableTemp++, remappedSrcLoc, srcDescPtr->strides.hStride, widthLimit, heightLimit);
+                    for (int c = 0; c < dstDescPtr->c; c++)
+                    {
+                        *dstPtrTempChannel = *(srcPtrTempChannel + remappedSrcLoc);
+                        dstPtrTempChannel += dstDescPtr->strides.cStride;
+                        srcPtrTempChannel += srcDescPtr->strides.cStride;
+                    }
+                    dstPtrTemp++;
+                }
+                dstPtrRow += dstDescPtr->strides.hStride;
+                rowRemapTableImage += remapTableDescPtr->strides.hStride;
+                colRemapTableImage += remapTableDescPtr->strides.hStride;
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+RppStatus remap_nn_f32_f32_host_tensor(Rpp32f *srcPtr,
+                                       RpptDescPtr srcDescPtr,
+                                       Rpp32f *dstPtr,
+                                       RpptDescPtr dstDescPtr,
+                                       Rpp32f *rowRemapTable,
+                                       Rpp32f *colRemapTable,
+                                       RpptDescPtr remapTableDescPtr,
+                                       RpptROIPtr roiTensorPtrSrc,
+                                       RpptRoiType roiType,
+                                       RppLayoutParams layoutParams,
+                                       rpp::Handle& handle)
+{
+    RpptROI roiDefault = {0, 0, (Rpp32s)srcDescPtr->w, (Rpp32s)srcDescPtr->h};
+    Rpp32u numThreads = handle.GetNumThreads();
+
+    __m128 pSrcChannel = lsx_set1_f32(srcDescPtr->c);
+    __m128 pSrcStride = lsx_set1_f32(srcDescPtr->strides.hStride);
+
+omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+    for(int batchCount = 0; batchCount < dstDescPtr->n; batchCount++)
+    {
+        RpptROI roi;
+        RpptROIPtr roiPtrInput = &roiTensorPtrSrc[batchCount];
+        compute_roi_validation_host(roiPtrInput, &roi, &roiDefault, roiType);
+
+        Rpp32f *srcPtrChannel, *dstPtrChannel, *srcPtrImage, *dstPtrImage;
+        srcPtrImage = srcPtr + batchCount * srcDescPtr->strides.nStride;
+        dstPtrImage = dstPtr + batchCount * dstDescPtr->strides.nStride;
+        srcPtrChannel = srcPtrImage + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
+        dstPtrChannel = dstPtrImage;
+
+        Rpp32f *rowRemapTableImage, *colRemapTableImage;
+        rowRemapTableImage = rowRemapTable + batchCount * remapTableDescPtr->strides.nStride;
+        colRemapTableImage = colRemapTable + batchCount * remapTableDescPtr->strides.nStride;
+        Rpp32u alignedLength = roi.xywhROI.roiWidth & ~3;   // Align dst width to process 4 dst pixels per iteration
+        Rpp32s vectorIncrementPerChannel = 4;
+        Rpp32s vectorIncrement = 12;
+        Rpp32s remappedSrcLoc;
+        Rpp32s remapSrcLocArray[4] = {0};     // Since 4 dst pixels are processed per iteration
+        Rpp32f widthLimit = roi.xywhROI.roiWidth - 1;
+        Rpp32f heightLimit = roi.xywhROI.roiHeight - 1;
+        __m128 pWidthLimit = lsx_set1_f32(widthLimit);
+        __m128 pHeightLimit = lsx_set1_f32(heightLimit);
+
+        // Remap with fused output-layout toggle (NHWC -> NCHW)
+        if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
+        {
+            Rpp32f *dstPtrRowR, *dstPtrRowG, *dstPtrRowB;
+            dstPtrRowR = dstPtrChannel;
+            dstPtrRowG = dstPtrRowR + dstDescPtr->strides.cStride;
+            dstPtrRowB = dstPtrRowG + dstDescPtr->strides.cStride;
+
+            for(int dstLocRow = 0; dstLocRow < roi.xywhROI.roiHeight; dstLocRow++)
+            {
+                Rpp32f *dstPtrTempR, *dstPtrTempG, *dstPtrTempB;
+                dstPtrTempR = dstPtrRowR;
+                dstPtrTempG = dstPtrRowG;
+                dstPtrTempB = dstPtrRowB;
+                Rpp32f *rowRemapTableTemp, *colRemapTableTemp;
+                rowRemapTableTemp = rowRemapTableImage;
+                colRemapTableTemp = colRemapTableImage;
+
+                int vectorLoopCount = 0;
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m128 pRow[3];
+                    compute_remap_src_loc_sse(rowRemapTableTemp, colRemapTableTemp, remapSrcLocArray, pSrcStride, pWidthLimit, pHeightLimit, pSrcChannel);
+                    rpp_simd_load(rpp_resize_nn_load_f32pkd3_to_f32pln3, srcPtrChannel, remapSrcLocArray, pRow);
+                    rpp_simd_store(rpp_store12_f32pln3_to_f32pln3, dstPtrTempR, dstPtrTempG, dstPtrTempB, pRow);
+                    dstPtrTempR += vectorIncrementPerChannel;
+                    dstPtrTempG += vectorIncrementPerChannel;
+                    dstPtrTempB += vectorIncrementPerChannel;
+                    rowRemapTableTemp += vectorIncrementPerChannel;
+                    colRemapTableTemp += vectorIncrementPerChannel;
+                }
+                for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
+                {
+                    compute_remap_src_loc(*rowRemapTableTemp++, *colRemapTableTemp++, remappedSrcLoc, srcDescPtr->strides.hStride, widthLimit, heightLimit, srcDescPtr->c);
+                    *dstPtrTempR++ = *(srcPtrChannel + remappedSrcLoc);
+                    *dstPtrTempG++ = *(srcPtrChannel + 1 + remappedSrcLoc);
+                    *dstPtrTempB++ = *(srcPtrChannel + 2 + remappedSrcLoc);
+                }
+                dstPtrRowR += dstDescPtr->strides.hStride;
+                dstPtrRowG += dstDescPtr->strides.hStride;
+                dstPtrRowB += dstDescPtr->strides.hStride;
+                rowRemapTableImage += remapTableDescPtr->strides.hStride;
+                colRemapTableImage += remapTableDescPtr->strides.hStride;
+            }
+        }
+
+        // Remap with fused output-layout toggle (NCHW -> NHWC)
+        else if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NHWC))
+        {
+            Rpp32f *srcPtrRowR, *srcPtrRowG, *srcPtrRowB;
+            srcPtrRowR = srcPtrChannel;
+            srcPtrRowG = srcPtrRowR + srcDescPtr->strides.cStride;
+            srcPtrRowB = srcPtrRowG + srcDescPtr->strides.cStride;
+            Rpp32f *dstPtrRow;
+            dstPtrRow = dstPtrChannel;
+
+            for(int dstLocRow = 0; dstLocRow < roi.xywhROI.roiHeight; dstLocRow++)
+            {
+                Rpp32f *dstPtrTemp;
+                dstPtrTemp = dstPtrRow;
+                Rpp32f *rowRemapTableTemp, *colRemapTableTemp;
+                rowRemapTableTemp = rowRemapTableImage;
+                colRemapTableTemp = colRemapTableImage;
+
+                int vectorLoopCount = 0;
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m128 pRow[4];
+                    compute_remap_src_loc_sse(rowRemapTableTemp, colRemapTableTemp, remapSrcLocArray, pSrcStride, pWidthLimit, pHeightLimit);
+                    rpp_simd_load(rpp_resize_nn_load_f32pln1, srcPtrRowR, remapSrcLocArray, pRow[0]);
+                    rpp_simd_load(rpp_resize_nn_load_f32pln1, srcPtrRowG, remapSrcLocArray, pRow[1]);
+                    rpp_simd_load(rpp_resize_nn_load_f32pln1, srcPtrRowB, remapSrcLocArray, pRow[2]);
+                    rpp_simd_store(rpp_store12_f32pln3_to_f32pkd3, dstPtrTemp, pRow);
+                    dstPtrTemp += vectorIncrement;
+                    rowRemapTableTemp += vectorIncrementPerChannel;
+                    colRemapTableTemp += vectorIncrementPerChannel;
+                }
+                for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
+                {
+                    compute_remap_src_loc(*rowRemapTableTemp++, *colRemapTableTemp++, remappedSrcLoc, srcDescPtr->strides.hStride, widthLimit, heightLimit);
+                    *dstPtrTemp++ = *(srcPtrRowR + remappedSrcLoc);
+                    *dstPtrTemp++ = *(srcPtrRowG + remappedSrcLoc);
+                    *dstPtrTemp++ = *(srcPtrRowB + remappedSrcLoc);
+                }
+                dstPtrRow += dstDescPtr->strides.hStride;
+                rowRemapTableImage += remapTableDescPtr->strides.hStride;
+                colRemapTableImage += remapTableDescPtr->strides.hStride;
+            }
+        }
+
+        // Remap without fused output-layout toggle (NHWC -> NHWC)
+        else if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NHWC))
+        {
+            Rpp32f *dstPtrRow;
+            dstPtrRow = dstPtrChannel;
+
+            for(int dstLocRow = 0; dstLocRow < roi.xywhROI.roiHeight; dstLocRow++)
+            {
+                Rpp32f *dstPtrTemp;
+                dstPtrTemp = dstPtrRow;
+                Rpp32f *rowRemapTableTemp, *colRemapTableTemp;
+                rowRemapTableTemp = rowRemapTableImage;
+                colRemapTableTemp = colRemapTableImage;
+
+                int vectorLoopCount = 0;
+                for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
+                {
+                    __m128 pRow;
+                    compute_remap_src_loc_sse(rowRemapTableTemp, colRemapTableTemp, remapSrcLocArray, pSrcStride, pWidthLimit, pHeightLimit, pSrcChannel);
+                    rpp_simd_load(rpp_load4_f32_to_f32, (srcPtrChannel + *remapSrcLocArray), &pRow);
+                    rpp_simd_store(rpp_store4_f32_to_f32, dstPtrTemp, &pRow);
+                    dstPtrTemp += 3;
+                    rowRemapTableTemp++;
+                    colRemapTableTemp++;
+                }
+                dstPtrRow += dstDescPtr->strides.hStride;
+                rowRemapTableImage += remapTableDescPtr->strides.hStride;
+                colRemapTableImage += remapTableDescPtr->strides.hStride;
+            }
+        }
+
+        // Remap without fused output-layout toggle (NCHW -> NCHW for 1 channel and 3 channel)
+        else if ((srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NCHW))
+        {
+            Rpp32f *dstPtrRow;
+            dstPtrRow = dstPtrChannel;
+
+            for(int dstLocRow = 0; dstLocRow < roi.xywhROI.roiHeight; dstLocRow++)
+            {
+                Rpp32f *dstPtrTemp;
+                dstPtrTemp = dstPtrRow;
+                Rpp32f *rowRemapTableTemp, *colRemapTableTemp;
+                rowRemapTableTemp = rowRemapTableImage;
+                colRemapTableTemp = colRemapTableImage;
+
+                int vectorLoopCount = 0;
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    Rpp32f *srcPtrTempChn, *dstPtrTempChn;
+                    srcPtrTempChn = srcPtrChannel;
+                    dstPtrTempChn = dstPtrTemp;
+                    compute_remap_src_loc_sse(rowRemapTableTemp, colRemapTableTemp, remapSrcLocArray, pSrcStride, pWidthLimit, pHeightLimit);
+
+                    for (int c = 0; c < dstDescPtr->c; c++)
+                    {
+                        __m128 pRow;
+                        rpp_simd_load(rpp_resize_nn_load_f32pln1, srcPtrTempChn, remapSrcLocArray, pRow);
+                        rpp_simd_store(rpp_store4_f32_to_f32, dstPtrTempChn, &pRow);
+                        srcPtrTempChn += srcDescPtr->strides.cStride;
+                        dstPtrTempChn += dstDescPtr->strides.cStride;
+                    }
+                    dstPtrTemp += vectorIncrementPerChannel;
+                    rowRemapTableTemp += vectorIncrementPerChannel;
+                    colRemapTableTemp += vectorIncrementPerChannel;
+                }
+                for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
+                {
+                    Rpp32f * dstPtrTempChannel = dstPtrTemp;
+                    Rpp32f * srcPtrTempChannel = srcPtrChannel;
+                    compute_remap_src_loc(*rowRemapTableTemp++, *colRemapTableTemp++, remappedSrcLoc, srcDescPtr->strides.hStride, widthLimit, heightLimit);
+                    for (int c = 0; c < dstDescPtr->c; c++)
+                    {
+                        *dstPtrTempChannel = *(srcPtrTempChannel + remappedSrcLoc);
+                        dstPtrTempChannel += dstDescPtr->strides.cStride;
+                        srcPtrTempChannel += srcDescPtr->strides.cStride;
+                    }
+                    dstPtrTemp++;
+                }
+                dstPtrRow += dstDescPtr->strides.hStride;
+                rowRemapTableImage += remapTableDescPtr->strides.hStride;
+                colRemapTableImage += remapTableDescPtr->strides.hStride;
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+RppStatus remap_nn_i8_i8_host_tensor(Rpp8s *srcPtr,
+                                     RpptDescPtr srcDescPtr,
+                                     Rpp8s *dstPtr,
+                                     RpptDescPtr dstDescPtr,
+                                     Rpp32f *rowRemapTable,
+                                     Rpp32f *colRemapTable,
+                                     RpptDescPtr remapTableDescPtr,
+                                     RpptROIPtr roiTensorPtrSrc,
+                                     RpptRoiType roiType,
+                                     RppLayoutParams layoutParams,
+                                     rpp::Handle& handle)
+{
+    RpptROI roiDefault = {0, 0, (Rpp32s)srcDescPtr->w, (Rpp32s)srcDescPtr->h};
+    Rpp32u numThreads = handle.GetNumThreads();
+
+    __m128 pSrcChannel = lsx_set1_f32(srcDescPtr->c);
+    __m128 pSrcStride = lsx_set1_f32(srcDescPtr->strides.hStride);
+
+omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+    for(int batchCount = 0; batchCount < dstDescPtr->n; batchCount++)
+    {
+        RpptROI roi;
+        RpptROIPtr roiPtrInput = &roiTensorPtrSrc[batchCount];
+        compute_roi_validation_host(roiPtrInput, &roi, &roiDefault, roiType);
+
+        Rpp8s *srcPtrChannel, *dstPtrChannel, *srcPtrImage, *dstPtrImage;
+        srcPtrImage = srcPtr + batchCount * srcDescPtr->strides.nStride;
+        dstPtrImage = dstPtr + batchCount * dstDescPtr->strides.nStride;
+        srcPtrChannel = srcPtrImage + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
+        dstPtrChannel = dstPtrImage;
+
+        Rpp32f *rowRemapTableImage, *colRemapTableImage;
+        rowRemapTableImage = rowRemapTable + batchCount * remapTableDescPtr->strides.nStride;
+        colRemapTableImage = colRemapTable + batchCount * remapTableDescPtr->strides.nStride;
+        Rpp32u alignedLength = roi.xywhROI.roiWidth & ~3;   // Align dst width to process 4 dst pixels per iteration
+        Rpp32s vectorIncrementPerChannel = 4;
+        Rpp32s vectorIncrement = 12;
+        Rpp32s remappedSrcLoc;
+        Rpp32s remapSrcLocArray[4] = {0};     // Since 4 dst pixels are processed per iteration
+        Rpp32f widthLimit = roi.xywhROI.roiWidth - 1;
+        Rpp32f heightLimit = roi.xywhROI.roiHeight - 1;
+        __m128 pWidthLimit = lsx_set1_f32(widthLimit);
+        __m128 pHeightLimit = lsx_set1_f32(heightLimit);
+
+        // Remap with fused output-layout toggle (NHWC -> NCHW)
+        if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
+        {
+            Rpp8s *dstPtrRowR, *dstPtrRowG, *dstPtrRowB;
+            dstPtrRowR = dstPtrChannel;
+            dstPtrRowG = dstPtrRowR + dstDescPtr->strides.cStride;
+            dstPtrRowB = dstPtrRowG + dstDescPtr->strides.cStride;
+
+            for(int dstLocRow = 0; dstLocRow < roi.xywhROI.roiHeight; dstLocRow++)
+            {
+                Rpp8s *dstPtrTempR, *dstPtrTempG, *dstPtrTempB;
+                dstPtrTempR = dstPtrRowR;
+                dstPtrTempG = dstPtrRowG;
+                dstPtrTempB = dstPtrRowB;
+                Rpp32f *rowRemapTableTemp, *colRemapTableTemp;
+                rowRemapTableTemp = rowRemapTableImage;
+                colRemapTableTemp = colRemapTableImage;
+
+                int vectorLoopCount = 0;
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m128i pxRow;
+                    compute_remap_src_loc_sse(rowRemapTableTemp, colRemapTableTemp, remapSrcLocArray, pSrcStride, pWidthLimit, pHeightLimit, pSrcChannel);
+                    rpp_simd_load(rpp_resize_nn_load_i8pkd3, srcPtrChannel, remapSrcLocArray, pxRow);
+                    rpp_simd_store(rpp_store12_i8pkd3_to_i8pln3, dstPtrTempR, dstPtrTempG, dstPtrTempB, pxRow);
+                    dstPtrTempR += vectorIncrementPerChannel;
+                    dstPtrTempG += vectorIncrementPerChannel;
+                    dstPtrTempB += vectorIncrementPerChannel;
+                    rowRemapTableTemp += vectorIncrementPerChannel;
+                    colRemapTableTemp += vectorIncrementPerChannel;
+                }
+                for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
+                {
+                    compute_remap_src_loc(*rowRemapTableTemp++, *colRemapTableTemp++, remappedSrcLoc, srcDescPtr->strides.hStride, widthLimit, heightLimit, srcDescPtr->c);
+                    *dstPtrTempR++ = *(srcPtrChannel + remappedSrcLoc);
+                    *dstPtrTempG++ = *(srcPtrChannel + 1 + remappedSrcLoc);
+                    *dstPtrTempB++ = *(srcPtrChannel + 2 + remappedSrcLoc);
+                }
+                dstPtrRowR += dstDescPtr->strides.hStride;
+                dstPtrRowG += dstDescPtr->strides.hStride;
+                dstPtrRowB += dstDescPtr->strides.hStride;
+                rowRemapTableImage += remapTableDescPtr->strides.hStride;
+                colRemapTableImage += remapTableDescPtr->strides.hStride;
+            }
+        }
+
+        // Remap with fused output-layout toggle (NCHW -> NHWC)
+        else if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NHWC))
+        {
+            Rpp8s *srcPtrRowR, *srcPtrRowG, *srcPtrRowB;
+            srcPtrRowR = srcPtrChannel;
+            srcPtrRowG = srcPtrRowR + srcDescPtr->strides.cStride;
+            srcPtrRowB = srcPtrRowG + srcDescPtr->strides.cStride;
+            Rpp8s *dstPtrRow;
+            dstPtrRow = dstPtrChannel;
+
+            for(int dstLocRow = 0; dstLocRow < roi.xywhROI.roiHeight; dstLocRow++)
+            {
+                Rpp8s *dstPtrTemp;
+                dstPtrTemp = dstPtrRow;
+                Rpp32f *rowRemapTableTemp, *colRemapTableTemp;
+                rowRemapTableTemp = rowRemapTableImage;
+                colRemapTableTemp = colRemapTableImage;
+
+                int vectorLoopCount = 0;
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m128i pxRow[3];
+                    compute_remap_src_loc_sse(rowRemapTableTemp, colRemapTableTemp, remapSrcLocArray, pSrcStride, pWidthLimit, pHeightLimit);
+                    rpp_simd_load(rpp_resize_nn_load_i8pln1, srcPtrRowR, remapSrcLocArray, pxRow[0]);
+                    rpp_simd_load(rpp_resize_nn_load_i8pln1, srcPtrRowG, remapSrcLocArray, pxRow[1]);
+                    rpp_simd_load(rpp_resize_nn_load_i8pln1, srcPtrRowB, remapSrcLocArray, pxRow[2]);
+                    rpp_simd_store(rpp_store12_i8pln3_to_i8pkd3, dstPtrTemp, pxRow);
+                    dstPtrTemp += vectorIncrement;
+                    rowRemapTableTemp += vectorIncrementPerChannel;
+                    colRemapTableTemp += vectorIncrementPerChannel;
+                }
+                for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
+                {
+                    compute_remap_src_loc(*rowRemapTableTemp++, *colRemapTableTemp++, remappedSrcLoc, srcDescPtr->strides.hStride, widthLimit, heightLimit);
+                    *dstPtrTemp++ = *(srcPtrRowR + remappedSrcLoc);
+                    *dstPtrTemp++ = *(srcPtrRowG + remappedSrcLoc);
+                    *dstPtrTemp++ = *(srcPtrRowB + remappedSrcLoc);
+                }
+                dstPtrRow += dstDescPtr->strides.hStride;
+                rowRemapTableImage += remapTableDescPtr->strides.hStride;
+                colRemapTableImage += remapTableDescPtr->strides.hStride;
+            }
+        }
+
+        // Remap without fused output-layout toggle (NHWC -> NHWC)
+        else if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NHWC))
+        {
+            Rpp8s *dstPtrRow;
+            dstPtrRow = dstPtrChannel;
+
+            for(int dstLocRow = 0; dstLocRow < roi.xywhROI.roiHeight; dstLocRow++)
+            {
+                Rpp8s *dstPtrTemp;
+                dstPtrTemp = dstPtrRow;
+                Rpp32f *rowRemapTableTemp, *colRemapTableTemp;
+                rowRemapTableTemp = rowRemapTableImage;
+                colRemapTableTemp = colRemapTableImage;
+
+                int vectorLoopCount = 0;
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m128i pxRow;
+                    compute_remap_src_loc_sse(rowRemapTableTemp, colRemapTableTemp, remapSrcLocArray, pSrcStride, pWidthLimit, pHeightLimit, pSrcChannel);
+                    rpp_simd_load(rpp_resize_nn_load_i8pkd3, srcPtrChannel, remapSrcLocArray, pxRow);
+                    rpp_simd_store(rpp_store12_i8_to_i8, dstPtrTemp, pxRow);
+                    dstPtrTemp += vectorIncrement;
+                    rowRemapTableTemp += vectorIncrementPerChannel;
+                    colRemapTableTemp += vectorIncrementPerChannel;
+                }
+                for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
+                {
+                    compute_remap_src_loc(*rowRemapTableTemp++, *colRemapTableTemp++, remappedSrcLoc, srcDescPtr->strides.hStride, widthLimit, heightLimit, srcDescPtr->c);
+                    *dstPtrTemp++ = *(srcPtrChannel + remappedSrcLoc);
+                    *dstPtrTemp++ = *(srcPtrChannel + 1 + remappedSrcLoc);
+                    *dstPtrTemp++ = *(srcPtrChannel + 2 + remappedSrcLoc);
+                }
+                dstPtrRow += dstDescPtr->strides.hStride;
+                rowRemapTableImage += remapTableDescPtr->strides.hStride;
+                colRemapTableImage += remapTableDescPtr->strides.hStride;
+            }
+        }
+
+        // Remap without fused output-layout toggle (NCHW -> NCHW for 1 channel and 3 channel)
+        else if ((srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NCHW))
+        {
+            Rpp8s *dstPtrRow;
+            dstPtrRow = dstPtrChannel;
+
+            for(int dstLocRow = 0; dstLocRow < roi.xywhROI.roiHeight; dstLocRow++)
+            {
+                Rpp8s *dstPtrTemp;
+                dstPtrTemp = dstPtrRow;
+                Rpp32f *rowRemapTableTemp, *colRemapTableTemp;
+                rowRemapTableTemp = rowRemapTableImage;
+                colRemapTableTemp = colRemapTableImage;
+
+                int vectorLoopCount = 0;
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    Rpp8s *srcPtrTempChn, *dstPtrTempChn;
+                    srcPtrTempChn = srcPtrChannel;
+                    dstPtrTempChn = dstPtrTemp;
+                    compute_remap_src_loc_sse(rowRemapTableTemp, colRemapTableTemp, remapSrcLocArray, pSrcStride, pWidthLimit, pHeightLimit);
+
+                    for (int c = 0; c < dstDescPtr->c; c++)
+                    {
+                        __m128i pxRow;
+                        rpp_simd_load(rpp_resize_nn_load_i8pln1, srcPtrTempChn, remapSrcLocArray, pxRow);
+                        rpp_simd_store(rpp_storeu_si32, dstPtrTempChn, pxRow);
+                        srcPtrTempChn += srcDescPtr->strides.cStride;
+                        dstPtrTempChn += dstDescPtr->strides.cStride;
+                    }
+                    dstPtrTemp += vectorIncrementPerChannel;
+                    rowRemapTableTemp += vectorIncrementPerChannel;
+                    colRemapTableTemp += vectorIncrementPerChannel;
+                }
+                for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
+                {
+                    Rpp8s * dstPtrTempChannel = dstPtrTemp;
+                    Rpp8s * srcPtrTempChannel = srcPtrChannel;
+                    compute_remap_src_loc(*rowRemapTableTemp++, *colRemapTableTemp++, remappedSrcLoc, srcDescPtr->strides.hStride, widthLimit, heightLimit);
+                    for (int c = 0; c < dstDescPtr->c; c++)
+                    {
+                        *dstPtrTempChannel = *(srcPtrTempChannel + remappedSrcLoc);
+                        dstPtrTempChannel += dstDescPtr->strides.cStride;
+                        srcPtrTempChannel += srcDescPtr->strides.cStride;
+                    }
+                    dstPtrTemp++;
+                }
+                dstPtrRow += dstDescPtr->strides.hStride;
+                rowRemapTableImage += remapTableDescPtr->strides.hStride;
+                colRemapTableImage += remapTableDescPtr->strides.hStride;
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+RppStatus remap_nn_f16_f16_host_tensor(Rpp16f *srcPtr,
+                                       RpptDescPtr srcDescPtr,
+                                       Rpp16f *dstPtr,
+                                       RpptDescPtr dstDescPtr,
+                                       Rpp32f *rowRemapTable,
+                                       Rpp32f *colRemapTable,
+                                       RpptDescPtr remapTableDescPtr,
+                                       RpptROIPtr roiTensorPtrSrc,
+                                       RpptRoiType roiType,
+                                       RppLayoutParams layoutParams,
+                                       rpp::Handle& handle)
+{
+    RpptROI roiDefault = {0, 0, (Rpp32s)srcDescPtr->w, (Rpp32s)srcDescPtr->h};
+    Rpp32u numThreads = handle.GetNumThreads();
+
+    __m128 pSrcChannel = lsx_set1_f32(srcDescPtr->c);
+    __m128 pSrcStride = lsx_set1_f32(srcDescPtr->strides.hStride);
+
+omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+    for(int batchCount = 0; batchCount < dstDescPtr->n; batchCount++)
+    {
+        RpptROI roi;
+        RpptROIPtr roiPtrInput = &roiTensorPtrSrc[batchCount];
+        compute_roi_validation_host(roiPtrInput, &roi, &roiDefault, roiType);
+
+        Rpp16f *srcPtrChannel, *dstPtrChannel, *srcPtrImage, *dstPtrImage;
+        srcPtrImage = srcPtr + batchCount * srcDescPtr->strides.nStride;
+        dstPtrImage = dstPtr + batchCount * dstDescPtr->strides.nStride;
+        srcPtrChannel = srcPtrImage + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
+        dstPtrChannel = dstPtrImage;
+
+        Rpp32f *rowRemapTableImage, *colRemapTableImage;
+        rowRemapTableImage = rowRemapTable + batchCount * remapTableDescPtr->strides.nStride;
+        colRemapTableImage = colRemapTable + batchCount * remapTableDescPtr->strides.nStride;
+
+        Rpp32f widthLimit = roi.xywhROI.roiWidth - 1;
+        Rpp32f heightLimit = roi.xywhROI.roiHeight - 1;
+        // Remap with 3 channel inputs and outputs
+        if (srcDescPtr->c == 3 && dstDescPtr->c == 3)
+        {
+            Rpp16f *srcPtrRowR, *srcPtrRowG, *srcPtrRowB;
+            srcPtrRowR = srcPtrChannel;
+            srcPtrRowG = srcPtrRowR + srcDescPtr->strides.cStride;
+            srcPtrRowB = srcPtrRowG + srcDescPtr->strides.cStride;
+            Rpp16f *dstPtrRowR, *dstPtrRowG, *dstPtrRowB;
+            dstPtrRowR = dstPtrChannel;
+            dstPtrRowG = dstPtrRowR + dstDescPtr->strides.cStride;
+            dstPtrRowB = dstPtrRowG + dstDescPtr->strides.cStride;
+
+            for(int dstLocRow = 0; dstLocRow < roi.xywhROI.roiHeight; dstLocRow++)
+            {
+                Rpp16f *dstPtrTempR, *dstPtrTempG, *dstPtrTempB;
+                dstPtrTempR = dstPtrRowR;
+                dstPtrTempG = dstPtrRowG;
+                dstPtrTempB = dstPtrRowB;
+                Rpp32f *rowRemapTableTemp, *colRemapTableTemp;
+                rowRemapTableTemp = rowRemapTableImage;
+                colRemapTableTemp = colRemapTableImage;
+
+                int vectorLoopCount = 0;
+                for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
+                {
+                    Rpp32s remappedSrcLoc;
+                    compute_remap_src_loc(*rowRemapTableTemp++, *colRemapTableTemp++, remappedSrcLoc, srcDescPtr->strides.hStride, widthLimit, heightLimit, layoutParams.bufferMultiplier);
+                    *dstPtrTempR = (Rpp16f)*(srcPtrRowR + remappedSrcLoc);
+                    *dstPtrTempG = (Rpp16f)*(srcPtrRowG + remappedSrcLoc);
+                    *dstPtrTempB = (Rpp16f)*(srcPtrRowB + remappedSrcLoc);
+                    dstPtrTempR = dstPtrTempR + dstDescPtr->strides.wStride;
+                    dstPtrTempG = dstPtrTempG + dstDescPtr->strides.wStride;
+                    dstPtrTempB = dstPtrTempB + dstDescPtr->strides.wStride;
+                }
+                dstPtrRowR += dstDescPtr->strides.hStride;
+                dstPtrRowG += dstDescPtr->strides.hStride;
+                dstPtrRowB += dstDescPtr->strides.hStride;
+                rowRemapTableImage += remapTableDescPtr->strides.hStride;
+                colRemapTableImage += remapTableDescPtr->strides.hStride;
+            }
+        }
+
+        // Remap with single channel inputs and outputs
+        else
+        {
+            Rpp16f *srcPtrRow, *dstPtrRow;
+            srcPtrRow = srcPtrChannel;
+            dstPtrRow = dstPtrChannel;
+
+            for(int dstLocRow = 0; dstLocRow < roi.xywhROI.roiHeight; dstLocRow++)
+            {
+                Rpp16f *dstPtrTemp;
+                dstPtrTemp = dstPtrRow;
+                Rpp32f *rowRemapTableTemp, *colRemapTableTemp;
+                rowRemapTableTemp = rowRemapTableImage;
+                colRemapTableTemp = colRemapTableImage;
+
+                int vectorLoopCount = 0;
+                for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
+                {
+                    Rpp32s remappedSrcLoc;
+                    compute_remap_src_loc(*rowRemapTableTemp++, *colRemapTableTemp++, remappedSrcLoc, srcDescPtr->strides.hStride, widthLimit, heightLimit);
+                    *dstPtrTemp++ = (Rpp16f)*(srcPtrRow + remappedSrcLoc);
+                }
+                dstPtrRow += dstDescPtr->strides.hStride;
+                rowRemapTableImage += remapTableDescPtr->strides.hStride;
+                colRemapTableImage += remapTableDescPtr->strides.hStride;
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+/************* BILINEAR INTERPOLATION *************/
+
+RppStatus remap_bilinear_u8_u8_host_tensor(Rpp8u *srcPtr,
+                                           RpptDescPtr srcDescPtr,
+                                           Rpp8u *dstPtr,
+                                           RpptDescPtr dstDescPtr,
+                                           Rpp32f *rowRemapTable,
+                                           Rpp32f *colRemapTable,
+                                           RpptDescPtr remapTableDescPtr,
+                                           RpptROIPtr roiTensorPtrSrc,
+                                           RpptRoiType roiType,
+                                           RppLayoutParams layoutParams,
+                                           rpp::Handle& handle)
+{
+    RpptROI roiDefault = {0, 0, (Rpp32s)srcDescPtr->w, (Rpp32s)srcDescPtr->h};
+    Rpp32u numThreads = handle.GetNumThreads();
+#if __loongarch_asx
+    __m256 pSrcChannel = lasx_set1_f32(srcDescPtr->c);
+    __m256 pSrcStrideH = lasx_set1_f32(srcDescPtr->strides.hStride);
+    __m256i pxSrcStridesCHW[3];
+    pxSrcStridesCHW[0] = __lasx_xvreplgr2vr_w(srcDescPtr->strides.cStride);
+    pxSrcStridesCHW[1] = __lasx_xvreplgr2vr_w(srcDescPtr->strides.hStride);
+    pxSrcStridesCHW[2] = __lasx_xvreplgr2vr_w(srcDescPtr->strides.wStride);
+#endif
+
+omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+    for(int batchCount = 0; batchCount < dstDescPtr->n; batchCount++)
+    {
+        RpptROI roi, roiLTRB;
+        RpptROIPtr roiPtrInput = &roiTensorPtrSrc[batchCount];
+        compute_roi_validation_host(roiPtrInput, &roi, &roiDefault, roiType);
+        compute_ltrb_from_xywh_host(&roi, &roiLTRB);
+
+        Rpp8u *srcPtrChannel, *dstPtrChannel, *srcPtrImage, *dstPtrImage;
+        srcPtrImage = srcPtr + batchCount * srcDescPtr->strides.nStride;
+        dstPtrImage = dstPtr + batchCount * dstDescPtr->strides.nStride;
+        srcPtrChannel = srcPtrImage;
+        dstPtrChannel = dstPtrImage;
+
+        Rpp32f *rowRemapTableImage, *colRemapTableImage;
+        rowRemapTableImage = rowRemapTable + batchCount * remapTableDescPtr->strides.nStride;
+        colRemapTableImage = colRemapTable + batchCount * remapTableDescPtr->strides.nStride;
+
+        Rpp32u alignedLength = roi.xywhROI.roiWidth & ~7;   // Align dst width to process 8 dst pixels per iteration
+        Rpp32s vectorIncrementPerChannel = 8;
+        Rpp32s vectorIncrement = 24;
+
+#if __loongarch_asx
+        __m256 pBilinearCoeffs[4];
+        __m256 pRoiLTRB[4];
+        pRoiLTRB[0] = lasx_set1_f32(roiLTRB.ltrbROI.lt.x);
+        pRoiLTRB[1] = lasx_set1_f32(roiLTRB.ltrbROI.lt.y);
+        pRoiLTRB[2] = lasx_set1_f32(roiLTRB.ltrbROI.rb.x);
+        pRoiLTRB[3] = lasx_set1_f32(roiLTRB.ltrbROI.rb.y);
+        RpptBilinearNbhoodLocsVecLen8 srcLocs;
+#endif
+
+        // Remap with fused output-layout toggle (NHWC -> NCHW)
+        if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
+        {
+            Rpp8u *dstPtrRowR, *dstPtrRowG, *dstPtrRowB;
+            dstPtrRowR = dstPtrChannel;
+            dstPtrRowG = dstPtrRowR + dstDescPtr->strides.cStride;
+            dstPtrRowB = dstPtrRowG + dstDescPtr->strides.cStride;
+
+            for(int dstLocRow = 0; dstLocRow < roi.xywhROI.roiHeight; dstLocRow++)
+            {
+                Rpp8u *dstPtrTempR, *dstPtrTempG, *dstPtrTempB;
+                dstPtrTempR = dstPtrRowR;
+                dstPtrTempG = dstPtrRowG;
+                dstPtrTempB = dstPtrRowB;
+
+                Rpp32f *rowRemapTableTemp, *colRemapTableTemp;
+                rowRemapTableTemp = rowRemapTableImage;
+                colRemapTableTemp = colRemapTableImage;
+
+                int vectorLoopCount = 0;
+#if __loongarch_asx
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256 pSrcX = (__m256)__lasx_xvld(colRemapTableTemp, 0);
+                    __m256 pSrcY = (__m256)__lasx_xvld(rowRemapTableTemp, 0);
+                    __m256 pSrc[12], pDst[3];
+                    compute_generic_bilinear_srclocs_3c_avx(pSrcY, pSrcX, srcLocs, pBilinearCoeffs, pSrcStrideH, pxSrcStridesCHW, srcDescPtr->c, pRoiLTRB, true);
+                    rpp_simd_load(rpp_generic_bilinear_load_3c_avx<Rpp8u>, srcPtrChannel, srcDescPtr, srcLocs, pSrcY, pSrcX, pRoiLTRB, pSrc);  // Load input pixels required for bilinear interpolation
+                    compute_bilinear_interpolation_3c_avx(pSrc, pBilinearCoeffs, pDst); // Compute Bilinear interpolation
+                    rpp_simd_store(rpp_store24_f32pln3_to_u8pln3_avx, dstPtrTempR, dstPtrTempG, dstPtrTempB, pDst); // Store dst pixels
+                    dstPtrTempR += vectorIncrementPerChannel;
+                    dstPtrTempG += vectorIncrementPerChannel;
+                    dstPtrTempB += vectorIncrementPerChannel;
+                    rowRemapTableTemp += vectorIncrementPerChannel;
+                    colRemapTableTemp += vectorIncrementPerChannel;
+                }
+#endif
+                for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
+                    compute_generic_bilinear_interpolation_pkd3_to_pln3(*rowRemapTableTemp++, *colRemapTableTemp++, &roiLTRB, dstPtrTempR++, dstPtrTempG++, dstPtrTempB++, srcPtrChannel, srcDescPtr);
+
+                dstPtrRowR += dstDescPtr->strides.hStride;
+                dstPtrRowG += dstDescPtr->strides.hStride;
+                dstPtrRowB += dstDescPtr->strides.hStride;
+                rowRemapTableImage += remapTableDescPtr->strides.hStride;
+                colRemapTableImage += remapTableDescPtr->strides.hStride;
+            }
+        }
+
+        // Remap with fused output-layout toggle (NCHW -> NHWC)
+        else if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NHWC))
+        {
+            Rpp8u *dstPtrRow;
+            dstPtrRow = dstPtrChannel;
+
+            for(int dstLocRow = 0; dstLocRow < roi.xywhROI.roiHeight; dstLocRow++)
+            {
+                Rpp8u *dstPtrTemp;
+                dstPtrTemp = dstPtrRow;
+
+                Rpp32f *rowRemapTableTemp, *colRemapTableTemp;
+                rowRemapTableTemp = rowRemapTableImage;
+                colRemapTableTemp = colRemapTableImage;
+
+                int vectorLoopCount = 0;
+#if __loongarch_asx
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256 pSrcX = (__m256)__lasx_xvld(colRemapTableTemp, 0);
+                    __m256 pSrcY = (__m256)__lasx_xvld(rowRemapTableTemp, 0);
+                    __m256 pSrc[12], pDst[3];
+                    compute_generic_bilinear_srclocs_3c_avx(pSrcY, pSrcX, srcLocs, pBilinearCoeffs, pSrcStrideH, pxSrcStridesCHW, srcDescPtr->c, pRoiLTRB, false);
+                    rpp_simd_load(rpp_generic_bilinear_load_3c_avx<Rpp8u>, srcPtrChannel, srcDescPtr, srcLocs, pSrcY, pSrcX, pRoiLTRB, pSrc);  // Load input pixels required for bilinear interpolation
+                    compute_bilinear_interpolation_3c_avx(pSrc, pBilinearCoeffs, pDst); // Compute Bilinear interpolation
+                    rpp_simd_store(rpp_store24_f32pln3_to_u8pkd3_avx, dstPtrTemp, pDst); // Store dst pixels
+                    dstPtrTemp += vectorIncrement;
+                    rowRemapTableTemp += vectorIncrementPerChannel;
+                    colRemapTableTemp += vectorIncrementPerChannel;
+                }
+#endif
+                for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
+                {
+                    compute_generic_bilinear_interpolation_pln3pkd3_to_pkd3(*rowRemapTableTemp++, *colRemapTableTemp++, &roiLTRB, dstPtrTemp, srcPtrChannel, srcDescPtr);
+                    dstPtrTemp += 3;
+                }
+
+                dstPtrRow += dstDescPtr->strides.hStride;
+                rowRemapTableImage += remapTableDescPtr->strides.hStride;
+                colRemapTableImage += remapTableDescPtr->strides.hStride;
+            }
+        }
+
+        // Remap without fused output-layout toggle (NHWC -> NHWC)
+        else if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NHWC))
+        {
+            Rpp8u *dstPtrRow;
+            dstPtrRow = dstPtrChannel;
+
+            for(int dstLocRow = 0; dstLocRow < roi.xywhROI.roiHeight; dstLocRow++)
+            {
+                Rpp8u *dstPtrTemp;
+                dstPtrTemp = dstPtrRow;
+
+                Rpp32f *rowRemapTableTemp, *colRemapTableTemp;
+                rowRemapTableTemp = rowRemapTableImage;
+                colRemapTableTemp = colRemapTableImage;
+
+                int vectorLoopCount = 0;
+#if __loongarch_asx
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256 pSrcX = (__m256)__lasx_xvld(colRemapTableTemp, 0);
+                    __m256 pSrcY = (__m256)__lasx_xvld(rowRemapTableTemp, 0);
+                    __m256 pSrc[12], pDst[3];
+                    compute_generic_bilinear_srclocs_3c_avx(pSrcY, pSrcX, srcLocs, pBilinearCoeffs, pSrcStrideH, pxSrcStridesCHW, srcDescPtr->c, pRoiLTRB, true);
+                    rpp_simd_load(rpp_generic_bilinear_load_3c_avx<Rpp8u>, srcPtrChannel, srcDescPtr, srcLocs, pSrcY, pSrcX, pRoiLTRB, pSrc);  // Load input pixels required for bilinear interpolation
+                    compute_bilinear_interpolation_3c_avx(pSrc, pBilinearCoeffs, pDst); // Compute Bilinear interpolation
+                    rpp_simd_store(rpp_store24_f32pln3_to_u8pkd3_avx, dstPtrTemp, pDst); // Store dst pixels
+                    dstPtrTemp += vectorIncrement;
+                    rowRemapTableTemp += vectorIncrementPerChannel;
+                    colRemapTableTemp += vectorIncrementPerChannel;
+                }
+#endif
+                for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
+                {
+                    compute_generic_bilinear_interpolation_pln3pkd3_to_pkd3(*rowRemapTableTemp++, *colRemapTableTemp++, &roiLTRB, dstPtrTemp, srcPtrChannel, srcDescPtr);
+                    dstPtrTemp += 3;
+                }
+
+                dstPtrRow += dstDescPtr->strides.hStride;
+                rowRemapTableImage += remapTableDescPtr->strides.hStride;
+                colRemapTableImage += remapTableDescPtr->strides.hStride;
+            }
+        }
+
+        // Remap without fused output-layout toggle (NCHW -> NCHW)
+        else if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NCHW))
+        {
+            Rpp8u *dstPtrRowR, *dstPtrRowG, *dstPtrRowB;
+            dstPtrRowR = dstPtrChannel;
+            dstPtrRowG = dstPtrRowR + dstDescPtr->strides.cStride;
+            dstPtrRowB = dstPtrRowG + dstDescPtr->strides.cStride;
+
+            for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+            {
+                Rpp8u *dstPtrTempR, *dstPtrTempG, *dstPtrTempB;
+                dstPtrTempR = dstPtrRowR;
+                dstPtrTempG = dstPtrRowG;
+                dstPtrTempB = dstPtrRowB;
+
+                Rpp32f *rowRemapTableTemp, *colRemapTableTemp;
+                rowRemapTableTemp = rowRemapTableImage;
+                colRemapTableTemp = colRemapTableImage;
+
+                int vectorLoopCount = 0;
+#if __loongarch_asx
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256 pSrcX = (__m256)__lasx_xvld(colRemapTableTemp, 0);
+                    __m256 pSrcY = (__m256)__lasx_xvld(rowRemapTableTemp, 0);
+                    __m256 pSrc[12], pDst[3];
+                    compute_generic_bilinear_srclocs_3c_avx(pSrcY, pSrcX, srcLocs, pBilinearCoeffs, pSrcStrideH, pxSrcStridesCHW, srcDescPtr->c, pRoiLTRB, false);
+                    rpp_simd_load(rpp_generic_bilinear_load_3c_avx<Rpp8u>, srcPtrChannel, srcDescPtr, srcLocs, pSrcY, pSrcX, pRoiLTRB, pSrc);  // Load input pixels required for bilinear interpolation
+                    compute_bilinear_interpolation_3c_avx(pSrc, pBilinearCoeffs, pDst); // Compute Bilinear interpolation
+                    rpp_simd_store(rpp_store24_f32pln3_to_u8pln3_avx, dstPtrTempR, dstPtrTempG, dstPtrTempB, pDst); // Store dst pixels
+                    dstPtrTempR += vectorIncrementPerChannel;
+                    dstPtrTempG += vectorIncrementPerChannel;
+                    dstPtrTempB += vectorIncrementPerChannel;
+                    rowRemapTableTemp += vectorIncrementPerChannel;
+                    colRemapTableTemp += vectorIncrementPerChannel;
+                }
+#endif
+                for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
+                    compute_generic_bilinear_interpolation_pln_to_pln(*rowRemapTableTemp++, *colRemapTableTemp++, &roiLTRB, dstPtrTempR++, srcPtrChannel, srcDescPtr, dstDescPtr);
+
+                dstPtrRowR += dstDescPtr->strides.hStride;
+                dstPtrRowG += dstDescPtr->strides.hStride;
+                dstPtrRowB += dstDescPtr->strides.hStride;
+                rowRemapTableImage += remapTableDescPtr->strides.hStride;
+                colRemapTableImage += remapTableDescPtr->strides.hStride;
+            }
+        }
+
+        // Remap without fused output-layout toggle single channel (NCHW -> NCHW)
+        else if ((srcDescPtr->c == 1) && (srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NCHW))
+        {
+            Rpp8u *dstPtrRow;
+            dstPtrRow = dstPtrChannel;
+
+            for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+            {
+                Rpp8u *dstPtrTemp;
+                dstPtrTemp = dstPtrRow;
+
+                Rpp32f *rowRemapTableTemp, *colRemapTableTemp;
+                rowRemapTableTemp = rowRemapTableImage;
+                colRemapTableTemp = colRemapTableImage;
+
+                int vectorLoopCount = 0;
+#if __loongarch_asx
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256 pSrcX = (__m256)__lasx_xvld(colRemapTableTemp, 0);
+                    __m256 pSrcY = (__m256)__lasx_xvld(rowRemapTableTemp, 0);
+                    __m256 pSrc[4], pDst;
+                    compute_generic_bilinear_srclocs_1c_avx(pSrcY, pSrcX, srcLocs, pBilinearCoeffs, pSrcStrideH, pxSrcStridesCHW, pRoiLTRB);
+                    rpp_simd_load(rpp_generic_bilinear_load_1c_avx<Rpp8u>, srcPtrChannel, srcDescPtr, srcLocs, pSrcY, pSrcX, pRoiLTRB, pSrc);  // Load input pixels required for bilinear interpolation
+                    compute_bilinear_interpolation_1c_avx(pSrc, pBilinearCoeffs, pDst); // Compute Bilinear interpolation
+                    rpp_simd_store(rpp_store8_f32pln1_to_u8pln1_avx, dstPtrTemp, pDst); // Store dst pixels
+                    dstPtrTemp += vectorIncrementPerChannel;
+                    rowRemapTableTemp += vectorIncrementPerChannel;
+                    colRemapTableTemp += vectorIncrementPerChannel;
+                }
+#endif
+                for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
+                    compute_generic_bilinear_interpolation_pln_to_pln(*rowRemapTableTemp++, *colRemapTableTemp++, &roiLTRB, dstPtrTemp++, srcPtrChannel, srcDescPtr, dstDescPtr);
+
+                dstPtrRow += dstDescPtr->strides.hStride;
+                rowRemapTableImage += remapTableDescPtr->strides.hStride;
+                colRemapTableImage += remapTableDescPtr->strides.hStride;
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+RppStatus remap_bilinear_f32_f32_host_tensor(Rpp32f *srcPtr,
+                                             RpptDescPtr srcDescPtr,
+                                             Rpp32f *dstPtr,
+                                             RpptDescPtr dstDescPtr,
+                                             Rpp32f *rowRemapTable,
+                                             Rpp32f *colRemapTable,
+                                             RpptDescPtr remapTableDescPtr,
+                                             RpptROIPtr roiTensorPtrSrc,
+                                             RpptRoiType roiType,
+                                             RppLayoutParams layoutParams,
+                                             rpp::Handle& handle)
+{
+    RpptROI roiDefault = {0, 0, (Rpp32s)srcDescPtr->w, (Rpp32s)srcDescPtr->h};
+    Rpp32u numThreads = handle.GetNumThreads();
+
+#if __loongarch_asx
+    __m256 pSrcChannel = lasx_set1_f32(srcDescPtr->c);
+    __m256 pSrcStrideH = lasx_set1_f32(srcDescPtr->strides.hStride);
+    __m256i pxSrcStridesCHW[3];
+    pxSrcStridesCHW[0] = __lasx_xvreplgr2vr_w(srcDescPtr->strides.cStride);
+    pxSrcStridesCHW[1] = __lasx_xvreplgr2vr_w(srcDescPtr->strides.hStride);
+    pxSrcStridesCHW[2] = __lasx_xvreplgr2vr_w(srcDescPtr->strides.wStride);
+#endif
+
+omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+    for(int batchCount = 0; batchCount < dstDescPtr->n; batchCount++)
+    {
+        RpptROI roi, roiLTRB;
+        RpptROIPtr roiPtrInput = &roiTensorPtrSrc[batchCount];
+        compute_roi_validation_host(roiPtrInput, &roi, &roiDefault, roiType);
+        compute_ltrb_from_xywh_host(&roi, &roiLTRB);
+
+        Rpp32f *srcPtrChannel, *dstPtrChannel, *srcPtrImage, *dstPtrImage;
+        srcPtrImage = srcPtr + batchCount * srcDescPtr->strides.nStride;
+        dstPtrImage = dstPtr + batchCount * dstDescPtr->strides.nStride;
+        srcPtrChannel = srcPtrImage;
+        dstPtrChannel = dstPtrImage;
+
+        Rpp32f *rowRemapTableImage, *colRemapTableImage;
+        rowRemapTableImage = rowRemapTable + batchCount * remapTableDescPtr->strides.nStride;
+        colRemapTableImage = colRemapTable + batchCount * remapTableDescPtr->strides.nStride;
+
+        Rpp32u alignedLength = roi.xywhROI.roiWidth & ~7;   // Align dst width to process 8 dst pixels per iteration
+        Rpp32s vectorIncrementPerChannel = 8;
+        Rpp32s vectorIncrement = 24;
+
+#if __loongarch_asx
+        __m256 pBilinearCoeffs[4];
+        __m256 pRoiLTRB[4];
+        pRoiLTRB[0] = lasx_set1_f32(roiLTRB.ltrbROI.lt.x);
+        pRoiLTRB[1] = lasx_set1_f32(roiLTRB.ltrbROI.lt.y);
+        pRoiLTRB[2] = lasx_set1_f32(roiLTRB.ltrbROI.rb.x);
+        pRoiLTRB[3] = lasx_set1_f32(roiLTRB.ltrbROI.rb.y);
+        RpptBilinearNbhoodLocsVecLen8 srcLocs;
+#endif
+
+        // Remap with fused output-layout toggle (NHWC -> NCHW)
+        if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
+        {
+            Rpp32f *dstPtrRowR, *dstPtrRowG, *dstPtrRowB;
+            dstPtrRowR = dstPtrChannel;
+            dstPtrRowG = dstPtrRowR + dstDescPtr->strides.cStride;
+            dstPtrRowB = dstPtrRowG + dstDescPtr->strides.cStride;
+
+            for(int dstLocRow = 0; dstLocRow < roi.xywhROI.roiHeight; dstLocRow++)
+            {
+                Rpp32f *dstPtrTempR, *dstPtrTempG, *dstPtrTempB;
+                dstPtrTempR = dstPtrRowR;
+                dstPtrTempG = dstPtrRowG;
+                dstPtrTempB = dstPtrRowB;
+
+                Rpp32f *rowRemapTableTemp, *colRemapTableTemp;
+                rowRemapTableTemp = rowRemapTableImage;
+                colRemapTableTemp = colRemapTableImage;
+
+                int vectorLoopCount = 0;
+#if __loongarch_asx
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256 pSrcX = (__m256)__lasx_xvld(colRemapTableTemp, 0);
+                    __m256 pSrcY = (__m256)__lasx_xvld(rowRemapTableTemp, 0);
+                    __m256 pSrc[12], pDst[3];
+                    compute_generic_bilinear_srclocs_3c_avx(pSrcY, pSrcX, srcLocs, pBilinearCoeffs, pSrcStrideH, pxSrcStridesCHW, srcDescPtr->c, pRoiLTRB, true);
+                    rpp_simd_load(rpp_generic_bilinear_load_3c_avx<Rpp32f>, srcPtrChannel, srcDescPtr, srcLocs, pSrcY, pSrcX, pRoiLTRB, pSrc);  // Load input pixels required for bilinear interpolation
+                    compute_bilinear_interpolation_3c_avx(pSrc, pBilinearCoeffs, pDst); // Compute Bilinear interpolation
+                    rpp_simd_store(rpp_store24_f32pln3_to_f32pln3_avx, dstPtrTempR, dstPtrTempG, dstPtrTempB, pDst); // Store dst pixels
+                    dstPtrTempR += vectorIncrementPerChannel;
+                    dstPtrTempG += vectorIncrementPerChannel;
+                    dstPtrTempB += vectorIncrementPerChannel;
+                    rowRemapTableTemp += vectorIncrementPerChannel;
+                    colRemapTableTemp += vectorIncrementPerChannel;
+                }
+#endif
+                for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
+                    compute_generic_bilinear_interpolation_pkd3_to_pln3(*rowRemapTableTemp++, *colRemapTableTemp++, &roiLTRB, dstPtrTempR++, dstPtrTempG++, dstPtrTempB++, srcPtrChannel, srcDescPtr);
+
+                dstPtrRowR += dstDescPtr->strides.hStride;
+                dstPtrRowG += dstDescPtr->strides.hStride;
+                dstPtrRowB += dstDescPtr->strides.hStride;
+                rowRemapTableImage += remapTableDescPtr->strides.hStride;
+                colRemapTableImage += remapTableDescPtr->strides.hStride;
+            }
+        }
+
+        // Remap with fused output-layout toggle (NCHW -> NHWC)
+        else if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NHWC))
+        {
+            Rpp32f *dstPtrRow;
+            dstPtrRow = dstPtrChannel;
+
+            for(int dstLocRow = 0; dstLocRow < roi.xywhROI.roiHeight; dstLocRow++)
+            {
+                Rpp32f *dstPtrTemp;
+                dstPtrTemp = dstPtrRow;
+
+                Rpp32f *rowRemapTableTemp, *colRemapTableTemp;
+                rowRemapTableTemp = rowRemapTableImage;
+                colRemapTableTemp = colRemapTableImage;
+
+                int vectorLoopCount = 0;
+#if __loongarch_asx
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256 pSrcX = (__m256)__lasx_xvld(colRemapTableTemp, 0);
+                    __m256 pSrcY = (__m256)__lasx_xvld(rowRemapTableTemp, 0);
+                    __m256 pSrc[12], pDst[3];
+                    compute_generic_bilinear_srclocs_3c_avx(pSrcY, pSrcX, srcLocs, pBilinearCoeffs, pSrcStrideH, pxSrcStridesCHW, srcDescPtr->c, pRoiLTRB, false);
+                    rpp_simd_load(rpp_generic_bilinear_load_3c_avx<Rpp32f>, srcPtrChannel, srcDescPtr, srcLocs, pSrcY, pSrcX, pRoiLTRB, pSrc);  // Load input pixels required for bilinear interpolation
+                    compute_bilinear_interpolation_3c_avx(pSrc, pBilinearCoeffs, pDst); // Compute Bilinear interpolation
+                    rpp_simd_store(rpp_store24_f32pln3_to_f32pkd3_avx, dstPtrTemp, pDst); // Store dst pixels
+                    dstPtrTemp += vectorIncrement;
+                    rowRemapTableTemp += vectorIncrementPerChannel;
+                    colRemapTableTemp += vectorIncrementPerChannel;
+                }
+#endif
+                for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
+                {
+                    compute_generic_bilinear_interpolation_pln3pkd3_to_pkd3(*rowRemapTableTemp++, *colRemapTableTemp++, &roiLTRB, dstPtrTemp, srcPtrChannel, srcDescPtr);
+                    dstPtrTemp += 3;
+                }
+
+                dstPtrRow += dstDescPtr->strides.hStride;
+                rowRemapTableImage += remapTableDescPtr->strides.hStride;
+                colRemapTableImage += remapTableDescPtr->strides.hStride;
+            }
+        }
+
+        // Remap without fused output-layout toggle (NHWC -> NHWC)
+        else if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NHWC))
+        {
+            Rpp32f *dstPtrRow;
+            dstPtrRow = dstPtrChannel;
+
+            for(int dstLocRow = 0; dstLocRow < roi.xywhROI.roiHeight; dstLocRow++)
+            {
+                Rpp32f *dstPtrTemp;
+                dstPtrTemp = dstPtrRow;
+
+                Rpp32f *rowRemapTableTemp, *colRemapTableTemp;
+                rowRemapTableTemp = rowRemapTableImage;
+                colRemapTableTemp = colRemapTableImage;
+
+                int vectorLoopCount = 0;
+#if __loongarch_asx
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256 pSrcX = (__m256)__lasx_xvld(colRemapTableTemp, 0);
+                    __m256 pSrcY = (__m256)__lasx_xvld(rowRemapTableTemp, 0);
+                    __m256 pSrc[12], pDst[3];
+                    compute_generic_bilinear_srclocs_3c_avx(pSrcY, pSrcX, srcLocs, pBilinearCoeffs, pSrcStrideH, pxSrcStridesCHW, srcDescPtr->c, pRoiLTRB, true);
+                    rpp_simd_load(rpp_generic_bilinear_load_3c_avx<Rpp32f>, srcPtrChannel, srcDescPtr, srcLocs, pSrcY, pSrcX, pRoiLTRB, pSrc);  // Load input pixels required for bilinear interpolation
+                    compute_bilinear_interpolation_3c_avx(pSrc, pBilinearCoeffs, pDst); // Compute Bilinear interpolation
+                    rpp_simd_store(rpp_store24_f32pln3_to_f32pkd3_avx, dstPtrTemp, pDst); // Store dst pixels
+                    dstPtrTemp += vectorIncrement;
+                    rowRemapTableTemp += vectorIncrementPerChannel;
+                    colRemapTableTemp += vectorIncrementPerChannel;
+                }
+#endif
+                for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
+                {
+                    compute_generic_bilinear_interpolation_pln3pkd3_to_pkd3(*rowRemapTableTemp++, *colRemapTableTemp++, &roiLTRB, dstPtrTemp, srcPtrChannel, srcDescPtr);
+                    dstPtrTemp += 3;
+                }
+
+                dstPtrRow += dstDescPtr->strides.hStride;
+                rowRemapTableImage += remapTableDescPtr->strides.hStride;
+                colRemapTableImage += remapTableDescPtr->strides.hStride;
+            }
+        }
+
+        // Remap without fused output-layout toggle (NCHW -> NCHW)
+        else if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NCHW))
+        {
+            Rpp32f *dstPtrRowR, *dstPtrRowG, *dstPtrRowB;
+            dstPtrRowR = dstPtrChannel;
+            dstPtrRowG = dstPtrRowR + dstDescPtr->strides.cStride;
+            dstPtrRowB = dstPtrRowG + dstDescPtr->strides.cStride;
+
+            for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+            {
+                Rpp32f *dstPtrTempR, *dstPtrTempG, *dstPtrTempB;
+                dstPtrTempR = dstPtrRowR;
+                dstPtrTempG = dstPtrRowG;
+                dstPtrTempB = dstPtrRowB;
+
+                Rpp32f *rowRemapTableTemp, *colRemapTableTemp;
+                rowRemapTableTemp = rowRemapTableImage;
+                colRemapTableTemp = colRemapTableImage;
+
+                int vectorLoopCount = 0;
+#if __loongarch_asx
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256 pSrcX = (__m256)__lasx_xvld(colRemapTableTemp, 0);
+                    __m256 pSrcY = (__m256)__lasx_xvld(rowRemapTableTemp, 0);
+                    __m256 pSrc[12], pDst[3];
+                    compute_generic_bilinear_srclocs_3c_avx(pSrcY, pSrcX, srcLocs, pBilinearCoeffs, pSrcStrideH, pxSrcStridesCHW, srcDescPtr->c, pRoiLTRB, false);
+                    rpp_simd_load(rpp_generic_bilinear_load_3c_avx<Rpp32f>, srcPtrChannel, srcDescPtr, srcLocs, pSrcY, pSrcX, pRoiLTRB, pSrc);  // Load input pixels required for bilinear interpolation
+                    compute_bilinear_interpolation_3c_avx(pSrc, pBilinearCoeffs, pDst); // Compute Bilinear interpolation
+                    rpp_simd_store(rpp_store24_f32pln3_to_f32pln3_avx, dstPtrTempR, dstPtrTempG, dstPtrTempB, pDst); // Store dst pixels
+                    dstPtrTempR += vectorIncrementPerChannel;
+                    dstPtrTempG += vectorIncrementPerChannel;
+                    dstPtrTempB += vectorIncrementPerChannel;
+                    rowRemapTableTemp += vectorIncrementPerChannel;
+                    colRemapTableTemp += vectorIncrementPerChannel;
+                }
+#endif
+                for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
+                    compute_generic_bilinear_interpolation_pln_to_pln(*rowRemapTableTemp++, *colRemapTableTemp++, &roiLTRB, dstPtrTempR++, srcPtrChannel, srcDescPtr, dstDescPtr);
+
+                dstPtrRowR += dstDescPtr->strides.hStride;
+                dstPtrRowG += dstDescPtr->strides.hStride;
+                dstPtrRowB += dstDescPtr->strides.hStride;
+                rowRemapTableImage += remapTableDescPtr->strides.hStride;
+                colRemapTableImage += remapTableDescPtr->strides.hStride;
+            }
+        }
+
+        // Remap without fused output-layout toggle single channel (NCHW -> NCHW)
+        else if ((srcDescPtr->c == 1) && (srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NCHW))
+        {
+            Rpp32f *dstPtrRow;
+            dstPtrRow = dstPtrChannel;
+
+            for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+            {
+                Rpp32f *dstPtrTemp;
+                dstPtrTemp = dstPtrRow;
+
+                Rpp32f *rowRemapTableTemp, *colRemapTableTemp;
+                rowRemapTableTemp = rowRemapTableImage;
+                colRemapTableTemp = colRemapTableImage;
+
+                int vectorLoopCount = 0;
+#if __loongarch_asx
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256 pSrcX = (__m256)__lasx_xvld(colRemapTableTemp, 0);
+                    __m256 pSrcY = (__m256)__lasx_xvld(rowRemapTableTemp, 0);
+                    __m256 pSrc[4], pDst;
+                    compute_generic_bilinear_srclocs_1c_avx(pSrcY, pSrcX, srcLocs, pBilinearCoeffs, pSrcStrideH, pxSrcStridesCHW, pRoiLTRB);
+                    rpp_simd_load(rpp_generic_bilinear_load_1c_avx<Rpp32f>, srcPtrChannel, srcDescPtr, srcLocs, pSrcY, pSrcX, pRoiLTRB, pSrc);  // Load input pixels required for bilinear interpolation
+                    compute_bilinear_interpolation_1c_avx(pSrc, pBilinearCoeffs, pDst); // Compute Bilinear interpolation
+                    rpp_simd_store(rpp_store8_f32pln1_to_f32pln1_avx, dstPtrTemp, pDst); // Store dst pixels
+                    dstPtrTemp += vectorIncrementPerChannel;
+                    rowRemapTableTemp += vectorIncrementPerChannel;
+                    colRemapTableTemp += vectorIncrementPerChannel;
+                }
+#endif
+                for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
+                    compute_generic_bilinear_interpolation_pln_to_pln(*rowRemapTableTemp++, *colRemapTableTemp++, &roiLTRB, dstPtrTemp++, srcPtrChannel, srcDescPtr, dstDescPtr);
+
+                dstPtrRow += dstDescPtr->strides.hStride;
+                rowRemapTableImage += remapTableDescPtr->strides.hStride;
+                colRemapTableImage += remapTableDescPtr->strides.hStride;
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+
+RppStatus remap_bilinear_i8_i8_host_tensor(Rpp8s *srcPtr,
+                                           RpptDescPtr srcDescPtr,
+                                           Rpp8s *dstPtr,
+                                           RpptDescPtr dstDescPtr,
+                                           Rpp32f *rowRemapTable,
+                                           Rpp32f *colRemapTable,
+                                           RpptDescPtr remapTableDescPtr,
+                                           RpptROIPtr roiTensorPtrSrc,
+                                           RpptRoiType roiType,
+                                           RppLayoutParams ,
+                                           rpp::Handle& handle)
+{
+    RpptROI roiDefault = {0, 0, (Rpp32s)srcDescPtr->w, (Rpp32s)srcDescPtr->h};
+    Rpp32u numThreads = handle.GetNumThreads();
+
+#if __loongarch_asx
+    __m256 pSrcChannel = lasx_set1_f32(srcDescPtr->c);
+    __m256 pSrcStrideH = lasx_set1_f32(srcDescPtr->strides.hStride);
+    __m256i pxSrcStridesCHW[3];
+    pxSrcStridesCHW[0] = __lasx_xvreplgr2vr_w(srcDescPtr->strides.cStride);
+    pxSrcStridesCHW[1] = __lasx_xvreplgr2vr_w(srcDescPtr->strides.hStride);
+    pxSrcStridesCHW[2] = __lasx_xvreplgr2vr_w(srcDescPtr->strides.wStride);
+#endif
+
+omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+    for(int batchCount = 0; batchCount < dstDescPtr->n; batchCount++)
+    {
+        RpptROI roi, roiLTRB;
+        RpptROIPtr roiPtrInput = &roiTensorPtrSrc[batchCount];
+        compute_roi_validation_host(roiPtrInput, &roi, &roiDefault, roiType);
+        compute_ltrb_from_xywh_host(&roi, &roiLTRB);
+
+        Rpp8s *srcPtrChannel, *dstPtrChannel, *srcPtrImage, *dstPtrImage;
+        srcPtrImage = srcPtr + batchCount * srcDescPtr->strides.nStride;
+        dstPtrImage = dstPtr + batchCount * dstDescPtr->strides.nStride;
+        srcPtrChannel = srcPtrImage;
+        dstPtrChannel = dstPtrImage;
+
+        Rpp32f *rowRemapTableImage, *colRemapTableImage;
+        rowRemapTableImage = rowRemapTable + batchCount * remapTableDescPtr->strides.nStride;
+        colRemapTableImage = colRemapTable + batchCount * remapTableDescPtr->strides.nStride;
+
+        Rpp32u alignedLength = roi.xywhROI.roiWidth & ~7;   // Align dst width to process 8 dst pixels per iteration
+        Rpp32s vectorIncrementPerChannel = 8;
+        Rpp32s vectorIncrement = 24;
+
+#if __loongarch_asx
+        __m256 pBilinearCoeffs[4];
+        __m256 pRoiLTRB[4];
+        pRoiLTRB[0] = lasx_set1_f32(roiLTRB.ltrbROI.lt.x);
+        pRoiLTRB[1] = lasx_set1_f32(roiLTRB.ltrbROI.lt.y);
+        pRoiLTRB[2] = lasx_set1_f32(roiLTRB.ltrbROI.rb.x);
+        pRoiLTRB[3] = lasx_set1_f32(roiLTRB.ltrbROI.rb.y);
+        RpptBilinearNbhoodLocsVecLen8 srcLocs;
+#endif
+
+        // Remap with fused output-layout toggle (NHWC -> NCHW)
+        if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
+        {
+            Rpp8s *dstPtrRowR, *dstPtrRowG, *dstPtrRowB;
+            dstPtrRowR = dstPtrChannel;
+            dstPtrRowG = dstPtrRowR + dstDescPtr->strides.cStride;
+            dstPtrRowB = dstPtrRowG + dstDescPtr->strides.cStride;
+
+            for(int dstLocRow = 0; dstLocRow < roi.xywhROI.roiHeight; dstLocRow++)
+            {
+                Rpp8s *dstPtrTempR, *dstPtrTempG, *dstPtrTempB;
+                dstPtrTempR = dstPtrRowR;
+                dstPtrTempG = dstPtrRowG;
+                dstPtrTempB = dstPtrRowB;
+
+                Rpp32f *rowRemapTableTemp, *colRemapTableTemp;
+                rowRemapTableTemp = rowRemapTableImage;
+                colRemapTableTemp = colRemapTableImage;
+
+                int vectorLoopCount = 0;
+#if __loongarch_asx
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256 pSrcX = (__m256)__lasx_xvld(colRemapTableTemp, 0);
+                    __m256 pSrcY = (__m256)__lasx_xvld(rowRemapTableTemp, 0);
+                    __m256 pSrc[12], pDst[3];
+                    compute_generic_bilinear_srclocs_3c_avx(pSrcY, pSrcX, srcLocs, pBilinearCoeffs, pSrcStrideH, pxSrcStridesCHW, srcDescPtr->c, pRoiLTRB, true);
+                    rpp_simd_load(rpp_generic_bilinear_load_3c_avx<Rpp8s>, srcPtrChannel, srcDescPtr, srcLocs, pSrcY, pSrcX, pRoiLTRB, pSrc);  // Load input pixels required for bilinear interpolation
+                    compute_bilinear_interpolation_3c_avx(pSrc, pBilinearCoeffs, pDst); // Compute Bilinear interpolation
+                    compute_offset_i8_3c_avx(pDst);
+                    rpp_simd_store(rpp_store24_f32pln3_to_i8pln3_avx, dstPtrTempR, dstPtrTempG, dstPtrTempB, pDst); // Store dst pixels
+                    dstPtrTempR += vectorIncrementPerChannel;
+                    dstPtrTempG += vectorIncrementPerChannel;
+                    dstPtrTempB += vectorIncrementPerChannel;
+                    rowRemapTableTemp += vectorIncrementPerChannel;
+                    colRemapTableTemp += vectorIncrementPerChannel;
+                }
+#endif
+                for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
+                    compute_generic_bilinear_interpolation_pkd3_to_pln3(*rowRemapTableTemp++, *colRemapTableTemp++, &roiLTRB, dstPtrTempR++, dstPtrTempG++, dstPtrTempB++, srcPtrChannel, srcDescPtr);
+
+                dstPtrRowR += dstDescPtr->strides.hStride;
+                dstPtrRowG += dstDescPtr->strides.hStride;
+                dstPtrRowB += dstDescPtr->strides.hStride;
+                rowRemapTableImage += remapTableDescPtr->strides.hStride;
+                colRemapTableImage += remapTableDescPtr->strides.hStride;
+            }
+        }
+
+        // Remap with fused output-layout toggle (NCHW -> NHWC)
+        else if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NHWC))
+        {
+            Rpp8s *dstPtrRow;
+            dstPtrRow = dstPtrChannel;
+
+            for(int dstLocRow = 0; dstLocRow < roi.xywhROI.roiHeight; dstLocRow++)
+            {
+                Rpp8s *dstPtrTemp;
+                dstPtrTemp = dstPtrRow;
+
+                Rpp32f *rowRemapTableTemp, *colRemapTableTemp;
+                rowRemapTableTemp = rowRemapTableImage;
+                colRemapTableTemp = colRemapTableImage;
+
+                int vectorLoopCount = 0;
+#if __loongarch_asx
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256 pSrcX = (__m256)__lasx_xvld(colRemapTableTemp, 0);
+                    __m256 pSrcY = (__m256)__lasx_xvld(rowRemapTableTemp, 0);
+                    __m256 pSrc[12], pDst[3];
+                    compute_generic_bilinear_srclocs_3c_avx(pSrcY, pSrcX, srcLocs, pBilinearCoeffs, pSrcStrideH, pxSrcStridesCHW, srcDescPtr->c, pRoiLTRB, false);
+                    rpp_simd_load(rpp_generic_bilinear_load_3c_avx<Rpp8s>, srcPtrChannel, srcDescPtr, srcLocs, pSrcY, pSrcX, pRoiLTRB, pSrc);  // Load input pixels required for bilinear interpolation
+                    compute_bilinear_interpolation_3c_avx(pSrc, pBilinearCoeffs, pDst); // Compute Bilinear interpolation
+                    compute_offset_i8_3c_avx(pDst);
+                    rpp_simd_store(rpp_store24_f32pln3_to_i8pkd3_avx, dstPtrTemp, pDst); // Store dst pixels
+                    dstPtrTemp += vectorIncrement;
+                    rowRemapTableTemp += vectorIncrementPerChannel;
+                    colRemapTableTemp += vectorIncrementPerChannel;
+                }
+#endif
+                for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
+                {
+                    compute_generic_bilinear_interpolation_pln3pkd3_to_pkd3(*rowRemapTableTemp++, *colRemapTableTemp++, &roiLTRB, dstPtrTemp, srcPtrChannel, srcDescPtr);
+                    dstPtrTemp += 3;
+                }
+
+                dstPtrRow += dstDescPtr->strides.hStride;
+                rowRemapTableImage += remapTableDescPtr->strides.hStride;
+                colRemapTableImage += remapTableDescPtr->strides.hStride;
+            }
+        }
+
+        // Remap without fused output-layout toggle (NHWC -> NHWC)
+        else if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NHWC))
+        {
+            Rpp8s *dstPtrRow;
+            dstPtrRow = dstPtrChannel;
+
+            for(int dstLocRow = 0; dstLocRow < roi.xywhROI.roiHeight; dstLocRow++)
+            {
+                Rpp8s *dstPtrTemp;
+                dstPtrTemp = dstPtrRow;
+
+                Rpp32f *rowRemapTableTemp, *colRemapTableTemp;
+                rowRemapTableTemp = rowRemapTableImage;
+                colRemapTableTemp = colRemapTableImage;
+
+                int vectorLoopCount = 0;
+#if __loongarch_asx
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256 pSrcX = (__m256)__lasx_xvld(colRemapTableTemp, 0);
+                    __m256 pSrcY = (__m256)__lasx_xvld(rowRemapTableTemp, 0);
+                    __m256 pSrc[12], pDst[3];
+                    compute_generic_bilinear_srclocs_3c_avx(pSrcY, pSrcX, srcLocs, pBilinearCoeffs, pSrcStrideH, pxSrcStridesCHW, srcDescPtr->c, pRoiLTRB, true);
+                    rpp_simd_load(rpp_generic_bilinear_load_3c_avx<Rpp8s>, srcPtrChannel, srcDescPtr, srcLocs, pSrcY, pSrcX, pRoiLTRB, pSrc);  // Load input pixels required for bilinear interpolation
+                    compute_bilinear_interpolation_3c_avx(pSrc, pBilinearCoeffs, pDst); // Compute Bilinear interpolation
+                    compute_offset_i8_3c_avx(pDst);
+                    rpp_simd_store(rpp_store24_f32pln3_to_i8pkd3_avx, dstPtrTemp, pDst); // Store dst pixels
+                    dstPtrTemp += vectorIncrement;
+                    rowRemapTableTemp += vectorIncrementPerChannel;
+                    colRemapTableTemp += vectorIncrementPerChannel;
+                }
+#endif
+                for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
+                {
+                    compute_generic_bilinear_interpolation_pln3pkd3_to_pkd3(*rowRemapTableTemp++, *colRemapTableTemp++, &roiLTRB, dstPtrTemp, srcPtrChannel, srcDescPtr);
+                    dstPtrTemp += 3;
+                }
+
+                dstPtrRow += dstDescPtr->strides.hStride;
+                rowRemapTableImage += remapTableDescPtr->strides.hStride;
+                colRemapTableImage += remapTableDescPtr->strides.hStride;
+            }
+        }
+
+        // Remap without fused output-layout toggle (NCHW -> NCHW)
+        else if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NCHW))
+        {
+            Rpp8s *dstPtrRowR, *dstPtrRowG, *dstPtrRowB;
+            dstPtrRowR = dstPtrChannel;
+            dstPtrRowG = dstPtrRowR + dstDescPtr->strides.cStride;
+            dstPtrRowB = dstPtrRowG + dstDescPtr->strides.cStride;
+
+            for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+            {
+                Rpp8s *dstPtrTempR, *dstPtrTempG, *dstPtrTempB;
+                dstPtrTempR = dstPtrRowR;
+                dstPtrTempG = dstPtrRowG;
+                dstPtrTempB = dstPtrRowB;
+
+                Rpp32f *rowRemapTableTemp, *colRemapTableTemp;
+                rowRemapTableTemp = rowRemapTableImage;
+                colRemapTableTemp = colRemapTableImage;
+
+                int vectorLoopCount = 0;
+#if __loongarch_asx
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256 pSrcX = (__m256)__lasx_xvld(colRemapTableTemp, 0);
+                    __m256 pSrcY = (__m256)__lasx_xvld(rowRemapTableTemp, 0);
+                    __m256 pSrc[12], pDst[3];
+                    compute_generic_bilinear_srclocs_3c_avx(pSrcY, pSrcX, srcLocs, pBilinearCoeffs, pSrcStrideH, pxSrcStridesCHW, srcDescPtr->c, pRoiLTRB, false);
+                    rpp_simd_load(rpp_generic_bilinear_load_3c_avx<Rpp8s>, srcPtrChannel, srcDescPtr, srcLocs, pSrcY, pSrcX, pRoiLTRB, pSrc);  // Load input pixels required for bilinear interpolation
+                    compute_bilinear_interpolation_3c_avx(pSrc, pBilinearCoeffs, pDst); // Compute Bilinear interpolation
+                    compute_offset_i8_3c_avx(pDst);
+                    rpp_simd_store(rpp_store24_f32pln3_to_i8pln3_avx, dstPtrTempR, dstPtrTempG, dstPtrTempB, pDst); // Store dst pixels
+                    dstPtrTempR += vectorIncrementPerChannel;
+                    dstPtrTempG += vectorIncrementPerChannel;
+                    dstPtrTempB += vectorIncrementPerChannel;
+                    rowRemapTableTemp += vectorIncrementPerChannel;
+                    colRemapTableTemp += vectorIncrementPerChannel;
+                }
+#endif
+                for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
+                    compute_generic_bilinear_interpolation_pln_to_pln(*rowRemapTableTemp++, *colRemapTableTemp++, &roiLTRB, dstPtrTempR++, srcPtrChannel, srcDescPtr, dstDescPtr);
+
+                dstPtrRowR += dstDescPtr->strides.hStride;
+                dstPtrRowG += dstDescPtr->strides.hStride;
+                dstPtrRowB += dstDescPtr->strides.hStride;
+                rowRemapTableImage += remapTableDescPtr->strides.hStride;
+                colRemapTableImage += remapTableDescPtr->strides.hStride;
+            }
+        }
+
+        // Remap without fused output-layout toggle single channel (NCHW -> NCHW)
+        else if ((srcDescPtr->c == 1) && (srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NCHW))
+        {
+            Rpp8s *dstPtrRow;
+            dstPtrRow = dstPtrChannel;
+
+            for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+            {
+                Rpp8s *dstPtrTemp;
+                dstPtrTemp = dstPtrRow;
+
+                Rpp32f *rowRemapTableTemp, *colRemapTableTemp;
+                rowRemapTableTemp = rowRemapTableImage;
+                colRemapTableTemp = colRemapTableImage;
+
+                int vectorLoopCount = 0;
+#if __loongarch_asx
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256 pSrcX = (__m256)__lasx_xvld(colRemapTableTemp, 0);
+                    __m256 pSrcY = (__m256)__lasx_xvld(rowRemapTableTemp, 0);
+                    __m256 pSrc[4], pDst;
+                    compute_generic_bilinear_srclocs_1c_avx(pSrcY, pSrcX, srcLocs, pBilinearCoeffs, pSrcStrideH, pxSrcStridesCHW, pRoiLTRB);
+                    rpp_simd_load(rpp_generic_bilinear_load_1c_avx<Rpp8s>, srcPtrChannel, srcDescPtr, srcLocs, pSrcY, pSrcX, pRoiLTRB, pSrc);  // Load input pixels required for bilinear interpolation
+                    compute_bilinear_interpolation_1c_avx(pSrc, pBilinearCoeffs, pDst); // Compute Bilinear interpolation
+                    compute_offset_i8_1c_avx(pDst);
+                    rpp_simd_store(rpp_store8_f32pln1_to_i8pln1_avx, dstPtrTemp, pDst); // Store dst pixels
+                    dstPtrTemp += vectorIncrementPerChannel;
+                    rowRemapTableTemp += vectorIncrementPerChannel;
+                    colRemapTableTemp += vectorIncrementPerChannel;
+                }
+#endif
+                for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
+                    compute_generic_bilinear_interpolation_pln_to_pln(*rowRemapTableTemp++, *colRemapTableTemp++, &roiLTRB, dstPtrTemp++, srcPtrChannel, srcDescPtr, dstDescPtr);
+
+                dstPtrRow += dstDescPtr->strides.hStride;
+                rowRemapTableImage += remapTableDescPtr->strides.hStride;
+                colRemapTableImage += remapTableDescPtr->strides.hStride;
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+RppStatus remap_bilinear_f16_f16_host_tensor(Rpp16f *srcPtr,
+                                             RpptDescPtr srcDescPtr,
+                                             Rpp16f *dstPtr,
+                                             RpptDescPtr dstDescPtr,
+                                             Rpp32f *rowRemapTable,
+                                             Rpp32f *colRemapTable,
+                                             RpptDescPtr remapTableDescPtr,
+                                             RpptROIPtr roiTensorPtrSrc,
+                                             RpptRoiType roiType,
+                                             RppLayoutParams layoutParams,
+                                             rpp::Handle& handle)
+{
+    RpptROI roiDefault = {0, 0, (Rpp32s)srcDescPtr->w, (Rpp32s)srcDescPtr->h};
+    Rpp32u numThreads = handle.GetNumThreads();
+
+#if __loongarch_asx
+    __m256 pSrcChannel = lasx_set1_f32(srcDescPtr->c);
+    __m256 pSrcStrideH = lasx_set1_f32(srcDescPtr->strides.hStride);
+    __m256i pxSrcStridesCHW[3];
+    pxSrcStridesCHW[0] = __lasx_xvreplgr2vr_w(srcDescPtr->strides.cStride);
+    pxSrcStridesCHW[1] = __lasx_xvreplgr2vr_w(srcDescPtr->strides.hStride);
+    pxSrcStridesCHW[2] = __lasx_xvreplgr2vr_w(srcDescPtr->strides.wStride);
+#endif
+
+omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+    for(int batchCount = 0; batchCount < dstDescPtr->n; batchCount++)
+    {
+        RpptROI roi, roiLTRB;
+        RpptROIPtr roiPtrInput = &roiTensorPtrSrc[batchCount];
+        compute_roi_validation_host(roiPtrInput, &roi, &roiDefault, roiType);
+        compute_ltrb_from_xywh_host(&roi, &roiLTRB);
+
+        Rpp16f *srcPtrChannel, *dstPtrChannel, *srcPtrImage, *dstPtrImage;
+        srcPtrImage = srcPtr + batchCount * srcDescPtr->strides.nStride;
+        dstPtrImage = dstPtr + batchCount * dstDescPtr->strides.nStride;
+        srcPtrChannel = srcPtrImage;
+        dstPtrChannel = dstPtrImage;
+
+        Rpp32f *rowRemapTableImage, *colRemapTableImage;
+        rowRemapTableImage = rowRemapTable + batchCount * remapTableDescPtr->strides.nStride;
+        colRemapTableImage = colRemapTable + batchCount * remapTableDescPtr->strides.nStride;
+
+        Rpp32u alignedLength = roi.xywhROI.roiWidth & ~7;   // Align dst width to process 8 dst pixels per iteration
+        Rpp32s vectorIncrementPerChannel = 8;
+        Rpp32s vectorIncrement = 24;
+
+#if __loongarch_asx
+        __m256 pBilinearCoeffs[4];
+        __m256 pRoiLTRB[4];
+        pRoiLTRB[0] = lasx_set1_f32(roiLTRB.ltrbROI.lt.x);
+        pRoiLTRB[1] = lasx_set1_f32(roiLTRB.ltrbROI.lt.y);
+        pRoiLTRB[2] = lasx_set1_f32(roiLTRB.ltrbROI.rb.x);
+        pRoiLTRB[3] = lasx_set1_f32(roiLTRB.ltrbROI.rb.y);
+        RpptBilinearNbhoodLocsVecLen8 srcLocs;
+#endif
+
+        // Remap with fused output-layout toggle (NHWC -> NCHW)
+        if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
+        {
+            Rpp16f *dstPtrRowR, *dstPtrRowG, *dstPtrRowB;
+            dstPtrRowR = dstPtrChannel;
+            dstPtrRowG = dstPtrRowR + dstDescPtr->strides.cStride;
+            dstPtrRowB = dstPtrRowG + dstDescPtr->strides.cStride;
+
+            for(int dstLocRow = 0; dstLocRow < roi.xywhROI.roiHeight; dstLocRow++)
+            {
+                Rpp16f *dstPtrTempR, *dstPtrTempG, *dstPtrTempB;
+                dstPtrTempR = dstPtrRowR;
+                dstPtrTempG = dstPtrRowG;
+                dstPtrTempB = dstPtrRowB;
+
+                Rpp32f *rowRemapTableTemp, *colRemapTableTemp;
+                rowRemapTableTemp = rowRemapTableImage;
+                colRemapTableTemp = colRemapTableImage;
+
+                int vectorLoopCount = 0;
+#if __loongarch_asx
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256 pSrcX = (__m256)__lasx_xvld(colRemapTableTemp, 0);
+                    __m256 pSrcY = (__m256)__lasx_xvld(rowRemapTableTemp, 0);
+                    __m256 pSrc[12], pDst[3];
+                    Rpp32f dstPtrTempR_ps[8], dstPtrTempG_ps[8], dstPtrTempB_ps[8];
+                    compute_generic_bilinear_srclocs_3c_avx(pSrcY, pSrcX, srcLocs, pBilinearCoeffs, pSrcStrideH, pxSrcStridesCHW, srcDescPtr->c, pRoiLTRB, true);
+                    rpp_simd_load(rpp_generic_bilinear_load_3c_avx<Rpp16f>, srcPtrChannel, srcDescPtr, srcLocs, pSrcY, pSrcX, pRoiLTRB, pSrc);  // Load input pixels required for bilinear interpolation
+                    compute_bilinear_interpolation_3c_avx(pSrc, pBilinearCoeffs, pDst); // Compute Bilinear interpolation
+                    rpp_simd_store(rpp_store24_f32pln3_to_f32pln3_avx, dstPtrTempR_ps, dstPtrTempG_ps, dstPtrTempB_ps, pDst); // Store dst pixels
+                    for(int cnt = 0; cnt < vectorIncrementPerChannel; cnt++)
+                    {
+                        dstPtrTempR[cnt] = (Rpp16f) dstPtrTempR_ps[cnt];
+                        dstPtrTempG[cnt] = (Rpp16f) dstPtrTempG_ps[cnt];
+                        dstPtrTempB[cnt] = (Rpp16f) dstPtrTempB_ps[cnt];
+                    }
+                    dstPtrTempR += vectorIncrementPerChannel;
+                    dstPtrTempG += vectorIncrementPerChannel;
+                    dstPtrTempB += vectorIncrementPerChannel;
+                    rowRemapTableTemp += vectorIncrementPerChannel;
+                    colRemapTableTemp += vectorIncrementPerChannel;
+                }
+#endif
+                for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
+                    compute_generic_bilinear_interpolation_pkd3_to_pln3(*rowRemapTableTemp++, *colRemapTableTemp++, &roiLTRB, dstPtrTempR++, dstPtrTempG++, dstPtrTempB++, srcPtrChannel, srcDescPtr);
+
+                dstPtrRowR += dstDescPtr->strides.hStride;
+                dstPtrRowG += dstDescPtr->strides.hStride;
+                dstPtrRowB += dstDescPtr->strides.hStride;
+                rowRemapTableImage += remapTableDescPtr->strides.hStride;
+                colRemapTableImage += remapTableDescPtr->strides.hStride;
+            }
+        }
+
+        // Remap with fused output-layout toggle (NCHW -> NHWC)
+        else if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NHWC))
+        {
+            Rpp16f *dstPtrRow;
+            dstPtrRow = dstPtrChannel;
+
+            for(int dstLocRow = 0; dstLocRow < roi.xywhROI.roiHeight; dstLocRow++)
+            {
+                Rpp16f *dstPtrTemp;
+                dstPtrTemp = dstPtrRow;
+
+                Rpp32f *rowRemapTableTemp, *colRemapTableTemp;
+                rowRemapTableTemp = rowRemapTableImage;
+                colRemapTableTemp = colRemapTableImage;
+
+                int vectorLoopCount = 0;
+#if __loongarch_asx
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256 pSrcX = (__m256)__lasx_xvld(colRemapTableTemp, 0);
+                    __m256 pSrcY = (__m256)__lasx_xvld(rowRemapTableTemp, 0);
+                    __m256 pSrc[12], pDst[3];
+                    Rpp32f dstPtrTemp_ps[25];
+                    compute_generic_bilinear_srclocs_3c_avx(pSrcY, pSrcX, srcLocs, pBilinearCoeffs, pSrcStrideH, pxSrcStridesCHW, srcDescPtr->c, pRoiLTRB, false);
+                    rpp_simd_load(rpp_generic_bilinear_load_3c_avx<Rpp16f>, srcPtrChannel, srcDescPtr, srcLocs, pSrcY, pSrcX, pRoiLTRB, pSrc);  // Load input pixels required for bilinear interpolation
+                    compute_bilinear_interpolation_3c_avx(pSrc, pBilinearCoeffs, pDst); // Compute Bilinear interpolation
+                    rpp_simd_store(rpp_store24_f32pln3_to_f32pkd3_avx, dstPtrTemp_ps, pDst); // Store dst pixels
+                    for(int cnt = 0; cnt < vectorIncrement; cnt++)
+                        dstPtrTemp[cnt] = (Rpp16f) dstPtrTemp_ps[cnt];
+                    dstPtrTemp += vectorIncrement;
+                    rowRemapTableTemp += vectorIncrementPerChannel;
+                    colRemapTableTemp += vectorIncrementPerChannel;
+                }
+#endif
+                for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
+                {
+                    compute_generic_bilinear_interpolation_pln3pkd3_to_pkd3(*rowRemapTableTemp++, *colRemapTableTemp++, &roiLTRB, dstPtrTemp, srcPtrChannel, srcDescPtr);
+                    dstPtrTemp += 3;
+                }
+
+                dstPtrRow += dstDescPtr->strides.hStride;
+                rowRemapTableImage += remapTableDescPtr->strides.hStride;
+                colRemapTableImage += remapTableDescPtr->strides.hStride;
+            }
+        }
+
+        // Remap without fused output-layout toggle (NHWC -> NHWC)
+        else if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NHWC))
+        {
+            Rpp16f *dstPtrRow;
+            dstPtrRow = dstPtrChannel;
+
+            for(int dstLocRow = 0; dstLocRow < roi.xywhROI.roiHeight; dstLocRow++)
+            {
+                Rpp16f *dstPtrTemp;
+                dstPtrTemp = dstPtrRow;
+
+                Rpp32f *rowRemapTableTemp, *colRemapTableTemp;
+                rowRemapTableTemp = rowRemapTableImage;
+                colRemapTableTemp = colRemapTableImage;
+
+                int vectorLoopCount = 0;
+#if __loongarch_asx
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256 pSrcX = (__m256)__lasx_xvld(colRemapTableTemp, 0);
+                    __m256 pSrcY = (__m256)__lasx_xvld(rowRemapTableTemp, 0);
+                    __m256 pSrc[12], pDst[3];
+                    Rpp32f dstPtrTemp_ps[25];
+                    compute_generic_bilinear_srclocs_3c_avx(pSrcY, pSrcX, srcLocs, pBilinearCoeffs, pSrcStrideH, pxSrcStridesCHW, srcDescPtr->c, pRoiLTRB, true);
+                    rpp_simd_load(rpp_generic_bilinear_load_3c_avx<Rpp16f>, srcPtrChannel, srcDescPtr, srcLocs, pSrcY, pSrcX, pRoiLTRB, pSrc);  // Load input pixels required for bilinear interpolation
+                    compute_bilinear_interpolation_3c_avx(pSrc, pBilinearCoeffs, pDst); // Compute Bilinear interpolation
+                    rpp_simd_store(rpp_store24_f32pln3_to_f32pkd3_avx, dstPtrTemp_ps, pDst); // Store dst pixels
+                    for(int cnt = 0; cnt < vectorIncrement; cnt++)
+                        dstPtrTemp[cnt] = (Rpp16f) dstPtrTemp_ps[cnt];
+                    dstPtrTemp += vectorIncrement;
+                    rowRemapTableTemp += vectorIncrementPerChannel;
+                    colRemapTableTemp += vectorIncrementPerChannel;
+                }
+#endif
+                for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
+                {
+                    compute_generic_bilinear_interpolation_pln3pkd3_to_pkd3(*rowRemapTableTemp++, *colRemapTableTemp++, &roiLTRB, dstPtrTemp, srcPtrChannel, srcDescPtr);
+                    dstPtrTemp += 3;
+                }
+
+                dstPtrRow += dstDescPtr->strides.hStride;
+                rowRemapTableImage += remapTableDescPtr->strides.hStride;
+                colRemapTableImage += remapTableDescPtr->strides.hStride;
+            }
+        }
+
+        // Remap without fused output-layout toggle (NCHW -> NCHW)
+        else if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NCHW))
+        {
+            Rpp16f *dstPtrRowR, *dstPtrRowG, *dstPtrRowB;
+            dstPtrRowR = dstPtrChannel;
+            dstPtrRowG = dstPtrRowR + dstDescPtr->strides.cStride;
+            dstPtrRowB = dstPtrRowG + dstDescPtr->strides.cStride;
+
+            for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+            {
+                Rpp16f *dstPtrTempR, *dstPtrTempG, *dstPtrTempB;
+                dstPtrTempR = dstPtrRowR;
+                dstPtrTempG = dstPtrRowG;
+                dstPtrTempB = dstPtrRowB;
+
+                Rpp32f *rowRemapTableTemp, *colRemapTableTemp;
+                rowRemapTableTemp = rowRemapTableImage;
+                colRemapTableTemp = colRemapTableImage;
+
+                int vectorLoopCount = 0;
+#if __loongarch_asx
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256 pSrcX = (__m256)__lasx_xvld(colRemapTableTemp, 0);
+                    __m256 pSrcY = (__m256)__lasx_xvld(rowRemapTableTemp, 0);
+                    __m256 pSrc[12], pDst[3];
+                    Rpp32f dstPtrTempR_ps[8], dstPtrTempG_ps[8], dstPtrTempB_ps[8];
+                    compute_generic_bilinear_srclocs_3c_avx(pSrcY, pSrcX, srcLocs, pBilinearCoeffs, pSrcStrideH, pxSrcStridesCHW, srcDescPtr->c, pRoiLTRB, false);
+                    rpp_simd_load(rpp_generic_bilinear_load_3c_avx<Rpp16f>, srcPtrChannel, srcDescPtr, srcLocs, pSrcY, pSrcX, pRoiLTRB, pSrc);  // Load input pixels required for bilinear interpolation
+                    compute_bilinear_interpolation_3c_avx(pSrc, pBilinearCoeffs, pDst); // Compute Bilinear interpolation
+                    rpp_simd_store(rpp_store24_f32pln3_to_f32pln3_avx, dstPtrTempR_ps, dstPtrTempG_ps, dstPtrTempB_ps, pDst); // Store dst pixels
+                    for(int cnt = 0; cnt < vectorIncrementPerChannel; cnt++)
+                    {
+                        dstPtrTempR[cnt] = (Rpp16f) dstPtrTempR_ps[cnt];
+                        dstPtrTempG[cnt] = (Rpp16f) dstPtrTempG_ps[cnt];
+                        dstPtrTempB[cnt] = (Rpp16f) dstPtrTempB_ps[cnt];
+                    }
+                    dstPtrTempR += vectorIncrementPerChannel;
+                    dstPtrTempG += vectorIncrementPerChannel;
+                    dstPtrTempB += vectorIncrementPerChannel;
+                    rowRemapTableTemp += vectorIncrementPerChannel;
+                    colRemapTableTemp += vectorIncrementPerChannel;
+                }
+#endif
+                for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
+                    compute_generic_bilinear_interpolation_pln_to_pln(*rowRemapTableTemp++, *colRemapTableTemp++, &roiLTRB, dstPtrTempR++, srcPtrChannel, srcDescPtr, dstDescPtr);
+
+                dstPtrRowR += dstDescPtr->strides.hStride;
+                dstPtrRowG += dstDescPtr->strides.hStride;
+                dstPtrRowB += dstDescPtr->strides.hStride;
+                rowRemapTableImage += remapTableDescPtr->strides.hStride;
+                colRemapTableImage += remapTableDescPtr->strides.hStride;
+            }
+        }
+
+        // Remap without fused output-layout toggle single channel (NCHW -> NCHW)
+        else if ((srcDescPtr->c == 1) && (srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NCHW))
+        {
+            Rpp16f *dstPtrRow;
+            dstPtrRow = dstPtrChannel;
+
+            for(int i = 0; i < roi.xywhROI.roiHeight; i++)
+            {
+                Rpp16f *dstPtrTemp;
+                dstPtrTemp = dstPtrRow;
+
+                Rpp32f *rowRemapTableTemp, *colRemapTableTemp;
+                rowRemapTableTemp = rowRemapTableImage;
+                colRemapTableTemp = colRemapTableImage;
+
+                int vectorLoopCount = 0;
+#if __loongarch_asx
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    __m256 pSrcX = (__m256)__lasx_xvld(colRemapTableTemp, 0);
+                    __m256 pSrcY = (__m256)__lasx_xvld(rowRemapTableTemp, 0);
+                    __m256 pSrc[4], pDst;
+                    Rpp32f dstPtrTemp_ps[8];
+                    compute_generic_bilinear_srclocs_1c_avx(pSrcY, pSrcX, srcLocs, pBilinearCoeffs, pSrcStrideH, pxSrcStridesCHW, pRoiLTRB);
+                    rpp_simd_load(rpp_generic_bilinear_load_1c_avx<Rpp16f>, srcPtrChannel, srcDescPtr, srcLocs, pSrcY, pSrcX, pRoiLTRB, pSrc);  // Load input pixels required for bilinear interpolation
+                    compute_bilinear_interpolation_1c_avx(pSrc, pBilinearCoeffs, pDst); // Compute Bilinear interpolation
+                    rpp_simd_store(rpp_store8_f32pln1_to_f32pln1_avx, dstPtrTemp_ps, pDst); // Store dst pixels
+                    for(int cnt = 0; cnt < vectorIncrementPerChannel; cnt++)
+                        dstPtrTemp[cnt] = (Rpp16f) dstPtrTemp_ps[cnt];
+                    dstPtrTemp += vectorIncrementPerChannel;
+                    rowRemapTableTemp += vectorIncrementPerChannel;
+                    colRemapTableTemp += vectorIncrementPerChannel;
+                }
+#endif
+                for (; vectorLoopCount < roi.xywhROI.roiWidth; vectorLoopCount++)
+                    compute_generic_bilinear_interpolation_pln_to_pln(*rowRemapTableTemp++, *colRemapTableTemp++, &roiLTRB, dstPtrTemp++, srcPtrChannel, srcDescPtr, dstDescPtr);
+
+                dstPtrRow += dstDescPtr->strides.hStride;
+                rowRemapTableImage += remapTableDescPtr->strides.hStride;
+                colRemapTableImage += remapTableDescPtr->strides.hStride;
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
diff --git a/src/modules/cpu/kernel/resample.hpp b/src/modules/cpu/kernel/resample.hpp
index 4c4d86c0..da9d340a 100644
--- a/src/modules/cpu/kernel/resample.hpp
+++ b/src/modules/cpu/kernel/resample.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 RppStatus resample_host_tensor(Rpp32f *srcPtr,
                                RpptDescPtr srcDescPtr,
@@ -80,6 +85,22 @@ RppStatus resample_host_tensor(Rpp32f *srcPtr,
                             loc1 = srcLength - inBlockRounded;
                         Rpp32s locInWindow = loc0;
                         Rpp32f locBegin = locInWindow - inPos;
+#ifdef __loongarch_sx
+                        __m128 pLocInWindow = __lsx_vfadd_s(lsx_set1_f32(locBegin), xmm_pDstLocInit);
+
+                        Rpp32f accum = 0.0f;
+                        __m128 pAccum = xmm_p0;
+                        for (; locInWindow + 3 < loc1; locInWindow += 4)
+                        {
+                            __m128 w4 = window(pLocInWindow);
+                            pAccum = __lsx_vfadd_s(pAccum, __lsx_vfmul_s((__m128)__lsx_vld(inBlockPtr + locInWindow, 0), w4));
+                            pLocInWindow = __lsx_vfadd_s(pLocInWindow, xmm_p4);
+                        }
+                        // sum all 4 values in the pAccum vector and store in accum
+                        pAccum = __lsx_vfadd_s(pAccum, (__m128)__lsx_vpermi_w((__m128i)pAccum, (__m128i)pAccum, _MM_SHUFFLE(1, 0, 3, 2)));
+                        pAccum = __lsx_vfadd_s(pAccum, (__m128)__lsx_vpermi_w((__m128i)pAccum, (__m128i)pAccum, _MM_SHUFFLE(0, 1, 0, 1)));
+                        accum = (((v4f32)(pAccum))[0]);
+#else
                         __m128 pLocInWindow = _mm_add_ps(_mm_set1_ps(locBegin), xmm_pDstLocInit);
 
                         Rpp32f accum = 0.0f;
@@ -94,6 +115,7 @@ RppStatus resample_host_tensor(Rpp32f *srcPtr,
                         pAccum = _mm_add_ps(pAccum, _mm_shuffle_ps(pAccum, pAccum, _MM_SHUFFLE(1, 0, 3, 2)));
                         pAccum = _mm_add_ps(pAccum, _mm_shuffle_ps(pAccum, pAccum, _MM_SHUFFLE(0, 1, 0, 1)));
                         accum = _mm_cvtss_f32(pAccum);
+#endif
 
                         Rpp32f x = locInWindow - inPos;
                         for (; locInWindow < loc1; locInWindow++, x++) {
diff --git a/src/modules/cpu/kernel/resize.hpp b/src/modules/cpu/kernel/resize.hpp
index ad3e47f8..464d6ebd 100644
--- a/src/modules/cpu/kernel/resize.hpp
+++ b/src/modules/cpu/kernel/resize.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 
 /************* NEAREST NEIGHBOR INTERPOLATION *************/
@@ -67,10 +72,17 @@ inline RppStatus resize_nn_u8_u8_host_tensor(Rpp8u *srcPtr,
         dstPtrChannel = dstPtrImage;
 
         Rpp32u alignedLength = dstImgSize[batchCount].width & ~3;   // Align dst width to process 4 dst pixels per iteration
+#ifdef __loongarch_sx
+        __m128 pWRatio = lsx_set1_f32(wRatio);
+        __m128 pWidthLimit = lsx_set1_f32((float)widthLimit);
+        __m128 pWOffset = lsx_set1_f32(wOffset);
+        __m128 pDstLoc;
+#else
         __m128 pWRatio = _mm_set1_ps(wRatio);
         __m128 pWidthLimit = _mm_set1_ps((float)widthLimit);
         __m128 pWOffset = _mm_set1_ps(wOffset);
         __m128 pDstLoc;
+#endif
         Rpp32s srcLocationColumnArray[4] = {0};     // Since 4 dst pixels are processed per iteration
         Rpp32s srcLocationRow, srcLocationColumn;
 
@@ -288,10 +300,17 @@ inline RppStatus resize_nn_f32_f32_host_tensor(Rpp32f *srcPtr,
         dstPtrChannel = dstPtrImage;
 
         Rpp32u alignedLength = dstImgSize[batchCount].width & ~3;   // Align dst width to process 4 dst pixels per iteration
+#ifdef __loongarch_sx
+        __m128 pWRatio = lsx_set1_f32(wRatio);
+        __m128 pWidthLimit = lsx_set1_f32((float)widthLimit);
+        __m128 pWOffset = lsx_set1_f32(wOffset);
+        __m128 pDstLoc;
+#else
         __m128 pWRatio = _mm_set1_ps(wRatio);
         __m128 pWidthLimit = _mm_set1_ps((float)widthLimit);
         __m128 pWOffset = _mm_set1_ps(wOffset);
         __m128 pDstLoc;
+#endif
         Rpp32s srcLocationColumnArray[4] = {0};     // Since 4 dst pixels are processed per iteration
         Rpp32s srcLocationRow, srcLocationColumn;
 
@@ -501,10 +520,17 @@ inline RppStatus resize_nn_i8_i8_host_tensor(Rpp8s *srcPtr,
         dstPtrChannel = dstPtrImage;
 
         Rpp32u alignedLength = dstImgSize[batchCount].width & ~3;   // Align dst width to process 4 dst pixels per iteration
+#ifdef __loongarch_sx
+        __m128 pWRatio = lsx_set1_f32(wRatio);
+        __m128 pWidthLimit = lsx_set1_f32((float)widthLimit);
+        __m128 pWOffset = lsx_set1_f32(wOffset);
+        __m128 pDstLoc;
+#else
         __m128 pWRatio = _mm_set1_ps(wRatio);
         __m128 pWidthLimit = _mm_set1_ps((float)widthLimit);
         __m128 pWOffset = _mm_set1_ps(wOffset);
         __m128 pDstLoc;
+#endif
         Rpp32s srcLocationColumnArray[4] = {0};     // Since 4 dst pixels are processed per iteration
         Rpp32s srcLocationRow, srcLocationColumn;
 
@@ -836,10 +862,17 @@ inline RppStatus resize_bilinear_u8_u8_host_tensor(Rpp8u *srcPtr,
         dstPtrChannel = dstPtrImage;
 
         Rpp32u alignedLength = dstImgSize[batchCount].width & ~7;   // Align dst width to process 8 dst pixels per iteration
+#ifdef __loongarch_asx
+        __m256 pWRatio = lasx_set1_f32(wRatio);
+        __m256 pWOffset = lasx_set1_f32(wOffset);
+        __m256i pxMaxSrcLoc = __lasx_xvreplgr2vr_w(maxWidthLimitMinusStride);
+        __m256 pWeightParams[noOfCoeffs], pBilinearCoeffs[noOfCoeffs], pDstLoc;
+#else
         __m256 pWRatio = _mm256_set1_ps(wRatio);
         __m256 pWOffset = _mm256_set1_ps(wOffset);
         __m256i pxMaxSrcLoc = _mm256_set1_epi32(maxWidthLimitMinusStride);
         __m256 pWeightParams[noOfCoeffs], pBilinearCoeffs[noOfCoeffs], pDstLoc;
+#endif
         Rpp32f weightParams[noOfCoeffs], bilinearCoeffs[noOfCoeffs];
         Rpp32s srcLocationColumnArray[8] = {0};     // Since 8 dst pixels are processed per iteration
         Rpp32s srcLocationRow, srcLocationColumn;
@@ -862,8 +895,13 @@ inline RppStatus resize_bilinear_u8_u8_host_tensor(Rpp8u *srcPtr,
                 Rpp8u *srcRowPtrsForInterp[2];     // kernelSize(2)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset); // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = avx_pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -909,8 +947,13 @@ inline RppStatus resize_bilinear_u8_u8_host_tensor(Rpp8u *srcPtr,
                 Rpp8u *srcRowPtrsForInterp[6];     // kernelSize(2) * numOfPlanes(3)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation_pln(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = avx_pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -951,8 +994,13 @@ inline RppStatus resize_bilinear_u8_u8_host_tensor(Rpp8u *srcPtr,
                 Rpp8u *srcRowPtrsForInterp[2];     // kernelSize(2)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = avx_pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -991,8 +1039,13 @@ inline RppStatus resize_bilinear_u8_u8_host_tensor(Rpp8u *srcPtr,
                 Rpp8u *srcRowPtrsForInterp[6];     // kernelSize(2) * numOfPlanes(3)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation_pln(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = avx_pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -1077,10 +1130,17 @@ inline RppStatus resize_bilinear_f32_f32_host_tensor(Rpp32f *srcPtr,
         dstPtrChannel = dstPtrImage;
 
         Rpp32u alignedLength = dstImgSize[batchCount].width & ~7;   // Align dst width to process 8 dst pixels per iteration
+#ifdef __loongarch_asx
+        __m256 pWRatio = lasx_set1_f32(wRatio);
+        __m256 pWOffset = lasx_set1_f32(wOffset);
+        __m256i pxMaxSrcLoc = __lasx_xvreplgr2vr_w(maxWidthLimitMinusStride);
+        __m256 pWeightParams[noOfCoeffs], pBilinearCoeffs[noOfCoeffs], pDstLoc;
+#else
         __m256 pWRatio = _mm256_set1_ps(wRatio);
         __m256 pWOffset = _mm256_set1_ps(wOffset);
         __m256i pxMaxSrcLoc = _mm256_set1_epi32(maxWidthLimitMinusStride);
         __m256 pWeightParams[noOfCoeffs], pBilinearCoeffs[noOfCoeffs], pDstLoc;
+#endif
         Rpp32f weightParams[noOfCoeffs], bilinearCoeffs[noOfCoeffs];
         Rpp32s srcLocationColumnArray[8] = {0};     // Since 8 dst pixels are processed per iteration
         Rpp32s srcLocationRow, srcLocationColumn;
@@ -1103,8 +1163,13 @@ inline RppStatus resize_bilinear_f32_f32_host_tensor(Rpp32f *srcPtr,
                 Rpp32f *srcRowPtrsForInterp[2];     // kernelSize(2)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = avx_pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -1150,8 +1215,13 @@ inline RppStatus resize_bilinear_f32_f32_host_tensor(Rpp32f *srcPtr,
                 Rpp32f *srcRowPtrsForInterp[6];     // kernelSize(2) * numOfPlanes(3)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation_pln(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = avx_pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -1193,8 +1263,13 @@ inline RppStatus resize_bilinear_f32_f32_host_tensor(Rpp32f *srcPtr,
                 Rpp32f *srcRowPtrsForInterp[2];     // kernelSize(2)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = avx_pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -1234,8 +1309,13 @@ inline RppStatus resize_bilinear_f32_f32_host_tensor(Rpp32f *srcPtr,
                 Rpp32f *srcRowPtrsForInterp[6];     // kernelSize(2) * numOfPlanes(3)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation_pln(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = avx_pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -1320,10 +1400,17 @@ inline RppStatus resize_bilinear_f16_f16_host_tensor(Rpp16f *srcPtr,
         dstPtrChannel = dstPtrImage;
 
         Rpp32u alignedLength = dstImgSize[batchCount].width & ~7;   // Align dst width to process 8 dst pixels per iteration
+#ifdef __loongarch_asx
+        __m256 pWRatio = lasx_set1_f32(wRatio);
+        __m256 pWOffset = lasx_set1_f32(wOffset);
+        __m256i pxMaxSrcLoc = __lasx_xvreplgr2vr_w(maxWidthLimitMinusStride);
+        __m256 pWeightParams[noOfCoeffs], pBilinearCoeffs[noOfCoeffs], pDstLoc;
+#else
         __m256 pWRatio = _mm256_set1_ps(wRatio);
         __m256 pWOffset = _mm256_set1_ps(wOffset);
         __m256i pxMaxSrcLoc = _mm256_set1_epi32(maxWidthLimitMinusStride);
         __m256 pWeightParams[noOfCoeffs], pBilinearCoeffs[noOfCoeffs], pDstLoc;
+#endif
         Rpp32f weightParams[noOfCoeffs], bilinearCoeffs[noOfCoeffs];
         Rpp32s srcLocationColumnArray[8] = {0};     // Since 8 dst pixels are processed per iteration
         Rpp32s srcLocationRow, srcLocationColumn;
@@ -1346,8 +1433,13 @@ inline RppStatus resize_bilinear_f16_f16_host_tensor(Rpp16f *srcPtr,
                 Rpp16f *srcRowPtrsForInterp[2];     // kernelSize(2)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = avx_pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -1393,8 +1485,13 @@ inline RppStatus resize_bilinear_f16_f16_host_tensor(Rpp16f *srcPtr,
                 Rpp16f *srcRowPtrsForInterp[6];     // kernelSize(2) * numOfPlanes(3)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation_pln(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = avx_pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -1436,8 +1533,13 @@ inline RppStatus resize_bilinear_f16_f16_host_tensor(Rpp16f *srcPtr,
                 Rpp16f *srcRowPtrsForInterp[2];     // kernelSize(2)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = avx_pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -1477,8 +1579,13 @@ inline RppStatus resize_bilinear_f16_f16_host_tensor(Rpp16f *srcPtr,
                 Rpp16f *srcRowPtrsForInterp[6];     // kernelSize(2) * numOfPlanes(3)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation_pln(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = avx_pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -1564,10 +1671,17 @@ inline RppStatus resize_bilinear_i8_i8_host_tensor(Rpp8s *srcPtr,
         dstPtrChannel = dstPtrImage;
 
         Rpp32u alignedLength = dstImgSize[batchCount].width & ~7;   // Align dst width to process 8 dst pixels per iteration
+#ifdef __loongarch_asx
+        __m256 pWRatio = lasx_set1_f32(wRatio);
+        __m256 pWOffset = lasx_set1_f32(wOffset);
+        __m256i pxMaxSrcLoc = __lasx_xvreplgr2vr_w(maxWidthLimitMinusStride);
+        __m256 pWeightParams[noOfCoeffs], pBilinearCoeffs[noOfCoeffs], pDstLoc;
+#else
         __m256 pWRatio = _mm256_set1_ps(wRatio);
         __m256 pWOffset = _mm256_set1_ps(wOffset);
         __m256i pxMaxSrcLoc = _mm256_set1_epi32(maxWidthLimitMinusStride);
         __m256 pWeightParams[noOfCoeffs], pBilinearCoeffs[noOfCoeffs], pDstLoc;
+#endif
         Rpp32f weightParams[noOfCoeffs], bilinearCoeffs[noOfCoeffs];
         Rpp32s srcLocationColumnArray[8] = {0};     // Since 8 dst pixels are processed per iteration
         Rpp32s srcLocationRow, srcLocationColumn;
@@ -1590,8 +1704,13 @@ inline RppStatus resize_bilinear_i8_i8_host_tensor(Rpp8s *srcPtr,
                 Rpp8s *srcRowPtrsForInterp[2];     // kernelSize(2)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = avx_pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -1637,8 +1756,13 @@ inline RppStatus resize_bilinear_i8_i8_host_tensor(Rpp8s *srcPtr,
                 Rpp8s *srcRowPtrsForInterp[6];     // kernelSize(2) * numOfPlanes(3)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation_pln(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = avx_pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -1679,8 +1803,13 @@ inline RppStatus resize_bilinear_i8_i8_host_tensor(Rpp8s *srcPtr,
                 Rpp8s *srcRowPtrsForInterp[2];     // kernelSize(2)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = avx_pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -1719,8 +1848,13 @@ inline RppStatus resize_bilinear_i8_i8_host_tensor(Rpp8s *srcPtr,
                 Rpp8s *srcRowPtrsForInterp[6];     // kernelSize(2) * numOfPlanes(3)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation_pln(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = avx_pDstLocInit;
 
                 int vectorLoopCount = 0;
diff --git a/src/modules/cpu/kernel/resize_crop_mirror.hpp b/src/modules/cpu/kernel/resize_crop_mirror.hpp
index 071db095..60195bd7 100644
--- a/src/modules/cpu/kernel/resize_crop_mirror.hpp
+++ b/src/modules/cpu/kernel/resize_crop_mirror.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 RppStatus resize_crop_mirror_u8_u8_host_tensor(Rpp8u *srcPtr,
                                                RpptDescPtr srcDescPtr,
@@ -66,10 +71,17 @@ RppStatus resize_crop_mirror_u8_u8_host_tensor(Rpp8u *srcPtr,
         dstPtrChannel = dstPtrImage;
 
         Rpp32u alignedLength = dstImgSize[batchCount].width & ~7;   // Align dst width to process 8 dst pixels per iteration
+#ifdef __loongarch_asx
+        __m256 pWRatio = lasx_set1_f32(wRatio);
+        __m256 pWOffset = lasx_set1_f32(wOffset);
+        __m256i pxMaxSrcLoc = __lasx_xvreplgr2vr_w(maxWidthLimitMinusStride);
+        __m256 pWeightParams[INTERP_BILINEAR_NUM_COEFFS], pBilinearCoeffs[INTERP_BILINEAR_NUM_COEFFS], pDstLoc;
+#else
         __m256 pWRatio = _mm256_set1_ps(wRatio);
         __m256 pWOffset = _mm256_set1_ps(wOffset);
         __m256i pxMaxSrcLoc = _mm256_set1_epi32(maxWidthLimitMinusStride);
         __m256 pWeightParams[INTERP_BILINEAR_NUM_COEFFS], pBilinearCoeffs[INTERP_BILINEAR_NUM_COEFFS], pDstLoc;
+#endif
         Rpp32f weightParams[INTERP_BILINEAR_NUM_COEFFS], bilinearCoeffs[INTERP_BILINEAR_NUM_COEFFS];
         Rpp32s srcLocationColumnArray[8] = {0};     // Since 8 dst pixels are processed per iteration
         Rpp32s srcLocationRow, srcLocationColumn;
@@ -80,7 +92,11 @@ RppStatus resize_crop_mirror_u8_u8_host_tensor(Rpp8u *srcPtr,
         auto computeFnSrcLocAvx = &compute_resize_bilinear_src_loc_and_weights_avx;
         if(mirrorFlag)
         {
+#ifdef __loongarch_asx
+            pDstLocInit =  lasx_setr_f32(width - 1, width - 2, width - 3, width - 4, width - 5, width - 6, width - 7, width - 8);
+#else
             pDstLocInit =  _mm256_setr_ps(width - 1, width - 2, width - 3, width - 4, width - 5, width - 6, width - 7, width - 8);
+#endif
             computeFnSrcLocAvx = &compute_resize_bilinear_src_loc_and_weights_mirror_avx;
         }
 
@@ -102,8 +118,13 @@ RppStatus resize_crop_mirror_u8_u8_host_tensor(Rpp8u *srcPtr,
                 Rpp8u *srcRowPtrsForInterp[2];     // kernelSize(2)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset); // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -150,8 +171,13 @@ RppStatus resize_crop_mirror_u8_u8_host_tensor(Rpp8u *srcPtr,
                 Rpp8u *srcRowPtrsForInterp[6];     // kernelSize(2) * numOfPlanes(3)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation_pln(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -193,8 +219,13 @@ RppStatus resize_crop_mirror_u8_u8_host_tensor(Rpp8u *srcPtr,
                 Rpp8u *srcRowPtrsForInterp[2];     // kernelSize(2)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -233,8 +264,13 @@ RppStatus resize_crop_mirror_u8_u8_host_tensor(Rpp8u *srcPtr,
                 Rpp8u *srcRowPtrsForInterp[6];     // kernelSize(2) * numOfPlanes(3)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation_pln(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = pDstLocInit;
 
                 Rpp8u *dstPtrTempChn;
@@ -317,10 +353,17 @@ RppStatus resize_crop_mirror_f32_f32_host_tensor(Rpp32f *srcPtr,
         dstPtrChannel = dstPtrImage;
 
         Rpp32u alignedLength = dstImgSize[batchCount].width & ~7;   // Align dst width to process 8 dst pixels per iteration
+#ifdef __loongarch_asx
+        __m256 pWRatio = lasx_set1_f32(wRatio);
+        __m256 pWOffset = lasx_set1_f32(wOffset);
+        __m256i pxMaxSrcLoc = __lasx_xvreplgr2vr_w(maxWidthLimitMinusStride);
+        __m256 pWeightParams[INTERP_BILINEAR_NUM_COEFFS], pBilinearCoeffs[INTERP_BILINEAR_NUM_COEFFS], pDstLoc;
+#else
         __m256 pWRatio = _mm256_set1_ps(wRatio);
         __m256 pWOffset = _mm256_set1_ps(wOffset);
         __m256i pxMaxSrcLoc = _mm256_set1_epi32(maxWidthLimitMinusStride);
         __m256 pWeightParams[INTERP_BILINEAR_NUM_COEFFS], pBilinearCoeffs[INTERP_BILINEAR_NUM_COEFFS], pDstLoc;
+#endif
         Rpp32f weightParams[INTERP_BILINEAR_NUM_COEFFS], bilinearCoeffs[INTERP_BILINEAR_NUM_COEFFS];
         Rpp32s srcLocationColumnArray[8] = {0};     // Since 8 dst pixels are processed per iteration
         Rpp32s srcLocationRow, srcLocationColumn;
@@ -332,7 +375,11 @@ RppStatus resize_crop_mirror_f32_f32_host_tensor(Rpp32f *srcPtr,
 
         if(mirrorFlag)
         {
+#ifdef __loongarch_asx
+            pDstLocInit =  lasx_setr_f32(width - 1, width - 2, width - 3, width - 4, width - 5, width - 6, width - 7, width - 8);
+#else
             pDstLocInit =  _mm256_setr_ps(width - 1, width - 2, width - 3, width - 4, width - 5, width - 6, width - 7, width - 8);
+#endif
             computeFnSrcLocAvx = &compute_resize_bilinear_src_loc_and_weights_mirror_avx;
         }
 
@@ -354,8 +401,13 @@ RppStatus resize_crop_mirror_f32_f32_host_tensor(Rpp32f *srcPtr,
                 Rpp32f *srcRowPtrsForInterp[2];     // kernelSize(2)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -402,8 +454,13 @@ RppStatus resize_crop_mirror_f32_f32_host_tensor(Rpp32f *srcPtr,
                 Rpp32f *srcRowPtrsForInterp[6];     // kernelSize(2) * numOfPlanes(3)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation_pln(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -446,8 +503,13 @@ RppStatus resize_crop_mirror_f32_f32_host_tensor(Rpp32f *srcPtr,
                 Rpp32f *srcRowPtrsForInterp[2];     // kernelSize(2)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -488,8 +550,13 @@ RppStatus resize_crop_mirror_f32_f32_host_tensor(Rpp32f *srcPtr,
                 Rpp32f *srcRowPtrsForInterp[6];     // kernelSize(2) * numOfPlanes(3)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation_pln(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -573,10 +640,17 @@ RppStatus resize_crop_mirror_f16_f16_host_tensor(Rpp16f *srcPtr,
         dstPtrChannel = dstPtrImage;
 
         Rpp32u alignedLength = dstImgSize[batchCount].width & ~7;   // Align dst width to process 8 dst pixels per iteration
+#ifdef __loongarch_asx
+        __m256 pWRatio = lasx_set1_f32(wRatio);
+        __m256 pWOffset = lasx_set1_f32(wOffset);
+        __m256i pxMaxSrcLoc = __lasx_xvreplgr2vr_w(maxWidthLimitMinusStride);
+        __m256 pWeightParams[INTERP_BILINEAR_NUM_COEFFS], pBilinearCoeffs[INTERP_BILINEAR_NUM_COEFFS], pDstLoc;
+#else
         __m256 pWRatio = _mm256_set1_ps(wRatio);
         __m256 pWOffset = _mm256_set1_ps(wOffset);
         __m256i pxMaxSrcLoc = _mm256_set1_epi32(maxWidthLimitMinusStride);
         __m256 pWeightParams[INTERP_BILINEAR_NUM_COEFFS], pBilinearCoeffs[INTERP_BILINEAR_NUM_COEFFS], pDstLoc;
+#endif
         Rpp32f weightParams[INTERP_BILINEAR_NUM_COEFFS], bilinearCoeffs[INTERP_BILINEAR_NUM_COEFFS];
         Rpp32s srcLocationColumnArray[8] = {0};     // Since 8 dst pixels are processed per iteration
         Rpp32s srcLocationRow, srcLocationColumn;
@@ -588,7 +662,11 @@ RppStatus resize_crop_mirror_f16_f16_host_tensor(Rpp16f *srcPtr,
 
         if(mirrorFlag)
         {
+#ifdef __loongarch_asx
+            pDstLocInit =  lasx_setr_f32(width - 1, width - 2, width - 3, width - 4, width - 5, width - 6, width - 7, width - 8);
+#else
             pDstLocInit =  _mm256_setr_ps(width - 1, width - 2, width - 3, width - 4, width - 5, width - 6, width - 7, width - 8);
+#endif
             computeFnSrcLocAvx = &compute_resize_bilinear_src_loc_and_weights_mirror_avx;
         }
 
@@ -610,8 +688,13 @@ RppStatus resize_crop_mirror_f16_f16_host_tensor(Rpp16f *srcPtr,
                 Rpp16f *srcRowPtrsForInterp[2];     // kernelSize(2)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -658,8 +741,13 @@ RppStatus resize_crop_mirror_f16_f16_host_tensor(Rpp16f *srcPtr,
                 Rpp16f *srcRowPtrsForInterp[6];     // kernelSize(2) * numOfPlanes(3)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation_pln(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -702,8 +790,13 @@ RppStatus resize_crop_mirror_f16_f16_host_tensor(Rpp16f *srcPtr,
                 Rpp16f *srcRowPtrsForInterp[2];     // kernelSize(2)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -744,8 +837,13 @@ RppStatus resize_crop_mirror_f16_f16_host_tensor(Rpp16f *srcPtr,
                 Rpp16f *srcRowPtrsForInterp[6];     // kernelSize(2) * numOfPlanes(3)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation_pln(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -829,10 +927,17 @@ RppStatus resize_crop_mirror_i8_i8_host_tensor(Rpp8s *srcPtr,
         dstPtrChannel = dstPtrImage;
 
         Rpp32u alignedLength = dstImgSize[batchCount].width & ~7;   // Align dst width to process 8 dst pixels per iteration
+#ifdef __loongarch_asx
+        __m256 pWRatio = lasx_set1_f32(wRatio);
+        __m256 pWOffset = lasx_set1_f32(wOffset);
+        __m256i pxMaxSrcLoc = __lasx_xvreplgr2vr_w(maxWidthLimitMinusStride);
+        __m256 pWeightParams[INTERP_BILINEAR_NUM_COEFFS], pBilinearCoeffs[INTERP_BILINEAR_NUM_COEFFS], pDstLoc;
+#else
         __m256 pWRatio = _mm256_set1_ps(wRatio);
         __m256 pWOffset = _mm256_set1_ps(wOffset);
         __m256i pxMaxSrcLoc = _mm256_set1_epi32(maxWidthLimitMinusStride);
         __m256 pWeightParams[INTERP_BILINEAR_NUM_COEFFS], pBilinearCoeffs[INTERP_BILINEAR_NUM_COEFFS], pDstLoc;
+#endif
         Rpp32f weightParams[INTERP_BILINEAR_NUM_COEFFS], bilinearCoeffs[INTERP_BILINEAR_NUM_COEFFS];
         Rpp32s srcLocationColumnArray[8] = {0};     // Since 8 dst pixels are processed per iteration
         Rpp32s srcLocationRow, srcLocationColumn;
@@ -843,7 +948,11 @@ RppStatus resize_crop_mirror_i8_i8_host_tensor(Rpp8s *srcPtr,
         auto computeFnSrcLocAvx = &compute_resize_bilinear_src_loc_and_weights_avx;
         if(mirrorFlag)
         {
+#ifdef __loongarch_asx
+            pDstLocInit =  lasx_setr_f32(width - 1, width - 2, width - 3, width - 4, width - 5, width - 6, width - 7, width - 8);
+#else
             pDstLocInit =  _mm256_setr_ps(width - 1, width - 2, width - 3, width - 4, width - 5, width - 6, width - 7, width - 8);
+#endif
             computeFnSrcLocAvx = &compute_resize_bilinear_src_loc_and_weights_mirror_avx;
         }
 
@@ -865,8 +974,13 @@ RppStatus resize_crop_mirror_i8_i8_host_tensor(Rpp8s *srcPtr,
                 Rpp8s *srcRowPtrsForInterp[2];     // kernelSize(2)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset); // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -913,8 +1027,13 @@ RppStatus resize_crop_mirror_i8_i8_host_tensor(Rpp8s *srcPtr,
                 Rpp8s *srcRowPtrsForInterp[6];     // kernelSize(2) * numOfPlanes(3)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation_pln(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -956,8 +1075,13 @@ RppStatus resize_crop_mirror_i8_i8_host_tensor(Rpp8s *srcPtr,
                 Rpp8s *srcRowPtrsForInterp[2];     // kernelSize(2)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -997,8 +1121,13 @@ RppStatus resize_crop_mirror_i8_i8_host_tensor(Rpp8s *srcPtr,
                 Rpp8s *srcRowPtrsForInterp[6];     // kernelSize(2) * numOfPlanes(3)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation_pln(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = pDstLocInit;
 
                 Rpp8s *dstPtrTempChn;
diff --git a/src/modules/cpu/kernel/resize_mirror_normalize.hpp b/src/modules/cpu/kernel/resize_mirror_normalize.hpp
index 1d930912..093f8500 100644
--- a/src/modules/cpu/kernel/resize_mirror_normalize.hpp
+++ b/src/modules/cpu/kernel/resize_mirror_normalize.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 RppStatus resize_mirror_normalize_u8_u8_host_tensor(Rpp8u *srcPtr,
                                                     RpptDescPtr srcDescPtr,
@@ -71,10 +76,17 @@ RppStatus resize_mirror_normalize_u8_u8_host_tensor(Rpp8u *srcPtr,
         memset(dstPtrImage, (Rpp8u)0, (size_t)dstDescPtr->strides.nStride);
 
         Rpp32u alignedLength = dstImgSize[batchCount].width & ~7;   // Align dst width to process 8 dst pixels per iteration
+#ifdef __loongarch_asx
+        __m256 pWRatio = lasx_set1_f32(wRatio);
+        __m256 pWOffset = lasx_set1_f32(wOffset);
+        __m256i pxMaxSrcLoc = __lasx_xvreplgr2vr_w(maxWidthLimitMinusStride);
+        __m256 pWeightParams[INTERP_BILINEAR_NUM_COEFFS], pBilinearCoeffs[INTERP_BILINEAR_NUM_COEFFS], pDstLoc;
+#else
         __m256 pWRatio = _mm256_set1_ps(wRatio);
         __m256 pWOffset = _mm256_set1_ps(wOffset);
         __m256i pxMaxSrcLoc = _mm256_set1_epi32(maxWidthLimitMinusStride);
         __m256 pWeightParams[INTERP_BILINEAR_NUM_COEFFS], pBilinearCoeffs[INTERP_BILINEAR_NUM_COEFFS], pDstLoc;
+#endif
         Rpp32f weightParams[INTERP_BILINEAR_NUM_COEFFS], bilinearCoeffs[INTERP_BILINEAR_NUM_COEFFS];
         Rpp32s srcLocationColumnArray[8] = {0};     // Since 8 dst pixels are processed per iteration
         Rpp32s srcLocationRow, srcLocationColumn;
@@ -86,8 +98,13 @@ RppStatus resize_mirror_normalize_u8_u8_host_tensor(Rpp8u *srcPtr,
         {
             mean[c] = meanTensor[incrementPerImage + c];
             invStdDev[c] = 1.0 / stdDevTensor[incrementPerImage + c];
+#ifdef __loongarch_asx
+            pRMNParams[2 * c] = lasx_set1_f32(mean[c]);
+            pRMNParams[2 * c + 1] = lasx_set1_f32(invStdDev[c]);
+#else
             pRMNParams[2 * c] = _mm256_set1_ps(mean[c]);
             pRMNParams[2 * c + 1] = _mm256_set1_ps(invStdDev[c]);
+#endif
         }
         Rpp32u mirrorFlag = mirrorTensor[batchCount];
         Rpp32u width = dstImgSize[batchCount].width;
@@ -96,7 +113,11 @@ RppStatus resize_mirror_normalize_u8_u8_host_tensor(Rpp8u *srcPtr,
         auto computeFnSrcLocAvx = &compute_resize_bilinear_src_loc_and_weights_avx;
         if(mirrorFlag)
         {
+#ifdef __loongarch_asx
+            pDstLocInit =  lasx_setr_f32(width - 1, width - 2, width - 3, width - 4, width - 5, width - 6, width - 7, width - 8);
+#else
             pDstLocInit =  _mm256_setr_ps(width - 1, width - 2, width - 3, width - 4, width - 5, width - 6, width - 7, width - 8);
+#endif
             computeFnSrcLocAvx = &compute_resize_bilinear_src_loc_and_weights_mirror_avx;
         }
 
@@ -118,8 +139,13 @@ RppStatus resize_mirror_normalize_u8_u8_host_tensor(Rpp8u *srcPtr,
                 Rpp8u *srcRowPtrsForInterp[2];     // kernelSize(2)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset); // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -170,8 +196,13 @@ RppStatus resize_mirror_normalize_u8_u8_host_tensor(Rpp8u *srcPtr,
                 Rpp8u *srcRowPtrsForInterp[6];     // kernelSize(2) * numOfPlanes(3)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation_pln(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -217,8 +248,13 @@ RppStatus resize_mirror_normalize_u8_u8_host_tensor(Rpp8u *srcPtr,
                 Rpp8u *srcRowPtrsForInterp[2];     // kernelSize(2)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -261,8 +297,13 @@ RppStatus resize_mirror_normalize_u8_u8_host_tensor(Rpp8u *srcPtr,
                 Rpp8u *srcRowPtrsForInterp[6];     // kernelSize(2) * numOfPlanes(3)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation_pln(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = pDstLocInit;
 
                 Rpp8u *dstPtrTempChn;
@@ -353,10 +394,17 @@ RppStatus resize_mirror_normalize_f32_f32_host_tensor(Rpp32f *srcPtr,
         memset(dstPtrImage, (Rpp32f)0, (size_t)dstDescPtr->strides.nStride);
 
         Rpp32u alignedLength = dstImgSize[batchCount].width & ~7;   // Align dst width to process 8 dst pixels per iteration
+#ifdef __loongarch_asx
+        __m256 pWRatio = lasx_set1_f32(wRatio);
+        __m256 pWOffset = lasx_set1_f32(wOffset);
+        __m256i pxMaxSrcLoc = __lasx_xvreplgr2vr_w(maxWidthLimitMinusStride);
+        __m256 pWeightParams[INTERP_BILINEAR_NUM_COEFFS], pBilinearCoeffs[INTERP_BILINEAR_NUM_COEFFS], pDstLoc;
+#else
         __m256 pWRatio = _mm256_set1_ps(wRatio);
         __m256 pWOffset = _mm256_set1_ps(wOffset);
         __m256i pxMaxSrcLoc = _mm256_set1_epi32(maxWidthLimitMinusStride);
         __m256 pWeightParams[INTERP_BILINEAR_NUM_COEFFS], pBilinearCoeffs[INTERP_BILINEAR_NUM_COEFFS], pDstLoc;
+#endif
         Rpp32f weightParams[INTERP_BILINEAR_NUM_COEFFS], bilinearCoeffs[INTERP_BILINEAR_NUM_COEFFS];
         Rpp32s srcLocationColumnArray[8] = {0};     // Since 8 dst pixels are processed per iteration
         Rpp32s srcLocationRow, srcLocationColumn;
@@ -368,8 +416,13 @@ RppStatus resize_mirror_normalize_f32_f32_host_tensor(Rpp32f *srcPtr,
         {
             mean[c] = meanTensor[incrementPerImage + c] * ONE_OVER_255;
             invStdDev[c] = 1.0 / stdDevTensor[incrementPerImage + c];
+#ifdef __loongarch_asx
+            pRMNParams[2 * c] = lasx_set1_f32(mean[c]);
+            pRMNParams[2 * c + 1] = lasx_set1_f32(invStdDev[c]);
+#else
             pRMNParams[2 * c] = _mm256_set1_ps(mean[c]);
             pRMNParams[2 * c + 1] = _mm256_set1_ps(invStdDev[c]);
+#endif
         }
         Rpp32u mirrorFlag = mirrorTensor[batchCount];
         Rpp32u width = dstImgSize[batchCount].width;
@@ -379,7 +432,11 @@ RppStatus resize_mirror_normalize_f32_f32_host_tensor(Rpp32f *srcPtr,
 
         if(mirrorFlag)
         {
+#ifdef __loongarch_asx
+            pDstLocInit =  lasx_setr_f32(width - 1, width - 2, width - 3, width - 4, width - 5, width - 6, width - 7, width - 8);
+#else
             pDstLocInit =  _mm256_setr_ps(width - 1, width - 2, width - 3, width - 4, width - 5, width - 6, width - 7, width - 8);
+#endif
             computeFnSrcLocAvx = &compute_resize_bilinear_src_loc_and_weights_mirror_avx;
         }
 
@@ -401,8 +458,13 @@ RppStatus resize_mirror_normalize_f32_f32_host_tensor(Rpp32f *srcPtr,
                 Rpp32f *srcRowPtrsForInterp[2];     // kernelSize(2)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -453,8 +515,13 @@ RppStatus resize_mirror_normalize_f32_f32_host_tensor(Rpp32f *srcPtr,
                 Rpp32f *srcRowPtrsForInterp[6];     // kernelSize(2) * numOfPlanes(3)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation_pln(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -501,8 +568,13 @@ RppStatus resize_mirror_normalize_f32_f32_host_tensor(Rpp32f *srcPtr,
                 Rpp32f *srcRowPtrsForInterp[2];     // kernelSize(2)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -547,8 +619,13 @@ RppStatus resize_mirror_normalize_f32_f32_host_tensor(Rpp32f *srcPtr,
                 Rpp32f *srcRowPtrsForInterp[6];     // kernelSize(2) * numOfPlanes(3)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation_pln(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -639,10 +716,17 @@ RppStatus resize_mirror_normalize_f16_f16_host_tensor(Rpp16f *srcPtr,
         memset(dstPtrImage, (Rpp16f)0, (size_t)dstDescPtr->strides.nStride);
 
         Rpp32u alignedLength = dstImgSize[batchCount].width & ~7;   // Align dst width to process 8 dst pixels per iteration
+#ifdef __loongarch_asx
+        __m256 pWRatio = lasx_set1_f32(wRatio);
+        __m256 pWOffset = lasx_set1_f32(wOffset);
+        __m256i pxMaxSrcLoc = __lasx_xvreplgr2vr_w(maxWidthLimitMinusStride);
+        __m256 pWeightParams[INTERP_BILINEAR_NUM_COEFFS], pBilinearCoeffs[INTERP_BILINEAR_NUM_COEFFS], pDstLoc;
+#else
         __m256 pWRatio = _mm256_set1_ps(wRatio);
         __m256 pWOffset = _mm256_set1_ps(wOffset);
         __m256i pxMaxSrcLoc = _mm256_set1_epi32(maxWidthLimitMinusStride);
         __m256 pWeightParams[INTERP_BILINEAR_NUM_COEFFS], pBilinearCoeffs[INTERP_BILINEAR_NUM_COEFFS], pDstLoc;
+#endif
         Rpp32f weightParams[INTERP_BILINEAR_NUM_COEFFS], bilinearCoeffs[INTERP_BILINEAR_NUM_COEFFS];
         Rpp32s srcLocationColumnArray[8] = {0};     // Since 8 dst pixels are processed per iteration
         Rpp32s srcLocationRow, srcLocationColumn;
@@ -654,8 +738,13 @@ RppStatus resize_mirror_normalize_f16_f16_host_tensor(Rpp16f *srcPtr,
         {
             mean[c] = meanTensor[incrementPerImage + c] * ONE_OVER_255;
             invStdDev[c] = 1.0 / stdDevTensor[incrementPerImage + c];
+#ifdef __loongarch_asx
+            pRMNParams[2 * c] = lasx_set1_f32(mean[c]);
+            pRMNParams[2 * c + 1] = lasx_set1_f32(invStdDev[c]);
+#else
             pRMNParams[2 * c] = _mm256_set1_ps(mean[c]);
             pRMNParams[2 * c + 1] = _mm256_set1_ps(invStdDev[c]);
+#endif
         }
         Rpp32u mirrorFlag = mirrorTensor[batchCount];
         Rpp32u width = dstImgSize[batchCount].width;
@@ -665,7 +754,11 @@ RppStatus resize_mirror_normalize_f16_f16_host_tensor(Rpp16f *srcPtr,
 
         if(mirrorFlag)
         {
+#ifdef __loongarch_asx
+            pDstLocInit =  lasx_setr_f32(width - 1, width - 2, width - 3, width - 4, width - 5, width - 6, width - 7, width - 8);
+#else
             pDstLocInit =  _mm256_setr_ps(width - 1, width - 2, width - 3, width - 4, width - 5, width - 6, width - 7, width - 8);
+#endif
             computeFnSrcLocAvx = &compute_resize_bilinear_src_loc_and_weights_mirror_avx;
         }
 
@@ -687,8 +780,13 @@ RppStatus resize_mirror_normalize_f16_f16_host_tensor(Rpp16f *srcPtr,
                 Rpp16f *srcRowPtrsForInterp[2];     // kernelSize(2)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -739,8 +837,13 @@ RppStatus resize_mirror_normalize_f16_f16_host_tensor(Rpp16f *srcPtr,
                 Rpp16f *srcRowPtrsForInterp[6];     // kernelSize(2) * numOfPlanes(3)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation_pln(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -787,8 +890,13 @@ RppStatus resize_mirror_normalize_f16_f16_host_tensor(Rpp16f *srcPtr,
                 Rpp16f *srcRowPtrsForInterp[2];     // kernelSize(2)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -833,8 +941,13 @@ RppStatus resize_mirror_normalize_f16_f16_host_tensor(Rpp16f *srcPtr,
                 Rpp16f *srcRowPtrsForInterp[6];     // kernelSize(2) * numOfPlanes(3)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation_pln(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -925,11 +1038,19 @@ RppStatus resize_mirror_normalize_i8_i8_host_tensor(Rpp8s *srcPtr,
         memset(dstPtrImage, (Rpp8s)0, (size_t)dstDescPtr->strides.nStride);
 
         Rpp32u alignedLength = dstImgSize[batchCount].width & ~7;   // Align dst width to process 8 dst pixels per iteration
+#ifdef __loongarch_asx
+        __m256 pWRatio = lasx_set1_f32(wRatio);
+        __m256 pWOffset = lasx_set1_f32(wOffset);
+
+        __m256i pxMaxSrcLoc = __lasx_xvreplgr2vr_w(maxWidthLimitMinusStride);
+        __m256 pWeightParams[INTERP_BILINEAR_NUM_COEFFS], pBilinearCoeffs[INTERP_BILINEAR_NUM_COEFFS], pDstLoc;
+#else
         __m256 pWRatio = _mm256_set1_ps(wRatio);
         __m256 pWOffset = _mm256_set1_ps(wOffset);
 
         __m256i pxMaxSrcLoc = _mm256_set1_epi32(maxWidthLimitMinusStride);
         __m256 pWeightParams[INTERP_BILINEAR_NUM_COEFFS], pBilinearCoeffs[INTERP_BILINEAR_NUM_COEFFS], pDstLoc;
+#endif
         Rpp32f weightParams[INTERP_BILINEAR_NUM_COEFFS], bilinearCoeffs[INTERP_BILINEAR_NUM_COEFFS];
         Rpp32s srcLocationColumnArray[8] = {0};     // Since 8 dst pixels are processed per iteration
         Rpp32s srcLocationRow, srcLocationColumn;
@@ -941,8 +1062,13 @@ RppStatus resize_mirror_normalize_i8_i8_host_tensor(Rpp8s *srcPtr,
         {
             mean[c] = meanTensor[incrementPerImage + c];
             invStdDev[c] = 1.0 / stdDevTensor[incrementPerImage + c];
+#ifdef __loongarch_asx
+            pRMNParams[2 * c] = lasx_set1_f32(mean[c]);
+            pRMNParams[2 * c + 1] = lasx_set1_f32(invStdDev[c]);
+#else
             pRMNParams[2 * c] = _mm256_set1_ps(mean[c]);
             pRMNParams[2 * c + 1] = _mm256_set1_ps(invStdDev[c]);
+#endif
         }
         Rpp32u mirrorFlag = mirrorTensor[batchCount];
         Rpp32u width = dstImgSize[batchCount].width;
@@ -951,7 +1077,11 @@ RppStatus resize_mirror_normalize_i8_i8_host_tensor(Rpp8s *srcPtr,
         auto computeFnSrcLocAvx = &compute_resize_bilinear_src_loc_and_weights_avx;
         if(mirrorFlag)
         {
+#ifdef __loongarch_asx
+            pDstLocInit =  lasx_setr_f32(width - 1, width - 2, width - 3, width - 4, width - 5, width - 6, width - 7, width - 8);
+#else
             pDstLocInit =  _mm256_setr_ps(width - 1, width - 2, width - 3, width - 4, width - 5, width - 6, width - 7, width - 8);
+#endif
             computeFnSrcLocAvx = &compute_resize_bilinear_src_loc_and_weights_mirror_avx;
         }
 
@@ -973,8 +1103,13 @@ RppStatus resize_mirror_normalize_i8_i8_host_tensor(Rpp8s *srcPtr,
                 Rpp8s *srcRowPtrsForInterp[2];     // kernelSize(2)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset); // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -1025,8 +1160,13 @@ RppStatus resize_mirror_normalize_i8_i8_host_tensor(Rpp8s *srcPtr,
                 Rpp8s *srcRowPtrsForInterp[6];     // kernelSize(2) * numOfPlanes(3)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation_pln(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -1072,8 +1212,13 @@ RppStatus resize_mirror_normalize_i8_i8_host_tensor(Rpp8s *srcPtr,
                 Rpp8s *srcRowPtrsForInterp[2];     // kernelSize(2)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -1117,8 +1262,13 @@ RppStatus resize_mirror_normalize_i8_i8_host_tensor(Rpp8s *srcPtr,
                 Rpp8s *srcRowPtrsForInterp[6];     // kernelSize(2) * numOfPlanes(3)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation_pln(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = pDstLocInit;
 
                 Rpp8s *dstPtrTempChn;
@@ -1213,10 +1363,17 @@ RppStatus resize_mirror_normalize_u8_f32_host_tensor(Rpp8u *srcPtr,
         memset(dstPtrImage, (Rpp32f)0, (size_t)dstDescPtr->strides.nStride);
 
         Rpp32u alignedLength = dstImgSize[batchCount].width & ~7;   // Align dst width to process 8 dst pixels per iteration
+#ifdef __loongarch_asx
+        __m256 pWRatio = lasx_set1_f32(wRatio);
+        __m256 pWOffset = lasx_set1_f32(wOffset);
+        __m256i pxMaxSrcLoc = __lasx_xvreplgr2vr_w(maxWidthLimitMinusStride);
+        __m256 pWeightParams[INTERP_BILINEAR_NUM_COEFFS], pBilinearCoeffs[INTERP_BILINEAR_NUM_COEFFS], pDstLoc;
+#else
         __m256 pWRatio = _mm256_set1_ps(wRatio);
         __m256 pWOffset = _mm256_set1_ps(wOffset);
         __m256i pxMaxSrcLoc = _mm256_set1_epi32(maxWidthLimitMinusStride);
         __m256 pWeightParams[INTERP_BILINEAR_NUM_COEFFS], pBilinearCoeffs[INTERP_BILINEAR_NUM_COEFFS], pDstLoc;
+#endif
         Rpp32f weightParams[INTERP_BILINEAR_NUM_COEFFS], bilinearCoeffs[INTERP_BILINEAR_NUM_COEFFS];
         Rpp32s srcLocationColumnArray[8] = {0};     // Since 8 dst pixels are processed per iteration
         Rpp32s srcLocationRow, srcLocationColumn;
@@ -1228,8 +1385,13 @@ RppStatus resize_mirror_normalize_u8_f32_host_tensor(Rpp8u *srcPtr,
         {
             mean[c] = meanTensor[incrementPerImage + c];
             invStdDev[c] = 1.0 / stdDevTensor[incrementPerImage + c];
+#ifdef __loongarch_asx
+            pRMNParams[2 * c] = lasx_set1_f32(mean[c]);
+            pRMNParams[2 * c + 1] = lasx_set1_f32(invStdDev[c]);
+#else
             pRMNParams[2 * c] = _mm256_set1_ps(mean[c]);
             pRMNParams[2 * c + 1] = _mm256_set1_ps(invStdDev[c]);
+#endif
         }
         Rpp32u mirrorFlag = mirrorTensor[batchCount];
         Rpp32u width = dstImgSize[batchCount].width;
@@ -1238,7 +1400,11 @@ RppStatus resize_mirror_normalize_u8_f32_host_tensor(Rpp8u *srcPtr,
         auto computeFnSrcLocAvx = &compute_resize_bilinear_src_loc_and_weights_avx;
         if(mirrorFlag)
         {
+#ifdef __loongarch_asx
+            pDstLocInit =  lasx_setr_f32(width - 1, width - 2, width - 3, width - 4, width - 5, width - 6, width - 7, width - 8);
+#else
             pDstLocInit =  _mm256_setr_ps(width - 1, width - 2, width - 3, width - 4, width - 5, width - 6, width - 7, width - 8);
+#endif
             computeFnSrcLocAvx = &compute_resize_bilinear_src_loc_and_weights_mirror_avx;
         }
 
@@ -1260,8 +1426,13 @@ RppStatus resize_mirror_normalize_u8_f32_host_tensor(Rpp8u *srcPtr,
                 Rpp8u *srcRowPtrsForInterp[2];     // kernelSize(2)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset); // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -1312,8 +1483,13 @@ RppStatus resize_mirror_normalize_u8_f32_host_tensor(Rpp8u *srcPtr,
                 Rpp8u *srcRowPtrsForInterp[6];     // kernelSize(2) * numOfPlanes(3)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation_pln(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -1359,8 +1535,13 @@ RppStatus resize_mirror_normalize_u8_f32_host_tensor(Rpp8u *srcPtr,
                 Rpp8u *srcRowPtrsForInterp[2];     // kernelSize(2)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -1403,8 +1584,13 @@ RppStatus resize_mirror_normalize_u8_f32_host_tensor(Rpp8u *srcPtr,
                 Rpp8u *srcRowPtrsForInterp[6];     // kernelSize(2) * numOfPlanes(3)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation_pln(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = pDstLocInit;
 
                 Rpp32f *dstPtrTempChn;
@@ -1497,10 +1683,17 @@ RppStatus resize_mirror_normalize_u8_f16_host_tensor(Rpp8u *srcPtr,
         memset(dstPtrImage, (Rpp16f)0, (size_t)dstDescPtr->strides.nStride);
 
         Rpp32u alignedLength = dstImgSize[batchCount].width & ~7;   // Align dst width to process 8 dst pixels per iteration
+#ifdef __loongarch_asx
+        __m256 pWRatio = lasx_set1_f32(wRatio);
+        __m256 pWOffset = lasx_set1_f32(wOffset);
+        __m256i pxMaxSrcLoc = __lasx_xvreplgr2vr_w(maxWidthLimitMinusStride);
+        __m256 pWeightParams[INTERP_BILINEAR_NUM_COEFFS], pBilinearCoeffs[INTERP_BILINEAR_NUM_COEFFS], pDstLoc;
+#else
         __m256 pWRatio = _mm256_set1_ps(wRatio);
         __m256 pWOffset = _mm256_set1_ps(wOffset);
         __m256i pxMaxSrcLoc = _mm256_set1_epi32(maxWidthLimitMinusStride);
         __m256 pWeightParams[INTERP_BILINEAR_NUM_COEFFS], pBilinearCoeffs[INTERP_BILINEAR_NUM_COEFFS], pDstLoc;
+#endif
         Rpp32f weightParams[INTERP_BILINEAR_NUM_COEFFS], bilinearCoeffs[INTERP_BILINEAR_NUM_COEFFS];
         Rpp32s srcLocationColumnArray[8] = {0};     // Since 8 dst pixels are processed per iteration
         Rpp32s srcLocationRow, srcLocationColumn;
@@ -1512,8 +1705,13 @@ RppStatus resize_mirror_normalize_u8_f16_host_tensor(Rpp8u *srcPtr,
         {
             mean[c] = meanTensor[incrementPerImage + c];
             invStdDev[c] = 1.0 / stdDevTensor[incrementPerImage + c];
+#ifdef __loongarch_asx
+            pRMNParams[2 * c] = lasx_set1_f32(mean[c]);
+            pRMNParams[2 * c + 1] = lasx_set1_f32(invStdDev[c]);
+#else
             pRMNParams[2 * c] = _mm256_set1_ps(mean[c]);
             pRMNParams[2 * c + 1] = _mm256_set1_ps(invStdDev[c]);
+#endif
         }
         Rpp32u mirrorFlag = mirrorTensor[batchCount];
         Rpp32u width = dstImgSize[batchCount].width;
@@ -1522,7 +1720,11 @@ RppStatus resize_mirror_normalize_u8_f16_host_tensor(Rpp8u *srcPtr,
         auto computeFnSrcLocAvx = &compute_resize_bilinear_src_loc_and_weights_avx;
         if(mirrorFlag)
         {
+#ifdef __loongarch_asx
+            pDstLocInit =  lasx_setr_f32(width - 1, width - 2, width - 3, width - 4, width - 5, width - 6, width - 7, width - 8);
+#else
             pDstLocInit =  _mm256_setr_ps(width - 1, width - 2, width - 3, width - 4, width - 5, width - 6, width - 7, width - 8);
+#endif
             computeFnSrcLocAvx = &compute_resize_bilinear_src_loc_and_weights_mirror_avx;
         }
 
@@ -1544,8 +1746,13 @@ RppStatus resize_mirror_normalize_u8_f16_host_tensor(Rpp8u *srcPtr,
                 Rpp8u *srcRowPtrsForInterp[2];     // kernelSize(2)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset); // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -1596,8 +1803,13 @@ RppStatus resize_mirror_normalize_u8_f16_host_tensor(Rpp8u *srcPtr,
                 Rpp8u *srcRowPtrsForInterp[6];     // kernelSize(2) * numOfPlanes(3)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation_pln(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -1643,8 +1855,13 @@ RppStatus resize_mirror_normalize_u8_f16_host_tensor(Rpp8u *srcPtr,
                 Rpp8u *srcRowPtrsForInterp[2];     // kernelSize(2)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = pDstLocInit;
 
                 int vectorLoopCount = 0;
@@ -1687,8 +1904,13 @@ RppStatus resize_mirror_normalize_u8_f16_host_tensor(Rpp8u *srcPtr,
                 Rpp8u *srcRowPtrsForInterp[6];     // kernelSize(2) * numOfPlanes(3)
                 compute_resize_bilinear_src_loc_and_weights(dstLocRow, hRatio, srcLocationRow, &weightParams[0], hOffset);  // Compute the src row location correspoding to the dst row location
                 compute_src_row_ptrs_for_bilinear_interpolation_pln(srcRowPtrsForInterp, srcPtrChannel, srcLocationRow, maxHeightLimit, srcDescPtr); // Compute the src row pointers for interpolation
+#ifdef __loongarch_asx
+                pWeightParams[0] = lasx_set1_f32(weightParams[0]);
+                pWeightParams[1]  = lasx_set1_f32(weightParams[1]);
+#else
                 pWeightParams[0] = _mm256_set1_ps(weightParams[0]);
                 pWeightParams[1]  = _mm256_set1_ps(weightParams[1]);
+#endif
                 pDstLoc = pDstLocInit;
 
                 Rpp16f *dstPtrTempChn;
diff --git a/src/modules/cpu/kernel/ricap.hpp b/src/modules/cpu/kernel/ricap.hpp
index a2975851..29a0e59d 100644
--- a/src/modules/cpu/kernel/ricap.hpp
+++ b/src/modules/cpu/kernel/ricap.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
     // RICAP output image profile as per "Data Augmentation using Random Image Cropping and Patching for Deep CNNs" available at https://arxiv.org/pdf/1811.09030.pdf
     // |------img-roi-1------|------img-roi-2------|
diff --git a/src/modules/cpu/kernel/slice.hpp b/src/modules/cpu/kernel/slice.hpp
index 37c3097c..d28c6636 100644
--- a/src/modules/cpu/kernel/slice.hpp
+++ b/src/modules/cpu/kernel/slice.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 template<typename T>
 RppStatus slice_host_tensor(T *srcPtr,
diff --git a/src/modules/cpu/kernel/spatter.hpp b/src/modules/cpu/kernel/spatter.hpp
index 9da65d26..a0107087 100644
--- a/src/modules/cpu/kernel/spatter.hpp
+++ b/src/modules/cpu/kernel/spatter.hpp
@@ -24,8 +24,13 @@ SOFTWARE.
 
 #include <random>
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 #include "spatter_mask.hpp"
 
 RppStatus spatter_u8_u8_host_tensor(Rpp8u *srcPtr,
@@ -84,11 +89,21 @@ RppStatus spatter_u8_u8_host_tensor(Rpp8u *srcPtr,
         if (srcDescPtr->c == 1)
             spatterValue[0] = spatterValue[1] = spatterValue[2] = (spatterValue[0] + spatterValue[1] + spatterValue[2]) * 0.3333;
 
-#if __AVX2__
+#if defined(__AVX2__)
         __m256 pSpatterValue[3];
         pSpatterValue[0] = _mm256_set1_ps(spatterValue[0]);
         pSpatterValue[1] = _mm256_set1_ps(spatterValue[1]);
         pSpatterValue[2] = _mm256_set1_ps(spatterValue[2]);
+#elif defined(__loongarch_asx)
+        __m256 pSpatterValue[3];
+        pSpatterValue[0] = lasx_set1_f32(spatterValue[0]);
+        pSpatterValue[1] = lasx_set1_f32(spatterValue[1]);
+        pSpatterValue[2] = lasx_set1_f32(spatterValue[2]);
+#elif defined(__loongarch_sx)
+        __m128 pSpatterValue[3];
+        pSpatterValue[0] = lsx_set1_f32(spatterValue[0]);
+        pSpatterValue[1] = lsx_set1_f32(spatterValue[1]);
+        pSpatterValue[2] = lsx_set1_f32(spatterValue[2]);
 #else
         __m128 pSpatterValue[3];
         pSpatterValue[0] = _mm_set1_ps(spatterValue[0]);
@@ -122,7 +137,7 @@ RppStatus spatter_u8_u8_host_tensor(Rpp8u *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 pSpatterMask[2], pSpatterMaskInv[2], p[6];
                     rpp_simd_load(rpp_load16_f32_to_f32_avx, spatterMaskPtrTemp, pSpatterMask);    // simd loads
                     rpp_simd_load(rpp_load16_f32_to_f32_avx, spatterMaskInvPtrTemp, pSpatterMaskInv);    // simd loads
@@ -193,7 +208,7 @@ RppStatus spatter_u8_u8_host_tensor(Rpp8u *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 pSpatterMask[2], pSpatterMaskInv[2], p[6];
                     rpp_simd_load(rpp_load16_f32_to_f32_avx, spatterMaskPtrTemp, pSpatterMask);    // simd loads
                     rpp_simd_load(rpp_load16_f32_to_f32_avx, spatterMaskInvPtrTemp, pSpatterMaskInv);    // simd loads
@@ -260,7 +275,7 @@ RppStatus spatter_u8_u8_host_tensor(Rpp8u *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 pSpatterMask[2], pSpatterMaskInv[2], p[6];
                     rpp_simd_load(rpp_load16_f32_to_f32_avx, spatterMaskPtrTemp, pSpatterMask);    // simd loads
                     rpp_simd_load(rpp_load16_f32_to_f32_avx, spatterMaskInvPtrTemp, pSpatterMaskInv);    // simd loads
@@ -323,7 +338,7 @@ RppStatus spatter_u8_u8_host_tensor(Rpp8u *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 pSpatterMask[2], pSpatterMaskInv[2];
                     rpp_simd_load(rpp_load16_f32_to_f32_avx, spatterMaskPtrTemp, pSpatterMask);    // simd loads
                     rpp_simd_load(rpp_load16_f32_to_f32_avx, spatterMaskInvPtrTemp, pSpatterMaskInv);    // simd loads
@@ -336,7 +351,7 @@ RppStatus spatter_u8_u8_host_tensor(Rpp8u *srcPtr,
                     dstPtrChannel = dstPtrTemp;
                     for(int c = 0; c < srcDescPtr->c; c++)
                     {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                         __m256 p[2];
                         rpp_simd_load(rpp_load16_u8_to_f32_avx, srcPtrChannel, p);    // simd loads
                         compute_spatter_16_host(p, pSpatterMaskInv, pSpatterMask, pSpatterValue[c]);    // spatter adjustment
@@ -434,7 +449,7 @@ RppStatus spatter_f32_f32_host_tensor(Rpp32f *srcPtr,
         if (srcDescPtr->c == 1)
             spatterValue[0] = spatterValue[1] = spatterValue[2] = (spatterValue[0] + spatterValue[1] + spatterValue[2]) * 0.3333;
 
-#if __AVX2__
+#if defined(__AVX2__)
         Rpp32u alignedLength = (bufferLength / 24) * 24;
         Rpp32u vectorIncrement = 24;
         Rpp32u vectorIncrementPerChannel = 8;
@@ -443,6 +458,24 @@ RppStatus spatter_f32_f32_host_tensor(Rpp32f *srcPtr,
         pSpatterValue[0] = _mm256_set1_ps(spatterValue[0]);
         pSpatterValue[1] = _mm256_set1_ps(spatterValue[1]);
         pSpatterValue[2] = _mm256_set1_ps(spatterValue[2]);
+#elif defined(__loongarch_asx)
+        Rpp32u alignedLength = (bufferLength / 24) * 24;
+        Rpp32u vectorIncrement = 24;
+        Rpp32u vectorIncrementPerChannel = 8;
+
+        __m256 pSpatterValue[3];
+        pSpatterValue[0] = lasx_set1_f32(spatterValue[0]);
+        pSpatterValue[1] = lasx_set1_f32(spatterValue[1]);
+        pSpatterValue[2] = lasx_set1_f32(spatterValue[2]);
+#elif defined(__loongarch_asx)
+        Rpp32u alignedLength = (bufferLength / 12) * 12;
+        Rpp32u vectorIncrement = 12;
+        Rpp32u vectorIncrementPerChannel = 4;
+
+        __m128 pSpatterValue[3];
+        pSpatterValue[0] = lsx_set1_f32(spatterValue[0]);
+        pSpatterValue[1] = lsx_set1_f32(spatterValue[1]);
+        pSpatterValue[2] = lsx_set1_f32(spatterValue[2]);
 #else
         Rpp32u alignedLength = (bufferLength / 12) * 12;
         Rpp32u vectorIncrement = 12;
@@ -480,7 +513,7 @@ RppStatus spatter_f32_f32_host_tensor(Rpp32f *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 pSpatterMask, pSpatterMaskInv, p[3];
                     rpp_simd_load(rpp_load8_f32_to_f32_avx, spatterMaskPtrTemp, &pSpatterMask);    // simd loads
                     rpp_simd_load(rpp_load8_f32_to_f32_avx, spatterMaskInvPtrTemp, &pSpatterMaskInv);    // simd loads
@@ -551,7 +584,7 @@ RppStatus spatter_f32_f32_host_tensor(Rpp32f *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 pSpatterMask, pSpatterMaskInv, p[3];
                     rpp_simd_load(rpp_load8_f32_to_f32_avx, spatterMaskPtrTemp, &pSpatterMask);    // simd loads
                     rpp_simd_load(rpp_load8_f32_to_f32_avx, spatterMaskInvPtrTemp, &pSpatterMaskInv);    // simd loads
@@ -618,7 +651,7 @@ RppStatus spatter_f32_f32_host_tensor(Rpp32f *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 pSpatterMask, pSpatterMaskInv, p[3];
                     rpp_simd_load(rpp_load8_f32_to_f32_avx, spatterMaskPtrTemp, &pSpatterMask);    // simd loads
                     rpp_simd_load(rpp_load8_f32_to_f32_avx, spatterMaskInvPtrTemp, &pSpatterMaskInv);    // simd loads
@@ -660,7 +693,7 @@ RppStatus spatter_f32_f32_host_tensor(Rpp32f *srcPtr,
         // Spatter without fused output-layout toggle (NCHW -> NCHW)
         else if ((srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NCHW))
         {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             alignedLength = bufferLength & ~7;
 #else
             alignedLength = bufferLength & ~3;
@@ -685,7 +718,7 @@ RppStatus spatter_f32_f32_host_tensor(Rpp32f *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 pSpatterMask, pSpatterMaskInv;
                     rpp_simd_load(rpp_load8_f32_to_f32_avx, spatterMaskPtrTemp, &pSpatterMask);    // simd loads
                     rpp_simd_load(rpp_load8_f32_to_f32_avx, spatterMaskInvPtrTemp, &pSpatterMaskInv);    // simd loads
@@ -698,7 +731,7 @@ RppStatus spatter_f32_f32_host_tensor(Rpp32f *srcPtr,
                     dstPtrChannel = dstPtrTemp;
                     for(int c = 0; c < srcDescPtr->c; c++)
                     {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                         __m256 p;
                         rpp_simd_load(rpp_load8_f32_to_f32_avx, srcPtrChannel, &p);    // simd loads
                         compute_spatter_8_host(&p, &pSpatterMaskInv, &pSpatterMask, &pSpatterValue[c]);    // spatter adjustment
@@ -796,7 +829,7 @@ RppStatus spatter_f16_f16_host_tensor(Rpp16f *srcPtr,
         if (srcDescPtr->c == 1)
             spatterValue[0] = spatterValue[1] = spatterValue[2] = (spatterValue[0] + spatterValue[1] + spatterValue[2]) * 0.3333;
 
-#if __AVX2__
+#if defined(__AVX2__)
         Rpp32u alignedLength = (bufferLength / 24) * 24;
         Rpp32u vectorIncrement = 24;
         Rpp32u vectorIncrementPerChannel = 8;
@@ -805,6 +838,24 @@ RppStatus spatter_f16_f16_host_tensor(Rpp16f *srcPtr,
         pSpatterValue[0] = _mm256_set1_ps(spatterValue[0]);
         pSpatterValue[1] = _mm256_set1_ps(spatterValue[1]);
         pSpatterValue[2] = _mm256_set1_ps(spatterValue[2]);
+#elif defined(__loongarch_asx)
+        Rpp32u alignedLength = (bufferLength / 24) * 24;
+        Rpp32u vectorIncrement = 24;
+        Rpp32u vectorIncrementPerChannel = 8;
+
+        __m256 pSpatterValue[3];
+        pSpatterValue[0] = lasx_set1_f32(spatterValue[0]);
+        pSpatterValue[1] = lasx_set1_f32(spatterValue[1]);
+        pSpatterValue[2] = lasx_set1_f32(spatterValue[2]);
+#elif defined(__loongarch_sx)
+        Rpp32u alignedLength = (bufferLength / 12) * 12;
+        Rpp32u vectorIncrement = 12;
+        Rpp32u vectorIncrementPerChannel = 4;
+
+        __m128 pSpatterValue[3];
+        pSpatterValue[0] = lsx_set1_f32(spatterValue[0]);
+        pSpatterValue[1] = lsx_set1_f32(spatterValue[1]);
+        pSpatterValue[2] = lsx_set1_f32(spatterValue[2]);
 #else
         Rpp32u alignedLength = (bufferLength / 12) * 12;
         Rpp32u vectorIncrement = 12;
@@ -846,7 +897,7 @@ RppStatus spatter_f16_f16_host_tensor(Rpp16f *srcPtr,
                     Rpp32f dstPtrTempR_ps[8], dstPtrTempG_ps[8], dstPtrTempB_ps[8];
                     for(int cnt = 0; cnt < vectorIncrement; cnt++)
                         srcPtrTemp_ps[cnt] = (Rpp32f) srcPtrTemp[cnt];
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 pSpatterMask, pSpatterMaskInv, p[3];
                     rpp_simd_load(rpp_load8_f32_to_f32_avx, spatterMaskPtrTemp, &pSpatterMask);    // simd loads
                     rpp_simd_load(rpp_load8_f32_to_f32_avx, spatterMaskInvPtrTemp, &pSpatterMaskInv);    // simd loads
@@ -931,7 +982,7 @@ RppStatus spatter_f16_f16_host_tensor(Rpp16f *srcPtr,
                         srcPtrTempG_ps[cnt] = (Rpp32f) srcPtrTempG[cnt];
                         srcPtrTempB_ps[cnt] = (Rpp32f) srcPtrTempB[cnt];
                     }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 pSpatterMask, pSpatterMaskInv, p[3];
                     rpp_simd_load(rpp_load8_f32_to_f32_avx, spatterMaskPtrTemp, &pSpatterMask);    // simd loads
                     rpp_simd_load(rpp_load8_f32_to_f32_avx, spatterMaskInvPtrTemp, &pSpatterMaskInv);    // simd loads
@@ -1004,7 +1055,7 @@ RppStatus spatter_f16_f16_host_tensor(Rpp16f *srcPtr,
                     Rpp32f dstPtrTemp_ps[25];
                     for(int cnt = 0; cnt < vectorIncrement; cnt++)
                         srcPtrTemp_ps[cnt] = (Rpp32f) srcPtrTemp[cnt];
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 pSpatterMask, pSpatterMaskInv, p[3];
                     rpp_simd_load(rpp_load8_f32_to_f32_avx, spatterMaskPtrTemp, &pSpatterMask);    // simd loads
                     rpp_simd_load(rpp_load8_f32_to_f32_avx, spatterMaskInvPtrTemp, &pSpatterMaskInv);    // simd loads
@@ -1048,7 +1099,7 @@ RppStatus spatter_f16_f16_host_tensor(Rpp16f *srcPtr,
         // Spatter without fused output-layout toggle (NCHW -> NCHW)
         else if ((srcDescPtr->layout == RpptLayout::NCHW) && (dstDescPtr->layout == RpptLayout::NCHW))
         {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             alignedLength = bufferLength & ~7;
 #else
             alignedLength = bufferLength & ~3;
@@ -1073,7 +1124,7 @@ RppStatus spatter_f16_f16_host_tensor(Rpp16f *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 pSpatterMask, pSpatterMaskInv;
                     rpp_simd_load(rpp_load8_f32_to_f32_avx, spatterMaskPtrTemp, &pSpatterMask);    // simd loads
                     rpp_simd_load(rpp_load8_f32_to_f32_avx, spatterMaskInvPtrTemp, &pSpatterMaskInv);    // simd loads
@@ -1089,7 +1140,7 @@ RppStatus spatter_f16_f16_host_tensor(Rpp16f *srcPtr,
                         Rpp32f srcPtrChannel_ps[8], dstPtrChannel_ps[8];
                         for(int cnt = 0; cnt < vectorIncrementPerChannel; cnt++)
                             srcPtrChannel_ps[cnt] = (Rpp32f) srcPtrChannel[cnt];
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                         __m256 p;
                         rpp_simd_load(rpp_load8_f32_to_f32_avx, srcPtrChannel_ps, &p);    // simd loads
                         compute_spatter_8_host(&p, &pSpatterMaskInv, &pSpatterMask, &pSpatterValue[c]);    // spatter adjustment
@@ -1193,11 +1244,21 @@ RppStatus spatter_i8_i8_host_tensor(Rpp8s *srcPtr,
         if (srcDescPtr->c == 1)
             spatterValue[0] = spatterValue[1] = spatterValue[2] = (spatterValue[0] + spatterValue[1] + spatterValue[2]) * 0.3333;
 
-#if __AVX2__
+#if defined(__AVX2__)
         __m256 pSpatterValue[3];
         pSpatterValue[0] = _mm256_set1_ps(spatterValue[0]);
         pSpatterValue[1] = _mm256_set1_ps(spatterValue[1]);
         pSpatterValue[2] = _mm256_set1_ps(spatterValue[2]);
+#elif defined(__loongarch_asx)
+        __m256 pSpatterValue[3];
+        pSpatterValue[0] = lasx_set1_f32(spatterValue[0]);
+        pSpatterValue[1] = lasx_set1_f32(spatterValue[1]);
+        pSpatterValue[2] = lasx_set1_f32(spatterValue[2]);
+#elif defined(__loongarch_sx)
+        __m128 pSpatterValue[3];
+        pSpatterValue[0] = lsx_set1_f32(spatterValue[0]);
+        pSpatterValue[1] = lsx_set1_f32(spatterValue[1]);
+        pSpatterValue[2] = lsx_set1_f32(spatterValue[2]);
 #else
         __m128 pSpatterValue[3];
         pSpatterValue[0] = _mm_set1_ps(spatterValue[0]);
@@ -1231,7 +1292,7 @@ RppStatus spatter_i8_i8_host_tensor(Rpp8s *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 pSpatterMask[2], pSpatterMaskInv[2], p[6];
                     rpp_simd_load(rpp_load16_f32_to_f32_avx, spatterMaskPtrTemp, pSpatterMask);    // simd loads
                     rpp_simd_load(rpp_load16_f32_to_f32_avx, spatterMaskInvPtrTemp, pSpatterMaskInv);    // simd loads
@@ -1302,7 +1363,7 @@ RppStatus spatter_i8_i8_host_tensor(Rpp8s *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 pSpatterMask[2], pSpatterMaskInv[2], p[6];
                     rpp_simd_load(rpp_load16_f32_to_f32_avx, spatterMaskPtrTemp, pSpatterMask);    // simd loads
                     rpp_simd_load(rpp_load16_f32_to_f32_avx, spatterMaskInvPtrTemp, pSpatterMaskInv);    // simd loads
@@ -1369,7 +1430,7 @@ RppStatus spatter_i8_i8_host_tensor(Rpp8s *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 pSpatterMask[2], pSpatterMaskInv[2], p[6];
                     rpp_simd_load(rpp_load16_f32_to_f32_avx, spatterMaskPtrTemp, pSpatterMask);    // simd loads
                     rpp_simd_load(rpp_load16_f32_to_f32_avx, spatterMaskInvPtrTemp, pSpatterMaskInv);    // simd loads
@@ -1432,7 +1493,7 @@ RppStatus spatter_i8_i8_host_tensor(Rpp8s *srcPtr,
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     __m256 pSpatterMask[2], pSpatterMaskInv[2];
                     rpp_simd_load(rpp_load16_f32_to_f32_avx, spatterMaskPtrTemp, pSpatterMask);    // simd loads
                     rpp_simd_load(rpp_load16_f32_to_f32_avx, spatterMaskInvPtrTemp, pSpatterMaskInv);    // simd loads
@@ -1445,7 +1506,7 @@ RppStatus spatter_i8_i8_host_tensor(Rpp8s *srcPtr,
                     dstPtrChannel = dstPtrTemp;
                     for(int c = 0; c < srcDescPtr->c; c++)
                     {
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                         __m256 p[2];
                         rpp_simd_load(rpp_load16_i8_to_f32_avx, srcPtrChannel, p);    // simd loads
                         compute_spatter_16_host(p, pSpatterMaskInv, pSpatterMask, pSpatterValue[c]);    // spatter adjustment
diff --git a/src/modules/cpu/kernel/spectrogram.hpp b/src/modules/cpu/kernel/spectrogram.hpp
index 1fda17fd..122a2f0c 100644
--- a/src/modules/cpu/kernel/spectrogram.hpp
+++ b/src/modules/cpu/kernel/spectrogram.hpp
@@ -32,6 +32,23 @@ inline bool can_use_real_impl(Rpp64s n) { return is_pow2(n); }
 inline Rpp64s size_in_buf(Rpp64s n) { return can_use_real_impl(n) ? n : 2 * n; }
 inline Rpp64s size_out_buf(Rpp64s n) { return can_use_real_impl(n) ? n + 2 : 2 * n; }
 
+#if defined(__loongarch64)
+static void* _mm_malloc(size_t size, size_t alignment)
+{
+    void* ptr = nullptr;
+    if (posix_memalign(&ptr, alignment, size) != 0)
+        return nullptr;
+    return ptr;
+}
+
+static void _mm_free(void* ptr)
+{
+    if (!ptr)
+        return;
+    free(ptr);
+}
+#endif
+
 RppStatus spectrogram_host_tensor(Rpp32f *srcPtr,
                                   RpptDescPtr srcDescPtr,
                                   Rpp32f *dstPtr,
@@ -118,10 +135,10 @@ RppStatus spectrogram_host_tensor(Rpp32f *srcPtr,
                 for (; t < alignedWindowLength; t += 8)
                 {
                     __m256 pSrc, pWindowFn;
-                    pSrc = _mm256_loadu_ps(srcPtrWindowTemp);
-                    pWindowFn = _mm256_loadu_ps(windowFnTemp);
-                    pSrc = _mm256_mul_ps(pSrc, pWindowFn);
-                    _mm256_storeu_ps(windowOutputTemp, pSrc);
+                    pSrc = (__m256)__lasx_xvld(srcPtrWindowTemp, 0);
+                    pWindowFn = (__m256)__lasx_xvld(windowFnTemp, 0);
+                    pSrc = __lasx_xvfmul_s(pSrc, pWindowFn);
+                    __lasx_xvst((__m256i)pSrc, windowOutputTemp, 0);
                     srcPtrWindowTemp += 8;
                     windowFnTemp += 8;
                     windowOutputTemp += 8;
@@ -144,6 +161,7 @@ RppStatus spectrogram_host_tensor(Rpp32f *srcPtr,
             exit(0);
         }
 
+
         // Set temporary buffers to 0
         Rpp32f FFTS_ALIGN(32) *fftInBuf = static_cast<Rpp32f*>(_mm_malloc(fftInSize * sizeof(Rpp32f), 32)); // ffts requires 32-byte aligned memory
         Rpp32f FFTS_ALIGN(32) *fftOutBuf = static_cast<Rpp32f*>(_mm_malloc(fftOutSize * sizeof(Rpp32f), 32)); // ffts requires 32-byte aligned memory
diff --git a/src/modules/cpu/kernel/subtract_scalar.hpp b/src/modules/cpu/kernel/subtract_scalar.hpp
index a40e6219..c0f3c9f3 100644
--- a/src/modules/cpu/kernel/subtract_scalar.hpp
+++ b/src/modules/cpu/kernel/subtract_scalar.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 RppStatus subtract_scalar_f32_f32_host_tensor(Rpp32f *srcPtr,
                                               RpptGenericDescPtr srcGenericDescPtr,
@@ -62,7 +67,7 @@ RppStatus subtract_scalar_f32_f32_host_tensor(Rpp32f *srcPtr,
         Rpp32u vectorIncrement = 16;
         Rpp32u bufferLength = roi.xyzwhdROI.roiWidth * layoutParams.bufferMultiplier;
         Rpp32u alignedLength = (bufferLength / vectorIncrement) * vectorIncrement;
-        __m256 pSubtractParam = _mm256_set1_ps(subtractParam);
+        __m256 pSubtractParam = lasx_set1_f32(subtractParam);
 
         // Subtract without fused output-layout toggle (NCDHW -> NCDHW)
         if((srcGenericDescPtr->layout == RpptLayout::NCDHW) && (dstGenericDescPtr->layout == RpptLayout::NCDHW))
diff --git a/src/modules/cpu/kernel/swap_channels.hpp b/src/modules/cpu/kernel/swap_channels.hpp
index 1677c012..b3332d3d 100644
--- a/src/modules/cpu/kernel/swap_channels.hpp
+++ b/src/modules/cpu/kernel/swap_channels.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 RppStatus swap_channels_u8_u8_host_tensor(Rpp8u *srcPtr,
                                           RpptDescPtr srcDescPtr,
diff --git a/src/modules/cpu/kernel/tensor_max.hpp b/src/modules/cpu/kernel/tensor_max.hpp
index 0380f4ef..32336427 100644
--- a/src/modules/cpu/kernel/tensor_max.hpp
+++ b/src/modules/cpu/kernel/tensor_max.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 RppStatus tensor_max_u8_u8_host(Rpp8u *srcPtr,
                                 RpptDescPtr srcDescPtr,
@@ -66,8 +71,10 @@ RppStatus tensor_max_u8_u8_host(Rpp8u *srcPtr,
 
             Rpp8u *srcPtrRow;
             srcPtrRow = srcPtrChannel;
-#if __AVX2__
+#if defined(__AVX2__)
                 __m256i pMax = _mm256_setzero_si256();
+#elif defined(__loongarch_asx)
+                __m256i pMax = __lasx_xvldi(0);
 #endif
                 for(int i = 0; i < roi.xywhROI.roiHeight; i++)
                 {
@@ -75,12 +82,20 @@ RppStatus tensor_max_u8_u8_host(Rpp8u *srcPtr,
                     srcPtrTemp = srcPtrRow;
 
                     int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                     {
                         __m256i p1 = _mm256_loadu_si256((__m256i *)srcPtrTemp);
                         pMax = _mm256_max_epu8(p1, pMax); //compare and store max of 32 values into global max
 
+                        srcPtrTemp += vectorIncrement;
+                    }
+#elif defined(__loongarch_asx)
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
+                    {
+                        __m256i p1 = __lasx_xvld((__m256i *)srcPtrTemp, 0);
+                        pMax = __lasx_xvmax_bu(p1, pMax); //compare and store max of 32 values into global max
+
                         srcPtrTemp += vectorIncrement;
                     }
 #endif
@@ -90,7 +105,7 @@ RppStatus tensor_max_u8_u8_host(Rpp8u *srcPtr,
                     }
                     srcPtrRow += srcDescPtr->strides.hStride;
                 }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m128i result;
                 reduce_max_32_host(&pMax, &result);
                 rpp_simd_store(rpp_store16_u8_to_u8, resultAvx, &result);
@@ -112,10 +127,14 @@ RppStatus tensor_max_u8_u8_host(Rpp8u *srcPtr,
                 srcPtrRowR = srcPtrChannel;
                 srcPtrRowG = srcPtrRowR + srcDescPtr->strides.cStride;
                 srcPtrRowB = srcPtrRowG + srcDescPtr->strides.cStride;
-#if __AVX2__
+#if defined(__AVX2__)
                 __m256i pMaxR = _mm256_setzero_si256();
                 __m256i pMaxG = pMaxR;
                 __m256i pMaxB = pMaxR;
+#elif defined(__loongarch_asx)
+                __m256i pMaxR = __lasx_xvldi(0);
+                __m256i pMaxG = pMaxR;
+                __m256i pMaxB = pMaxR;
 #endif
                 for(int i = 0; i < roi.xywhROI.roiHeight; i++)
                 {
@@ -125,7 +144,7 @@ RppStatus tensor_max_u8_u8_host(Rpp8u *srcPtr,
                     srcPtrTempB = srcPtrRowB;
 
                     int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                     {
                         __m256i p[3];
@@ -147,7 +166,7 @@ RppStatus tensor_max_u8_u8_host(Rpp8u *srcPtr,
                     srcPtrRowG += srcDescPtr->strides.hStride;
                     srcPtrRowB += srcDescPtr->strides.hStride;
                 }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m128i result;
                 reduce_max_96_host(&pMaxR, &pMaxG, &pMaxB, &result);
                 rpp_simd_store(rpp_store16_u8_to_u8, resultAvx, &result);
@@ -177,10 +196,15 @@ RppStatus tensor_max_u8_u8_host(Rpp8u *srcPtr,
             {
                 Rpp8u *srcPtrRow;
                 srcPtrRow = srcPtrChannel;
-
+#if defined(__loongarch_sx)
+                __m128i pMaxR = __lsx_vldi(0);
+                __m128i pMaxG = pMaxR;
+                __m128i pMaxB = pMaxR;
+#else
                 __m128i pMaxR = _mm_setzero_si128();
                 __m128i pMaxG = pMaxR;
                 __m128i pMaxB = pMaxR;
+#endif
 
                 for(int i = 0; i < roi.xywhROI.roiHeight; i++)
                 {
@@ -188,7 +212,7 @@ RppStatus tensor_max_u8_u8_host(Rpp8u *srcPtr,
                     srcPtrTemp = srcPtrRow;
 
                     int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                     {
                         __m128i p[3];
@@ -207,7 +231,7 @@ RppStatus tensor_max_u8_u8_host(Rpp8u *srcPtr,
                     }
                     srcPtrRow += srcDescPtr->strides.hStride;
                 }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m128i result;
                 reduce_max_48_host(&pMaxR, &pMaxG, &pMaxB, &result);
                 rpp_simd_store(rpp_store16_u8_to_u8, resultAvx, &result);
@@ -267,8 +291,10 @@ RppStatus tensor_max_f32_f32_host(Rpp32f *srcPtr,
 
             Rpp32f *srcPtrRow;
             srcPtrRow = srcPtrChannel;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256 pMax = _mm256_setzero_ps();
+#elif defined(__loongarch_asx)
+            __m256 pMax = (__m256)__lasx_xvldi(0);
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -276,7 +302,7 @@ RppStatus tensor_max_f32_f32_host(Rpp32f *srcPtr,
                 srcPtrTemp = srcPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256 p1;
@@ -292,7 +318,7 @@ RppStatus tensor_max_f32_f32_host(Rpp32f *srcPtr,
                 }
                 srcPtrRow += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             __m128 result;
             reduce_max_float8_host(&pMax, &result);
             rpp_simd_store(rpp_store4_f32_to_f32, resultAvx, &result);
@@ -312,10 +338,14 @@ RppStatus tensor_max_f32_f32_host(Rpp32f *srcPtr,
             srcPtrRowR = srcPtrChannel;
             srcPtrRowG = srcPtrRowR + srcDescPtr->strides.cStride;
             srcPtrRowB = srcPtrRowG + srcDescPtr->strides.cStride;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256 pMaxR = _mm256_setzero_ps();
             __m256 pMaxG = pMaxR;
             __m256 pMaxB = pMaxR;
+#elif defined(__loongarch_asx)
+            __m256 pMaxR = (__m256)__lasx_xvldi(0);
+            __m256 pMaxG = pMaxR;
+            __m256 pMaxB = pMaxR;
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -325,7 +355,7 @@ RppStatus tensor_max_f32_f32_host(Rpp32f *srcPtr,
                 srcPtrTempB = srcPtrRowB;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 p[3];
@@ -347,7 +377,7 @@ RppStatus tensor_max_f32_f32_host(Rpp32f *srcPtr,
                 srcPtrRowG += srcDescPtr->strides.hStride;
                 srcPtrRowB += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             __m256 result;
             reduce_max_float24_host(&pMaxR, &pMaxG, &pMaxB, &result);
             rpp_simd_store(rpp_store8_f32_to_f32_avx, resultAvx, &result);
@@ -377,10 +407,14 @@ RppStatus tensor_max_f32_f32_host(Rpp32f *srcPtr,
                 Rpp32f *srcPtrRow;
                 srcPtrRow = srcPtrChannel;
 
-#if __AVX2__
+#if defined(__AVX2__)
                 __m256 pMaxR = _mm256_setzero_ps();
                 __m256 pMaxG = pMaxR;
                 __m256 pMaxB = pMaxR;
+#elif defined(__loongarch_asx)
+                __m256 pMaxR = (__m256)__lasx_xvldi(0);
+                __m256 pMaxG = pMaxR;
+                __m256 pMaxB = pMaxR;
 #endif
                 for(int i = 0; i < roi.xywhROI.roiHeight; i++)
                 {
@@ -388,7 +422,7 @@ RppStatus tensor_max_f32_f32_host(Rpp32f *srcPtr,
                     srcPtrTemp = srcPtrRow;
 
                     int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                     {
                         __m256 p[3];
@@ -407,7 +441,7 @@ RppStatus tensor_max_f32_f32_host(Rpp32f *srcPtr,
                     }
                     srcPtrRow += srcDescPtr->strides.hStride;
                 }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m256 result;
                 reduce_max_float24_host(&pMaxR, &pMaxG, &pMaxB, &result);
                 rpp_simd_store(rpp_store8_f32_to_f32_avx, resultAvx, &result);
@@ -467,8 +501,10 @@ RppStatus tensor_max_f16_f16_host(Rpp16f *srcPtr,
 
             Rpp16f *srcPtrRow;
             srcPtrRow = srcPtrChannel;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256 pMax = _mm256_setzero_ps();
+#elif defined(__loongarch_asx)
+            __m256 pMax = (__m256)__lasx_xvldi(0);
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -476,7 +512,7 @@ RppStatus tensor_max_f16_f16_host(Rpp16f *srcPtr,
                 srcPtrTemp = srcPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     Rpp32f srcPtrTemp_ps[8];
@@ -497,7 +533,7 @@ RppStatus tensor_max_f16_f16_host(Rpp16f *srcPtr,
                 }
                 srcPtrRow += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             __m128 result;
             reduce_max_float8_host(&pMax, &result);
             rpp_simd_store(rpp_store4_f32_to_f32, resultAvx, &result);
@@ -517,10 +553,14 @@ RppStatus tensor_max_f16_f16_host(Rpp16f *srcPtr,
             srcPtrRowR = srcPtrChannel;
             srcPtrRowG = srcPtrRowR + srcDescPtr->strides.cStride;
             srcPtrRowB = srcPtrRowG + srcDescPtr->strides.cStride;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256 pMaxR = _mm256_setzero_ps();
             __m256 pMaxG = pMaxR;
             __m256 pMaxB = pMaxR;
+#elif defined(__loongarch_asx)
+            __m256 pMaxR = (__m256)__lasx_xvldi(0);
+            __m256 pMaxG = pMaxR;
+            __m256 pMaxB = pMaxR;
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -530,7 +570,7 @@ RppStatus tensor_max_f16_f16_host(Rpp16f *srcPtr,
                 srcPtrTempB = srcPtrRowB;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     Rpp32f srcPtrTempR_ps[8], srcPtrTempG_ps[8], srcPtrTempB_ps[8];
@@ -559,7 +599,7 @@ RppStatus tensor_max_f16_f16_host(Rpp16f *srcPtr,
                 srcPtrRowG += srcDescPtr->strides.hStride;
                 srcPtrRowB += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             __m256 result;
             reduce_max_float24_host(&pMaxR, &pMaxG, &pMaxB, &result);
             rpp_simd_store(rpp_store8_f32_to_f32_avx, resultAvx, &result);
@@ -590,10 +630,14 @@ RppStatus tensor_max_f16_f16_host(Rpp16f *srcPtr,
                 Rpp16f *srcPtrRow;
                 srcPtrRow = srcPtrChannel;
 
-#if __AVX2__
+#if defined(__AVX2__)
                 __m256 pMaxR = _mm256_setzero_ps();
                 __m256 pMaxG = pMaxR;
                 __m256 pMaxB = pMaxR;
+#elif defined(__loongarch_asx)
+                __m256 pMaxR = (__m256)__lasx_xvldi(0);
+                __m256 pMaxG = pMaxR;
+                __m256 pMaxB = pMaxR;
 #endif
                 for(int i = 0; i < roi.xywhROI.roiHeight; i++)
                 {
@@ -601,7 +645,7 @@ RppStatus tensor_max_f16_f16_host(Rpp16f *srcPtr,
                     srcPtrTemp = srcPtrRow;
 
                     int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                     {
                         Rpp32f srcPtrTemp_ps[24];
@@ -625,7 +669,7 @@ RppStatus tensor_max_f16_f16_host(Rpp16f *srcPtr,
                     }
                     srcPtrRow += srcDescPtr->strides.hStride;
                 }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m256 result;
                 reduce_max_float24_host(&pMaxR, &pMaxG, &pMaxB, &result);
                 rpp_simd_store(rpp_store8_f32_to_f32_avx, resultAvx, &result);
@@ -685,8 +729,10 @@ RppStatus tensor_max_i8_i8_host(Rpp8s *srcPtr,
 
             Rpp8s *srcPtrRow;
             srcPtrRow = srcPtrChannel;
-#if __AVX2__
+#if defined(__AVX2__)
                 __m256i pMax = _mm256_set1_epi8(INT8_MIN);
+#elif defined(__loongarch_asx)
+                __m256i pMax = __lasx_xvreplgr2vr_b(INT8_MIN);
 #endif
                 for(int i = 0; i < roi.xywhROI.roiHeight; i++)
                 {
@@ -694,12 +740,20 @@ RppStatus tensor_max_i8_i8_host(Rpp8s *srcPtr,
                     srcPtrTemp = srcPtrRow;
 
                     int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                     {
                         __m256i p1 = _mm256_load_si256((__m256i *)srcPtrTemp);
                         pMax = _mm256_max_epi8(p1, pMax); //compare and store max of 32 values into global max
 
+                        srcPtrTemp += vectorIncrement;
+                    }
+#elif defined(__loongarch_asx)
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
+                    {
+                        __m256i p1 = __lasx_xvld((__m256i *)srcPtrTemp, 0);
+                        pMax = __lasx_xvmax_b(p1, pMax); //compare and store max of 32 values into global max
+
                         srcPtrTemp += vectorIncrement;
                     }
 #endif
@@ -709,7 +763,7 @@ RppStatus tensor_max_i8_i8_host(Rpp8s *srcPtr,
                     }
                     srcPtrRow += srcDescPtr->strides.hStride;
                 }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m128i result;
                 reduce_max_i32_host(&pMax, &result);
                 rpp_simd_store(rpp_store16_i8, resultAvx, &result);
@@ -731,10 +785,14 @@ RppStatus tensor_max_i8_i8_host(Rpp8s *srcPtr,
                 srcPtrRowR = srcPtrChannel;
                 srcPtrRowG = srcPtrRowR + srcDescPtr->strides.cStride;
                 srcPtrRowB = srcPtrRowG + srcDescPtr->strides.cStride;
-#if __AVX2__
+#if defined(__AVX2__)
                 __m256i pMaxR = _mm256_set1_epi8(INT8_MIN);
                 __m256i pMaxG = pMaxR;
                 __m256i pMaxB = pMaxR;
+#elif defined(__loongarch_asx)
+                __m256i pMaxR = __lasx_xvreplgr2vr_b(INT8_MIN);
+                __m256i pMaxG = pMaxR;
+                __m256i pMaxB = pMaxR;
 #endif
                 for(int i = 0; i < roi.xywhROI.roiHeight; i++)
                 {
@@ -744,7 +802,7 @@ RppStatus tensor_max_i8_i8_host(Rpp8s *srcPtr,
                     srcPtrTempB = srcPtrRowB;
 
                     int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                     {
                         __m256i p[3];
@@ -766,7 +824,7 @@ RppStatus tensor_max_i8_i8_host(Rpp8s *srcPtr,
                     srcPtrRowG += srcDescPtr->strides.hStride;
                     srcPtrRowB += srcDescPtr->strides.hStride;
                 }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m128i result;
                 reduce_max_i96_host(&pMaxR, &pMaxG, &pMaxB, &result);
                 rpp_simd_store(rpp_store16_i8, resultAvx, &result);
@@ -796,10 +854,15 @@ RppStatus tensor_max_i8_i8_host(Rpp8s *srcPtr,
             {
                 Rpp8s *srcPtrRow;
                 srcPtrRow = srcPtrChannel;
-
+#if defined(__loongarch_sx)
+                __m128i pMaxR = __lsx_vreplgr2vr_b(INT8_MIN);
+                __m128i pMaxG = pMaxR;
+                __m128i pMaxB = pMaxR;
+#else
                 __m128i pMaxR = _mm_set1_epi8(INT8_MIN);
                 __m128i pMaxG = pMaxR;
                 __m128i pMaxB = pMaxR;
+#endif
 
                 for(int i = 0; i < roi.xywhROI.roiHeight; i++)
                 {
@@ -807,7 +870,7 @@ RppStatus tensor_max_i8_i8_host(Rpp8s *srcPtr,
                     srcPtrTemp = srcPtrRow;
 
                     int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                     {
                         __m128i p[3];
@@ -826,7 +889,7 @@ RppStatus tensor_max_i8_i8_host(Rpp8s *srcPtr,
                     }
                     srcPtrRow += srcDescPtr->strides.hStride;
                 }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m128i result;
                 reduce_max_i48_host(&pMaxR, &pMaxG, &pMaxB, &result);
                 rpp_simd_store(rpp_store16_i8, resultAvx, &result);
diff --git a/src/modules/cpu/kernel/tensor_mean.hpp b/src/modules/cpu/kernel/tensor_mean.hpp
index 9536e258..4cf74d6b 100644
--- a/src/modules/cpu/kernel/tensor_mean.hpp
+++ b/src/modules/cpu/kernel/tensor_mean.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 #include "reduction.hpp"
 
 RppStatus tensor_mean_u8_f32_host(Rpp8u *srcPtr,
@@ -66,7 +71,7 @@ RppStatus tensor_mean_u8_f32_host(Rpp8u *srcPtr,
 
             Rpp8u *srcPtrRow;
             srcPtrRow = srcPtrChannel;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             __m256i pSum = avx_px0;
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
@@ -75,7 +80,7 @@ RppStatus tensor_mean_u8_f32_host(Rpp8u *srcPtr,
                 srcPtrTemp = srcPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256i p1[2];
@@ -88,9 +93,12 @@ RppStatus tensor_mean_u8_f32_host(Rpp8u *srcPtr,
                     sum += static_cast<Rpp32u>(*srcPtrTemp++);
                 srcPtrRow += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__)
             _mm256_store_si256((__m256i *)sumAvx, pSum);
             sum += (sumAvx[0] + sumAvx[1] + sumAvx[2] + sumAvx[3] + sumAvx[4] + sumAvx[5] + sumAvx[6] + sumAvx[7]);
+#elif defined(__loongarch_asx)
+            __lasx_xvst(pSum, (__m256i *)sumAvx, 0);
+            sum += (sumAvx[0] + sumAvx[1] + sumAvx[2] + sumAvx[3] + sumAvx[4] + sumAvx[5] + sumAvx[6] + sumAvx[7]);
 #endif
             mean = static_cast<Rpp32f>(sum) / totalPixelsPerChannel;
             tensorMeanArr[batchCount] = mean;
@@ -110,7 +118,7 @@ RppStatus tensor_mean_u8_f32_host(Rpp8u *srcPtr,
             srcPtrRowR = srcPtrChannel;
             srcPtrRowG = srcPtrRowR + srcDescPtr->strides.cStride;
             srcPtrRowB = srcPtrRowG + srcDescPtr->strides.cStride;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             __m256i pSumR = avx_px0;
             __m256i pSumG = avx_px0;
             __m256i pSumB = avx_px0;
@@ -123,7 +131,7 @@ RppStatus tensor_mean_u8_f32_host(Rpp8u *srcPtr,
                 srcPtrTempB = srcPtrRowB;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256i p[6];
@@ -144,13 +152,20 @@ RppStatus tensor_mean_u8_f32_host(Rpp8u *srcPtr,
                 srcPtrRowG += srcDescPtr->strides.hStride;
                 srcPtrRowB += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__)
             _mm256_store_si256((__m256i *)sumAvxR, pSumR);
             _mm256_store_si256((__m256i *)sumAvxG, pSumG);
             _mm256_store_si256((__m256i *)sumAvxB, pSumB);
             sumR += (sumAvxR[0] + sumAvxR[1] + sumAvxR[2] + sumAvxR[3] + sumAvxR[4] + sumAvxR[5] + sumAvxR[6] + sumAvxR[7]);
             sumG += (sumAvxG[0] + sumAvxG[1] + sumAvxG[2] + sumAvxG[3] + sumAvxG[4] + sumAvxG[5] + sumAvxG[6] + sumAvxG[7]);
             sumB += (sumAvxB[0] + sumAvxB[1] + sumAvxB[2] + sumAvxB[3] + sumAvxB[4] + sumAvxB[5] + sumAvxB[6] + sumAvxB[7]);
+#elif defined(__loongarch_asx)
+            __lasx_xvst(pSumR, (__m256i *)sumAvxR, 0);
+            __lasx_xvst(pSumG, (__m256i *)sumAvxG, 0);
+            __lasx_xvst(pSumB, (__m256i *)sumAvxB, 0);
+            sumR += (sumAvxR[0] + sumAvxR[1] + sumAvxR[2] + sumAvxR[3] + sumAvxR[4] + sumAvxR[5] + sumAvxR[6] + sumAvxR[7]);
+            sumG += (sumAvxG[0] + sumAvxG[1] + sumAvxG[2] + sumAvxG[3] + sumAvxG[4] + sumAvxG[5] + sumAvxG[6] + sumAvxG[7]);
+            sumB += (sumAvxB[0] + sumAvxB[1] + sumAvxB[2] + sumAvxB[3] + sumAvxB[4] + sumAvxB[5] + sumAvxB[6] + sumAvxB[7]);
 #endif
             sum = static_cast<Rpp64u>(sumR) + static_cast<Rpp64u>(sumG) + static_cast<Rpp64u>(sumB);
             mean = (static_cast<Rpp64f>(sum) / (totalPixelsPerChannel * 3));
@@ -175,7 +190,7 @@ RppStatus tensor_mean_u8_f32_host(Rpp8u *srcPtr,
 
             Rpp8u *srcPtrRow;
             srcPtrRow = srcPtrChannel;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             __m256i pSumR = avx_px0;
             __m256i pSumG = avx_px0;
             __m256i pSumB = avx_px0;
@@ -186,7 +201,7 @@ RppStatus tensor_mean_u8_f32_host(Rpp8u *srcPtr,
                 srcPtrTemp = srcPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256i p[6];
@@ -204,13 +219,20 @@ RppStatus tensor_mean_u8_f32_host(Rpp8u *srcPtr,
                 }
                 srcPtrRow += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__)
             _mm256_store_si256((__m256i *)sumAvxR, pSumR);
             _mm256_store_si256((__m256i *)sumAvxG, pSumG);
             _mm256_store_si256((__m256i *)sumAvxB, pSumB);
             sumR += (sumAvxR[0] + sumAvxR[1] + sumAvxR[2] + sumAvxR[3] + sumAvxR[4] + sumAvxR[5] + sumAvxR[6] + sumAvxR[7]);
             sumG += (sumAvxG[0] + sumAvxG[1] + sumAvxG[2] + sumAvxG[3] + sumAvxG[4] + sumAvxG[5] + sumAvxG[6] + sumAvxG[7]);
             sumB += (sumAvxB[0] + sumAvxB[1] + sumAvxB[2] + sumAvxB[3] + sumAvxB[4] + sumAvxB[5] + sumAvxB[6] + sumAvxB[7]);
+#elif defined(__loongarch_asx)
+            __lasx_xvst(pSumR, (__m256i *)sumAvxR, 0);
+            __lasx_xvst(pSumG, (__m256i *)sumAvxG, 0);
+            __lasx_xvst(pSumB, (__m256i *)sumAvxB, 0);
+            sumR += (sumAvxR[0] + sumAvxR[1] + sumAvxR[2] + sumAvxR[3] + sumAvxR[4] + sumAvxR[5] + sumAvxR[6] + sumAvxR[7]);
+            sumG += (sumAvxG[0] + sumAvxG[1] + sumAvxG[2] + sumAvxG[3] + sumAvxG[4] + sumAvxG[5] + sumAvxG[6] + sumAvxG[7]);
+            sumB += (sumAvxB[0] + sumAvxB[1] + sumAvxB[2] + sumAvxB[3] + sumAvxB[4] + sumAvxB[5] + sumAvxB[6] + sumAvxB[7]);
 #endif
             sum = static_cast<Rpp64u>(sumR) + static_cast<Rpp64u>(sumG) + static_cast<Rpp64u>(sumB);
             mean = (static_cast<Rpp64f>(sum) / (totalPixelsPerChannel * 3));
@@ -267,8 +289,10 @@ RppStatus tensor_mean_f32_f32_host(Rpp32f *srcPtr,
 
             Rpp32f *srcPtrRow;
             srcPtrRow = srcPtrChannel;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256d pSum = _mm256_setzero_pd();
+#elif defined(__loongarch_asx)
+            __m256d pSum = (__m256d)__lasx_xvldi(0);
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -276,7 +300,7 @@ RppStatus tensor_mean_f32_f32_host(Rpp32f *srcPtr,
                 srcPtrTemp = srcPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256d p1[2];
@@ -289,9 +313,12 @@ RppStatus tensor_mean_f32_f32_host(Rpp32f *srcPtr,
                     sum += static_cast<Rpp64f>(*srcPtrTemp++);
                 srcPtrRow += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__)
             _mm256_storeu_pd(sumAvx, pSum);
             sum += (sumAvx[0] + sumAvx[1] + sumAvx[2] + sumAvx[3]);
+#elif defined(__loongarch_asx)
+            __lasx_xvst((__m256i)pSum, sumAvx, 0);
+            sum += (sumAvx[0] + sumAvx[1] + sumAvx[2] + sumAvx[3]);
 #endif
             mean = static_cast<Rpp32f>(sum / totalPixelsPerChannel);
             tensorMeanArr[batchCount] = mean;
@@ -310,10 +337,14 @@ RppStatus tensor_mean_f32_f32_host(Rpp32f *srcPtr,
             srcPtrRowR = srcPtrChannel;
             srcPtrRowG = srcPtrRowR + srcDescPtr->strides.cStride;
             srcPtrRowB = srcPtrRowG + srcDescPtr->strides.cStride;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256d pSumR = _mm256_setzero_pd();
             __m256d pSumG = _mm256_setzero_pd();
             __m256d pSumB = _mm256_setzero_pd();
+#elif defined(__loongarch_asx)
+            __m256d pSumR = (__m256d)__lasx_xvldi(0);
+            __m256d pSumG = (__m256d)__lasx_xvldi(0);
+            __m256d pSumB = (__m256d)__lasx_xvldi(0);
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -323,7 +354,7 @@ RppStatus tensor_mean_f32_f32_host(Rpp32f *srcPtr,
                 srcPtrTempB = srcPtrRowB;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256d p[6];
@@ -344,13 +375,20 @@ RppStatus tensor_mean_f32_f32_host(Rpp32f *srcPtr,
                 srcPtrRowG += srcDescPtr->strides.hStride;
                 srcPtrRowB += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__)
             _mm256_storeu_pd(sumAvxR, pSumR);
             _mm256_storeu_pd(sumAvxG, pSumG);
             _mm256_storeu_pd(sumAvxB, pSumB);
             sumR += (sumAvxR[0] + sumAvxR[1] + sumAvxR[2] + sumAvxR[3]);
             sumG += (sumAvxG[0] + sumAvxG[1] + sumAvxG[2] + sumAvxG[3]);
             sumB += (sumAvxB[0] + sumAvxB[1] + sumAvxB[2] + sumAvxB[3]);
+#elif defined(__loongarch_asx)
+            __lasx_xvst((__m256i)pSumR, sumAvxR, 0);
+            __lasx_xvst((__m256i)pSumG, sumAvxG, 0);
+            __lasx_xvst((__m256i)pSumB, sumAvxB, 0);
+            sumR += (sumAvxR[0] + sumAvxR[1] + sumAvxR[2] + sumAvxR[3]);
+            sumG += (sumAvxG[0] + sumAvxG[1] + sumAvxG[2] + sumAvxG[3]);
+            sumB += (sumAvxB[0] + sumAvxB[1] + sumAvxB[2] + sumAvxB[3]);
 #endif
 
             sum = sumR + sumG + sumB;
@@ -375,10 +413,14 @@ RppStatus tensor_mean_f32_f32_host(Rpp32f *srcPtr,
 
             Rpp32f *srcPtrRow;
             srcPtrRow = srcPtrChannel;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256d pSumR = _mm256_setzero_pd();
             __m256d pSumG = _mm256_setzero_pd();
             __m256d pSumB = _mm256_setzero_pd();
+#elif defined(__loongarch_asx)
+            __m256d pSumR = (__m256d)__lasx_xvldi(0);
+            __m256d pSumG = (__m256d)__lasx_xvldi(0);
+            __m256d pSumB = (__m256d)__lasx_xvldi(0);
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -386,7 +428,7 @@ RppStatus tensor_mean_f32_f32_host(Rpp32f *srcPtr,
                 srcPtrTemp = srcPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256d p[6];
@@ -404,13 +446,20 @@ RppStatus tensor_mean_f32_f32_host(Rpp32f *srcPtr,
                 }
                 srcPtrRow += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__)
             _mm256_storeu_pd(sumAvxR, pSumR);
             _mm256_storeu_pd(sumAvxG, pSumG);
             _mm256_storeu_pd(sumAvxB, pSumB);
             sumR += (sumAvxR[0] + sumAvxR[1] + sumAvxR[2] + sumAvxR[3]);
             sumG += (sumAvxG[0] + sumAvxG[1] + sumAvxG[2] + sumAvxG[3]);
             sumB += (sumAvxB[0] + sumAvxB[1] + sumAvxB[2] + sumAvxB[3]);
+#elif defined(__loongarch_asx)
+            __lasx_xvst((__m256i)pSumR, sumAvxR, 0);
+            __lasx_xvst((__m256i)pSumG, sumAvxG, 0);
+            __lasx_xvst((__m256i)pSumB, sumAvxB, 0);
+            sumR += (sumAvxR[0] + sumAvxR[1] + sumAvxR[2] + sumAvxR[3]);
+            sumG += (sumAvxG[0] + sumAvxG[1] + sumAvxG[2] + sumAvxG[3]);
+            sumB += (sumAvxB[0] + sumAvxB[1] + sumAvxB[2] + sumAvxB[3]);
 #endif
             sum = sumR + sumG + sumB;
             mean = static_cast<Rpp32f>(sum / (totalPixelsPerChannel * 3));
@@ -467,8 +516,10 @@ RppStatus tensor_mean_f16_f32_host(Rpp16f *srcPtr,
 
             Rpp16f *srcPtrRow;
             srcPtrRow = srcPtrChannel;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256d pSum = _mm256_setzero_pd();
+#elif defined(__loongarch_asx)
+            __m256d pSum = (__m256d)__lasx_xvldi(0);
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -476,7 +527,7 @@ RppStatus tensor_mean_f16_f32_host(Rpp16f *srcPtr,
                 srcPtrTemp = srcPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     Rpp32f srcPtrTemp_ps[8];
@@ -492,9 +543,12 @@ RppStatus tensor_mean_f16_f32_host(Rpp16f *srcPtr,
                     sum += static_cast<Rpp64f>(*srcPtrTemp++);
                 srcPtrRow += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__)
             _mm256_storeu_pd(sumAvx, pSum);
             sum += (sumAvx[0] + sumAvx[1] + sumAvx[2] + sumAvx[3]);
+#elif defined(__loongarch_asx)
+            __lasx_xvst((__m256i)pSum, sumAvx, 0);
+            sum += (sumAvx[0] + sumAvx[1] + sumAvx[2] + sumAvx[3]);
 #endif
             mean = static_cast<Rpp32f>(sum / totalPixelsPerChannel);
             tensorMeanArr[batchCount] = mean;
@@ -513,10 +567,14 @@ RppStatus tensor_mean_f16_f32_host(Rpp16f *srcPtr,
             srcPtrRowR = srcPtrChannel;
             srcPtrRowG = srcPtrRowR + srcDescPtr->strides.cStride;
             srcPtrRowB = srcPtrRowG + srcDescPtr->strides.cStride;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256d pSumR = _mm256_setzero_pd();
             __m256d pSumG = _mm256_setzero_pd();
             __m256d pSumB = _mm256_setzero_pd();
+#elif defined(__loongarch_asx)
+            __m256d pSumR = (__m256d)__lasx_xvldi(0);
+            __m256d pSumG = (__m256d)__lasx_xvldi(0);
+            __m256d pSumB = (__m256d)__lasx_xvldi(0);
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -526,7 +584,7 @@ RppStatus tensor_mean_f16_f32_host(Rpp16f *srcPtr,
                 srcPtrTempB = srcPtrRowB;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     Rpp32f srcPtrTempR_ps[8], srcPtrTempG_ps[8], srcPtrTempB_ps[8];
@@ -554,13 +612,20 @@ RppStatus tensor_mean_f16_f32_host(Rpp16f *srcPtr,
                 srcPtrRowG += srcDescPtr->strides.hStride;
                 srcPtrRowB += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__)
             _mm256_storeu_pd(sumAvxR, pSumR);
             _mm256_storeu_pd(sumAvxG, pSumG);
             _mm256_storeu_pd(sumAvxB, pSumB);
             sumR += (sumAvxR[0] + sumAvxR[1] + sumAvxR[2] + sumAvxR[3]);
             sumG += (sumAvxG[0] + sumAvxG[1] + sumAvxG[2] + sumAvxG[3]);
             sumB += (sumAvxB[0] + sumAvxB[1] + sumAvxB[2] + sumAvxB[3]);
+#elif defined(__loongarch_asx)
+            __lasx_xvst((__m256i)pSumR, sumAvxR, 0);
+            __lasx_xvst((__m256i)pSumG, sumAvxG, 0);
+            __lasx_xvst((__m256i)pSumB, sumAvxB, 0);
+            sumR += (sumAvxR[0] + sumAvxR[1] + sumAvxR[2] + sumAvxR[3]);
+            sumG += (sumAvxG[0] + sumAvxG[1] + sumAvxG[2] + sumAvxG[3]);
+            sumB += (sumAvxB[0] + sumAvxB[1] + sumAvxB[2] + sumAvxB[3]);
 #endif
             sum = sumR + sumG + sumB;
             mean = static_cast<Rpp32f>(sum / (totalPixelsPerChannel * 3));
@@ -584,10 +649,14 @@ RppStatus tensor_mean_f16_f32_host(Rpp16f *srcPtr,
 
             Rpp16f *srcPtrRow;
             srcPtrRow = srcPtrChannel;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256d pSumR = _mm256_setzero_pd();
             __m256d pSumG = _mm256_setzero_pd();
             __m256d pSumB = _mm256_setzero_pd();
+#elif defined(__loongarch_asx)
+            __m256d pSumR = (__m256d)__lasx_xvldi(0);
+            __m256d pSumG = (__m256d)__lasx_xvldi(0);
+            __m256d pSumB = (__m256d)__lasx_xvldi(0);
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -595,7 +664,7 @@ RppStatus tensor_mean_f16_f32_host(Rpp16f *srcPtr,
                 srcPtrTemp = srcPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     Rpp32f srcPtrTemp_ps[24];
@@ -616,13 +685,20 @@ RppStatus tensor_mean_f16_f32_host(Rpp16f *srcPtr,
                 }
                 srcPtrRow += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__)
             _mm256_storeu_pd(sumAvxR, pSumR);
             _mm256_storeu_pd(sumAvxG, pSumG);
             _mm256_storeu_pd(sumAvxB, pSumB);
             sumR += (sumAvxR[0] + sumAvxR[1] + sumAvxR[2] + sumAvxR[3]);
             sumG += (sumAvxG[0] + sumAvxG[1] + sumAvxG[2] + sumAvxG[3]);
             sumB += (sumAvxB[0] + sumAvxB[1] + sumAvxB[2] + sumAvxB[3]);
+#elif defined(__loongarch_asx)
+            __lasx_xvst((__m256i)pSumR, sumAvxR, 0);
+            __lasx_xvst((__m256i)pSumG, sumAvxG, 0);
+            __lasx_xvst((__m256i)pSumB, sumAvxB, 0);
+            sumR += (sumAvxR[0] + sumAvxR[1] + sumAvxR[2] + sumAvxR[3]);
+            sumG += (sumAvxG[0] + sumAvxG[1] + sumAvxG[2] + sumAvxG[3]);
+            sumB += (sumAvxB[0] + sumAvxB[1] + sumAvxB[2] + sumAvxB[3]);
 #endif
             sum = sumR + sumG + sumB;
             mean = static_cast<Rpp32f>(sum / (totalPixelsPerChannel * 3));
@@ -679,7 +755,7 @@ RppStatus tensor_mean_i8_f32_host(Rpp8s *srcPtr,
 
             Rpp8s *srcPtrRow;
             srcPtrRow = srcPtrChannel;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             __m256i pSum = avx_px0;
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
@@ -688,7 +764,7 @@ RppStatus tensor_mean_i8_f32_host(Rpp8s *srcPtr,
                 srcPtrTemp = srcPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256i p1[2];
@@ -701,9 +777,12 @@ RppStatus tensor_mean_i8_f32_host(Rpp8s *srcPtr,
                     sum += static_cast<Rpp32s>(*srcPtrTemp++);
                 srcPtrRow += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__)
             _mm256_store_si256((__m256i *)sumAvx, pSum);
             sum += (sumAvx[0] + sumAvx[1] + sumAvx[2] + sumAvx[3] + sumAvx[4] + sumAvx[5] + sumAvx[6] + sumAvx[7]);
+#elif defined(__loongarch_asx)
+            __lasx_xvst(pSum, (__m256i *)sumAvx, 0);
+            sum += (sumAvx[0] + sumAvx[1] + sumAvx[2] + sumAvx[3] + sumAvx[4] + sumAvx[5] + sumAvx[6] + sumAvx[7]);
 #endif
             mean = static_cast<Rpp32f>(sum)  / totalPixelsPerChannel;
             tensorMeanArr[batchCount] = mean;
@@ -723,7 +802,7 @@ RppStatus tensor_mean_i8_f32_host(Rpp8s *srcPtr,
             srcPtrRowR = srcPtrChannel;
             srcPtrRowG = srcPtrRowR + srcDescPtr->strides.cStride;
             srcPtrRowB = srcPtrRowG + srcDescPtr->strides.cStride;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             __m256i pSumR = avx_px0;
             __m256i pSumG = avx_px0;
             __m256i pSumB = avx_px0;
@@ -736,7 +815,7 @@ RppStatus tensor_mean_i8_f32_host(Rpp8s *srcPtr,
                 srcPtrTempB = srcPtrRowB;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256i p[6];
@@ -757,13 +836,20 @@ RppStatus tensor_mean_i8_f32_host(Rpp8s *srcPtr,
                 srcPtrRowG += srcDescPtr->strides.hStride;
                 srcPtrRowB += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__)
             _mm256_store_si256((__m256i *)sumAvxR, pSumR);
             _mm256_store_si256((__m256i *)sumAvxG, pSumG);
             _mm256_store_si256((__m256i *)sumAvxB, pSumB);
             sumR += (sumAvxR[0] + sumAvxR[1] + sumAvxR[2] + sumAvxR[3] + sumAvxR[4] + sumAvxR[5] + sumAvxR[6] + sumAvxR[7]);
             sumG += (sumAvxG[0] + sumAvxG[1] + sumAvxG[2] + sumAvxG[3] + sumAvxG[4] + sumAvxG[5] + sumAvxG[6] + sumAvxG[7]);
             sumB += (sumAvxB[0] + sumAvxB[1] + sumAvxB[2] + sumAvxB[3] + sumAvxB[4] + sumAvxB[5] + sumAvxB[6] + sumAvxB[7]);
+#elif defined(__loongarch_asx)
+            __lasx_xvst(pSumR, (__m256i *)sumAvxR, 0);
+            __lasx_xvst(pSumG, (__m256i *)sumAvxG, 0);
+            __lasx_xvst(pSumB, (__m256i *)sumAvxB, 0);
+            sumR += (sumAvxR[0] + sumAvxR[1] + sumAvxR[2] + sumAvxR[3] + sumAvxR[4] + sumAvxR[5] + sumAvxR[6] + sumAvxR[7]);
+            sumG += (sumAvxG[0] + sumAvxG[1] + sumAvxG[2] + sumAvxG[3] + sumAvxG[4] + sumAvxG[5] + sumAvxG[6] + sumAvxG[7]);
+            sumB += (sumAvxB[0] + sumAvxB[1] + sumAvxB[2] + sumAvxB[3] + sumAvxB[4] + sumAvxB[5] + sumAvxB[6] + sumAvxB[7]);
 #endif
 
             sum = static_cast<Rpp64u>(sum) + static_cast<Rpp64u>(sumG) + static_cast<Rpp64u>(sumB);
@@ -789,7 +875,7 @@ RppStatus tensor_mean_i8_f32_host(Rpp8s *srcPtr,
 
             Rpp8s *srcPtrRow;
             srcPtrRow = srcPtrChannel;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             __m256i pSumR = avx_px0;
             __m256i pSumG = avx_px0;
             __m256i pSumB = avx_px0;
@@ -800,7 +886,7 @@ RppStatus tensor_mean_i8_f32_host(Rpp8s *srcPtr,
                 srcPtrTemp = srcPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256i p[6];
@@ -818,13 +904,20 @@ RppStatus tensor_mean_i8_f32_host(Rpp8s *srcPtr,
                 }
                 srcPtrRow += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__)
             _mm256_store_si256((__m256i *)sumAvxR, pSumR);
             _mm256_store_si256((__m256i *)sumAvxG, pSumG);
             _mm256_store_si256((__m256i *)sumAvxB, pSumB);
             sumR += (sumAvxR[0] + sumAvxR[1] + sumAvxR[2] + sumAvxR[3] + sumAvxR[4] + sumAvxR[5] + sumAvxR[6] + sumAvxR[7]);
             sumG += (sumAvxG[0] + sumAvxG[1] + sumAvxG[2] + sumAvxG[3] + sumAvxG[4] + sumAvxG[5] + sumAvxG[6] + sumAvxG[7]);
             sumB += (sumAvxB[0] + sumAvxB[1] + sumAvxB[2] + sumAvxB[3] + sumAvxB[4] + sumAvxB[5] + sumAvxB[6] + sumAvxB[7]);
+#elif defined(__loongarch_asx)
+            __lasx_xvst(pSumR, (__m256i *)sumAvxR, 0);
+            __lasx_xvst(pSumG, (__m256i *)sumAvxG, 0);
+            __lasx_xvst(pSumB, (__m256i *)sumAvxB, 0);
+            sumR += (sumAvxR[0] + sumAvxR[1] + sumAvxR[2] + sumAvxR[3] + sumAvxR[4] + sumAvxR[5] + sumAvxR[6] + sumAvxR[7]);
+            sumG += (sumAvxG[0] + sumAvxG[1] + sumAvxG[2] + sumAvxG[3] + sumAvxG[4] + sumAvxG[5] + sumAvxG[6] + sumAvxG[7]);
+            sumB += (sumAvxB[0] + sumAvxB[1] + sumAvxB[2] + sumAvxB[3] + sumAvxB[4] + sumAvxB[5] + sumAvxB[6] + sumAvxB[7]);
 #endif
             sum = static_cast<Rpp64u>(sumR) + static_cast<Rpp64u>(sumG) + static_cast<Rpp64u>(sumB);
             mean = (static_cast<Rpp64f>(sum) / (totalPixelsPerChannel * 3));
diff --git a/src/modules/cpu/kernel/tensor_min.hpp b/src/modules/cpu/kernel/tensor_min.hpp
index 15b9b77b..a9f938b9 100644
--- a/src/modules/cpu/kernel/tensor_min.hpp
+++ b/src/modules/cpu/kernel/tensor_min.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 RppStatus tensor_min_u8_u8_host(Rpp8u *srcPtr,
                                 RpptDescPtr srcDescPtr,
@@ -66,8 +71,10 @@ RppStatus tensor_min_u8_u8_host(Rpp8u *srcPtr,
 
             Rpp8u *srcPtrRow;
             srcPtrRow = srcPtrChannel;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256i pMin = _mm256_set1_epi8((char)255);
+#elif defined(__loongarch_asx)
+            __m256i pMin = __lasx_xvreplgr2vr_b((char)255);
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -75,12 +82,20 @@ RppStatus tensor_min_u8_u8_host(Rpp8u *srcPtr,
                 srcPtrTemp = srcPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256i p1 = _mm256_loadu_si256((__m256i *)srcPtrTemp);
                     pMin = _mm256_min_epu8(p1, pMin);
 
+                    srcPtrTemp += vectorIncrement;
+                }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
+                {
+                    __m256i p1 = __lasx_xvld((__m256i *)srcPtrTemp, 0);
+                    pMin = __lasx_xvmin_bu(p1, pMin);
+
                     srcPtrTemp += vectorIncrement;
                 }
 #endif
@@ -90,7 +105,7 @@ RppStatus tensor_min_u8_u8_host(Rpp8u *srcPtr,
                 }
                 srcPtrRow += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             __m128i result;
             reduce_min_32_host(&pMin, &result);
             rpp_simd_store(rpp_store16_u8_to_u8, resultAvx, &result);
@@ -111,10 +126,14 @@ RppStatus tensor_min_u8_u8_host(Rpp8u *srcPtr,
             srcPtrRowR = srcPtrChannel;
             srcPtrRowG = srcPtrRowR + srcDescPtr->strides.cStride;
             srcPtrRowB = srcPtrRowG + srcDescPtr->strides.cStride;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256i pMinR = _mm256_set1_epi8((char)255);
             __m256i pMinG = pMinR;
             __m256i pMinB = pMinR;
+#elif defined(__loongarch_asx)
+            __m256i pMinR = __lasx_xvreplgr2vr_b((char)255);
+            __m256i pMinG = pMinR;
+            __m256i pMinB = pMinR;
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -124,7 +143,7 @@ RppStatus tensor_min_u8_u8_host(Rpp8u *srcPtr,
                 srcPtrTempB = srcPtrRowB;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256i p[3];
@@ -146,7 +165,7 @@ RppStatus tensor_min_u8_u8_host(Rpp8u *srcPtr,
                 srcPtrRowG += srcDescPtr->strides.hStride;
                 srcPtrRowB += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             __m128i result;
             reduce_min_96_host(&pMinR, &pMinG, &pMinB, &result);
             rpp_simd_store(rpp_store16_u8_to_u8, resultAvx, &result);
@@ -176,9 +195,15 @@ RppStatus tensor_min_u8_u8_host(Rpp8u *srcPtr,
                 Rpp8u *srcPtrRow;
                 srcPtrRow = srcPtrChannel;
 
+#if defined(__loongarch_sx)
+                __m128i pMinR = __lsx_vreplgr2vr_b((char)255);
+                __m128i pMinG = pMinR;
+                __m128i pMinB = pMinR;
+#else
                 __m128i pMinR = _mm_set1_epi8((char)255);
                 __m128i pMinG = pMinR;
                 __m128i pMinB = pMinR;
+#endif
 
                 for(int i = 0; i < roi.xywhROI.roiHeight; i++)
                 {
@@ -263,8 +288,10 @@ RppStatus tensor_min_f32_f32_host(Rpp32f *srcPtr,
 
             Rpp32f *srcPtrRow;
             srcPtrRow = srcPtrChannel;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256 pMin = _mm256_set1_ps(255.0);
+#elif defined(__loongarch_asx)
+            __m256 pMin = lasx_set1_f32(255.0);
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -272,7 +299,7 @@ RppStatus tensor_min_f32_f32_host(Rpp32f *srcPtr,
                 srcPtrTemp = srcPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256 p1;
@@ -289,7 +316,7 @@ RppStatus tensor_min_f32_f32_host(Rpp32f *srcPtr,
                 srcPtrRow += srcDescPtr->strides.hStride;
             }
 
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             __m128 result;
             reduce_min_float8_host(&pMin, &result);
             rpp_simd_store(rpp_store4_f32_to_f32, resultAvx, &result);
@@ -309,10 +336,14 @@ RppStatus tensor_min_f32_f32_host(Rpp32f *srcPtr,
             srcPtrRowR = srcPtrChannel;
             srcPtrRowG = srcPtrRowR + srcDescPtr->strides.cStride;
             srcPtrRowB = srcPtrRowG + srcDescPtr->strides.cStride;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256 pMinR = _mm256_set1_ps(255.0);
             __m256 pMinG = pMinR;
             __m256 pMinB = pMinR;
+#elif defined(__loongarch_asx)
+            __m256 pMinR = lasx_set1_f32(255.0);
+            __m256 pMinG = pMinR;
+            __m256 pMinB = pMinR;
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -322,7 +353,7 @@ RppStatus tensor_min_f32_f32_host(Rpp32f *srcPtr,
                 srcPtrTempB = srcPtrRowB;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 p[3];
@@ -344,7 +375,7 @@ RppStatus tensor_min_f32_f32_host(Rpp32f *srcPtr,
                 srcPtrRowG += srcDescPtr->strides.hStride;
                 srcPtrRowB += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             __m256 result;
             reduce_min_float24_host(&pMinR, &pMinG, &pMinB, &result);
             rpp_simd_store(rpp_store8_f32_to_f32_avx, resultAvx, &result);
@@ -374,10 +405,14 @@ RppStatus tensor_min_f32_f32_host(Rpp32f *srcPtr,
                 Rpp32f *srcPtrRow;
                 srcPtrRow = srcPtrChannel;
 
-#if __AVX2__
+#if defined(__AVX2__)
                 __m256 pMinR = _mm256_set1_ps(255.0);
                 __m256 pMinG = pMinR;
                 __m256 pMinB = pMinR;
+#elif defined(__loongarch_asx)
+                __m256 pMinR = lasx_set1_f32(255.0);
+                __m256 pMinG = pMinR;
+                __m256 pMinB = pMinR;
 #endif
                 for(int i = 0; i < roi.xywhROI.roiHeight; i++)
                 {
@@ -385,7 +420,7 @@ RppStatus tensor_min_f32_f32_host(Rpp32f *srcPtr,
                     srcPtrTemp = srcPtrRow;
 
                     int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                     {
                         __m256 p[3];
@@ -405,7 +440,7 @@ RppStatus tensor_min_f32_f32_host(Rpp32f *srcPtr,
                     srcPtrRow += srcDescPtr->strides.hStride;
                 }
 
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m256 result;
                 reduce_min_float24_host(&pMinR, &pMinG, &pMinB, &result);
                 rpp_simd_store(rpp_store8_f32_to_f32_avx, resultAvx, &result);
@@ -465,8 +500,10 @@ RppStatus tensor_min_f16_f16_host(Rpp16f *srcPtr,
 
             Rpp16f *srcPtrRow;
             srcPtrRow = srcPtrChannel;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256 pMin = _mm256_set1_ps(255.0);
+#elif defined(__loongarch_asx)
+            __m256 pMin = lasx_set1_f32(255.0);
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -474,7 +511,7 @@ RppStatus tensor_min_f16_f16_host(Rpp16f *srcPtr,
                 srcPtrTemp = srcPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     Rpp32f srcPtrTemp_ps[8];
@@ -496,7 +533,7 @@ RppStatus tensor_min_f16_f16_host(Rpp16f *srcPtr,
                 srcPtrRow += srcDescPtr->strides.hStride;
             }
 
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             __m128 result;
             reduce_min_float8_host(&pMin, &result);
             rpp_simd_store(rpp_store4_f32_to_f32, resultAvx, &result);
@@ -516,10 +553,14 @@ RppStatus tensor_min_f16_f16_host(Rpp16f *srcPtr,
             srcPtrRowR = srcPtrChannel;
             srcPtrRowG = srcPtrRowR + srcDescPtr->strides.cStride;
             srcPtrRowB = srcPtrRowG + srcDescPtr->strides.cStride;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256 pMinR = _mm256_set1_ps(255.0);
             __m256 pMinG = pMinR;
             __m256 pMinB = pMinR;
+#elif defined(__loongarch_asx)
+            __m256 pMinR = lasx_set1_f32(255.0);
+            __m256 pMinG = pMinR;
+            __m256 pMinB = pMinR;
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -529,7 +570,7 @@ RppStatus tensor_min_f16_f16_host(Rpp16f *srcPtr,
                 srcPtrTempB = srcPtrRowB;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     Rpp32f srcPtrTempR_ps[8], srcPtrTempG_ps[8], srcPtrTempB_ps[8];
@@ -558,7 +599,7 @@ RppStatus tensor_min_f16_f16_host(Rpp16f *srcPtr,
                 srcPtrRowG += srcDescPtr->strides.hStride;
                 srcPtrRowB += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             __m256 result;
             reduce_min_float24_host(&pMinR, &pMinG, &pMinB, &result);
             rpp_simd_store(rpp_store8_f32_to_f32_avx, resultAvx, &result);
@@ -588,10 +629,14 @@ RppStatus tensor_min_f16_f16_host(Rpp16f *srcPtr,
                 Rpp16f *srcPtrRow;
                 srcPtrRow = srcPtrChannel;
 
-#if __AVX2__
+#if defined(__AVX2__)
                 __m256 pMinR = _mm256_set1_ps(255.0);
                 __m256 pMinG = pMinR;
                 __m256 pMinB = pMinR;
+#elif defined(__loongarch_asx)
+                __m256 pMinR = lasx_set1_f32(255.0);
+                __m256 pMinG = pMinR;
+                __m256 pMinB = pMinR;
 #endif
                 for(int i = 0; i < roi.xywhROI.roiHeight; i++)
                 {
@@ -599,7 +644,7 @@ RppStatus tensor_min_f16_f16_host(Rpp16f *srcPtr,
                     srcPtrTemp = srcPtrRow;
 
                     int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                     {
                         Rpp32f srcPtrTemp_ps[24];
@@ -624,7 +669,7 @@ RppStatus tensor_min_f16_f16_host(Rpp16f *srcPtr,
                     srcPtrRow += srcDescPtr->strides.hStride;
                 }
 
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m256 result;
                 reduce_min_float24_host(&pMinR, &pMinG, &pMinB, &result);
                 rpp_simd_store(rpp_store8_f32_to_f32_avx, resultAvx, &result);
@@ -684,8 +729,10 @@ RppStatus tensor_min_i8_i8_host(Rpp8s *srcPtr,
 
             Rpp8s *srcPtrRow;
             srcPtrRow = srcPtrChannel;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256i pMin = _mm256_set1_epi8((char)127);
+#elif defined(__loongarch_asx)
+            __m256i pMin = __lasx_xvreplgr2vr_b((char)127);
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -693,12 +740,20 @@ RppStatus tensor_min_i8_i8_host(Rpp8s *srcPtr,
                 srcPtrTemp = srcPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256i p1 = _mm256_load_si256((__m256i *)srcPtrTemp);
                     pMin = _mm256_min_epi8(p1, pMin); //compare and store min of 32 values into global min
 
+                    srcPtrTemp += vectorIncrement;
+                }
+#elif defined(__loongarch_asx)
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
+                {
+                    __m256i p1 = __lasx_xvld((__m256i *)srcPtrTemp, 0);
+                    pMin = __lasx_xvmin_b(p1, pMin); //compare and store min of 32 values into global min
+
                     srcPtrTemp += vectorIncrement;
                 }
 #endif
@@ -709,7 +764,7 @@ RppStatus tensor_min_i8_i8_host(Rpp8s *srcPtr,
                 srcPtrRow += srcDescPtr->strides.hStride;
             }
 
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             __m128i result;
             reduce_min_i32_host(&pMin, &result);
             rpp_simd_store(rpp_store16_i8, resultAvx, &result);
@@ -730,10 +785,14 @@ RppStatus tensor_min_i8_i8_host(Rpp8s *srcPtr,
             srcPtrRowR = srcPtrChannel;
             srcPtrRowG = srcPtrRowR + srcDescPtr->strides.cStride;
             srcPtrRowB = srcPtrRowG + srcDescPtr->strides.cStride;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256i pMinR = _mm256_set1_epi8((char)127);
             __m256i pMinG = pMinR;
             __m256i pMinB = pMinR;
+#elif defined(__loongarch_asx)
+            __m256i pMinR = __lasx_xvreplgr2vr_b((char)127);
+            __m256i pMinG = pMinR;
+            __m256i pMinB = pMinR;
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -743,7 +802,7 @@ RppStatus tensor_min_i8_i8_host(Rpp8s *srcPtr,
                 srcPtrTempB = srcPtrRowB;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256i p[3];
@@ -765,7 +824,7 @@ RppStatus tensor_min_i8_i8_host(Rpp8s *srcPtr,
                 srcPtrRowG += srcDescPtr->strides.hStride;
                 srcPtrRowB += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             __m128i result;
             reduce_min_i96_host(&pMinR, &pMinG, &pMinB, &result);
             rpp_simd_store(rpp_store16_i8, resultAvx, &result);
@@ -795,9 +854,15 @@ RppStatus tensor_min_i8_i8_host(Rpp8s *srcPtr,
                 Rpp8s *srcPtrRow;
                 srcPtrRow = srcPtrChannel;
 
+#if defined(__loongarch_sx)
+                __m128i pMinR = __lsx_vreplgr2vr_b((char)127);
+                __m128i pMinG = pMinR;
+                __m128i pMinB = pMinR;
+#else
                 __m128i pMinR = _mm_set1_epi8((char)127);
                 __m128i pMinG = pMinR;
                 __m128i pMinB = pMinR;
+#endif
 
                 for(int i = 0; i < roi.xywhROI.roiHeight; i++)
                 {
@@ -805,7 +870,7 @@ RppStatus tensor_min_i8_i8_host(Rpp8s *srcPtr,
                     srcPtrTemp = srcPtrRow;
 
                     int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                     for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                     {
                         __m128i p[3];
@@ -824,7 +889,7 @@ RppStatus tensor_min_i8_i8_host(Rpp8s *srcPtr,
                     }
                     srcPtrRow += srcDescPtr->strides.hStride;
                 }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m128i result;
                 reduce_min_i48_host(&pMinR, &pMinG, &pMinB, &result);
                 rpp_simd_store(rpp_store16_i8, resultAvx, &result);
diff --git a/src/modules/cpu/kernel/tensor_stddev.hpp b/src/modules/cpu/kernel/tensor_stddev.hpp
index 2f64e93a..d7774e55 100644
--- a/src/modules/cpu/kernel/tensor_stddev.hpp
+++ b/src/modules/cpu/kernel/tensor_stddev.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 #include "reduction.hpp"
 
 RppStatus tensor_stddev_u8_f32_host(Rpp8u *srcPtr,
@@ -68,9 +73,12 @@ RppStatus tensor_stddev_u8_f32_host(Rpp8u *srcPtr,
 
             Rpp8u *srcPtrRow;
             srcPtrRow = srcPtrChannel;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256d pMean = _mm256_set1_pd(mean);
             __m256d pVar = _mm256_setzero_pd();
+#elif defined(__loongarch_asx)
+            __m256d pMean = lasx_set1_f64(mean);
+            __m256d pVar = (__m256d)__lasx_xvldi(0);
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -78,7 +86,7 @@ RppStatus tensor_stddev_u8_f32_host(Rpp8u *srcPtr,
                 srcPtrTemp = srcPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256d p1[2];
@@ -95,9 +103,12 @@ RppStatus tensor_stddev_u8_f32_host(Rpp8u *srcPtr,
                 }
                 srcPtrRow += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__)
             _mm256_storeu_pd(varAvx, pVar);
             var += (varAvx[0] + varAvx[1] + varAvx[2] + varAvx[3]);
+#elif defined(__loongarch_asx)
+            __lasx_xvst((__m256i)pVar, varAvx, 0);
+            var += (varAvx[0] + varAvx[1] + varAvx[2] + varAvx[3]);
 #endif
             stddev = sqrt(var / totalPixelsPerChannel);
             tensorStddevArr[batchCount] = static_cast<Rpp32f>(stddev);
@@ -125,7 +136,7 @@ RppStatus tensor_stddev_u8_f32_host(Rpp8u *srcPtr,
             srcPtrRowR = srcPtrChannel;
             srcPtrRowG = srcPtrRowR + srcDescPtr->strides.cStride;
             srcPtrRowB = srcPtrRowG + srcDescPtr->strides.cStride;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256d pMeanR     = _mm256_set1_pd(meanR);
             __m256d pMeanG     = _mm256_set1_pd(meanG);
             __m256d pMeanB     = _mm256_set1_pd(meanB);
@@ -133,6 +144,14 @@ RppStatus tensor_stddev_u8_f32_host(Rpp8u *srcPtr,
             __m256d pVarR, pVarG, pVarB;
             __m256d pVarImageR, pVarImageG, pVarImageB;
             pVarR = pVarG = pVarB = pVarImageR = pVarImageG = pVarImageB = _mm256_setzero_pd();
+#elif defined(__loongarch_asx)
+            __m256d pMeanR     = lasx_set1_f64(meanR);
+            __m256d pMeanG     = lasx_set1_f64(meanG);
+            __m256d pMeanB     = lasx_set1_f64(meanB);
+            __m256d pMeanImage = lasx_set1_f64(meanImage);
+            __m256d pVarR, pVarG, pVarB;
+            __m256d pVarImageR, pVarImageG, pVarImageB;
+            pVarR = pVarG = pVarB = pVarImageR = pVarImageG = pVarImageB = (__m256d)__lasx_xvldi(0);
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -142,7 +161,7 @@ RppStatus tensor_stddev_u8_f32_host(Rpp8u *srcPtr,
                 srcPtrTempB = srcPtrRowB;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256d p[6];
@@ -173,7 +192,7 @@ RppStatus tensor_stddev_u8_f32_host(Rpp8u *srcPtr,
                 srcPtrRowG += srcDescPtr->strides.hStride;
                 srcPtrRowB += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__)
             _mm256_storeu_pd(varAvxR, pVarR);
             _mm256_storeu_pd(varAvxG, pVarG);
             _mm256_storeu_pd(varAvxB, pVarB);
@@ -181,6 +200,20 @@ RppStatus tensor_stddev_u8_f32_host(Rpp8u *srcPtr,
             _mm256_storeu_pd(varAvxImageG, pVarImageG);
             _mm256_storeu_pd(varAvxImageB, pVarImageB);
 
+            varR += (varAvxR[0] + varAvxR[1] + varAvxR[2] + varAvxR[3]);
+            varG += (varAvxG[0] + varAvxG[1] + varAvxG[2] + varAvxG[3]);
+            varB += (varAvxB[0] + varAvxB[1] + varAvxB[2] + varAvxB[3]);
+            varImageR += (varAvxImageR[0] + varAvxImageR[1] + varAvxImageR[2] + varAvxImageR[3]);
+            varImageG += (varAvxImageG[0] + varAvxImageG[1] + varAvxImageG[2] + varAvxImageG[3]);
+            varImageB += (varAvxImageB[0] + varAvxImageB[1] + varAvxImageB[2] + varAvxImageB[3]);
+#elif defined(__loongarch_asx)
+            __lasx_xvst((__m256i)pVarR, varAvxR, 0);
+            __lasx_xvst((__m256i)pVarG, varAvxG, 0);
+            __lasx_xvst((__m256i)pVarB, varAvxB, 0);
+            __lasx_xvst((__m256i)pVarImageR, varAvxImageR, 0);
+            __lasx_xvst((__m256i)pVarImageG, varAvxImageG, 0);
+            __lasx_xvst((__m256i)pVarImageB, varAvxImageB, 0);
+
             varR += (varAvxR[0] + varAvxR[1] + varAvxR[2] + varAvxR[3]);
             varG += (varAvxG[0] + varAvxG[1] + varAvxG[2] + varAvxG[3]);
             varB += (varAvxB[0] + varAvxB[1] + varAvxB[2] + varAvxB[3]);
@@ -219,7 +252,7 @@ RppStatus tensor_stddev_u8_f32_host(Rpp8u *srcPtr,
 
             Rpp8u *srcPtrRow;
             srcPtrRow = srcPtrChannel;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256d pMeanR     = _mm256_set1_pd(meanR);
             __m256d pMeanG     = _mm256_set1_pd(meanG);
             __m256d pMeanB     = _mm256_set1_pd(meanB);
@@ -227,6 +260,14 @@ RppStatus tensor_stddev_u8_f32_host(Rpp8u *srcPtr,
             __m256d pVarR, pVarG, pVarB;
             __m256d pVarImageR, pVarImageG, pVarImageB;
             pVarR = pVarG = pVarB = pVarImageR = pVarImageG = pVarImageB = _mm256_setzero_pd();
+#elif defined(__loongarch_asx)
+            __m256d pMeanR     = lasx_set1_f64(meanR);
+            __m256d pMeanG     = lasx_set1_f64(meanG);
+            __m256d pMeanB     = lasx_set1_f64(meanB);
+            __m256d pMeanImage = lasx_set1_f64(meanImage);
+            __m256d pVarR, pVarG, pVarB;
+            __m256d pVarImageR, pVarImageG, pVarImageB;
+            pVarR = pVarG = pVarB = pVarImageR = pVarImageG = pVarImageB = (__m256d)__lasx_xvldi(0);
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -234,7 +275,7 @@ RppStatus tensor_stddev_u8_f32_host(Rpp8u *srcPtr,
                 srcPtrTemp = srcPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256d p[6];
@@ -259,7 +300,7 @@ RppStatus tensor_stddev_u8_f32_host(Rpp8u *srcPtr,
                 }
                 srcPtrRow += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__)
             _mm256_storeu_pd(varAvxR, pVarR);
             _mm256_storeu_pd(varAvxG, pVarG);
             _mm256_storeu_pd(varAvxB, pVarB);
@@ -267,6 +308,20 @@ RppStatus tensor_stddev_u8_f32_host(Rpp8u *srcPtr,
             _mm256_storeu_pd(varAvxImageG, pVarImageG);
             _mm256_storeu_pd(varAvxImageB, pVarImageB);
 
+            varR += (varAvxR[0] + varAvxR[1] + varAvxR[2] + varAvxR[3]);
+            varG += (varAvxG[0] + varAvxG[1] + varAvxG[2] + varAvxG[3]);
+            varB += (varAvxB[0] + varAvxB[1] + varAvxB[2] + varAvxB[3]);
+            varImageR += (varAvxImageR[0] + varAvxImageR[1] + varAvxImageR[2] + varAvxImageR[3]);
+            varImageG += (varAvxImageG[0] + varAvxImageG[1] + varAvxImageG[2] + varAvxImageG[3]);
+            varImageB += (varAvxImageB[0] + varAvxImageB[1] + varAvxImageB[2] + varAvxImageB[3]);
+#elif defined(__loongarch_asx)
+            __lasx_xvst((__m256i)pVarR, varAvxR, 0);
+            __lasx_xvst((__m256i)pVarG, varAvxG, 0);
+            __lasx_xvst((__m256i)pVarB, varAvxB, 0);
+            __lasx_xvst((__m256i)pVarImageR, varAvxImageR, 0);
+            __lasx_xvst((__m256i)pVarImageG, varAvxImageG, 0);
+            __lasx_xvst((__m256i)pVarImageB, varAvxImageB, 0);
+
             varR += (varAvxR[0] + varAvxR[1] + varAvxR[2] + varAvxR[3]);
             varG += (varAvxG[0] + varAvxG[1] + varAvxG[2] + varAvxG[3]);
             varB += (varAvxB[0] + varAvxB[1] + varAvxB[2] + varAvxB[3]);
@@ -330,9 +385,12 @@ RppStatus tensor_stddev_f32_f32_host(Rpp32f *srcPtr,
 
             Rpp32f *srcPtrRow;
             srcPtrRow = srcPtrChannel;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256d pMean = _mm256_set1_pd(mean);
             __m256d pVar = _mm256_setzero_pd();
+#elif defined(__loongarch_asx)
+            __m256d pMean = lasx_set1_f64(mean);
+            __m256d pVar = (__m256d)__lasx_xvldi(0);
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -340,7 +398,7 @@ RppStatus tensor_stddev_f32_f32_host(Rpp32f *srcPtr,
                 srcPtrTemp = srcPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256d p1[2];
@@ -356,9 +414,12 @@ RppStatus tensor_stddev_f32_f32_host(Rpp32f *srcPtr,
                 }
                 srcPtrRow += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__)
             _mm256_storeu_pd(varAvx, pVar);
             var += (varAvx[0] + varAvx[1] + varAvx[2] + varAvx[3]);
+#elif defined(__loongarch_asx)
+            __lasx_xvst((__m256i)pVar, varAvx, 0);
+            var += (varAvx[0] + varAvx[1] + varAvx[2] + varAvx[3]);
 #endif
             stddev = sqrt(var / totalPixelsPerChannel) * 255;
             tensorStddevArr[batchCount] = static_cast<Rpp32f>(stddev);
@@ -386,7 +447,7 @@ RppStatus tensor_stddev_f32_f32_host(Rpp32f *srcPtr,
             srcPtrRowR = srcPtrChannel;
             srcPtrRowG = srcPtrRowR + srcDescPtr->strides.cStride;
             srcPtrRowB = srcPtrRowG + srcDescPtr->strides.cStride;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256d pMeanR     = _mm256_set1_pd(meanR);
             __m256d pMeanG     = _mm256_set1_pd(meanG);
             __m256d pMeanB     = _mm256_set1_pd(meanB);
@@ -394,6 +455,14 @@ RppStatus tensor_stddev_f32_f32_host(Rpp32f *srcPtr,
             __m256d pVarR, pVarG, pVarB;
             __m256d pVarImageR, pVarImageG, pVarImageB;
             pVarR = pVarG = pVarB = pVarImageR = pVarImageG = pVarImageB = _mm256_setzero_pd();
+#elif defined(__loongarch_asx)
+            __m256d pMeanR     = lasx_set1_f64(meanR);
+            __m256d pMeanG     = lasx_set1_f64(meanG);
+            __m256d pMeanB     = lasx_set1_f64(meanB);
+            __m256d pMeanImage = lasx_set1_f64(meanImage);
+            __m256d pVarR, pVarG, pVarB;
+            __m256d pVarImageR, pVarImageG, pVarImageB;
+            pVarR = pVarG = pVarB = pVarImageR = pVarImageG = pVarImageB = (__m256d)__lasx_xvldi(0);
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -403,7 +472,7 @@ RppStatus tensor_stddev_f32_f32_host(Rpp32f *srcPtr,
                 srcPtrTempB = srcPtrRowB;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256d p[6];
@@ -434,7 +503,7 @@ RppStatus tensor_stddev_f32_f32_host(Rpp32f *srcPtr,
                 srcPtrRowG += srcDescPtr->strides.hStride;
                 srcPtrRowB += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__)
             _mm256_storeu_pd(varAvxR, pVarR);
             _mm256_storeu_pd(varAvxG, pVarG);
             _mm256_storeu_pd(varAvxB, pVarB);
@@ -442,6 +511,20 @@ RppStatus tensor_stddev_f32_f32_host(Rpp32f *srcPtr,
             _mm256_storeu_pd(varAvxImageG, pVarImageG);
             _mm256_storeu_pd(varAvxImageB, pVarImageB);
 
+            varR += (varAvxR[0] + varAvxR[1] + varAvxR[2] + varAvxR[3]);
+            varG += (varAvxG[0] + varAvxG[1] + varAvxG[2] + varAvxG[3]);
+            varB += (varAvxB[0] + varAvxB[1] + varAvxB[2] + varAvxB[3]);
+            varImageR += (varAvxImageR[0] + varAvxImageR[1] + varAvxImageR[2] + varAvxImageR[3]);
+            varImageG += (varAvxImageG[0] + varAvxImageG[1] + varAvxImageG[2] + varAvxImageG[3]);
+            varImageB += (varAvxImageB[0] + varAvxImageB[1] + varAvxImageB[2] + varAvxImageB[3]);
+#elif defined(__loongarch_asx)
+            __lasx_xvst((__m256i)pVarR, varAvxR, 0);
+            __lasx_xvst((__m256i)pVarG, varAvxG, 0);
+            __lasx_xvst((__m256i)pVarB, varAvxB, 0);
+            __lasx_xvst((__m256i)pVarImageR, varAvxImageR, 0);
+            __lasx_xvst((__m256i)pVarImageG, varAvxImageG, 0);
+            __lasx_xvst((__m256i)pVarImageB, varAvxImageB, 0);
+
             varR += (varAvxR[0] + varAvxR[1] + varAvxR[2] + varAvxR[3]);
             varG += (varAvxG[0] + varAvxG[1] + varAvxG[2] + varAvxG[3]);
             varB += (varAvxB[0] + varAvxB[1] + varAvxB[2] + varAvxB[3]);
@@ -480,7 +563,7 @@ RppStatus tensor_stddev_f32_f32_host(Rpp32f *srcPtr,
 
             Rpp32f *srcPtrRow;
             srcPtrRow = srcPtrChannel;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256d pMeanR     = _mm256_set1_pd(meanR);
             __m256d pMeanG     = _mm256_set1_pd(meanG);
             __m256d pMeanB     = _mm256_set1_pd(meanB);
@@ -488,6 +571,14 @@ RppStatus tensor_stddev_f32_f32_host(Rpp32f *srcPtr,
             __m256d pVarR, pVarG, pVarB;
             __m256d pVarImageR, pVarImageG, pVarImageB;
             pVarR = pVarG = pVarB = pVarImageR = pVarImageG = pVarImageB = _mm256_setzero_pd();
+#elif defined(__loongarch_asx)
+            __m256d pMeanR     = lasx_set1_f64(meanR);
+            __m256d pMeanG     = lasx_set1_f64(meanG);
+            __m256d pMeanB     = lasx_set1_f64(meanB);
+            __m256d pMeanImage = lasx_set1_f64(meanImage);
+            __m256d pVarR, pVarG, pVarB;
+            __m256d pVarImageR, pVarImageG, pVarImageB;
+            pVarR = pVarG = pVarB = pVarImageR = pVarImageG = pVarImageB = (__m256d)__lasx_xvldi(0);
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -495,7 +586,7 @@ RppStatus tensor_stddev_f32_f32_host(Rpp32f *srcPtr,
                 srcPtrTemp = srcPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256d p[6];
@@ -520,7 +611,7 @@ RppStatus tensor_stddev_f32_f32_host(Rpp32f *srcPtr,
                 }
                 srcPtrRow += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__)
             _mm256_storeu_pd(varAvxR, pVarR);
             _mm256_storeu_pd(varAvxG, pVarG);
             _mm256_storeu_pd(varAvxB, pVarB);
@@ -528,6 +619,20 @@ RppStatus tensor_stddev_f32_f32_host(Rpp32f *srcPtr,
             _mm256_storeu_pd(varAvxImageG, pVarImageG);
             _mm256_storeu_pd(varAvxImageB, pVarImageB);
 
+            varR += (varAvxR[0] + varAvxR[1] + varAvxR[2] + varAvxR[3]);
+            varG += (varAvxG[0] + varAvxG[1] + varAvxG[2] + varAvxG[3]);
+            varB += (varAvxB[0] + varAvxB[1] + varAvxB[2] + varAvxB[3]);
+            varImageR += (varAvxImageR[0] + varAvxImageR[1] + varAvxImageR[2] + varAvxImageR[3]);
+            varImageG += (varAvxImageG[0] + varAvxImageG[1] + varAvxImageG[2] + varAvxImageG[3]);
+            varImageB += (varAvxImageB[0] + varAvxImageB[1] + varAvxImageB[2] + varAvxImageB[3]);
+#elif defined(__loongarch_asx)
+            __lasx_xvst((__m256i)pVarR, varAvxR, 0);
+            __lasx_xvst((__m256i)pVarG, varAvxG, 0);
+            __lasx_xvst((__m256i)pVarB, varAvxB, 0);
+            __lasx_xvst((__m256i)pVarImageR, varAvxImageR, 0);
+            __lasx_xvst((__m256i)pVarImageG, varAvxImageG, 0);
+            __lasx_xvst((__m256i)pVarImageB, varAvxImageB, 0);
+
             varR += (varAvxR[0] + varAvxR[1] + varAvxR[2] + varAvxR[3]);
             varG += (varAvxG[0] + varAvxG[1] + varAvxG[2] + varAvxG[3]);
             varB += (varAvxB[0] + varAvxB[1] + varAvxB[2] + varAvxB[3]);
@@ -591,9 +696,12 @@ RppStatus tensor_stddev_f16_f32_host(Rpp16f *srcPtr,
 
             Rpp16f *srcPtrRow;
             srcPtrRow = srcPtrChannel;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256d pMean = _mm256_set1_pd(mean);
             __m256d pVar = _mm256_setzero_pd();
+#elif defined(__loongarch_asx)
+            __m256d pMean = lasx_set1_f64(mean);
+            __m256d pVar = (__m256d)__lasx_xvldi(0);
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -601,7 +709,7 @@ RppStatus tensor_stddev_f16_f32_host(Rpp16f *srcPtr,
                 srcPtrTemp = srcPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     Rpp32f srcPtrTemp_ps[8];
@@ -622,9 +730,12 @@ RppStatus tensor_stddev_f16_f32_host(Rpp16f *srcPtr,
                 }
                 srcPtrRow += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__)
             _mm256_storeu_pd(varAvx, pVar);
             var += (varAvx[0] + varAvx[1] + varAvx[2] + varAvx[3]);
+#elif defined(__loongarch_asx)
+            __lasx_xvst((__m256i)pVar, varAvx, 0);
+            var += (varAvx[0] + varAvx[1] + varAvx[2] + varAvx[3]);
 #endif
             stddev = sqrt(var / totalPixelsPerChannel) * 255;
             tensorStddevArr[batchCount] = static_cast<Rpp32f>(stddev);
@@ -652,7 +763,7 @@ RppStatus tensor_stddev_f16_f32_host(Rpp16f *srcPtr,
             srcPtrRowR = srcPtrChannel;
             srcPtrRowG = srcPtrRowR + srcDescPtr->strides.cStride;
             srcPtrRowB = srcPtrRowG + srcDescPtr->strides.cStride;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256d pMeanR     = _mm256_set1_pd(meanR);
             __m256d pMeanG     = _mm256_set1_pd(meanG);
             __m256d pMeanB     = _mm256_set1_pd(meanB);
@@ -660,6 +771,14 @@ RppStatus tensor_stddev_f16_f32_host(Rpp16f *srcPtr,
             __m256d pVarR, pVarG, pVarB;
             __m256d pVarImageR, pVarImageG, pVarImageB;
             pVarR = pVarG = pVarB = pVarImageR = pVarImageG = pVarImageB = _mm256_setzero_pd();
+#elif defined(__loongarch_asx)
+            __m256d pMeanR     = lasx_set1_f64(meanR);
+            __m256d pMeanG     = lasx_set1_f64(meanG);
+            __m256d pMeanB     = lasx_set1_f64(meanB);
+            __m256d pMeanImage = lasx_set1_f64(meanImage);
+            __m256d pVarR, pVarG, pVarB;
+            __m256d pVarImageR, pVarImageG, pVarImageB;
+            pVarR = pVarG = pVarB = pVarImageR = pVarImageG = pVarImageB = (__m256d)__lasx_xvldi(0);
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -669,7 +788,7 @@ RppStatus tensor_stddev_f16_f32_host(Rpp16f *srcPtr,
                 srcPtrTempB = srcPtrRowB;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     Rpp32f srcPtrTempR_ps[8], srcPtrTempG_ps[8], srcPtrTempB_ps[8];
@@ -708,7 +827,7 @@ RppStatus tensor_stddev_f16_f32_host(Rpp16f *srcPtr,
                 srcPtrRowG += srcDescPtr->strides.hStride;
                 srcPtrRowB += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__)
             _mm256_storeu_pd(varAvxR, pVarR);
             _mm256_storeu_pd(varAvxG, pVarG);
             _mm256_storeu_pd(varAvxB, pVarB);
@@ -716,6 +835,20 @@ RppStatus tensor_stddev_f16_f32_host(Rpp16f *srcPtr,
             _mm256_storeu_pd(varAvxImageG, pVarImageG);
             _mm256_storeu_pd(varAvxImageB, pVarImageB);
 
+            varR += (varAvxR[0] + varAvxR[1] + varAvxR[2] + varAvxR[3]);
+            varG += (varAvxG[0] + varAvxG[1] + varAvxG[2] + varAvxG[3]);
+            varB += (varAvxB[0] + varAvxB[1] + varAvxB[2] + varAvxB[3]);
+            varImageR += (varAvxImageR[0] + varAvxImageR[1] + varAvxImageR[2] + varAvxImageR[3]);
+            varImageG += (varAvxImageG[0] + varAvxImageG[1] + varAvxImageG[2] + varAvxImageG[3]);
+            varImageB += (varAvxImageB[0] + varAvxImageB[1] + varAvxImageB[2] + varAvxImageB[3]);
+#elif defined(__loongarch_asx)
+            __lasx_xvst((__m256i)pVarR, varAvxR, 0);
+            __lasx_xvst((__m256i)pVarG, varAvxG, 0);
+            __lasx_xvst((__m256i)pVarB, varAvxB, 0);
+            __lasx_xvst((__m256i)pVarImageR, varAvxImageR, 0);
+            __lasx_xvst((__m256i)pVarImageG, varAvxImageG, 0);
+            __lasx_xvst((__m256i)pVarImageB, varAvxImageB, 0);
+
             varR += (varAvxR[0] + varAvxR[1] + varAvxR[2] + varAvxR[3]);
             varG += (varAvxG[0] + varAvxG[1] + varAvxG[2] + varAvxG[3]);
             varB += (varAvxB[0] + varAvxB[1] + varAvxB[2] + varAvxB[3]);
@@ -754,7 +887,7 @@ RppStatus tensor_stddev_f16_f32_host(Rpp16f *srcPtr,
 
             Rpp16f *srcPtrRow;
             srcPtrRow = srcPtrChannel;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256d pMeanR     = _mm256_set1_pd(meanR);
             __m256d pMeanG     = _mm256_set1_pd(meanG);
             __m256d pMeanB     = _mm256_set1_pd(meanB);
@@ -762,6 +895,14 @@ RppStatus tensor_stddev_f16_f32_host(Rpp16f *srcPtr,
             __m256d pVarR, pVarG, pVarB;
             __m256d pVarImageR, pVarImageG, pVarImageB;
             pVarR = pVarG = pVarB = pVarImageR = pVarImageG = pVarImageB = _mm256_setzero_pd();
+#elif defined(__loongarch_asx)
+            __m256d pMeanR     = lasx_set1_f64(meanR);
+            __m256d pMeanG     = lasx_set1_f64(meanG);
+            __m256d pMeanB     = lasx_set1_f64(meanB);
+            __m256d pMeanImage = lasx_set1_f64(meanImage);
+            __m256d pVarR, pVarG, pVarB;
+            __m256d pVarImageR, pVarImageG, pVarImageB;
+            pVarR = pVarG = pVarB = pVarImageR = pVarImageG = pVarImageB = (__m256d)__lasx_xvldi(0);
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -769,7 +910,7 @@ RppStatus tensor_stddev_f16_f32_host(Rpp16f *srcPtr,
                 srcPtrTemp = srcPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     Rpp32f srcPtrTemp_ps[24];
@@ -799,7 +940,7 @@ RppStatus tensor_stddev_f16_f32_host(Rpp16f *srcPtr,
                 }
                 srcPtrRow += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__)
             _mm256_storeu_pd(varAvxR, pVarR);
             _mm256_storeu_pd(varAvxG, pVarG);
             _mm256_storeu_pd(varAvxB, pVarB);
@@ -807,6 +948,20 @@ RppStatus tensor_stddev_f16_f32_host(Rpp16f *srcPtr,
             _mm256_storeu_pd(varAvxImageG, pVarImageG);
             _mm256_storeu_pd(varAvxImageB, pVarImageB);
 
+            varR += (varAvxR[0] + varAvxR[1] + varAvxR[2] + varAvxR[3]);
+            varG += (varAvxG[0] + varAvxG[1] + varAvxG[2] + varAvxG[3]);
+            varB += (varAvxB[0] + varAvxB[1] + varAvxB[2] + varAvxB[3]);
+            varImageR += (varAvxImageR[0] + varAvxImageR[1] + varAvxImageR[2] + varAvxImageR[3]);
+            varImageG += (varAvxImageG[0] + varAvxImageG[1] + varAvxImageG[2] + varAvxImageG[3]);
+            varImageB += (varAvxImageB[0] + varAvxImageB[1] + varAvxImageB[2] + varAvxImageB[3]);
+#elif defined(__loongarch_asx)
+            __lasx_xvst((__m256i)pVarR, varAvxR, 0);
+            __lasx_xvst((__m256i)pVarG, varAvxG, 0);
+            __lasx_xvst((__m256i)pVarB, varAvxB, 0);
+            __lasx_xvst((__m256i)pVarImageR, varAvxImageR, 0);
+            __lasx_xvst((__m256i)pVarImageG, varAvxImageG, 0);
+            __lasx_xvst((__m256i)pVarImageB, varAvxImageB, 0);
+
             varR += (varAvxR[0] + varAvxR[1] + varAvxR[2] + varAvxR[3]);
             varG += (varAvxG[0] + varAvxG[1] + varAvxG[2] + varAvxG[3]);
             varB += (varAvxB[0] + varAvxB[1] + varAvxB[2] + varAvxB[3]);
@@ -870,9 +1025,12 @@ RppStatus tensor_stddev_i8_f32_host(Rpp8s *srcPtr,
 
             Rpp8s *srcPtrRow;
             srcPtrRow = srcPtrChannel;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256d pMean = _mm256_set1_pd(mean);
             __m256d pVar = _mm256_setzero_pd();
+#elif defined(__loongarch_asx)
+            __m256d pMean = lasx_set1_f64(mean);
+            __m256d pVar = (__m256d)__lasx_xvldi(0);
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -880,7 +1038,7 @@ RppStatus tensor_stddev_i8_f32_host(Rpp8s *srcPtr,
                 srcPtrTemp = srcPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256d p1[2];
@@ -897,9 +1055,12 @@ RppStatus tensor_stddev_i8_f32_host(Rpp8s *srcPtr,
                 }
                 srcPtrRow += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__)
             _mm256_storeu_pd(varAvx, pVar);
             var += (varAvx[0] + varAvx[1] + varAvx[2] + varAvx[3]);
+#elif defined(__loongarch_asx)
+            __lasx_xvst((__m256i)pVar, varAvx, 0);
+            var += (varAvx[0] + varAvx[1] + varAvx[2] + varAvx[3]);
 #endif
             stddev = sqrt(var / totalPixelsPerChannel);
             tensorStddevArr[batchCount] = static_cast<Rpp32f>(stddev);
@@ -927,7 +1088,7 @@ RppStatus tensor_stddev_i8_f32_host(Rpp8s *srcPtr,
             srcPtrRowR = srcPtrChannel;
             srcPtrRowG = srcPtrRowR + srcDescPtr->strides.cStride;
             srcPtrRowB = srcPtrRowG + srcDescPtr->strides.cStride;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256d pMeanR     = _mm256_set1_pd(meanR);
             __m256d pMeanG     = _mm256_set1_pd(meanG);
             __m256d pMeanB     = _mm256_set1_pd(meanB);
@@ -935,6 +1096,14 @@ RppStatus tensor_stddev_i8_f32_host(Rpp8s *srcPtr,
             __m256d pVarR, pVarG, pVarB;
             __m256d pVarImageR, pVarImageG, pVarImageB;
             pVarR = pVarG = pVarB = pVarImageR = pVarImageG = pVarImageB = _mm256_setzero_pd();
+#elif defined(__loongarch_asx)
+            __m256d pMeanR     = lasx_set1_f64(meanR);
+            __m256d pMeanG     = lasx_set1_f64(meanG);
+            __m256d pMeanB     = lasx_set1_f64(meanB);
+            __m256d pMeanImage = lasx_set1_f64(meanImage);
+            __m256d pVarR, pVarG, pVarB;
+            __m256d pVarImageR, pVarImageG, pVarImageB;
+            pVarR = pVarG = pVarB = pVarImageR = pVarImageG = pVarImageB = (__m256d)__lasx_xvldi(0);
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -944,7 +1113,7 @@ RppStatus tensor_stddev_i8_f32_host(Rpp8s *srcPtr,
                 srcPtrTempB = srcPtrRowB;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256d p[6];
@@ -975,7 +1144,7 @@ RppStatus tensor_stddev_i8_f32_host(Rpp8s *srcPtr,
                 srcPtrRowG += srcDescPtr->strides.hStride;
                 srcPtrRowB += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__)
             _mm256_storeu_pd(varAvxR, pVarR);
             _mm256_storeu_pd(varAvxG, pVarG);
             _mm256_storeu_pd(varAvxB, pVarB);
@@ -983,6 +1152,20 @@ RppStatus tensor_stddev_i8_f32_host(Rpp8s *srcPtr,
             _mm256_storeu_pd(varAvxImageG, pVarImageG);
             _mm256_storeu_pd(varAvxImageB, pVarImageB);
 
+            varR += (varAvxR[0] + varAvxR[1] + varAvxR[2] + varAvxR[3]);
+            varG += (varAvxG[0] + varAvxG[1] + varAvxG[2] + varAvxG[3]);
+            varB += (varAvxB[0] + varAvxB[1] + varAvxB[2] + varAvxB[3]);
+            varImageR += (varAvxImageR[0] + varAvxImageR[1] + varAvxImageR[2] + varAvxImageR[3]);
+            varImageG += (varAvxImageG[0] + varAvxImageG[1] + varAvxImageG[2] + varAvxImageG[3]);
+            varImageB += (varAvxImageB[0] + varAvxImageB[1] + varAvxImageB[2] + varAvxImageB[3]);
+#elif defined(__loongarch_asx)
+            __lasx_xvst((__m256i)pVarR, varAvxR, 0);
+            __lasx_xvst((__m256i)pVarG, varAvxG, 0);
+            __lasx_xvst((__m256i)pVarB, varAvxB, 0);
+            __lasx_xvst((__m256i)pVarImageR, varAvxImageR, 0);
+            __lasx_xvst((__m256i)pVarImageG, varAvxImageG, 0);
+            __lasx_xvst((__m256i)pVarImageB, varAvxImageB, 0);
+
             varR += (varAvxR[0] + varAvxR[1] + varAvxR[2] + varAvxR[3]);
             varG += (varAvxG[0] + varAvxG[1] + varAvxG[2] + varAvxG[3]);
             varB += (varAvxB[0] + varAvxB[1] + varAvxB[2] + varAvxB[3]);
@@ -1021,7 +1204,7 @@ RppStatus tensor_stddev_i8_f32_host(Rpp8s *srcPtr,
 
             Rpp8s *srcPtrRow;
             srcPtrRow = srcPtrChannel;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256d pMeanR     = _mm256_set1_pd(meanR);
             __m256d pMeanG     = _mm256_set1_pd(meanG);
             __m256d pMeanB     = _mm256_set1_pd(meanB);
@@ -1029,6 +1212,14 @@ RppStatus tensor_stddev_i8_f32_host(Rpp8s *srcPtr,
             __m256d pVarR, pVarG, pVarB;
             __m256d pVarImageR, pVarImageG, pVarImageB;
             pVarR = pVarG = pVarB = pVarImageR = pVarImageG = pVarImageB = _mm256_setzero_pd();
+#elif defined(__loongarch_asx)
+            __m256d pMeanR     = lasx_set1_f64(meanR);
+            __m256d pMeanG     = lasx_set1_f64(meanG);
+            __m256d pMeanB     = lasx_set1_f64(meanB);
+            __m256d pMeanImage = lasx_set1_f64(meanImage);
+            __m256d pVarR, pVarG, pVarB;
+            __m256d pVarImageR, pVarImageG, pVarImageB;
+            pVarR = pVarG = pVarB = pVarImageR = pVarImageG = pVarImageB = (__m256d)__lasx_xvldi(0);
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -1036,7 +1227,7 @@ RppStatus tensor_stddev_i8_f32_host(Rpp8s *srcPtr,
                 srcPtrTemp = srcPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256d p[6];
@@ -1061,7 +1252,7 @@ RppStatus tensor_stddev_i8_f32_host(Rpp8s *srcPtr,
                 }
                 srcPtrRow += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__)
             _mm256_storeu_pd(varAvxR, pVarR);
             _mm256_storeu_pd(varAvxG, pVarG);
             _mm256_storeu_pd(varAvxB, pVarB);
@@ -1069,6 +1260,20 @@ RppStatus tensor_stddev_i8_f32_host(Rpp8s *srcPtr,
             _mm256_storeu_pd(varAvxImageG, pVarImageG);
             _mm256_storeu_pd(varAvxImageB, pVarImageB);
 
+            varR += (varAvxR[0] + varAvxR[1] + varAvxR[2] + varAvxR[3]);
+            varG += (varAvxG[0] + varAvxG[1] + varAvxG[2] + varAvxG[3]);
+            varB += (varAvxB[0] + varAvxB[1] + varAvxB[2] + varAvxB[3]);
+            varImageR += (varAvxImageR[0] + varAvxImageR[1] + varAvxImageR[2] + varAvxImageR[3]);
+            varImageG += (varAvxImageG[0] + varAvxImageG[1] + varAvxImageG[2] + varAvxImageG[3]);
+            varImageB += (varAvxImageB[0] + varAvxImageB[1] + varAvxImageB[2] + varAvxImageB[3]);
+#elif defined(__loongarch_asx)
+            __lasx_xvst((__m256i)pVarR, varAvxR, 0);
+            __lasx_xvst((__m256i)pVarG, varAvxG, 0);
+            __lasx_xvst((__m256i)pVarB, varAvxB, 0);
+            __lasx_xvst((__m256i)pVarImageR, varAvxImageR, 0);
+            __lasx_xvst((__m256i)pVarImageG, varAvxImageG, 0);
+            __lasx_xvst((__m256i)pVarImageB, varAvxImageB, 0);
+
             varR += (varAvxR[0] + varAvxR[1] + varAvxR[2] + varAvxR[3]);
             varG += (varAvxG[0] + varAvxG[1] + varAvxG[2] + varAvxG[3]);
             varB += (varAvxB[0] + varAvxB[1] + varAvxB[2] + varAvxB[3]);
diff --git a/src/modules/cpu/kernel/tensor_sum.hpp b/src/modules/cpu/kernel/tensor_sum.hpp
index 72eef2b7..acfc0236 100644
--- a/src/modules/cpu/kernel/tensor_sum.hpp
+++ b/src/modules/cpu/kernel/tensor_sum.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 #include "reduction.hpp"
 
 RppStatus tensor_sum_u8_u64_host(Rpp8u *srcPtr,
@@ -65,8 +70,10 @@ RppStatus tensor_sum_u8_u64_host(Rpp8u *srcPtr,
 
             Rpp8u *srcPtrRow;
             srcPtrRow = srcPtrChannel;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256i psum = _mm256_setzero_si256();
+#elif defined(__loongarch_asx)
+            __m256i psum = __lasx_xvldi(0);
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -74,7 +81,7 @@ RppStatus tensor_sum_u8_u64_host(Rpp8u *srcPtr,
                 srcPtrTemp = srcPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256i p1[2];
@@ -89,7 +96,7 @@ RppStatus tensor_sum_u8_u64_host(Rpp8u *srcPtr,
                 }
                 srcPtrRow += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             rpp_simd_store(rpp_store8_u32_to_u32_avx, sumAvx, &psum);
             sum += (sumAvx[0] + sumAvx[1] + sumAvx[2] + sumAvx[3] + sumAvx[4] + sumAvx[5] + sumAvx[6] + sumAvx[7]);
 #endif
@@ -109,10 +116,14 @@ RppStatus tensor_sum_u8_u64_host(Rpp8u *srcPtr,
             srcPtrRowR = srcPtrChannel;
             srcPtrRowG = srcPtrRowR + srcDescPtr->strides.cStride;
             srcPtrRowB = srcPtrRowG + srcDescPtr->strides.cStride;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256i psumR = _mm256_setzero_si256();
             __m256i psumG = _mm256_setzero_si256();
             __m256i psumB = _mm256_setzero_si256();
+#elif defined(__loongarch_asx)
+            __m256i psumR = __lasx_xvldi(0);
+            __m256i psumG = __lasx_xvldi(0);
+            __m256i psumB = __lasx_xvldi(0);
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -122,7 +133,7 @@ RppStatus tensor_sum_u8_u64_host(Rpp8u *srcPtr,
                 srcPtrTempB = srcPtrRowB;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256i p[6];
@@ -143,7 +154,7 @@ RppStatus tensor_sum_u8_u64_host(Rpp8u *srcPtr,
                 srcPtrRowG += srcDescPtr->strides.hStride;
                 srcPtrRowB += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             rpp_simd_store(rpp_store8_u32_to_u32_avx, sumAvxR, &psumR);
             rpp_simd_store(rpp_store8_u32_to_u32_avx, sumAvxG, &psumG);
             rpp_simd_store(rpp_store8_u32_to_u32_avx, sumAvxB, &psumB);
@@ -170,10 +181,14 @@ RppStatus tensor_sum_u8_u64_host(Rpp8u *srcPtr,
 
             Rpp8u *srcPtrRow;
             srcPtrRow = srcPtrChannel;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256i psumR = _mm256_setzero_si256();
             __m256i psumG = _mm256_setzero_si256();
             __m256i psumB = _mm256_setzero_si256();
+#elif defined(__loongarch_asx)
+            __m256i psumR = __lasx_xvldi(0);
+            __m256i psumG = __lasx_xvldi(0);
+            __m256i psumB = __lasx_xvldi(0);
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -181,7 +196,7 @@ RppStatus tensor_sum_u8_u64_host(Rpp8u *srcPtr,
                 srcPtrTemp = srcPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256i p[6];
@@ -199,7 +214,7 @@ RppStatus tensor_sum_u8_u64_host(Rpp8u *srcPtr,
                 }
                 srcPtrRow += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             rpp_simd_store(rpp_store8_u32_to_u32_avx, sumAvxR, &psumR);
             rpp_simd_store(rpp_store8_u32_to_u32_avx, sumAvxG, &psumG);
             rpp_simd_store(rpp_store8_u32_to_u32_avx, sumAvxB, &psumB);
@@ -259,8 +274,10 @@ RppStatus tensor_sum_f32_f32_host(Rpp32f *srcPtr,
 
             Rpp32f *srcPtrRow;
             srcPtrRow = srcPtrChannel;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256d psum = _mm256_setzero_pd();
+#elif defined(__loongarch_asx)
+            __m256d psum = (__m256d)__lasx_xvldi(0);
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -268,7 +285,7 @@ RppStatus tensor_sum_f32_f32_host(Rpp32f *srcPtr,
                 srcPtrTemp = srcPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256d p1[2];
@@ -283,7 +300,7 @@ RppStatus tensor_sum_f32_f32_host(Rpp32f *srcPtr,
                 }
                 srcPtrRow += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             rpp_simd_store(rpp_store4_f64_to_f64_avx, sumAvx, &psum);
             sum += (sumAvx[0] + sumAvx[1] + sumAvx[2] + sumAvx[3]);
 #endif
@@ -303,10 +320,14 @@ RppStatus tensor_sum_f32_f32_host(Rpp32f *srcPtr,
             srcPtrRowR = srcPtrChannel;
             srcPtrRowG = srcPtrRowR + srcDescPtr->strides.cStride;
             srcPtrRowB = srcPtrRowG + srcDescPtr->strides.cStride;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256d psumR = _mm256_setzero_pd();
             __m256d psumG = _mm256_setzero_pd();
             __m256d psumB = _mm256_setzero_pd();
+#elif defined(__loongarch_asx)
+            __m256d psumR = (__m256d)__lasx_xvldi(0);
+            __m256d psumG = (__m256d)__lasx_xvldi(0);
+            __m256d psumB = (__m256d)__lasx_xvldi(0);
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -316,7 +337,7 @@ RppStatus tensor_sum_f32_f32_host(Rpp32f *srcPtr,
                 srcPtrTempB = srcPtrRowB;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256d p[6];
@@ -337,7 +358,7 @@ RppStatus tensor_sum_f32_f32_host(Rpp32f *srcPtr,
                 srcPtrRowG += srcDescPtr->strides.hStride;
                 srcPtrRowB += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             rpp_simd_store(rpp_store4_f64_to_f64_avx, sumAvxR, &psumR);
             rpp_simd_store(rpp_store4_f64_to_f64_avx, sumAvxG, &psumG);
             rpp_simd_store(rpp_store4_f64_to_f64_avx, sumAvxB, &psumB);
@@ -363,10 +384,14 @@ RppStatus tensor_sum_f32_f32_host(Rpp32f *srcPtr,
 
             Rpp32f *srcPtrRow;
             srcPtrRow = srcPtrChannel;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256d psumR = _mm256_setzero_pd();
             __m256d psumG = _mm256_setzero_pd();
             __m256d psumB = _mm256_setzero_pd();
+#elif defined(__loongarch_asx)
+            __m256d psumR = (__m256d)__lasx_xvldi(0);
+            __m256d psumG = (__m256d)__lasx_xvldi(0);
+            __m256d psumB = (__m256d)__lasx_xvldi(0);
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -374,7 +399,7 @@ RppStatus tensor_sum_f32_f32_host(Rpp32f *srcPtr,
                 srcPtrTemp = srcPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256d p[6];
@@ -392,7 +417,7 @@ RppStatus tensor_sum_f32_f32_host(Rpp32f *srcPtr,
                 }
                 srcPtrRow += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             rpp_simd_store(rpp_store4_f64_to_f64_avx, sumAvxR, &psumR);
             rpp_simd_store(rpp_store4_f64_to_f64_avx, sumAvxG, &psumG);
             rpp_simd_store(rpp_store4_f64_to_f64_avx, sumAvxB, &psumB);
@@ -451,8 +476,10 @@ RppStatus tensor_sum_f16_f32_host(Rpp16f *srcPtr,
 
             Rpp16f *srcPtrRow;
             srcPtrRow = srcPtrChannel;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256d psum = _mm256_setzero_pd();
+#elif defined(__loongarch_asx)
+            __m256d psum = (__m256d)__lasx_xvldi(0);
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -460,7 +487,7 @@ RppStatus tensor_sum_f16_f32_host(Rpp16f *srcPtr,
                 srcPtrTemp = srcPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     Rpp32f srcPtrTemp_ps[8];
@@ -478,7 +505,7 @@ RppStatus tensor_sum_f16_f32_host(Rpp16f *srcPtr,
                 }
                 srcPtrRow += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             rpp_simd_store(rpp_store4_f64_to_f64_avx, sumAvx, &psum);
             sum += (sumAvx[0] + sumAvx[1] + sumAvx[2] + sumAvx[3]);
 #endif
@@ -497,10 +524,14 @@ RppStatus tensor_sum_f16_f32_host(Rpp16f *srcPtr,
             srcPtrRowR = srcPtrChannel;
             srcPtrRowG = srcPtrRowR + srcDescPtr->strides.cStride;
             srcPtrRowB = srcPtrRowG + srcDescPtr->strides.cStride;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256d psumR = _mm256_setzero_pd();
             __m256d psumG = _mm256_setzero_pd();
             __m256d psumB = _mm256_setzero_pd();
+#elif defined(__loongarch_asx)
+            __m256d psumR = (__m256d)__lasx_xvldi(0);
+            __m256d psumG = (__m256d)__lasx_xvldi(0);
+            __m256d psumB = (__m256d)__lasx_xvldi(0);
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -510,7 +541,7 @@ RppStatus tensor_sum_f16_f32_host(Rpp16f *srcPtr,
                 srcPtrTempB = srcPtrRowB;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     Rpp32f srcPtrTempR_ps[8], srcPtrTempG_ps[8], srcPtrTempB_ps[8];
@@ -538,7 +569,7 @@ RppStatus tensor_sum_f16_f32_host(Rpp16f *srcPtr,
                 srcPtrRowG += srcDescPtr->strides.hStride;
                 srcPtrRowB += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             rpp_simd_store(rpp_store4_f64_to_f64_avx, sumAvxR, &psumR);
             rpp_simd_store(rpp_store4_f64_to_f64_avx, sumAvxG, &psumG);
             rpp_simd_store(rpp_store4_f64_to_f64_avx, sumAvxB, &psumB);
@@ -564,10 +595,14 @@ RppStatus tensor_sum_f16_f32_host(Rpp16f *srcPtr,
 
             Rpp16f *srcPtrRow;
             srcPtrRow = srcPtrChannel;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256d psumR = _mm256_setzero_pd();
             __m256d psumG = _mm256_setzero_pd();
             __m256d psumB = _mm256_setzero_pd();
+#elif defined(__loongarch_asx)
+            __m256d psumR = (__m256d)__lasx_xvldi(0);
+            __m256d psumG = (__m256d)__lasx_xvldi(0);
+            __m256d psumB = (__m256d)__lasx_xvldi(0);
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -575,7 +610,7 @@ RppStatus tensor_sum_f16_f32_host(Rpp16f *srcPtr,
                 srcPtrTemp = srcPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     Rpp32f srcPtrTemp_ps[24];
@@ -596,7 +631,7 @@ RppStatus tensor_sum_f16_f32_host(Rpp16f *srcPtr,
                 }
                 srcPtrRow += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             rpp_simd_store(rpp_store4_f64_to_f64_avx, sumAvxR, &psumR);
             rpp_simd_store(rpp_store4_f64_to_f64_avx, sumAvxG, &psumG);
             rpp_simd_store(rpp_store4_f64_to_f64_avx, sumAvxB, &psumB);
@@ -656,8 +691,10 @@ RppStatus tensor_sum_i8_i64_host(Rpp8s *srcPtr,
 
             Rpp8s *srcPtrRow;
             srcPtrRow = srcPtrChannel;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256i psum = _mm256_setzero_si256();
+#elif defined(__loongarch_asx)
+            __m256i psum = __lasx_xvldi(0);
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -665,7 +702,7 @@ RppStatus tensor_sum_i8_i64_host(Rpp8s *srcPtr,
                 srcPtrTemp = srcPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256i p1[2];
@@ -680,7 +717,7 @@ RppStatus tensor_sum_i8_i64_host(Rpp8s *srcPtr,
                 }
                 srcPtrRow += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             rpp_simd_store(rpp_store8_i32_to_i32_avx, sumAvx, &psum);
             sum += (sumAvx[0] + sumAvx[1] + sumAvx[2] + sumAvx[3] + sumAvx[4] + sumAvx[5] + sumAvx[6] + sumAvx[7]);
 #endif
@@ -700,10 +737,14 @@ RppStatus tensor_sum_i8_i64_host(Rpp8s *srcPtr,
             srcPtrRowR = srcPtrChannel;
             srcPtrRowG = srcPtrRowR + srcDescPtr->strides.cStride;
             srcPtrRowB = srcPtrRowG + srcDescPtr->strides.cStride;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256i psumR = _mm256_setzero_si256();
             __m256i psumG = _mm256_setzero_si256();
             __m256i psumB = _mm256_setzero_si256();
+#elif defined(__loongarch_asx)
+            __m256i psumR = __lasx_xvldi(0);
+            __m256i psumG = __lasx_xvldi(0);
+            __m256i psumB = __lasx_xvldi(0);
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -713,7 +754,7 @@ RppStatus tensor_sum_i8_i64_host(Rpp8s *srcPtr,
                 srcPtrTempB = srcPtrRowB;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256i p[6];
@@ -734,7 +775,7 @@ RppStatus tensor_sum_i8_i64_host(Rpp8s *srcPtr,
                 srcPtrRowG += srcDescPtr->strides.hStride;
                 srcPtrRowB += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             rpp_simd_store(rpp_store8_i32_to_i32_avx, sumAvxR, &psumR);
             rpp_simd_store(rpp_store8_i32_to_i32_avx, sumAvxG, &psumG);
             rpp_simd_store(rpp_store8_i32_to_i32_avx, sumAvxB, &psumB);
@@ -761,10 +802,14 @@ RppStatus tensor_sum_i8_i64_host(Rpp8s *srcPtr,
 
             Rpp8s *srcPtrRow;
             srcPtrRow = srcPtrChannel;
-#if __AVX2__
+#if defined(__AVX2__)
             __m256i psumR = _mm256_setzero_si256();
             __m256i psumG = _mm256_setzero_si256();
             __m256i psumB = _mm256_setzero_si256();
+#elif defined(__loongarch_asx)
+            __m256i psumR = __lasx_xvldi(0);
+            __m256i psumG = __lasx_xvldi(0);
+            __m256i psumB = __lasx_xvldi(0);
 #endif
             for(int i = 0; i < roi.xywhROI.roiHeight; i++)
             {
@@ -772,7 +817,7 @@ RppStatus tensor_sum_i8_i64_host(Rpp8s *srcPtr,
                 srcPtrTemp = srcPtrRow;
 
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256i p[6];
@@ -790,7 +835,7 @@ RppStatus tensor_sum_i8_i64_host(Rpp8s *srcPtr,
                 }
                 srcPtrRow += srcDescPtr->strides.hStride;
             }
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
             rpp_simd_store(rpp_store8_i32_to_i32_avx, sumAvxR, &psumR);
             rpp_simd_store(rpp_store8_i32_to_i32_avx, sumAvxG, &psumG);
             rpp_simd_store(rpp_store8_i32_to_i32_avx, sumAvxB, &psumB);
diff --git a/src/modules/cpu/kernel/threshold.hpp b/src/modules/cpu/kernel/threshold.hpp
index dc157957..0b1b5c0f 100644
--- a/src/modules/cpu/kernel/threshold.hpp
+++ b/src/modules/cpu/kernel/threshold.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 RppStatus threshold_u8_u8_host_tensor(Rpp8u *srcPtr,
                                       RpptDescPtr srcDescPtr,
@@ -69,13 +74,20 @@ RppStatus threshold_u8_u8_host_tensor(Rpp8u *srcPtr,
             minThreshold[c] = minTensor[batchIndex + c];
             maxThreshold[c] = maxTensor[batchIndex + c];
         }
-#if __AVX2__
+#if defined(__AVX2__)
             __m256 pThresholdParams[6];
             for (int c = 0, i = 0; c < 3; c++, i += 2)
             {
                 pThresholdParams[i] = _mm256_set1_ps(minThreshold[c]);
                 pThresholdParams[i + 1] = _mm256_set1_ps(maxThreshold[c]);
             }
+#elif defined(__loongarch_asx)
+            __m256 pThresholdParams[6];
+            for (int c = 0, i = 0; c < 3; c++, i += 2)
+            {
+                pThresholdParams[i] = lasx_set1_f32(minThreshold[c]);
+                pThresholdParams[i + 1] = lasx_set1_f32(maxThreshold[c]);
+            }
 #endif
         // Threshold with fused output-layout toggle (NHWC -> NCHW)
         if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
@@ -93,7 +105,7 @@ RppStatus threshold_u8_u8_host_tensor(Rpp8u *srcPtr,
                 dstPtrTempG = dstPtrRowG;
                 dstPtrTempB = dstPtrRowB;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 16)
                 {
                     __m256 p[6];
@@ -146,7 +158,7 @@ RppStatus threshold_u8_u8_host_tensor(Rpp8u *srcPtr,
                 srcPtrTempB = srcPtrRowB;
                 dstPtrTemp = dstPtrRow;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 16)
                 {
                     __m256 p[6];
@@ -194,7 +206,7 @@ RppStatus threshold_u8_u8_host_tensor(Rpp8u *srcPtr,
                 srcPtrTemp = srcPtrRow;
                 dstPtrTemp = dstPtrRow;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256 p[6];
@@ -249,7 +261,7 @@ RppStatus threshold_u8_u8_host_tensor(Rpp8u *srcPtr,
                 dstPtrTempG = dstPtrRowG;
                 dstPtrTempB = dstPtrRowB;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 p[6];
@@ -300,7 +312,7 @@ RppStatus threshold_u8_u8_host_tensor(Rpp8u *srcPtr,
                 srcPtrTemp = srcPtrRow;
                 dstPtrTemp = dstPtrRow;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 p[2];
@@ -368,13 +380,20 @@ RppStatus threshold_f32_f32_host_tensor(Rpp32f *srcPtr,
             minThreshold[c] = minTensor[batchIndex + c];
             maxThreshold[c] = maxTensor[batchIndex + c];
         }
-#if __AVX2__
+#if defined(__AVX2__)
             __m256 pThresholdParams[6];
             for (int c = 0, i = 0; c < 3; c++, i += 2)
             {
                 pThresholdParams[i] = _mm256_set1_ps(minThreshold[c]);
                 pThresholdParams[i + 1] = _mm256_set1_ps(maxThreshold[c]);
             }
+#elif defined(__loongarch_asx)
+            __m256 pThresholdParams[6];
+            for (int c = 0, i = 0; c < 3; c++, i += 2)
+            {
+                pThresholdParams[i] = lasx_set1_f32(minThreshold[c]);
+                pThresholdParams[i + 1] = lasx_set1_f32(maxThreshold[c]);
+            }
 #endif
         // Threshold with fused output-layout toggle (NHWC -> NCHW)
         if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
@@ -392,7 +411,7 @@ RppStatus threshold_f32_f32_host_tensor(Rpp32f *srcPtr,
                 dstPtrTempG = dstPtrRowG;
                 dstPtrTempB = dstPtrRowB;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
                 {
                     __m256 p[3];
@@ -445,7 +464,7 @@ RppStatus threshold_f32_f32_host_tensor(Rpp32f *srcPtr,
                 srcPtrTempB = srcPtrRowB;
                 dstPtrTemp = dstPtrRow;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
                 {
                     __m256 p[3];
@@ -493,7 +512,7 @@ RppStatus threshold_f32_f32_host_tensor(Rpp32f *srcPtr,
                 srcPtrTemp = srcPtrRow;
                 dstPtrTemp = dstPtrRow;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256 p[3];
@@ -548,7 +567,7 @@ RppStatus threshold_f32_f32_host_tensor(Rpp32f *srcPtr,
                 dstPtrTempG = dstPtrRowG;
                 dstPtrTempB = dstPtrRowB;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 p[3];
@@ -599,7 +618,7 @@ RppStatus threshold_f32_f32_host_tensor(Rpp32f *srcPtr,
                 srcPtrTemp = srcPtrRow;
                 dstPtrTemp = dstPtrRow;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 p[1];
@@ -667,13 +686,20 @@ RppStatus threshold_i8_i8_host_tensor(Rpp8s *srcPtr,
             minThreshold[c] = minTensor[batchIndex + c] + static_cast<Rpp32f>(128);
             maxThreshold[c] = maxTensor[batchIndex + c] + static_cast<Rpp32f>(128);
         }
-#if __AVX2__
+#if defined(__AVX2__)
             __m256 pThresholdParams[6];
             for (int c = 0, i = 0; c < 3; c++, i += 2)
             {
                 pThresholdParams[i] = _mm256_set1_ps(minThreshold[c]);
                 pThresholdParams[i + 1] = _mm256_set1_ps(maxThreshold[c]);
             }
+#elif defined(__loongarch_asx)
+            __m256 pThresholdParams[6];
+            for (int c = 0, i = 0; c < 3; c++, i += 2)
+            {
+                pThresholdParams[i] = lasx_set1_f32(minThreshold[c]);
+                pThresholdParams[i + 1] = lasx_set1_f32(maxThreshold[c]);
+            }
 #endif
         // Threshold with fused output-layout toggle (NHWC -> NCHW)
         if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
@@ -691,7 +717,7 @@ RppStatus threshold_i8_i8_host_tensor(Rpp8s *srcPtr,
                 dstPtrTempG = dstPtrRowG;
                 dstPtrTempB = dstPtrRowB;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 16)
                 {
                     __m256 p[6];
@@ -744,7 +770,7 @@ RppStatus threshold_i8_i8_host_tensor(Rpp8s *srcPtr,
                 srcPtrTempB = srcPtrRowB;
                 dstPtrTemp = dstPtrRow;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 16)
                 {
                     __m256 p[6];
@@ -792,7 +818,7 @@ RppStatus threshold_i8_i8_host_tensor(Rpp8s *srcPtr,
                 srcPtrTemp = srcPtrRow;
                 dstPtrTemp = dstPtrRow;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256 p[6];
@@ -847,7 +873,7 @@ RppStatus threshold_i8_i8_host_tensor(Rpp8s *srcPtr,
                 dstPtrTempG = dstPtrRowG;
                 dstPtrTempB = dstPtrRowB;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 p[6];
@@ -898,7 +924,7 @@ RppStatus threshold_i8_i8_host_tensor(Rpp8s *srcPtr,
                 srcPtrTemp = srcPtrRow;
                 dstPtrTemp = dstPtrRow;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 p[2];
@@ -966,13 +992,20 @@ RppStatus threshold_f16_f16_host_tensor(Rpp16f *srcPtr,
             minThreshold[c] = minTensor[batchIndex + c];
             maxThreshold[c] = maxTensor[batchIndex + c];
         }
-#if __AVX2__
+#if defined(__AVX2__)
             __m256 pThresholdParams[6];
             for (int c = 0, i = 0; c < 3; c++, i += 2)
             {
                 pThresholdParams[i] = _mm256_set1_ps(minThreshold[c]);
                 pThresholdParams[i + 1] = _mm256_set1_ps(maxThreshold[c]);
             }
+#elif defined(__loongarch_asx)
+            __m256 pThresholdParams[6];
+            for (int c = 0, i = 0; c < 3; c++, i += 2)
+            {
+                pThresholdParams[i] = lasx_set1_f32(minThreshold[c]);
+                pThresholdParams[i + 1] = lasx_set1_f32(maxThreshold[c]);
+            }
 #endif
         // Threshold with fused output-layout toggle (NHWC -> NCHW)
         if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
@@ -990,7 +1023,7 @@ RppStatus threshold_f16_f16_host_tensor(Rpp16f *srcPtr,
                 dstPtrTempG = dstPtrRowG;
                 dstPtrTempB = dstPtrRowB;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
                 {
                     __m256 p[3];
@@ -1043,7 +1076,7 @@ RppStatus threshold_f16_f16_host_tensor(Rpp16f *srcPtr,
                 srcPtrTempB = srcPtrRowB;
                 dstPtrTemp = dstPtrRow;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
                 {
                     __m256 p[3];
@@ -1091,7 +1124,7 @@ RppStatus threshold_f16_f16_host_tensor(Rpp16f *srcPtr,
                 srcPtrTemp = srcPtrRow;
                 dstPtrTemp = dstPtrRow;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                 {
                     __m256 p[3];
@@ -1146,7 +1179,7 @@ RppStatus threshold_f16_f16_host_tensor(Rpp16f *srcPtr,
                 dstPtrTempG = dstPtrRowG;
                 dstPtrTempB = dstPtrRowB;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 p[3];
@@ -1197,7 +1230,7 @@ RppStatus threshold_f16_f16_host_tensor(Rpp16f *srcPtr,
                 srcPtrTemp = srcPtrRow;
                 dstPtrTemp = dstPtrRow;
                 int vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 p[1];
diff --git a/src/modules/cpu/kernel/transpose.hpp b/src/modules/cpu/kernel/transpose.hpp
index 233db104..7892a138 100644
--- a/src/modules/cpu/kernel/transpose.hpp
+++ b/src/modules/cpu/kernel/transpose.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 using namespace std;
 
 inline void increment_ndim_ptr(Rpp32f **dstPtr, Rpp32u tensorDims, Rpp32u increment)
@@ -35,10 +40,17 @@ inline void increment_ndim_ptr(Rpp32f **dstPtr, Rpp32u tensorDims, Rpp32u increm
 
 inline void rpp_store16_f32_f32_channelwise(Rpp32f **dstPtr, __m128 *p)
 {
+#if defined(__loongarch_sx)
+    __lsx_vst(p[0], dstPtr[0], 0);
+    __lsx_vst(p[1], dstPtr[1], 0);
+    __lsx_vst(p[2], dstPtr[2], 0);
+    __lsx_vst(p[3], dstPtr[3], 0);
+#else
     _mm_storeu_ps(dstPtr[0], p[0]);
     _mm_storeu_ps(dstPtr[1], p[1]);
     _mm_storeu_ps(dstPtr[2], p[2]);
     _mm_storeu_ps(dstPtr[3], p[3]);
+#endif
 }
 
 inline void compute_2d_pln1_transpose(Rpp32f *srcPtrTemp, Rpp32f *dstPtrTemp, Rpp32u height, Rpp32u width, Rpp32u srcRowStride, Rpp32u dstRowStride)
@@ -346,7 +358,7 @@ RppStatus transpose_f32_f32_host_tensor(Rpp32f *srcPtr,
                                 Rpp32f *dstPtr3 = dstPtr2;
 
                                 Rpp32u vectorLoopCount = 0;
-#if __AVX2__
+#if defined(__AVX2__)
                                 for( ; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
                                 {
                                     __m256 pSrc = _mm256_setr_ps(srcPtr3[stridesIncrement[0]], srcPtr3[stridesIncrement[1]], srcPtr3[stridesIncrement[2]], srcPtr3[stridesIncrement[3]],
@@ -355,6 +367,15 @@ RppStatus transpose_f32_f32_host_tensor(Rpp32f *srcPtr,
                                     srcPtr3 += srcIncrement;
                                     dstPtr3 += vectorIncrement;
                                 }
+#elif defined(__loongarch_asx)
+                                for( ; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrement)
+                                {
+                                    __m256 pSrc = lasx_setr_f32(srcPtr3[stridesIncrement[0]], srcPtr3[stridesIncrement[1]], srcPtr3[stridesIncrement[2]], srcPtr3[stridesIncrement[3]],
+                                                                 srcPtr3[stridesIncrement[4]], srcPtr3[stridesIncrement[5]], srcPtr3[stridesIncrement[6]], srcPtr3[stridesIncrement[7]]);
+                                    rpp_simd_store(rpp_store8_f32_to_f32_avx, dstPtr3, &pSrc);
+                                    srcPtr3 += srcIncrement;
+                                    dstPtr3 += vectorIncrement;
+                                }
 #endif
                                 for( ; vectorLoopCount < bufferLength; vectorLoopCount++)
                                 {
diff --git a/src/modules/cpu/kernel/vignette.hpp b/src/modules/cpu/kernel/vignette.hpp
index 4043588a..aec3d8c4 100644
--- a/src/modules/cpu/kernel/vignette.hpp
+++ b/src/modules/cpu/kernel/vignette.hpp
@@ -23,8 +23,13 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 RppStatus vignette_u8_u8_host_tensor(Rpp8u *srcPtr,
                                      RpptDescPtr srcDescPtr,
@@ -64,8 +69,8 @@ RppStatus vignette_u8_u8_host_tensor(Rpp8u *srcPtr,
         srcPtrChannel = srcPtrImage + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
         dstPtrChannel = dstPtrImage;
 
-        __m256 pMultiplier = _mm256_set1_ps(multiplier);
-        __m256 pHalfWidth = _mm256_set1_ps(halfWidth);
+        __m256 pMultiplier = lasx_set1_f32(multiplier);
+        __m256 pHalfWidth = lasx_set1_f32(halfWidth);
 
         // Vignette with fused output-layout toggle (NHWC -> NCHW)
         if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
@@ -86,8 +91,8 @@ RppStatus vignette_u8_u8_host_tensor(Rpp8u *srcPtr,
 
                 Rpp32s iLoc = i - halfHeight;
                 Rpp32f iLocComponent = iLoc * iLoc;
-                __m256 pILocComponent = _mm256_set1_ps(iLocComponent);
-                __m256 pJLocComponent = _mm256_sub_ps(avx_pDstLocInit , pHalfWidth);
+                __m256 pILocComponent = lasx_set1_f32(iLocComponent);
+                __m256 pJLocComponent = __lasx_xvfsub_s(avx_pDstLocInit , pHalfWidth);
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 16)
@@ -139,8 +144,8 @@ RppStatus vignette_u8_u8_host_tensor(Rpp8u *srcPtr,
 
                 Rpp32s iLoc = i - halfHeight;
                 Rpp32f iLocComponent = iLoc * iLoc;
-                __m256 pILocComponent = _mm256_set1_ps(iLocComponent);
-                __m256 pJLocComponent = _mm256_sub_ps(avx_pDstLocInit , pHalfWidth);
+                __m256 pILocComponent = lasx_set1_f32(iLocComponent);
+                __m256 pJLocComponent = __lasx_xvfsub_s(avx_pDstLocInit , pHalfWidth);
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 16)
@@ -191,8 +196,8 @@ RppStatus vignette_u8_u8_host_tensor(Rpp8u *srcPtr,
 
                 Rpp32s iLoc = i - halfHeight;
                 Rpp32f iLocComponent = iLoc * iLoc;
-                __m256 pILocComponent = _mm256_set1_ps(iLocComponent);
-                __m256 pJLocComponent = _mm256_sub_ps(avx_pDstLocInit , pHalfWidth);
+                __m256 pILocComponent = lasx_set1_f32(iLocComponent);
+                __m256 pJLocComponent = __lasx_xvfsub_s(avx_pDstLocInit , pHalfWidth);
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 16)
@@ -246,8 +251,8 @@ RppStatus vignette_u8_u8_host_tensor(Rpp8u *srcPtr,
 
                 Rpp32s iLoc = i - halfHeight;
                 Rpp32f iLocComponent = iLoc * iLoc;
-                __m256 pILocComponent = _mm256_set1_ps(iLocComponent);
-                __m256 pJLocComponent = _mm256_sub_ps(avx_pDstLocInit , pHalfWidth);
+                __m256 pILocComponent = lasx_set1_f32(iLocComponent);
+                __m256 pJLocComponent = __lasx_xvfsub_s(avx_pDstLocInit , pHalfWidth);
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 16)
@@ -301,8 +306,8 @@ RppStatus vignette_u8_u8_host_tensor(Rpp8u *srcPtr,
 
                 Rpp32s iLoc = i - halfHeight;
                 Rpp32f iLocComponent = iLoc * iLoc;
-                __m256 pILocComponent = _mm256_set1_ps(iLocComponent);
-                __m256 pJLocComponent = _mm256_sub_ps(avx_pDstLocInit , pHalfWidth);
+                __m256 pILocComponent = lasx_set1_f32(iLocComponent);
+                __m256 pJLocComponent = __lasx_xvfsub_s(avx_pDstLocInit , pHalfWidth);
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 16)
@@ -370,8 +375,8 @@ RppStatus vignette_f32_f32_host_tensor(Rpp32f *srcPtr,
         srcPtrChannel = srcPtrImage + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
         dstPtrChannel = dstPtrImage;
 
-        __m256 pMultiplier = _mm256_set1_ps(multiplier);
-        __m256 pHalfWidth = _mm256_set1_ps(halfWidth);
+        __m256 pMultiplier = lasx_set1_f32(multiplier);
+        __m256 pHalfWidth = lasx_set1_f32(halfWidth);
 
         // Vignette with fused output-layout toggle (NHWC -> NCHW)
         if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
@@ -392,8 +397,8 @@ RppStatus vignette_f32_f32_host_tensor(Rpp32f *srcPtr,
 
                 Rpp32s iLoc = i - halfHeight;
                 Rpp32f iLocComponent = iLoc * iLoc;
-                __m256 pILocComponent = _mm256_set1_ps(iLocComponent);
-                __m256 pJLocComponent = _mm256_sub_ps(avx_pDstLocInit , pHalfWidth);
+                __m256 pILocComponent = lasx_set1_f32(iLocComponent);
+                __m256 pJLocComponent = __lasx_xvfsub_s(avx_pDstLocInit , pHalfWidth);
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
@@ -445,8 +450,8 @@ RppStatus vignette_f32_f32_host_tensor(Rpp32f *srcPtr,
 
                 Rpp32s iLoc = i - halfHeight;
                 Rpp32f iLocComponent = iLoc * iLoc;
-                __m256 pILocComponent = _mm256_set1_ps(iLocComponent);
-                __m256 pJLocComponent = _mm256_sub_ps(avx_pDstLocInit , pHalfWidth);
+                __m256 pILocComponent = lasx_set1_f32(iLocComponent);
+                __m256 pJLocComponent = __lasx_xvfsub_s(avx_pDstLocInit , pHalfWidth);
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
@@ -497,8 +502,8 @@ RppStatus vignette_f32_f32_host_tensor(Rpp32f *srcPtr,
 
                 Rpp32s iLoc = i - halfHeight;
                 Rpp32f iLocComponent = iLoc * iLoc;
-                __m256 pILocComponent = _mm256_set1_ps(iLocComponent);
-                __m256 pJLocComponent = _mm256_sub_ps(avx_pDstLocInit , pHalfWidth);
+                __m256 pILocComponent = lasx_set1_f32(iLocComponent);
+                __m256 pJLocComponent = __lasx_xvfsub_s(avx_pDstLocInit , pHalfWidth);
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
@@ -552,8 +557,8 @@ RppStatus vignette_f32_f32_host_tensor(Rpp32f *srcPtr,
 
                 Rpp32s iLoc = i - halfHeight;
                 Rpp32f iLocComponent = iLoc * iLoc;
-                __m256 pILocComponent = _mm256_set1_ps(iLocComponent);
-                __m256 pJLocComponent = _mm256_sub_ps(avx_pDstLocInit , pHalfWidth);
+                __m256 pILocComponent = lasx_set1_f32(iLocComponent);
+                __m256 pJLocComponent = __lasx_xvfsub_s(avx_pDstLocInit , pHalfWidth);
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
@@ -607,8 +612,8 @@ RppStatus vignette_f32_f32_host_tensor(Rpp32f *srcPtr,
 
                 Rpp32s iLoc = i - halfHeight;
                 Rpp32f iLocComponent = iLoc * iLoc;
-                __m256 pILocComponent = _mm256_set1_ps(iLocComponent);
-                __m256 pJLocComponent = _mm256_sub_ps(avx_pDstLocInit , pHalfWidth);
+                __m256 pILocComponent = lasx_set1_f32(iLocComponent);
+                __m256 pJLocComponent = __lasx_xvfsub_s(avx_pDstLocInit , pHalfWidth);
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
@@ -676,8 +681,8 @@ RppStatus vignette_i8_i8_host_tensor(Rpp8s *srcPtr,
         srcPtrChannel = srcPtrImage + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
         dstPtrChannel = dstPtrImage;
 
-        __m256 pMultiplier = _mm256_set1_ps(multiplier);
-        __m256 pHalfWidth = _mm256_set1_ps(halfWidth);
+        __m256 pMultiplier = lasx_set1_f32(multiplier);
+        __m256 pHalfWidth = lasx_set1_f32(halfWidth);
 
         // Vignette with fused output-layout toggle (NHWC -> NCHW)
         if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
@@ -698,8 +703,8 @@ RppStatus vignette_i8_i8_host_tensor(Rpp8s *srcPtr,
 
                 Rpp32s iLoc = i - halfHeight;
                 Rpp32f iLocComponent = iLoc * iLoc;
-                __m256 pILocComponent = _mm256_set1_ps(iLocComponent);
-                __m256 pJLocComponent = _mm256_sub_ps(avx_pDstLocInit , pHalfWidth);
+                __m256 pILocComponent = lasx_set1_f32(iLocComponent);
+                __m256 pJLocComponent = __lasx_xvfsub_s(avx_pDstLocInit , pHalfWidth);
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 16)
@@ -755,8 +760,8 @@ RppStatus vignette_i8_i8_host_tensor(Rpp8s *srcPtr,
 
                 Rpp32s iLoc = i - halfHeight;
                 Rpp32f iLocComponent = iLoc * iLoc;
-                __m256 pILocComponent = _mm256_set1_ps(iLocComponent);
-                __m256 pJLocComponent = _mm256_sub_ps(avx_pDstLocInit , pHalfWidth);
+                __m256 pILocComponent = lasx_set1_f32(iLocComponent);
+                __m256 pJLocComponent = __lasx_xvfsub_s(avx_pDstLocInit , pHalfWidth);
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 16)
@@ -811,8 +816,8 @@ RppStatus vignette_i8_i8_host_tensor(Rpp8s *srcPtr,
 
                 Rpp32s iLoc = i - halfHeight;
                 Rpp32f iLocComponent = iLoc * iLoc;
-                __m256 pILocComponent = _mm256_set1_ps(iLocComponent);
-                __m256 pJLocComponent = _mm256_sub_ps(avx_pDstLocInit , pHalfWidth);
+                __m256 pILocComponent = lasx_set1_f32(iLocComponent);
+                __m256 pJLocComponent = __lasx_xvfsub_s(avx_pDstLocInit , pHalfWidth);
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 16)
@@ -870,8 +875,8 @@ RppStatus vignette_i8_i8_host_tensor(Rpp8s *srcPtr,
 
                 Rpp32s iLoc = i - halfHeight;
                 Rpp32f iLocComponent = iLoc * iLoc;
-                __m256 pILocComponent = _mm256_set1_ps(iLocComponent);
-                __m256 pJLocComponent = _mm256_sub_ps(avx_pDstLocInit , pHalfWidth);
+                __m256 pILocComponent = lasx_set1_f32(iLocComponent);
+                __m256 pJLocComponent = __lasx_xvfsub_s(avx_pDstLocInit , pHalfWidth);
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 16)
@@ -929,8 +934,8 @@ RppStatus vignette_i8_i8_host_tensor(Rpp8s *srcPtr,
 
                 Rpp32s iLoc = i - halfHeight;
                 Rpp32f iLocComponent = iLoc * iLoc;
-                __m256 pILocComponent = _mm256_set1_ps(iLocComponent);
-                __m256 pJLocComponent = _mm256_sub_ps(avx_pDstLocInit , pHalfWidth);
+                __m256 pILocComponent = lasx_set1_f32(iLocComponent);
+                __m256 pJLocComponent = __lasx_xvfsub_s(avx_pDstLocInit , pHalfWidth);
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 16)
@@ -1001,8 +1006,8 @@ RppStatus vignette_f16_f16_host_tensor(Rpp16f *srcPtr,
         srcPtrChannel = srcPtrImage + (roi.xywhROI.xy.y * srcDescPtr->strides.hStride) + (roi.xywhROI.xy.x * layoutParams.bufferMultiplier);
         dstPtrChannel = dstPtrImage;
 
-        __m256 pMultiplier = _mm256_set1_ps(multiplier);
-        __m256 pHalfWidth = _mm256_set1_ps(halfWidth);
+        __m256 pMultiplier = lasx_set1_f32(multiplier);
+        __m256 pHalfWidth = lasx_set1_f32(halfWidth);
 
         // Vignette with fused output-layout toggle (NHWC -> NCHW)
         if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
@@ -1023,8 +1028,8 @@ RppStatus vignette_f16_f16_host_tensor(Rpp16f *srcPtr,
 
                 Rpp32s iLoc = i - halfHeight;
                 Rpp32f iLocComponent = iLoc * iLoc;
-                __m256 pILocComponent = _mm256_set1_ps(iLocComponent);
-                __m256 pJLocComponent = _mm256_sub_ps(avx_pDstLocInit , pHalfWidth);
+                __m256 pILocComponent = lasx_set1_f32(iLocComponent);
+                __m256 pJLocComponent = __lasx_xvfsub_s(avx_pDstLocInit , pHalfWidth);
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
@@ -1088,8 +1093,8 @@ RppStatus vignette_f16_f16_host_tensor(Rpp16f *srcPtr,
 
                 Rpp32s iLoc = i - halfHeight;
                 Rpp32f iLocComponent = iLoc * iLoc;
-                __m256 pILocComponent = _mm256_set1_ps(iLocComponent);
-                __m256 pJLocComponent = _mm256_sub_ps(avx_pDstLocInit , pHalfWidth);
+                __m256 pILocComponent = lasx_set1_f32(iLocComponent);
+                __m256 pJLocComponent = __lasx_xvfsub_s(avx_pDstLocInit , pHalfWidth);
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
@@ -1150,8 +1155,8 @@ RppStatus vignette_f16_f16_host_tensor(Rpp16f *srcPtr,
 
                 Rpp32s iLoc = i - halfHeight;
                 Rpp32f iLocComponent = iLoc * iLoc;
-                __m256 pILocComponent = _mm256_set1_ps(iLocComponent);
-                __m256 pJLocComponent = _mm256_sub_ps(avx_pDstLocInit , pHalfWidth);
+                __m256 pILocComponent = lasx_set1_f32(iLocComponent);
+                __m256 pJLocComponent = __lasx_xvfsub_s(avx_pDstLocInit , pHalfWidth);
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
@@ -1213,8 +1218,8 @@ RppStatus vignette_f16_f16_host_tensor(Rpp16f *srcPtr,
 
                 Rpp32s iLoc = i - halfHeight;
                 Rpp32f iLocComponent = iLoc * iLoc;
-                __m256 pILocComponent = _mm256_set1_ps(iLocComponent);
-                __m256 pJLocComponent = _mm256_sub_ps(avx_pDstLocInit , pHalfWidth);
+                __m256 pILocComponent = lasx_set1_f32(iLocComponent);
+                __m256 pJLocComponent = __lasx_xvfsub_s(avx_pDstLocInit , pHalfWidth);
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
@@ -1282,8 +1287,8 @@ RppStatus vignette_f16_f16_host_tensor(Rpp16f *srcPtr,
 
                 Rpp32s iLoc = i - halfHeight;
                 Rpp32f iLocComponent = iLoc * iLoc;
-                __m256 pILocComponent = _mm256_set1_ps(iLocComponent);
-                __m256 pJLocComponent = _mm256_sub_ps(avx_pDstLocInit , pHalfWidth);
+                __m256 pILocComponent = lasx_set1_f32(iLocComponent);
+                __m256 pJLocComponent = __lasx_xvfsub_s(avx_pDstLocInit , pHalfWidth);
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += 8)
diff --git a/src/modules/cpu/kernel/warp_affine.hpp b/src/modules/cpu/kernel/warp_affine.hpp
index 4150637a..1d045c27 100644
--- a/src/modules/cpu/kernel/warp_affine.hpp
+++ b/src/modules/cpu/kernel/warp_affine.hpp
@@ -23,21 +23,36 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 /************* warp_affine helpers *************/
 
 inline void compute_warp_affine_src_loc_next_term_sse(__m128 &pSrcY, __m128 &pSrcX, __m128 &pAffineMatrixTerm3Incr, __m128 &pAffineMatrixTerm0Incr)
 {
+#ifdef __loongarch_sx
+    pSrcY = __lsx_vfadd_s(pSrcY, pAffineMatrixTerm3Incr);   // Vectorized computation of next 4 src Y locations by adding the delta from previous location
+    pSrcX = __lsx_vfadd_s(pSrcX, pAffineMatrixTerm0Incr);   // Vectorized computation of next 4 src X locations by adding the delta from previous location
+#else
     pSrcY = _mm_add_ps(pSrcY, pAffineMatrixTerm3Incr);   // Vectorized computation of next 4 src Y locations by adding the delta from previous location
     pSrcX = _mm_add_ps(pSrcX, pAffineMatrixTerm0Incr);   // Vectorized computation of next 4 src X locations by adding the delta from previous location
+#endif
 }
 
 inline void compute_warp_affine_src_loc_next_term_avx(__m256 &pSrcY, __m256 &pSrcX, __m256 &pAffineMatrixTerm3Incr, __m256 &pAffineMatrixTerm0Incr)
 {
+#ifdef __loongarch_sx
+    pSrcY = __lasx_xvfadd_s(pSrcY, pAffineMatrixTerm3Incr);   // Vectorized computation of next 8 src Y locations by adding the delta from previous location
+    pSrcX = __lasx_xvfadd_s(pSrcX, pAffineMatrixTerm0Incr);   // Vectorized computation of next 8 src X locations by adding the delta from previous location
+#else
     pSrcY = _mm256_add_ps(pSrcY, pAffineMatrixTerm3Incr);   // Vectorized computation of next 8 src Y locations by adding the delta from previous location
     pSrcX = _mm256_add_ps(pSrcX, pAffineMatrixTerm0Incr);   // Vectorized computation of next 8 src X locations by adding the delta from previous location
+#endif
 }
 
 inline void compute_warp_affine_src_loc(Rpp32s dstY, Rpp32s dstX, Rpp32f &srcY, Rpp32f &srcX, Rpp32f6 *affineMatrix_f6, Rpp32s roiHalfHeight, Rpp32s roiHalfWidth)
@@ -96,6 +111,18 @@ RppStatus warp_affine_nn_u8_u8_host_tensor(Rpp8u *srcPtr,
         Rpp32s srcLoc[4] = {0};         // Since 4 dst pixels are processed per iteration
         Rpp32s invalidLoad[4] = {0};    // Since 4 dst pixels are processed per iteration
 
+#ifdef __loongarch_sx
+        __m128 pSrcStrideH = lsx_set1_f32(srcDescPtr->strides.hStride);
+        __m128 pAffineMatrixTerm0 = lsx_setr_f32(0, affineMatrix_f6->data[0], affineMatrix_f6->data[0] * 2, affineMatrix_f6->data[0] * 3);
+        __m128 pAffineMatrixTerm3 = lsx_setr_f32(0, affineMatrix_f6->data[3], affineMatrix_f6->data[3] * 2, affineMatrix_f6->data[3] * 3);
+        __m128 pAffineMatrixTerm0Incr = lsx_set1_f32(affineMatrix_f6->data[0] * 4);
+        __m128 pAffineMatrixTerm3Incr = lsx_set1_f32(affineMatrix_f6->data[3] * 4);
+        __m128 pRoiLTRB[4];
+        pRoiLTRB[0] = lsx_set1_f32(roiLTRB.ltrbROI.lt.x);
+        pRoiLTRB[1] = lsx_set1_f32(roiLTRB.ltrbROI.lt.y);
+        pRoiLTRB[2] = lsx_set1_f32(roiLTRB.ltrbROI.rb.x);
+        pRoiLTRB[3] = lsx_set1_f32(roiLTRB.ltrbROI.rb.y);
+#else
         __m128 pSrcStrideH = _mm_set1_ps(srcDescPtr->strides.hStride);
         __m128 pAffineMatrixTerm0 = _mm_setr_ps(0, affineMatrix_f6->data[0], affineMatrix_f6->data[0] * 2, affineMatrix_f6->data[0] * 3);
         __m128 pAffineMatrixTerm3 = _mm_setr_ps(0, affineMatrix_f6->data[3], affineMatrix_f6->data[3] * 2, affineMatrix_f6->data[3] * 3);
@@ -106,6 +133,7 @@ RppStatus warp_affine_nn_u8_u8_host_tensor(Rpp8u *srcPtr,
         pRoiLTRB[1] = _mm_set1_ps(roiLTRB.ltrbROI.lt.y);
         pRoiLTRB[2] = _mm_set1_ps(roiLTRB.ltrbROI.rb.x);
         pRoiLTRB[3] = _mm_set1_ps(roiLTRB.ltrbROI.rb.y);
+#endif
 
         // Warp Affine with fused output-layout toggle (NHWC -> NCHW)
         if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
@@ -126,8 +154,13 @@ RppStatus warp_affine_nn_u8_u8_host_tensor(Rpp8u *srcPtr,
                 Rpp32f srcX, srcY;
                 __m128 pSrcX, pSrcY;
                 compute_warp_affine_src_loc(i, vectorLoopCount, srcY, srcX, affineMatrix_f6, roiHalfHeight, roiHalfWidth);
+#ifdef __loongarch_sx
+                pSrcY = __lsx_vfadd_s(lsx_set1_f32(srcY), pAffineMatrixTerm3);
+                pSrcX = __lsx_vfadd_s(lsx_set1_f32(srcX), pAffineMatrixTerm0);
+#else
                 pSrcY = _mm_add_ps(_mm_set1_ps(srcY), pAffineMatrixTerm3);
                 pSrcX = _mm_add_ps(_mm_set1_ps(srcX), pAffineMatrixTerm0);
+#endif
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m128i pRow;
@@ -171,8 +204,13 @@ RppStatus warp_affine_nn_u8_u8_host_tensor(Rpp8u *srcPtr,
                 Rpp32f srcX, srcY;
                 __m128 pSrcX, pSrcY;
                 compute_warp_affine_src_loc(i, vectorLoopCount, srcY, srcX, affineMatrix_f6, roiHalfHeight, roiHalfWidth);
+#ifdef __loongarch_sx
+                pSrcY = __lsx_vfadd_s(lsx_set1_f32(srcY), pAffineMatrixTerm3);
+                pSrcX = __lsx_vfadd_s(lsx_set1_f32(srcX), pAffineMatrixTerm0);
+#else
                 pSrcY = _mm_add_ps(_mm_set1_ps(srcY), pAffineMatrixTerm3);
                 pSrcX = _mm_add_ps(_mm_set1_ps(srcX), pAffineMatrixTerm0);
+#endif
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m128i pRow[3];
@@ -211,8 +249,13 @@ RppStatus warp_affine_nn_u8_u8_host_tensor(Rpp8u *srcPtr,
                 Rpp32f srcX, srcY;
                 __m128 pSrcX, pSrcY;
                 compute_warp_affine_src_loc(i, vectorLoopCount, srcY, srcX, affineMatrix_f6, roiHalfHeight, roiHalfWidth);
+#ifdef __loongarch_sx
+                pSrcY = __lsx_vfadd_s(lsx_set1_f32(srcY), pAffineMatrixTerm3);
+                pSrcX = __lsx_vfadd_s(lsx_set1_f32(srcX), pAffineMatrixTerm0);
+#else
                 pSrcY = _mm_add_ps(_mm_set1_ps(srcY), pAffineMatrixTerm3);
                 pSrcX = _mm_add_ps(_mm_set1_ps(srcX), pAffineMatrixTerm0);
+#endif
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m128i pRow;
@@ -249,8 +292,13 @@ RppStatus warp_affine_nn_u8_u8_host_tensor(Rpp8u *srcPtr,
                 Rpp32f srcX, srcY;
                 __m128 pSrcX, pSrcY;
                 compute_warp_affine_src_loc(i, vectorLoopCount, srcY, srcX, affineMatrix_f6, roiHalfHeight, roiHalfWidth);
+#ifdef __loongarch_sx
+                pSrcY = __lsx_vfadd_s(lsx_set1_f32(srcY), pAffineMatrixTerm3);
+                pSrcX = __lsx_vfadd_s(lsx_set1_f32(srcX), pAffineMatrixTerm0);
+#else
                 pSrcY = _mm_add_ps(_mm_set1_ps(srcY), pAffineMatrixTerm3);
                 pSrcX = _mm_add_ps(_mm_set1_ps(srcX), pAffineMatrixTerm0);
+#endif
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     Rpp8u *dstPtrTempChn, *srcPtrTempChn;
@@ -323,6 +371,18 @@ RppStatus warp_affine_nn_f32_f32_host_tensor(Rpp32f *srcPtr,
         Rpp32s srcLoc[4] = {0};         // Since 4 dst pixels are processed per iteration
         Rpp32s invalidLoad[4] = {0};    // Since 4 dst pixels are processed per iteration
 
+#ifdef __loongarch_sx
+        __m128 pSrcStrideH = lsx_set1_f32(srcDescPtr->strides.hStride);
+        __m128 pAffineMatrixTerm0 = lsx_setr_f32(0, affineMatrix_f6->data[0], affineMatrix_f6->data[0] * 2, affineMatrix_f6->data[0] * 3);
+        __m128 pAffineMatrixTerm3 = lsx_setr_f32(0, affineMatrix_f6->data[3], affineMatrix_f6->data[3] * 2, affineMatrix_f6->data[3] * 3);
+        __m128 pAffineMatrixTerm0Incr = lsx_set1_f32(affineMatrix_f6->data[0] * 4);
+        __m128 pAffineMatrixTerm3Incr = lsx_set1_f32(affineMatrix_f6->data[3] * 4);
+        __m128 pRoiLTRB[4];
+        pRoiLTRB[0] = lsx_set1_f32(roiLTRB.ltrbROI.lt.x);
+        pRoiLTRB[1] = lsx_set1_f32(roiLTRB.ltrbROI.lt.y);
+        pRoiLTRB[2] = lsx_set1_f32(roiLTRB.ltrbROI.rb.x);
+        pRoiLTRB[3] = lsx_set1_f32(roiLTRB.ltrbROI.rb.y);
+#else
         __m128 pSrcStrideH = _mm_set1_ps(srcDescPtr->strides.hStride);
         __m128 pAffineMatrixTerm0 = _mm_setr_ps(0, affineMatrix_f6->data[0], affineMatrix_f6->data[0] * 2, affineMatrix_f6->data[0] * 3);
         __m128 pAffineMatrixTerm3 = _mm_setr_ps(0, affineMatrix_f6->data[3], affineMatrix_f6->data[3] * 2, affineMatrix_f6->data[3] * 3);
@@ -333,6 +393,7 @@ RppStatus warp_affine_nn_f32_f32_host_tensor(Rpp32f *srcPtr,
         pRoiLTRB[1] = _mm_set1_ps(roiLTRB.ltrbROI.lt.y);
         pRoiLTRB[2] = _mm_set1_ps(roiLTRB.ltrbROI.rb.x);
         pRoiLTRB[3] = _mm_set1_ps(roiLTRB.ltrbROI.rb.y);
+#endif
 
         // Warp Affine with fused output-layout toggle (NHWC -> NCHW)
         if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
@@ -353,8 +414,13 @@ RppStatus warp_affine_nn_f32_f32_host_tensor(Rpp32f *srcPtr,
                 Rpp32f srcX, srcY;
                 __m128 pSrcX, pSrcY;
                 compute_warp_affine_src_loc(i, vectorLoopCount, srcY, srcX, affineMatrix_f6, roiHalfHeight, roiHalfWidth);
+#ifdef __loongarch_sx
+                pSrcY = __lsx_vfadd_s(lsx_set1_f32(srcY), pAffineMatrixTerm3);
+                pSrcX = __lsx_vfadd_s(lsx_set1_f32(srcX), pAffineMatrixTerm0);
+#else
                 pSrcY = _mm_add_ps(_mm_set1_ps(srcY), pAffineMatrixTerm3);
                 pSrcX = _mm_add_ps(_mm_set1_ps(srcX), pAffineMatrixTerm0);
+#endif
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m128 pRow[3];
@@ -398,8 +464,13 @@ RppStatus warp_affine_nn_f32_f32_host_tensor(Rpp32f *srcPtr,
                 Rpp32f srcX, srcY;
                 __m128 pSrcX, pSrcY;
                 compute_warp_affine_src_loc(i, vectorLoopCount, srcY, srcX, affineMatrix_f6, roiHalfHeight, roiHalfWidth);
+#ifdef __loongarch_sx
+                pSrcY = __lsx_vfadd_s(lsx_set1_f32(srcY), pAffineMatrixTerm3);
+                pSrcX = __lsx_vfadd_s(lsx_set1_f32(srcX), pAffineMatrixTerm0);
+#else
                 pSrcY = _mm_add_ps(_mm_set1_ps(srcY), pAffineMatrixTerm3);
                 pSrcX = _mm_add_ps(_mm_set1_ps(srcX), pAffineMatrixTerm0);
+#endif
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m128 pRow[4];
@@ -438,8 +509,13 @@ RppStatus warp_affine_nn_f32_f32_host_tensor(Rpp32f *srcPtr,
                 Rpp32f srcX, srcY;
                 __m128 pSrcX, pSrcY;
                 compute_warp_affine_src_loc(i, vectorLoopCount, srcY, srcX, affineMatrix_f6, roiHalfHeight, roiHalfWidth);
+#ifdef __loongarch_sx
+                pSrcY = __lsx_vfadd_s(lsx_set1_f32(srcY), pAffineMatrixTerm3);
+                pSrcX = __lsx_vfadd_s(lsx_set1_f32(srcX), pAffineMatrixTerm0);
+#else
                 pSrcY = _mm_add_ps(_mm_set1_ps(srcY), pAffineMatrixTerm3);
                 pSrcX = _mm_add_ps(_mm_set1_ps(srcX), pAffineMatrixTerm0);
+#endif
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m128 pRow[4];
@@ -476,8 +552,13 @@ RppStatus warp_affine_nn_f32_f32_host_tensor(Rpp32f *srcPtr,
                 Rpp32f srcX, srcY;
                 __m128 pSrcX, pSrcY;
                 compute_warp_affine_src_loc(i, vectorLoopCount, srcY, srcX, affineMatrix_f6, roiHalfHeight, roiHalfWidth);
+#ifdef __loongarch_sx
+                pSrcY = __lsx_vfadd_s(lsx_set1_f32(srcY), pAffineMatrixTerm3);
+                pSrcX = __lsx_vfadd_s(lsx_set1_f32(srcX), pAffineMatrixTerm0);
+#else
                 pSrcY = _mm_add_ps(_mm_set1_ps(srcY), pAffineMatrixTerm3);
                 pSrcX = _mm_add_ps(_mm_set1_ps(srcX), pAffineMatrixTerm0);
+#endif
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     Rpp32f *dstPtrTempChn, *srcPtrTempChn;
@@ -550,6 +631,18 @@ RppStatus warp_affine_nn_i8_i8_host_tensor(Rpp8s *srcPtr,
         Rpp32s srcLoc[4] = {0};         // Since 4 dst pixels are processed per iteration
         Rpp32s invalidLoad[4] = {0};    // Since 4 dst pixels are processed per iteration
 
+#ifdef __loongarch_sx
+        __m128 pSrcStrideH = lsx_set1_f32(srcDescPtr->strides.hStride);
+        __m128 pAffineMatrixTerm0 = lsx_setr_f32(0, affineMatrix_f6->data[0], affineMatrix_f6->data[0] * 2, affineMatrix_f6->data[0] * 3);
+        __m128 pAffineMatrixTerm3 = lsx_setr_f32(0, affineMatrix_f6->data[3], affineMatrix_f6->data[3] * 2, affineMatrix_f6->data[3] * 3);
+        __m128 pAffineMatrixTerm0Incr = lsx_set1_f32(affineMatrix_f6->data[0] * 4);
+        __m128 pAffineMatrixTerm3Incr = lsx_set1_f32(affineMatrix_f6->data[3] * 4);
+        __m128 pRoiLTRB[4];
+        pRoiLTRB[0] = lsx_set1_f32(roiLTRB.ltrbROI.lt.x);
+        pRoiLTRB[1] = lsx_set1_f32(roiLTRB.ltrbROI.lt.y);
+        pRoiLTRB[2] = lsx_set1_f32(roiLTRB.ltrbROI.rb.x);
+        pRoiLTRB[3] = lsx_set1_f32(roiLTRB.ltrbROI.rb.y);
+#else
         __m128 pSrcStrideH = _mm_set1_ps(srcDescPtr->strides.hStride);
         __m128 pAffineMatrixTerm0 = _mm_setr_ps(0, affineMatrix_f6->data[0], affineMatrix_f6->data[0] * 2, affineMatrix_f6->data[0] * 3);
         __m128 pAffineMatrixTerm3 = _mm_setr_ps(0, affineMatrix_f6->data[3], affineMatrix_f6->data[3] * 2, affineMatrix_f6->data[3] * 3);
@@ -560,6 +653,7 @@ RppStatus warp_affine_nn_i8_i8_host_tensor(Rpp8s *srcPtr,
         pRoiLTRB[1] = _mm_set1_ps(roiLTRB.ltrbROI.lt.y);
         pRoiLTRB[2] = _mm_set1_ps(roiLTRB.ltrbROI.rb.x);
         pRoiLTRB[3] = _mm_set1_ps(roiLTRB.ltrbROI.rb.y);
+#endif
 
         // Warp Affine with fused output-layout toggle (NHWC -> NCHW)
         if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
@@ -580,8 +674,13 @@ RppStatus warp_affine_nn_i8_i8_host_tensor(Rpp8s *srcPtr,
                 Rpp32f srcX, srcY;
                 __m128 pSrcX, pSrcY;
                 compute_warp_affine_src_loc(i, vectorLoopCount, srcY, srcX, affineMatrix_f6, roiHalfHeight, roiHalfWidth);
+#ifdef __loongarch_sx
+                pSrcY = __lsx_vfadd_s(lsx_set1_f32(srcY), pAffineMatrixTerm3);
+                pSrcX = __lsx_vfadd_s(lsx_set1_f32(srcX), pAffineMatrixTerm0);
+#else
                 pSrcY = _mm_add_ps(_mm_set1_ps(srcY), pAffineMatrixTerm3);
                 pSrcX = _mm_add_ps(_mm_set1_ps(srcX), pAffineMatrixTerm0);
+#endif
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m128i pRow;
@@ -625,8 +724,13 @@ RppStatus warp_affine_nn_i8_i8_host_tensor(Rpp8s *srcPtr,
                 Rpp32f srcX, srcY;
                 __m128 pSrcX, pSrcY;
                 compute_warp_affine_src_loc(i, vectorLoopCount, srcY, srcX, affineMatrix_f6, roiHalfHeight, roiHalfWidth);
+#ifdef __loongarch_sx
+                pSrcY = __lsx_vfadd_s(lsx_set1_f32(srcY), pAffineMatrixTerm3);
+                pSrcX = __lsx_vfadd_s(lsx_set1_f32(srcX), pAffineMatrixTerm0);
+#else
                 pSrcY = _mm_add_ps(_mm_set1_ps(srcY), pAffineMatrixTerm3);
                 pSrcX = _mm_add_ps(_mm_set1_ps(srcX), pAffineMatrixTerm0);
+#endif
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m128i pRow[3];
@@ -665,8 +769,13 @@ RppStatus warp_affine_nn_i8_i8_host_tensor(Rpp8s *srcPtr,
                 Rpp32f srcX, srcY;
                 __m128 pSrcX, pSrcY;
                 compute_warp_affine_src_loc(i, vectorLoopCount, srcY, srcX, affineMatrix_f6, roiHalfHeight, roiHalfWidth);
+#ifdef __loongarch_sx
+                pSrcY = __lsx_vfadd_s(lsx_set1_f32(srcY), pAffineMatrixTerm3);
+                pSrcX = __lsx_vfadd_s(lsx_set1_f32(srcX), pAffineMatrixTerm0);
+#else
                 pSrcY = _mm_add_ps(_mm_set1_ps(srcY), pAffineMatrixTerm3);
                 pSrcX = _mm_add_ps(_mm_set1_ps(srcX), pAffineMatrixTerm0);
+#endif
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m128i pRow;
@@ -703,8 +812,13 @@ RppStatus warp_affine_nn_i8_i8_host_tensor(Rpp8s *srcPtr,
                 Rpp32f srcX, srcY;
                 __m128 pSrcX, pSrcY;
                 compute_warp_affine_src_loc(i, vectorLoopCount, srcY, srcX, affineMatrix_f6, roiHalfHeight, roiHalfWidth);
+#ifdef __loongarch_sx
+                pSrcY = __lsx_vfadd_s(lsx_set1_f32(srcY), pAffineMatrixTerm3);
+                pSrcX = __lsx_vfadd_s(lsx_set1_f32(srcX), pAffineMatrixTerm0);
+#else
                 pSrcY = _mm_add_ps(_mm_set1_ps(srcY), pAffineMatrixTerm3);
                 pSrcX = _mm_add_ps(_mm_set1_ps(srcX), pAffineMatrixTerm0);
+#endif
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     Rpp8s *dstPtrTempChn, *srcPtrTempChn;
@@ -919,6 +1033,24 @@ RppStatus warp_affine_bilinear_u8_u8_host_tensor(Rpp8u *srcPtr,
         Rpp32u bufferLength = roi.xywhROI.roiWidth;
         Rpp32u alignedLength = bufferLength & ~7;   // Align dst width to process 8 dst pixels per iteration
 
+#ifdef __loongarch_asx
+        __m256 pBilinearCoeffs[4];
+        __m256 pSrcStrideH = lasx_set1_f32(srcDescPtr->strides.hStride);
+        __m256 pAffineMatrixTerm0 = lasx_setr_f32(0, affineMatrix_f6->data[0], affineMatrix_f6->data[0] * 2, affineMatrix_f6->data[0] * 3, affineMatrix_f6->data[0] * 4, affineMatrix_f6->data[0] * 5, affineMatrix_f6->data[0] * 6, affineMatrix_f6->data[0] * 7);
+        __m256 pAffineMatrixTerm3 = lasx_setr_f32(0, affineMatrix_f6->data[3], affineMatrix_f6->data[3] * 2, affineMatrix_f6->data[3] * 3, affineMatrix_f6->data[3] * 4, affineMatrix_f6->data[3] * 5, affineMatrix_f6->data[3] * 6, affineMatrix_f6->data[3] * 7);
+        __m256 pAffineMatrixTerm0Incr = lasx_set1_f32(affineMatrix_f6->data[0] * 8);
+        __m256 pAffineMatrixTerm3Incr = lasx_set1_f32(affineMatrix_f6->data[3] * 8);
+        __m256 pRoiLTRB[4];
+        pRoiLTRB[0] = lasx_set1_f32(roiLTRB.ltrbROI.lt.x);
+        pRoiLTRB[1] = lasx_set1_f32(roiLTRB.ltrbROI.lt.y);
+        pRoiLTRB[2] = lasx_set1_f32(roiLTRB.ltrbROI.rb.x);
+        pRoiLTRB[3] = lasx_set1_f32(roiLTRB.ltrbROI.rb.y);
+        __m256i pxSrcStridesCHW[3];
+        pxSrcStridesCHW[0] = __lasx_xvreplgr2vr_w(srcDescPtr->strides.cStride);
+        pxSrcStridesCHW[1] = __lasx_xvreplgr2vr_w(srcDescPtr->strides.hStride);
+        pxSrcStridesCHW[2] = __lasx_xvreplgr2vr_w(srcDescPtr->strides.wStride);
+        RpptBilinearNbhoodLocsVecLen8 srcLocs;
+#else
         __m256 pBilinearCoeffs[4];
         __m256 pSrcStrideH = _mm256_set1_ps(srcDescPtr->strides.hStride);
         __m256 pAffineMatrixTerm0 = _mm256_setr_ps(0, affineMatrix_f6->data[0], affineMatrix_f6->data[0] * 2, affineMatrix_f6->data[0] * 3, affineMatrix_f6->data[0] * 4, affineMatrix_f6->data[0] * 5, affineMatrix_f6->data[0] * 6, affineMatrix_f6->data[0] * 7);
@@ -935,6 +1067,7 @@ RppStatus warp_affine_bilinear_u8_u8_host_tensor(Rpp8u *srcPtr,
         pxSrcStridesCHW[1] = _mm256_set1_epi32(srcDescPtr->strides.hStride);
         pxSrcStridesCHW[2] = _mm256_set1_epi32(srcDescPtr->strides.wStride);
         RpptBilinearNbhoodLocsVecLen8 srcLocs;
+#endif
 
         // Warp Affine with fused output-layout toggle (NHWC -> NCHW)
         if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
@@ -955,8 +1088,13 @@ RppStatus warp_affine_bilinear_u8_u8_host_tensor(Rpp8u *srcPtr,
                 Rpp32f srcX, srcY;
                 __m256 pSrcX, pSrcY;
                 compute_warp_affine_src_loc(i, vectorLoopCount, srcY, srcX, affineMatrix_f6, roiHalfHeight, roiHalfWidth);
+#ifdef __loongarch_asx
+                pSrcY = __lasx_xvfadd_s(lasx_set1_f32(srcY), pAffineMatrixTerm3);
+                pSrcX = __lasx_xvfadd_s(lasx_set1_f32(srcX), pAffineMatrixTerm0);
+#else
                 pSrcY = _mm256_add_ps(_mm256_set1_ps(srcY), pAffineMatrixTerm3);
                 pSrcX = _mm256_add_ps(_mm256_set1_ps(srcX), pAffineMatrixTerm0);
+#endif
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pSrc[12], pDst[3];
@@ -997,8 +1135,13 @@ RppStatus warp_affine_bilinear_u8_u8_host_tensor(Rpp8u *srcPtr,
                 Rpp32f srcX, srcY;
                 __m256 pSrcX, pSrcY;
                 compute_warp_affine_src_loc(i, vectorLoopCount, srcY, srcX, affineMatrix_f6, roiHalfHeight, roiHalfWidth);
+#ifdef __loongarch_asx
+                pSrcY = __lasx_xvfadd_s(lasx_set1_f32(srcY), pAffineMatrixTerm3);
+                pSrcX = __lasx_xvfadd_s(lasx_set1_f32(srcX), pAffineMatrixTerm0);
+#else
                 pSrcY = _mm256_add_ps(_mm256_set1_ps(srcY), pAffineMatrixTerm3);
                 pSrcX = _mm256_add_ps(_mm256_set1_ps(srcX), pAffineMatrixTerm0);
+#endif
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pSrc[12], pDst[3];
@@ -1036,8 +1179,13 @@ RppStatus warp_affine_bilinear_u8_u8_host_tensor(Rpp8u *srcPtr,
                 Rpp32f srcX, srcY;
                 __m256 pSrcX, pSrcY;
                 compute_warp_affine_src_loc(i, vectorLoopCount, srcY, srcX, affineMatrix_f6, roiHalfHeight, roiHalfWidth);
+#ifdef __loongarch_asx
+                pSrcY = __lasx_xvfadd_s(lasx_set1_f32(srcY), pAffineMatrixTerm3);
+                pSrcX = __lasx_xvfadd_s(lasx_set1_f32(srcX), pAffineMatrixTerm0);
+#else
                 pSrcY = _mm256_add_ps(_mm256_set1_ps(srcY), pAffineMatrixTerm3);
                 pSrcX = _mm256_add_ps(_mm256_set1_ps(srcX), pAffineMatrixTerm0);
+#endif
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pSrc[12], pDst[3];
@@ -1079,8 +1227,13 @@ RppStatus warp_affine_bilinear_u8_u8_host_tensor(Rpp8u *srcPtr,
                 Rpp32f srcX, srcY;
                 __m256 pSrcX, pSrcY;
                 compute_warp_affine_src_loc(i, vectorLoopCount, srcY, srcX, affineMatrix_f6, roiHalfHeight, roiHalfWidth);
+#ifdef __loongarch_asx
+                pSrcY = __lasx_xvfadd_s(lasx_set1_f32(srcY), pAffineMatrixTerm3);
+                pSrcX = __lasx_xvfadd_s(lasx_set1_f32(srcX), pAffineMatrixTerm0);
+#else
                 pSrcY = _mm256_add_ps(_mm256_set1_ps(srcY), pAffineMatrixTerm3);
                 pSrcX = _mm256_add_ps(_mm256_set1_ps(srcX), pAffineMatrixTerm0);
+#endif
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pSrc[12], pDst[3];
@@ -1121,8 +1274,13 @@ RppStatus warp_affine_bilinear_u8_u8_host_tensor(Rpp8u *srcPtr,
                 Rpp32f srcX, srcY;
                 __m256 pSrcX, pSrcY;
                 compute_warp_affine_src_loc(i, vectorLoopCount, srcY, srcX, affineMatrix_f6, roiHalfHeight, roiHalfWidth);
+#ifdef __loongarch_asx
+                pSrcY = __lasx_xvfadd_s(lasx_set1_f32(srcY), pAffineMatrixTerm3);
+                pSrcX = __lasx_xvfadd_s(lasx_set1_f32(srcX), pAffineMatrixTerm0);
+#else
                 pSrcY = _mm256_add_ps(_mm256_set1_ps(srcY), pAffineMatrixTerm3);
                 pSrcX = _mm256_add_ps(_mm256_set1_ps(srcX), pAffineMatrixTerm0);
+#endif
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pSrc[4], pDst;
@@ -1186,6 +1344,24 @@ RppStatus warp_affine_bilinear_f32_f32_host_tensor(Rpp32f *srcPtr,
         Rpp32u bufferLength = roi.xywhROI.roiWidth;
         Rpp32u alignedLength = bufferLength & ~7;   // Align dst width to process 8 dst pixels per iteration
 
+#ifdef __loongarch_asx
+        __m256 pBilinearCoeffs[4];
+        __m256 pSrcStrideH = lasx_set1_f32(srcDescPtr->strides.hStride);
+        __m256 pAffineMatrixTerm0 = lasx_setr_f32(0, affineMatrix_f6->data[0], affineMatrix_f6->data[0] * 2, affineMatrix_f6->data[0] * 3, affineMatrix_f6->data[0] * 4, affineMatrix_f6->data[0] * 5, affineMatrix_f6->data[0] * 6, affineMatrix_f6->data[0] * 7);
+        __m256 pAffineMatrixTerm3 = lasx_setr_f32(0, affineMatrix_f6->data[3], affineMatrix_f6->data[3] * 2, affineMatrix_f6->data[3] * 3, affineMatrix_f6->data[3] * 4, affineMatrix_f6->data[3] * 5, affineMatrix_f6->data[3] * 6, affineMatrix_f6->data[3] * 7);
+        __m256 pAffineMatrixTerm0Incr = lasx_set1_f32(affineMatrix_f6->data[0] * 8);
+        __m256 pAffineMatrixTerm3Incr = lasx_set1_f32(affineMatrix_f6->data[3] * 8);
+        __m256 pRoiLTRB[4];
+        pRoiLTRB[0] = lasx_set1_f32(roiLTRB.ltrbROI.lt.x);
+        pRoiLTRB[1] = lasx_set1_f32(roiLTRB.ltrbROI.lt.y);
+        pRoiLTRB[2] = lasx_set1_f32(roiLTRB.ltrbROI.rb.x);
+        pRoiLTRB[3] = lasx_set1_f32(roiLTRB.ltrbROI.rb.y);
+        __m256i pxSrcStridesCHW[3];
+        pxSrcStridesCHW[0] = __lasx_xvreplgr2vr_w(srcDescPtr->strides.cStride);
+        pxSrcStridesCHW[1] = __lasx_xvreplgr2vr_w(srcDescPtr->strides.hStride);
+        pxSrcStridesCHW[2] = __lasx_xvreplgr2vr_w(srcDescPtr->strides.wStride);
+        RpptBilinearNbhoodLocsVecLen8 srcLocs;
+#else
         __m256 pBilinearCoeffs[4];
         __m256 pSrcStrideH = _mm256_set1_ps(srcDescPtr->strides.hStride);
         __m256 pAffineMatrixTerm0 = _mm256_setr_ps(0, affineMatrix_f6->data[0], affineMatrix_f6->data[0] * 2, affineMatrix_f6->data[0] * 3, affineMatrix_f6->data[0] * 4, affineMatrix_f6->data[0] * 5, affineMatrix_f6->data[0] * 6, affineMatrix_f6->data[0] * 7);
@@ -1202,6 +1378,7 @@ RppStatus warp_affine_bilinear_f32_f32_host_tensor(Rpp32f *srcPtr,
         pxSrcStridesCHW[1] = _mm256_set1_epi32(srcDescPtr->strides.hStride);
         pxSrcStridesCHW[2] = _mm256_set1_epi32(srcDescPtr->strides.wStride);
         RpptBilinearNbhoodLocsVecLen8 srcLocs;
+#endif
 
         // Warp Affine with fused output-layout toggle (NHWC -> NCHW)
         if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
@@ -1222,8 +1399,13 @@ RppStatus warp_affine_bilinear_f32_f32_host_tensor(Rpp32f *srcPtr,
                 Rpp32f srcX, srcY;
                 __m256 pSrcX, pSrcY;
                 compute_warp_affine_src_loc(i, vectorLoopCount, srcY, srcX, affineMatrix_f6, roiHalfHeight, roiHalfWidth);
+#ifdef __loongarch_asx
+                pSrcY = __lasx_xvfadd_s(lasx_set1_f32(srcY), pAffineMatrixTerm3);
+                pSrcX = __lasx_xvfadd_s(lasx_set1_f32(srcX), pAffineMatrixTerm0);
+#else
                 pSrcY = _mm256_add_ps(_mm256_set1_ps(srcY), pAffineMatrixTerm3);
                 pSrcX = _mm256_add_ps(_mm256_set1_ps(srcX), pAffineMatrixTerm0);
+#endif
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pSrc[12], pDst[3];
@@ -1264,8 +1446,13 @@ RppStatus warp_affine_bilinear_f32_f32_host_tensor(Rpp32f *srcPtr,
                 Rpp32f srcX, srcY;
                 __m256 pSrcX, pSrcY;
                 compute_warp_affine_src_loc(i, vectorLoopCount, srcY, srcX, affineMatrix_f6, roiHalfHeight, roiHalfWidth);
+#ifdef __loongarch_asx
+                pSrcY = __lasx_xvfadd_s(lasx_set1_f32(srcY), pAffineMatrixTerm3);
+                pSrcX = __lasx_xvfadd_s(lasx_set1_f32(srcX), pAffineMatrixTerm0);
+#else
                 pSrcY = _mm256_add_ps(_mm256_set1_ps(srcY), pAffineMatrixTerm3);
                 pSrcX = _mm256_add_ps(_mm256_set1_ps(srcX), pAffineMatrixTerm0);
+#endif
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pSrc[12], pDst[3];
@@ -1303,8 +1490,13 @@ RppStatus warp_affine_bilinear_f32_f32_host_tensor(Rpp32f *srcPtr,
                 Rpp32f srcX, srcY;
                 __m256 pSrcX, pSrcY;
                 compute_warp_affine_src_loc(i, vectorLoopCount, srcY, srcX, affineMatrix_f6, roiHalfHeight, roiHalfWidth);
+#ifdef __loongarch_asx
+                pSrcY = __lasx_xvfadd_s(lasx_set1_f32(srcY), pAffineMatrixTerm3);
+                pSrcX = __lasx_xvfadd_s(lasx_set1_f32(srcX), pAffineMatrixTerm0);
+#else
                 pSrcY = _mm256_add_ps(_mm256_set1_ps(srcY), pAffineMatrixTerm3);
                 pSrcX = _mm256_add_ps(_mm256_set1_ps(srcX), pAffineMatrixTerm0);
+#endif
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pSrc[12], pDst[3];
@@ -1346,8 +1538,13 @@ RppStatus warp_affine_bilinear_f32_f32_host_tensor(Rpp32f *srcPtr,
                 Rpp32f srcX, srcY;
                 __m256 pSrcX, pSrcY;
                 compute_warp_affine_src_loc(i, vectorLoopCount, srcY, srcX, affineMatrix_f6, roiHalfHeight, roiHalfWidth);
+#ifdef __loongarch_asx
+                pSrcY = __lasx_xvfadd_s(lasx_set1_f32(srcY), pAffineMatrixTerm3);
+                pSrcX = __lasx_xvfadd_s(lasx_set1_f32(srcX), pAffineMatrixTerm0);
+#else
                 pSrcY = _mm256_add_ps(_mm256_set1_ps(srcY), pAffineMatrixTerm3);
                 pSrcX = _mm256_add_ps(_mm256_set1_ps(srcX), pAffineMatrixTerm0);
+#endif
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pSrc[12], pDst[3];
@@ -1388,8 +1585,13 @@ RppStatus warp_affine_bilinear_f32_f32_host_tensor(Rpp32f *srcPtr,
                 Rpp32f srcX, srcY;
                 __m256 pSrcX, pSrcY;
                 compute_warp_affine_src_loc(i, vectorLoopCount, srcY, srcX, affineMatrix_f6, roiHalfHeight, roiHalfWidth);
+#ifdef __loongarch_asx
+                pSrcY = __lasx_xvfadd_s(lasx_set1_f32(srcY), pAffineMatrixTerm3);
+                pSrcX = __lasx_xvfadd_s(lasx_set1_f32(srcX), pAffineMatrixTerm0);
+#else
                 pSrcY = _mm256_add_ps(_mm256_set1_ps(srcY), pAffineMatrixTerm3);
                 pSrcX = _mm256_add_ps(_mm256_set1_ps(srcX), pAffineMatrixTerm0);
+#endif
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pSrc[4], pDst;
@@ -1453,6 +1655,24 @@ RppStatus warp_affine_bilinear_i8_i8_host_tensor(Rpp8s *srcPtr,
         Rpp32u bufferLength = roi.xywhROI.roiWidth;
         Rpp32u alignedLength = bufferLength & ~7;   // Align dst width to process 8 dst pixels per iteration
 
+#ifdef __loongarch_asx
+        __m256 pBilinearCoeffs[4];
+        __m256 pSrcStrideH = lasx_set1_f32(srcDescPtr->strides.hStride);
+        __m256 pAffineMatrixTerm0 = lasx_setr_f32(0, affineMatrix_f6->data[0], affineMatrix_f6->data[0] * 2, affineMatrix_f6->data[0] * 3, affineMatrix_f6->data[0] * 4, affineMatrix_f6->data[0] * 5, affineMatrix_f6->data[0] * 6, affineMatrix_f6->data[0] * 7);
+        __m256 pAffineMatrixTerm3 = lasx_setr_f32(0, affineMatrix_f6->data[3], affineMatrix_f6->data[3] * 2, affineMatrix_f6->data[3] * 3, affineMatrix_f6->data[3] * 4, affineMatrix_f6->data[3] * 5, affineMatrix_f6->data[3] * 6, affineMatrix_f6->data[3] * 7);
+        __m256 pAffineMatrixTerm0Incr = lasx_set1_f32(affineMatrix_f6->data[0] * 8);
+        __m256 pAffineMatrixTerm3Incr = lasx_set1_f32(affineMatrix_f6->data[3] * 8);
+        __m256 pRoiLTRB[4];
+        pRoiLTRB[0] = lasx_set1_f32(roiLTRB.ltrbROI.lt.x);
+        pRoiLTRB[1] = lasx_set1_f32(roiLTRB.ltrbROI.lt.y);
+        pRoiLTRB[2] = lasx_set1_f32(roiLTRB.ltrbROI.rb.x);
+        pRoiLTRB[3] = lasx_set1_f32(roiLTRB.ltrbROI.rb.y);
+        __m256i pxSrcStridesCHW[3];
+        pxSrcStridesCHW[0] = __lasx_xvreplgr2vr_w(srcDescPtr->strides.cStride);
+        pxSrcStridesCHW[1] = __lasx_xvreplgr2vr_w(srcDescPtr->strides.hStride);
+        pxSrcStridesCHW[2] = __lasx_xvreplgr2vr_w(srcDescPtr->strides.wStride);
+        RpptBilinearNbhoodLocsVecLen8 srcLocs;
+#else
         __m256 pBilinearCoeffs[4];
         __m256 pSrcStrideH = _mm256_set1_ps(srcDescPtr->strides.hStride);
         __m256 pAffineMatrixTerm0 = _mm256_setr_ps(0, affineMatrix_f6->data[0], affineMatrix_f6->data[0] * 2, affineMatrix_f6->data[0] * 3, affineMatrix_f6->data[0] * 4, affineMatrix_f6->data[0] * 5, affineMatrix_f6->data[0] * 6, affineMatrix_f6->data[0] * 7);
@@ -1469,6 +1689,7 @@ RppStatus warp_affine_bilinear_i8_i8_host_tensor(Rpp8s *srcPtr,
         pxSrcStridesCHW[1] = _mm256_set1_epi32(srcDescPtr->strides.hStride);
         pxSrcStridesCHW[2] = _mm256_set1_epi32(srcDescPtr->strides.wStride);
         RpptBilinearNbhoodLocsVecLen8 srcLocs;
+#endif
 
         // Warp Affine with fused output-layout toggle (NHWC -> NCHW)
         if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
@@ -1489,8 +1710,13 @@ RppStatus warp_affine_bilinear_i8_i8_host_tensor(Rpp8s *srcPtr,
                 Rpp32f srcX, srcY;
                 __m256 pSrcX, pSrcY;
                 compute_warp_affine_src_loc(i, vectorLoopCount, srcY, srcX, affineMatrix_f6, roiHalfHeight, roiHalfWidth);
+#ifdef __loongarch_asx
+                pSrcY = __lasx_xvfadd_s(lasx_set1_f32(srcY), pAffineMatrixTerm3);
+                pSrcX = __lasx_xvfadd_s(lasx_set1_f32(srcX), pAffineMatrixTerm0);
+#else
                 pSrcY = _mm256_add_ps(_mm256_set1_ps(srcY), pAffineMatrixTerm3);
                 pSrcX = _mm256_add_ps(_mm256_set1_ps(srcX), pAffineMatrixTerm0);
+#endif
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pSrc[12], pDst[3];
@@ -1532,8 +1758,13 @@ RppStatus warp_affine_bilinear_i8_i8_host_tensor(Rpp8s *srcPtr,
                 Rpp32f srcX, srcY;
                 __m256 pSrcX, pSrcY;
                 compute_warp_affine_src_loc(i, vectorLoopCount, srcY, srcX, affineMatrix_f6, roiHalfHeight, roiHalfWidth);
+#ifdef __loongarch_asx
+                pSrcY = __lasx_xvfadd_s(lasx_set1_f32(srcY), pAffineMatrixTerm3);
+                pSrcX = __lasx_xvfadd_s(lasx_set1_f32(srcX), pAffineMatrixTerm0);
+#else
                 pSrcY = _mm256_add_ps(_mm256_set1_ps(srcY), pAffineMatrixTerm3);
                 pSrcX = _mm256_add_ps(_mm256_set1_ps(srcX), pAffineMatrixTerm0);
+#endif
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pSrc[12], pDst[3];
@@ -1572,8 +1803,13 @@ RppStatus warp_affine_bilinear_i8_i8_host_tensor(Rpp8s *srcPtr,
                 Rpp32f srcX, srcY;
                 __m256 pSrcX, pSrcY;
                 compute_warp_affine_src_loc(i, vectorLoopCount, srcY, srcX, affineMatrix_f6, roiHalfHeight, roiHalfWidth);
+#ifdef __loongarch_asx
+                pSrcY = __lasx_xvfadd_s(lasx_set1_f32(srcY), pAffineMatrixTerm3);
+                pSrcX = __lasx_xvfadd_s(lasx_set1_f32(srcX), pAffineMatrixTerm0);
+#else
                 pSrcY = _mm256_add_ps(_mm256_set1_ps(srcY), pAffineMatrixTerm3);
                 pSrcX = _mm256_add_ps(_mm256_set1_ps(srcX), pAffineMatrixTerm0);
+#endif
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pSrc[12], pDst[3];
@@ -1616,8 +1852,13 @@ RppStatus warp_affine_bilinear_i8_i8_host_tensor(Rpp8s *srcPtr,
                 Rpp32f srcX, srcY;
                 __m256 pSrcX, pSrcY;
                 compute_warp_affine_src_loc(i, vectorLoopCount, srcY, srcX, affineMatrix_f6, roiHalfHeight, roiHalfWidth);
+#ifdef __loongarch_asx
+                pSrcY = __lasx_xvfadd_s(lasx_set1_f32(srcY), pAffineMatrixTerm3);
+                pSrcX = __lasx_xvfadd_s(lasx_set1_f32(srcX), pAffineMatrixTerm0);
+#else
                 pSrcY = _mm256_add_ps(_mm256_set1_ps(srcY), pAffineMatrixTerm3);
                 pSrcX = _mm256_add_ps(_mm256_set1_ps(srcX), pAffineMatrixTerm0);
+#endif
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pSrc[12], pDst[3];
@@ -1659,8 +1900,13 @@ RppStatus warp_affine_bilinear_i8_i8_host_tensor(Rpp8s *srcPtr,
                 Rpp32f srcX, srcY;
                 __m256 pSrcX, pSrcY;
                 compute_warp_affine_src_loc(i, vectorLoopCount, srcY, srcX, affineMatrix_f6, roiHalfHeight, roiHalfWidth);
+#ifdef __loongarch_asx
+                pSrcY = __lasx_xvfadd_s(lasx_set1_f32(srcY), pAffineMatrixTerm3);
+                pSrcX = __lasx_xvfadd_s(lasx_set1_f32(srcX), pAffineMatrixTerm0);
+#else
                 pSrcY = _mm256_add_ps(_mm256_set1_ps(srcY), pAffineMatrixTerm3);
                 pSrcX = _mm256_add_ps(_mm256_set1_ps(srcX), pAffineMatrixTerm0);
+#endif
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pSrc[4], pDst;
@@ -1725,6 +1971,24 @@ RppStatus warp_affine_bilinear_f16_f16_host_tensor(Rpp16f *srcPtr,
         Rpp32u bufferLength = roi.xywhROI.roiWidth;
         Rpp32u alignedLength = bufferLength & ~7;   // Align dst width to process 8 dst pixels per iteration
 
+#ifdef __loongarch_asx
+        __m256 pBilinearCoeffs[4];
+        __m256 pSrcStrideH = lasx_set1_f32(srcDescPtr->strides.hStride);
+        __m256 pAffineMatrixTerm0 = lasx_setr_f32(0, affineMatrix_f6->data[0], affineMatrix_f6->data[0] * 2, affineMatrix_f6->data[0] * 3, affineMatrix_f6->data[0] * 4, affineMatrix_f6->data[0] * 5, affineMatrix_f6->data[0] * 6, affineMatrix_f6->data[0] * 7);
+        __m256 pAffineMatrixTerm3 = lasx_setr_f32(0, affineMatrix_f6->data[3], affineMatrix_f6->data[3] * 2, affineMatrix_f6->data[3] * 3, affineMatrix_f6->data[3] * 4, affineMatrix_f6->data[3] * 5, affineMatrix_f6->data[3] * 6, affineMatrix_f6->data[3] * 7);
+        __m256 pAffineMatrixTerm0Incr = lasx_set1_f32(affineMatrix_f6->data[0] * 8);
+        __m256 pAffineMatrixTerm3Incr = lasx_set1_f32(affineMatrix_f6->data[3] * 8);
+        __m256 pRoiLTRB[4];
+        pRoiLTRB[0] = lasx_set1_f32(roiLTRB.ltrbROI.lt.x);
+        pRoiLTRB[1] = lasx_set1_f32(roiLTRB.ltrbROI.lt.y);
+        pRoiLTRB[2] = lasx_set1_f32(roiLTRB.ltrbROI.rb.x);
+        pRoiLTRB[3] = lasx_set1_f32(roiLTRB.ltrbROI.rb.y);
+        __m256i pxSrcStridesCHW[3];
+        pxSrcStridesCHW[0] = __lasx_xvreplgr2vr_w(srcDescPtr->strides.cStride);
+        pxSrcStridesCHW[1] = __lasx_xvreplgr2vr_w(srcDescPtr->strides.hStride);
+        pxSrcStridesCHW[2] = __lasx_xvreplgr2vr_w(srcDescPtr->strides.wStride);
+        RpptBilinearNbhoodLocsVecLen8 srcLocs;
+#else
         __m256 pBilinearCoeffs[4];
         __m256 pSrcStrideH = _mm256_set1_ps(srcDescPtr->strides.hStride);
         __m256 pAffineMatrixTerm0 = _mm256_setr_ps(0, affineMatrix_f6->data[0], affineMatrix_f6->data[0] * 2, affineMatrix_f6->data[0] * 3, affineMatrix_f6->data[0] * 4, affineMatrix_f6->data[0] * 5, affineMatrix_f6->data[0] * 6, affineMatrix_f6->data[0] * 7);
@@ -1741,6 +2005,7 @@ RppStatus warp_affine_bilinear_f16_f16_host_tensor(Rpp16f *srcPtr,
         pxSrcStridesCHW[1] = _mm256_set1_epi32(srcDescPtr->strides.hStride);
         pxSrcStridesCHW[2] = _mm256_set1_epi32(srcDescPtr->strides.wStride);
         RpptBilinearNbhoodLocsVecLen8 srcLocs;
+#endif
 
         // Warp Affine with fused output-layout toggle (NHWC -> NCHW)
         if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
@@ -1761,8 +2026,13 @@ RppStatus warp_affine_bilinear_f16_f16_host_tensor(Rpp16f *srcPtr,
                 Rpp32f srcX, srcY;
                 __m256 pSrcX, pSrcY;
                 compute_warp_affine_src_loc(i, vectorLoopCount, srcY, srcX, affineMatrix_f6, roiHalfHeight, roiHalfWidth);
+#ifdef __loongarch_asx
+                pSrcY = __lasx_xvfadd_s(lasx_set1_f32(srcY), pAffineMatrixTerm3);
+                pSrcX = __lasx_xvfadd_s(lasx_set1_f32(srcX), pAffineMatrixTerm0);
+#else
                 pSrcY = _mm256_add_ps(_mm256_set1_ps(srcY), pAffineMatrixTerm3);
                 pSrcX = _mm256_add_ps(_mm256_set1_ps(srcX), pAffineMatrixTerm0);
+#endif
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pSrc[12], pDst[3];
@@ -1810,8 +2080,13 @@ RppStatus warp_affine_bilinear_f16_f16_host_tensor(Rpp16f *srcPtr,
                 Rpp32f srcX, srcY;
                 __m256 pSrcX, pSrcY;
                 compute_warp_affine_src_loc(i, vectorLoopCount, srcY, srcX, affineMatrix_f6, roiHalfHeight, roiHalfWidth);
+#ifdef __loongarch_asx
+                pSrcY = __lasx_xvfadd_s(lasx_set1_f32(srcY), pAffineMatrixTerm3);
+                pSrcX = __lasx_xvfadd_s(lasx_set1_f32(srcX), pAffineMatrixTerm0);
+#else
                 pSrcY = _mm256_add_ps(_mm256_set1_ps(srcY), pAffineMatrixTerm3);
                 pSrcX = _mm256_add_ps(_mm256_set1_ps(srcX), pAffineMatrixTerm0);
+#endif
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pSrc[12], pDst[3];
@@ -1852,8 +2127,13 @@ RppStatus warp_affine_bilinear_f16_f16_host_tensor(Rpp16f *srcPtr,
                 Rpp32f srcX, srcY;
                 __m256 pSrcX, pSrcY;
                 compute_warp_affine_src_loc(i, vectorLoopCount, srcY, srcX, affineMatrix_f6, roiHalfHeight, roiHalfWidth);
+#ifdef __loongarch_asx
+                pSrcY = __lasx_xvfadd_s(lasx_set1_f32(srcY), pAffineMatrixTerm3);
+                pSrcX = __lasx_xvfadd_s(lasx_set1_f32(srcX), pAffineMatrixTerm0);
+#else
                 pSrcY = _mm256_add_ps(_mm256_set1_ps(srcY), pAffineMatrixTerm3);
                 pSrcX = _mm256_add_ps(_mm256_set1_ps(srcX), pAffineMatrixTerm0);
+#endif
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pSrc[12], pDst[3];
@@ -1898,8 +2178,13 @@ RppStatus warp_affine_bilinear_f16_f16_host_tensor(Rpp16f *srcPtr,
                 Rpp32f srcX, srcY;
                 __m256 pSrcX, pSrcY;
                 compute_warp_affine_src_loc(i, vectorLoopCount, srcY, srcX, affineMatrix_f6, roiHalfHeight, roiHalfWidth);
+#ifdef __loongarch_asx
+                pSrcY = __lasx_xvfadd_s(lasx_set1_f32(srcY), pAffineMatrixTerm3);
+                pSrcX = __lasx_xvfadd_s(lasx_set1_f32(srcX), pAffineMatrixTerm0);
+#else
                 pSrcY = _mm256_add_ps(_mm256_set1_ps(srcY), pAffineMatrixTerm3);
                 pSrcX = _mm256_add_ps(_mm256_set1_ps(srcX), pAffineMatrixTerm0);
+#endif
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pSrc[12], pDst[3];
@@ -1947,8 +2232,13 @@ RppStatus warp_affine_bilinear_f16_f16_host_tensor(Rpp16f *srcPtr,
                 Rpp32f srcX, srcY;
                 __m256 pSrcX, pSrcY;
                 compute_warp_affine_src_loc(i, vectorLoopCount, srcY, srcX, affineMatrix_f6, roiHalfHeight, roiHalfWidth);
+#ifdef __loongarch_asx
+                pSrcY = __lasx_xvfadd_s(lasx_set1_f32(srcY), pAffineMatrixTerm3);
+                pSrcX = __lasx_xvfadd_s(lasx_set1_f32(srcX), pAffineMatrixTerm0);
+#else
                 pSrcY = _mm256_add_ps(_mm256_set1_ps(srcY), pAffineMatrixTerm3);
                 pSrcX = _mm256_add_ps(_mm256_set1_ps(srcX), pAffineMatrixTerm0);
+#endif
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pSrc[4], pDst;
diff --git a/src/modules/cpu/kernel/warp_perspective.hpp b/src/modules/cpu/kernel/warp_perspective.hpp
index 24d4de17..46cfa7e8 100644
--- a/src/modules/cpu/kernel/warp_perspective.hpp
+++ b/src/modules/cpu/kernel/warp_perspective.hpp
@@ -23,12 +23,17 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 /************* warp_perspective helpers *************/
 
-#if __AVX2__
+#if defined(__AVX2__)
 inline void compute_warp_perspective_src_loc_next_term_avx(__m256 &plocW, __m256 &plocY, __m256 &plocX, __m256 &pSrcY, __m256 &pSrcX, __m256 &pPerspectiveMatrixTerm6Incr, __m256 &pPerspectiveMatrixTerm3Incr, __m256 &pPerspectiveMatrixTerm0Incr, __m256 &pRoiHalfHeight, __m256 &pRoiHalfWidth)
 {
     plocW = _mm256_add_ps(plocW, pPerspectiveMatrixTerm6Incr);
@@ -44,6 +49,22 @@ inline void compute_warp_perspective_src_loc_first_term_avx(Rpp32f locX, Rpp32f
     pSrcY = _mm256_add_ps(_mm256_div_ps(plocY, plocW), pRoiHalfHeight);
     pSrcX = _mm256_add_ps(_mm256_div_ps(plocX, plocW), pRoiHalfWidth);
 }
+#elif defined(__loongarch_asx)
+inline void compute_warp_perspective_src_loc_next_term_avx(__m256 &plocW, __m256 &plocY, __m256 &plocX, __m256 &pSrcY, __m256 &pSrcX, __m256 &pPerspectiveMatrixTerm6Incr, __m256 &pPerspectiveMatrixTerm3Incr, __m256 &pPerspectiveMatrixTerm0Incr, __m256 &pRoiHalfHeight, __m256 &pRoiHalfWidth)
+{
+    plocW = __lasx_xvfadd_s(plocW, pPerspectiveMatrixTerm6Incr);
+    plocY = __lasx_xvfadd_s(plocY, pPerspectiveMatrixTerm3Incr);
+    plocX = __lasx_xvfadd_s(plocX, pPerspectiveMatrixTerm0Incr);
+    pSrcY = __lasx_xvfadd_s(__lasx_xvfdiv_s(plocY, plocW), pRoiHalfHeight);
+    pSrcX = __lasx_xvfadd_s(__lasx_xvfdiv_s(plocX, plocW), pRoiHalfWidth);
+}
+inline void compute_warp_perspective_src_loc_first_term_avx(Rpp32f locX, Rpp32f locY, Rpp32f locW, __m256 &plocW, __m256 &plocY, __m256 &plocX, __m256 &pSrcY, __m256 &pSrcX, __m256 &pPerspectiveMatrixTerm6, __m256 &pPerspectiveMatrixTerm3, __m256 &pPerspectiveMatrixTerm0, __m256 &pRoiHalfHeight, __m256 &pRoiHalfWidth) {
+    plocX = __lasx_xvfadd_s(lasx_set1_f32(locX), pPerspectiveMatrixTerm0);
+    plocY = __lasx_xvfadd_s(lasx_set1_f32(locY), pPerspectiveMatrixTerm3);
+    plocW = __lasx_xvfadd_s(lasx_set1_f32(locW), pPerspectiveMatrixTerm6);
+    pSrcY = __lasx_xvfadd_s(__lasx_xvfdiv_s(plocY, plocW), pRoiHalfHeight);
+    pSrcX = __lasx_xvfadd_s(__lasx_xvfdiv_s(plocX, plocW), pRoiHalfWidth);
+}
 #endif
 
 inline void compute_warp_perspective_src_loc_params(Rpp32s dstY, Rpp32s dstX, Rpp32f &locW, Rpp32f &locY, Rpp32f &locX, Rpp32f9 *perspectiveMatrix_f9, Rpp32s roiHalfHeight, Rpp32s roiHalfWidth)
@@ -106,7 +127,7 @@ RppStatus warp_perspective_nn_u8_u8_host_tensor(Rpp8u *srcPtr,
         Rpp32s srcLoc[8] = {0};         // Since 4 dst pixels are processed per iteration
         Rpp32s invalidLoad[8] = {0};    // Since 4 dst pixels are processed per iteration
 
-#if __AVX2__
+#if defined(__AVX2__)
         __m256 pSrcStrideH = _mm256_set1_ps(srcDescPtr->strides.hStride);
         __m256 pPerspectiveMatrixTerm0 = _mm256_setr_ps(0, perspectiveMatrix_f9->data[0], perspectiveMatrix_f9->data[0] * 2, perspectiveMatrix_f9->data[0] * 3, perspectiveMatrix_f9->data[0] * 4, perspectiveMatrix_f9->data[0] * 5, perspectiveMatrix_f9->data[0] * 6, perspectiveMatrix_f9->data[0] * 7);
         __m256 pPerspectiveMatrixTerm3 = _mm256_setr_ps(0, perspectiveMatrix_f9->data[3], perspectiveMatrix_f9->data[3] * 2, perspectiveMatrix_f9->data[3] * 3, perspectiveMatrix_f9->data[3] * 4, perspectiveMatrix_f9->data[3] * 5, perspectiveMatrix_f9->data[3] * 6, perspectiveMatrix_f9->data[3] * 7);
@@ -121,6 +142,21 @@ RppStatus warp_perspective_nn_u8_u8_host_tensor(Rpp8u *srcPtr,
         pRoiLTRB[1] = _mm256_set1_ps(roiLTRB.ltrbROI.lt.y);
         pRoiLTRB[2] = _mm256_set1_ps(roiLTRB.ltrbROI.rb.x);
         pRoiLTRB[3] = _mm256_set1_ps(roiLTRB.ltrbROI.rb.y);
+#elif defined(__loongarch_asx)
+        __m256 pSrcStrideH = lasx_set1_f32(srcDescPtr->strides.hStride);
+        __m256 pPerspectiveMatrixTerm0 = lasx_setr_f32(0, perspectiveMatrix_f9->data[0], perspectiveMatrix_f9->data[0] * 2, perspectiveMatrix_f9->data[0] * 3, perspectiveMatrix_f9->data[0] * 4, perspectiveMatrix_f9->data[0] * 5, perspectiveMatrix_f9->data[0] * 6, perspectiveMatrix_f9->data[0] * 7);
+        __m256 pPerspectiveMatrixTerm3 = lasx_setr_f32(0, perspectiveMatrix_f9->data[3], perspectiveMatrix_f9->data[3] * 2, perspectiveMatrix_f9->data[3] * 3, perspectiveMatrix_f9->data[3] * 4, perspectiveMatrix_f9->data[3] * 5, perspectiveMatrix_f9->data[3] * 6, perspectiveMatrix_f9->data[3] * 7);
+        __m256 pPerspectiveMatrixTerm6 = lasx_setr_f32(0, perspectiveMatrix_f9->data[6], perspectiveMatrix_f9->data[6] * 2, perspectiveMatrix_f9->data[6] * 3, perspectiveMatrix_f9->data[6] * 4, perspectiveMatrix_f9->data[6] * 5, perspectiveMatrix_f9->data[6] * 6, perspectiveMatrix_f9->data[6] * 7);
+        __m256 pPerspectiveMatrixTerm0Incr = lasx_set1_f32(perspectiveMatrix_f9->data[0] * 8);
+        __m256 pPerspectiveMatrixTerm3Incr = lasx_set1_f32(perspectiveMatrix_f9->data[3] * 8);
+        __m256 pPerspectiveMatrixTerm6Incr = lasx_set1_f32(perspectiveMatrix_f9->data[6] * 8);
+        __m256 pRoiHalfHeight = lasx_set1_f32(roiHalfHeight);
+        __m256 pRoiHalfWidth = lasx_set1_f32(roiHalfWidth);
+        __m256 pRoiLTRB[4];
+        pRoiLTRB[0] = lasx_set1_f32(roiLTRB.ltrbROI.lt.x);
+        pRoiLTRB[1] = lasx_set1_f32(roiLTRB.ltrbROI.lt.y);
+        pRoiLTRB[2] = lasx_set1_f32(roiLTRB.ltrbROI.rb.x);
+        pRoiLTRB[3] = lasx_set1_f32(roiLTRB.ltrbROI.rb.y);
 #endif
 
         // Warp perspective with fused output-layout toggle (NHWC -> NCHW)
@@ -140,7 +176,7 @@ RppStatus warp_perspective_nn_u8_u8_host_tensor(Rpp8u *srcPtr,
                 int vectorLoopCount = 0;
                 Rpp32f locX, locY, locW, srcX, srcY;
                 compute_warp_perspective_src_loc_params(i, vectorLoopCount, locW, locY, locX, perspectiveMatrix_f9, roiHalfHeight, roiHalfWidth);
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m256 plocX, plocY, plocW, pSrcX, pSrcY;
                 compute_warp_perspective_src_loc_first_term_avx(locX, locY, locW, plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6, pPerspectiveMatrixTerm3, pPerspectiveMatrixTerm0, pRoiHalfHeight, pRoiHalfWidth);
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
@@ -188,7 +224,7 @@ RppStatus warp_perspective_nn_u8_u8_host_tensor(Rpp8u *srcPtr,
                 int vectorLoopCount = 0;
                 Rpp32f locX, locY, locW, srcX, srcY;
                 compute_warp_perspective_src_loc_params(i, vectorLoopCount, locW, locY, locX, perspectiveMatrix_f9, roiHalfHeight, roiHalfWidth);
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m256 plocX, plocY, plocW, pSrcX, pSrcY;
                 compute_warp_perspective_src_loc_first_term_avx(locX, locY, locW, plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6, pPerspectiveMatrixTerm3, pPerspectiveMatrixTerm0, pRoiHalfHeight, pRoiHalfWidth);
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
@@ -231,7 +267,7 @@ RppStatus warp_perspective_nn_u8_u8_host_tensor(Rpp8u *srcPtr,
                 int vectorLoopCount = 0;
                 Rpp32f locX, locY, locW, srcX, srcY;
                 compute_warp_perspective_src_loc_params(i, vectorLoopCount, locW, locY, locX, perspectiveMatrix_f9, roiHalfHeight, roiHalfWidth);
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m256 plocX, plocY, plocW, pSrcX, pSrcY;
                 compute_warp_perspective_src_loc_first_term_avx(locX, locY, locW, plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6, pPerspectiveMatrixTerm3, pPerspectiveMatrixTerm0, pRoiHalfHeight, pRoiHalfWidth);
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
@@ -271,7 +307,7 @@ RppStatus warp_perspective_nn_u8_u8_host_tensor(Rpp8u *srcPtr,
                 int vectorLoopCount = 0;
                 Rpp32f locX, locY, locW, srcX, srcY;
                 compute_warp_perspective_src_loc_params(i, vectorLoopCount, locW, locY, locX, perspectiveMatrix_f9, roiHalfHeight, roiHalfWidth);
-#if __AVX2__
+#if defined(__AVX2__)
                 __m256 plocX, plocY, plocW, pSrcX, pSrcY;
                 compute_warp_perspective_src_loc_first_term_avx(locX, locY, locW, plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6, pPerspectiveMatrixTerm3, pPerspectiveMatrixTerm0, pRoiHalfHeight, pRoiHalfWidth);
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
@@ -291,6 +327,26 @@ RppStatus warp_perspective_nn_u8_u8_host_tensor(Rpp8u *srcPtr,
                     compute_warp_perspective_src_loc_next_term_avx(plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6Incr, pPerspectiveMatrixTerm3Incr, pPerspectiveMatrixTerm0Incr, pRoiHalfHeight, pRoiHalfWidth);
                     dstPtrTemp += vectorIncrementPerChannel;
                 }
+#elif defined(__loongarch_asx)
+                __m256 plocX, plocY, plocW, pSrcX, pSrcY;
+                compute_warp_perspective_src_loc_first_term_avx(locX, locY, locW, plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6, pPerspectiveMatrixTerm3, pPerspectiveMatrixTerm0, pRoiHalfHeight, pRoiHalfWidth);
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    Rpp8u *dstPtrTempChn, *srcPtrTempChn;
+                    srcPtrTempChn = srcPtrChannel;
+                    dstPtrTempChn = dstPtrTemp;
+                    compute_generic_nn_srclocs_and_validate_avx(pSrcY, pSrcX, pRoiLTRB, pSrcStrideH, srcLoc, invalidLoad);
+                    for (int c = 0; c < srcDescPtr->c; c++)
+                    {
+                        __m256i pRow;
+                        rpp_simd_load(rpp_generic_nn_load_u8pln1_avx, srcPtrTempChn, srcLoc, invalidLoad, pRow);
+                        rpp_storeu_si64(reinterpret_cast<__m128i *>(dstPtrTempChn), __lasx_cvt_256_128(pRow));
+                        srcPtrTempChn += srcDescPtr->strides.cStride;
+                        dstPtrTempChn += dstDescPtr->strides.cStride;
+                    }
+                    compute_warp_perspective_src_loc_next_term_avx(plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6Incr, pPerspectiveMatrixTerm3Incr, pPerspectiveMatrixTerm0Incr, pRoiHalfHeight, pRoiHalfWidth);
+                    dstPtrTemp += vectorIncrementPerChannel;
+                }
 #endif
                 locW += (perspectiveMatrix_f9->data[6] * vectorLoopCount);
                 locY += (perspectiveMatrix_f9->data[3] * vectorLoopCount);
@@ -351,7 +407,7 @@ RppStatus warp_perspective_nn_f32_f32_host_tensor(Rpp32f *srcPtr,
         Rpp32s srcLoc[8] = {0};         // Since 4 dst pixels are processed per iteration
         Rpp32s invalidLoad[8] = {0};    // Since 4 dst pixels are processed per iteration
 
-#if __AVX2__
+#if defined(__AVX2__)
         __m256 pSrcStrideH = _mm256_set1_ps(srcDescPtr->strides.hStride);
         __m256 pPerspectiveMatrixTerm0 = _mm256_setr_ps(0, perspectiveMatrix_f9->data[0], perspectiveMatrix_f9->data[0] * 2, perspectiveMatrix_f9->data[0] * 3, perspectiveMatrix_f9->data[0] * 4, perspectiveMatrix_f9->data[0] * 5, perspectiveMatrix_f9->data[0] * 6, perspectiveMatrix_f9->data[0] * 7);
         __m256 pPerspectiveMatrixTerm3 = _mm256_setr_ps(0, perspectiveMatrix_f9->data[3], perspectiveMatrix_f9->data[3] * 2, perspectiveMatrix_f9->data[3] * 3, perspectiveMatrix_f9->data[3] * 4, perspectiveMatrix_f9->data[3] * 5, perspectiveMatrix_f9->data[3] * 6, perspectiveMatrix_f9->data[3] * 7);
@@ -366,6 +422,21 @@ RppStatus warp_perspective_nn_f32_f32_host_tensor(Rpp32f *srcPtr,
         pRoiLTRB[1] = _mm256_set1_ps(roiLTRB.ltrbROI.lt.y);
         pRoiLTRB[2] = _mm256_set1_ps(roiLTRB.ltrbROI.rb.x);
         pRoiLTRB[3] = _mm256_set1_ps(roiLTRB.ltrbROI.rb.y);
+#elif defined(__loongarch_asx)
+        __m256 pSrcStrideH = lasx_set1_f32(srcDescPtr->strides.hStride);
+        __m256 pPerspectiveMatrixTerm0 = lasx_setr_f32(0, perspectiveMatrix_f9->data[0], perspectiveMatrix_f9->data[0] * 2, perspectiveMatrix_f9->data[0] * 3, perspectiveMatrix_f9->data[0] * 4, perspectiveMatrix_f9->data[0] * 5, perspectiveMatrix_f9->data[0] * 6, perspectiveMatrix_f9->data[0] * 7);
+        __m256 pPerspectiveMatrixTerm3 = lasx_setr_f32(0, perspectiveMatrix_f9->data[3], perspectiveMatrix_f9->data[3] * 2, perspectiveMatrix_f9->data[3] * 3, perspectiveMatrix_f9->data[3] * 4, perspectiveMatrix_f9->data[3] * 5, perspectiveMatrix_f9->data[3] * 6, perspectiveMatrix_f9->data[3] * 7);
+        __m256 pPerspectiveMatrixTerm6 = lasx_setr_f32(0, perspectiveMatrix_f9->data[6], perspectiveMatrix_f9->data[6] * 2, perspectiveMatrix_f9->data[6] * 3, perspectiveMatrix_f9->data[6] * 4, perspectiveMatrix_f9->data[6] * 5, perspectiveMatrix_f9->data[6] * 6, perspectiveMatrix_f9->data[6] * 7);
+        __m256 pPerspectiveMatrixTerm0Incr = lasx_set1_f32(perspectiveMatrix_f9->data[0] * 8);
+        __m256 pPerspectiveMatrixTerm3Incr = lasx_set1_f32(perspectiveMatrix_f9->data[3] * 8);
+        __m256 pPerspectiveMatrixTerm6Incr = lasx_set1_f32(perspectiveMatrix_f9->data[6] * 8);
+        __m256 pRoiHalfHeight = lasx_set1_f32(roiHalfHeight);
+        __m256 pRoiHalfWidth = lasx_set1_f32(roiHalfWidth);
+        __m256 pRoiLTRB[4];
+        pRoiLTRB[0] = lasx_set1_f32(roiLTRB.ltrbROI.lt.x);
+        pRoiLTRB[1] = lasx_set1_f32(roiLTRB.ltrbROI.lt.y);
+        pRoiLTRB[2] = lasx_set1_f32(roiLTRB.ltrbROI.rb.x);
+        pRoiLTRB[3] = lasx_set1_f32(roiLTRB.ltrbROI.rb.y);
 #endif
 
         // Warp perspective with fused output-layout toggle (NHWC -> NCHW)
@@ -385,7 +456,7 @@ RppStatus warp_perspective_nn_f32_f32_host_tensor(Rpp32f *srcPtr,
                 int vectorLoopCount = 0;
                 Rpp32f locX, locY, locW, srcX, srcY;
                 compute_warp_perspective_src_loc_params(i, vectorLoopCount, locW, locY, locX, perspectiveMatrix_f9, roiHalfHeight, roiHalfWidth);
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m256 plocX, plocY, plocW, pSrcX, pSrcY;
                 compute_warp_perspective_src_loc_first_term_avx(locX, locY, locW, plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6, pPerspectiveMatrixTerm3, pPerspectiveMatrixTerm0, pRoiHalfHeight, pRoiHalfWidth);
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
@@ -433,7 +504,7 @@ RppStatus warp_perspective_nn_f32_f32_host_tensor(Rpp32f *srcPtr,
                 int vectorLoopCount = 0;
                 Rpp32f locX, locY, locW, srcX, srcY;
                 compute_warp_perspective_src_loc_params(i, vectorLoopCount, locW, locY, locX, perspectiveMatrix_f9, roiHalfHeight, roiHalfWidth);
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m256 plocX, plocY, plocW, pSrcX, pSrcY;
                 compute_warp_perspective_src_loc_first_term_avx(locX, locY, locW, plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6, pPerspectiveMatrixTerm3, pPerspectiveMatrixTerm0, pRoiHalfHeight, pRoiHalfWidth);
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
@@ -476,7 +547,7 @@ RppStatus warp_perspective_nn_f32_f32_host_tensor(Rpp32f *srcPtr,
                 int vectorLoopCount = 0;
                 Rpp32f locX, locY, locW, srcX, srcY;
                 compute_warp_perspective_src_loc_params(i, vectorLoopCount, locW, locY, locX, perspectiveMatrix_f9, roiHalfHeight, roiHalfWidth);
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m256 plocX, plocY, plocW, pSrcX, pSrcY;
                 compute_warp_perspective_src_loc_first_term_avx(locX, locY, locW, plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6, pPerspectiveMatrixTerm3, pPerspectiveMatrixTerm0, pRoiHalfHeight, pRoiHalfWidth);
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
@@ -516,7 +587,7 @@ RppStatus warp_perspective_nn_f32_f32_host_tensor(Rpp32f *srcPtr,
                 int vectorLoopCount = 0;
                 Rpp32f locX, locY, locW, srcX, srcY;
                 compute_warp_perspective_src_loc_params(i, vectorLoopCount, locW, locY, locX, perspectiveMatrix_f9, roiHalfHeight, roiHalfWidth);
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m256 plocX, plocY, plocW, pSrcX, pSrcY;
                 compute_warp_perspective_src_loc_first_term_avx(locX, locY, locW, plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6, pPerspectiveMatrixTerm3, pPerspectiveMatrixTerm0, pRoiHalfHeight, pRoiHalfWidth);
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
@@ -595,7 +666,7 @@ RppStatus warp_perspective_nn_i8_i8_host_tensor(Rpp8s *srcPtr,
         Rpp32s srcLoc[8] = {0};         // Since 4 dst pixels are processed per iteration
         Rpp32s invalidLoad[8] = {0};    // Since 4 dst pixels are processed per iteration
 
-#if __AVX2__
+#if defined(__AVX2__)
         __m256 pSrcStrideH = _mm256_set1_ps(srcDescPtr->strides.hStride);
         __m256 pPerspectiveMatrixTerm0 = _mm256_setr_ps(0, perspectiveMatrix_f9->data[0], perspectiveMatrix_f9->data[0] * 2, perspectiveMatrix_f9->data[0] * 3, perspectiveMatrix_f9->data[0] * 4, perspectiveMatrix_f9->data[0] * 5, perspectiveMatrix_f9->data[0] * 6, perspectiveMatrix_f9->data[0] * 7);
         __m256 pPerspectiveMatrixTerm3 = _mm256_setr_ps(0, perspectiveMatrix_f9->data[3], perspectiveMatrix_f9->data[3] * 2, perspectiveMatrix_f9->data[3] * 3, perspectiveMatrix_f9->data[3] * 4, perspectiveMatrix_f9->data[3] * 5, perspectiveMatrix_f9->data[3] * 6, perspectiveMatrix_f9->data[3] * 7);
@@ -610,6 +681,21 @@ RppStatus warp_perspective_nn_i8_i8_host_tensor(Rpp8s *srcPtr,
         pRoiLTRB[1] = _mm256_set1_ps(roiLTRB.ltrbROI.lt.y);
         pRoiLTRB[2] = _mm256_set1_ps(roiLTRB.ltrbROI.rb.x);
         pRoiLTRB[3] = _mm256_set1_ps(roiLTRB.ltrbROI.rb.y);
+#elif defined(__loongarch_asx)
+        __m256 pSrcStrideH = lasx_set1_f32(srcDescPtr->strides.hStride);
+        __m256 pPerspectiveMatrixTerm0 = lasx_setr_f32(0, perspectiveMatrix_f9->data[0], perspectiveMatrix_f9->data[0] * 2, perspectiveMatrix_f9->data[0] * 3, perspectiveMatrix_f9->data[0] * 4, perspectiveMatrix_f9->data[0] * 5, perspectiveMatrix_f9->data[0] * 6, perspectiveMatrix_f9->data[0] * 7);
+        __m256 pPerspectiveMatrixTerm3 = lasx_setr_f32(0, perspectiveMatrix_f9->data[3], perspectiveMatrix_f9->data[3] * 2, perspectiveMatrix_f9->data[3] * 3, perspectiveMatrix_f9->data[3] * 4, perspectiveMatrix_f9->data[3] * 5, perspectiveMatrix_f9->data[3] * 6, perspectiveMatrix_f9->data[3] * 7);
+        __m256 pPerspectiveMatrixTerm6 = lasx_setr_f32(0, perspectiveMatrix_f9->data[6], perspectiveMatrix_f9->data[6] * 2, perspectiveMatrix_f9->data[6] * 3, perspectiveMatrix_f9->data[6] * 4, perspectiveMatrix_f9->data[6] * 5, perspectiveMatrix_f9->data[6] * 6, perspectiveMatrix_f9->data[6] * 7);
+        __m256 pPerspectiveMatrixTerm0Incr = lasx_set1_f32(perspectiveMatrix_f9->data[0] * 8);
+        __m256 pPerspectiveMatrixTerm3Incr = lasx_set1_f32(perspectiveMatrix_f9->data[3] * 8);
+        __m256 pPerspectiveMatrixTerm6Incr = lasx_set1_f32(perspectiveMatrix_f9->data[6] * 8);
+        __m256 pRoiHalfHeight = lasx_set1_f32(roiHalfHeight);
+        __m256 pRoiHalfWidth = lasx_set1_f32(roiHalfWidth);
+        __m256 pRoiLTRB[4];
+        pRoiLTRB[0] = lasx_set1_f32(roiLTRB.ltrbROI.lt.x);
+        pRoiLTRB[1] = lasx_set1_f32(roiLTRB.ltrbROI.lt.y);
+        pRoiLTRB[2] = lasx_set1_f32(roiLTRB.ltrbROI.rb.x);
+        pRoiLTRB[3] = lasx_set1_f32(roiLTRB.ltrbROI.rb.y);
 #endif
 
         // Warp perspective with fused output-layout toggle (NHWC -> NCHW)
@@ -629,7 +715,7 @@ RppStatus warp_perspective_nn_i8_i8_host_tensor(Rpp8s *srcPtr,
                 int vectorLoopCount = 0;
                 Rpp32f locX, locY, locW, srcX, srcY;
                 compute_warp_perspective_src_loc_params(i, vectorLoopCount, locW, locY, locX, perspectiveMatrix_f9, roiHalfHeight, roiHalfWidth);
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m256 plocX, plocY, plocW, pSrcX, pSrcY;
                 compute_warp_perspective_src_loc_first_term_avx(locX, locY, locW, plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6, pPerspectiveMatrixTerm3, pPerspectiveMatrixTerm0, pRoiHalfHeight, pRoiHalfWidth);
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
@@ -677,7 +763,7 @@ RppStatus warp_perspective_nn_i8_i8_host_tensor(Rpp8s *srcPtr,
                 int vectorLoopCount = 0;
                 Rpp32f locX, locY, locW, srcX, srcY;
                 compute_warp_perspective_src_loc_params(i, vectorLoopCount, locW, locY, locX, perspectiveMatrix_f9, roiHalfHeight, roiHalfWidth);
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m256 plocX, plocY, plocW, pSrcX, pSrcY;
                 compute_warp_perspective_src_loc_first_term_avx(locX, locY, locW, plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6, pPerspectiveMatrixTerm3, pPerspectiveMatrixTerm0, pRoiHalfHeight, pRoiHalfWidth);
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
@@ -720,7 +806,7 @@ RppStatus warp_perspective_nn_i8_i8_host_tensor(Rpp8s *srcPtr,
                 int vectorLoopCount = 0;
                 Rpp32f locX, locY, locW, srcX, srcY;
                 compute_warp_perspective_src_loc_params(i, vectorLoopCount, locW, locY, locX, perspectiveMatrix_f9, roiHalfHeight, roiHalfWidth);
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m256 plocX, plocY, plocW, pSrcX, pSrcY;
                 compute_warp_perspective_src_loc_first_term_avx(locX, locY, locW, plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6, pPerspectiveMatrixTerm3, pPerspectiveMatrixTerm0, pRoiHalfHeight, pRoiHalfWidth);
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
@@ -760,7 +846,7 @@ RppStatus warp_perspective_nn_i8_i8_host_tensor(Rpp8s *srcPtr,
                 int vectorLoopCount = 0;
                 Rpp32f locX, locY, locW, srcX, srcY;
                 compute_warp_perspective_src_loc_params(i, vectorLoopCount, locW, locY, locX, perspectiveMatrix_f9, roiHalfHeight, roiHalfWidth);
-#if __AVX2__
+#if defined(__AVX2__)
                 __m256 plocX, plocY, plocW, pSrcX, pSrcY;
                 compute_warp_perspective_src_loc_first_term_avx(locX, locY, locW, plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6, pPerspectiveMatrixTerm3, pPerspectiveMatrixTerm0, pRoiHalfHeight, pRoiHalfWidth);
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
@@ -780,6 +866,26 @@ RppStatus warp_perspective_nn_i8_i8_host_tensor(Rpp8s *srcPtr,
                     compute_warp_perspective_src_loc_next_term_avx(plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6Incr, pPerspectiveMatrixTerm3Incr, pPerspectiveMatrixTerm0Incr, pRoiHalfHeight, pRoiHalfWidth);
                     dstPtrTemp += vectorIncrementPerChannel;
                 }
+#elif defined(__loongarch_asx)
+                __m256 plocX, plocY, plocW, pSrcX, pSrcY;
+                compute_warp_perspective_src_loc_first_term_avx(locX, locY, locW, plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6, pPerspectiveMatrixTerm3, pPerspectiveMatrixTerm0, pRoiHalfHeight, pRoiHalfWidth);
+                for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
+                {
+                    Rpp8s *dstPtrTempChn, *srcPtrTempChn;
+                    srcPtrTempChn = srcPtrChannel;
+                    dstPtrTempChn = dstPtrTemp;
+                    compute_generic_nn_srclocs_and_validate_avx(pSrcY, pSrcX, pRoiLTRB, pSrcStrideH, srcLoc, invalidLoad);
+                    for (int c = 0; c < srcDescPtr->c; c++)
+                    {
+                        __m256i pRow;
+                        rpp_simd_load(rpp_generic_nn_load_i8pln1_avx, srcPtrTempChn, srcLoc, invalidLoad, pRow);
+                        rpp_storeu_si64(reinterpret_cast<__m128i *>(dstPtrTempChn), __lasx_cvt_256_128(pRow));
+                        srcPtrTempChn += srcDescPtr->strides.cStride;
+                        dstPtrTempChn += dstDescPtr->strides.cStride;
+                    }
+                    compute_warp_perspective_src_loc_next_term_avx(plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6Incr, pPerspectiveMatrixTerm3Incr, pPerspectiveMatrixTerm0Incr, pRoiHalfHeight, pRoiHalfWidth);
+                    dstPtrTemp += vectorIncrementPerChannel;
+                }
 #endif
                 locW += (perspectiveMatrix_f9->data[6] * vectorLoopCount);
                 locY += (perspectiveMatrix_f9->data[3] * vectorLoopCount);
@@ -839,7 +945,7 @@ RppStatus warp_perspective_nn_f16_f16_host_tensor(Rpp16f *srcPtr,
         Rpp32s srcLoc[8] = {0};         // Since 4 dst pixels are processed per iteration
         Rpp32s invalidLoad[8] = {0};    // Since 4 dst pixels are processed per iteration
 
-#if __AVX2__
+#if defined(__AVX2__)
         __m256 pSrcStrideH = _mm256_set1_ps(srcDescPtr->strides.hStride);
         __m256 pPerspectiveMatrixTerm0 = _mm256_setr_ps(0, perspectiveMatrix_f9->data[0], perspectiveMatrix_f9->data[0] * 2, perspectiveMatrix_f9->data[0] * 3, perspectiveMatrix_f9->data[0] * 4, perspectiveMatrix_f9->data[0] * 5, perspectiveMatrix_f9->data[0] * 6, perspectiveMatrix_f9->data[0] * 7);
         __m256 pPerspectiveMatrixTerm3 = _mm256_setr_ps(0, perspectiveMatrix_f9->data[3], perspectiveMatrix_f9->data[3] * 2, perspectiveMatrix_f9->data[3] * 3, perspectiveMatrix_f9->data[3] * 4, perspectiveMatrix_f9->data[3] * 5, perspectiveMatrix_f9->data[3] * 6, perspectiveMatrix_f9->data[3] * 7);
@@ -854,6 +960,21 @@ RppStatus warp_perspective_nn_f16_f16_host_tensor(Rpp16f *srcPtr,
         pRoiLTRB[1] = _mm256_set1_ps(roiLTRB.ltrbROI.lt.y);
         pRoiLTRB[2] = _mm256_set1_ps(roiLTRB.ltrbROI.rb.x);
         pRoiLTRB[3] = _mm256_set1_ps(roiLTRB.ltrbROI.rb.y);
+#elif defined(__loongarch_asx)
+        __m256 pSrcStrideH = lasx_set1_f32(srcDescPtr->strides.hStride);
+        __m256 pPerspectiveMatrixTerm0 = lasx_setr_f32(0, perspectiveMatrix_f9->data[0], perspectiveMatrix_f9->data[0] * 2, perspectiveMatrix_f9->data[0] * 3, perspectiveMatrix_f9->data[0] * 4, perspectiveMatrix_f9->data[0] * 5, perspectiveMatrix_f9->data[0] * 6, perspectiveMatrix_f9->data[0] * 7);
+        __m256 pPerspectiveMatrixTerm3 = lasx_setr_f32(0, perspectiveMatrix_f9->data[3], perspectiveMatrix_f9->data[3] * 2, perspectiveMatrix_f9->data[3] * 3, perspectiveMatrix_f9->data[3] * 4, perspectiveMatrix_f9->data[3] * 5, perspectiveMatrix_f9->data[3] * 6, perspectiveMatrix_f9->data[3] * 7);
+        __m256 pPerspectiveMatrixTerm6 = lasx_setr_f32(0, perspectiveMatrix_f9->data[6], perspectiveMatrix_f9->data[6] * 2, perspectiveMatrix_f9->data[6] * 3, perspectiveMatrix_f9->data[6] * 4, perspectiveMatrix_f9->data[6] * 5, perspectiveMatrix_f9->data[6] * 6, perspectiveMatrix_f9->data[6] * 7);
+        __m256 pPerspectiveMatrixTerm0Incr = lasx_set1_f32(perspectiveMatrix_f9->data[0] * 8);
+        __m256 pPerspectiveMatrixTerm3Incr = lasx_set1_f32(perspectiveMatrix_f9->data[3] * 8);
+        __m256 pPerspectiveMatrixTerm6Incr = lasx_set1_f32(perspectiveMatrix_f9->data[6] * 8);
+        __m256 pRoiHalfHeight = lasx_set1_f32(roiHalfHeight);
+        __m256 pRoiHalfWidth = lasx_set1_f32(roiHalfWidth);
+        __m256 pRoiLTRB[4];
+        pRoiLTRB[0] = lasx_set1_f32(roiLTRB.ltrbROI.lt.x);
+        pRoiLTRB[1] = lasx_set1_f32(roiLTRB.ltrbROI.lt.y);
+        pRoiLTRB[2] = lasx_set1_f32(roiLTRB.ltrbROI.rb.x);
+        pRoiLTRB[3] = lasx_set1_f32(roiLTRB.ltrbROI.rb.y);
 #endif
 
         // Warp perspective with fused output-layout toggle (NHWC -> NCHW)
@@ -873,7 +994,7 @@ RppStatus warp_perspective_nn_f16_f16_host_tensor(Rpp16f *srcPtr,
                 int vectorLoopCount = 0;
                 Rpp32f locX, locY, locW, srcX, srcY;
                 compute_warp_perspective_src_loc_params(i, vectorLoopCount, locW, locY, locX, perspectiveMatrix_f9, roiHalfHeight, roiHalfWidth);
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m256 plocX, plocY, plocW, pSrcX, pSrcY;
                 compute_warp_perspective_src_loc_first_term_avx(locX, locY, locW, plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6, pPerspectiveMatrixTerm3, pPerspectiveMatrixTerm0, pRoiHalfHeight, pRoiHalfWidth);
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
@@ -921,7 +1042,7 @@ RppStatus warp_perspective_nn_f16_f16_host_tensor(Rpp16f *srcPtr,
                 int vectorLoopCount = 0;
                 Rpp32f locX, locY, locW, srcX, srcY;
                 compute_warp_perspective_src_loc_params(i, vectorLoopCount, locW, locY, locX, perspectiveMatrix_f9, roiHalfHeight, roiHalfWidth);
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m256 plocX, plocY, plocW, pSrcX, pSrcY;
                 compute_warp_perspective_src_loc_first_term_avx(locX, locY, locW, plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6, pPerspectiveMatrixTerm3, pPerspectiveMatrixTerm0, pRoiHalfHeight, pRoiHalfWidth);
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
@@ -963,7 +1084,7 @@ RppStatus warp_perspective_nn_f16_f16_host_tensor(Rpp16f *srcPtr,
                 int vectorLoopCount = 0;
                 Rpp32f locX, locY, locW, srcX, srcY;
                 compute_warp_perspective_src_loc_params(i, vectorLoopCount, locW, locY, locX, perspectiveMatrix_f9, roiHalfHeight, roiHalfWidth);
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m256 plocX, plocY, plocW, pSrcX, pSrcY;
                 compute_warp_perspective_src_loc_first_term_avx(locX, locY, locW, plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6, pPerspectiveMatrixTerm3, pPerspectiveMatrixTerm0, pRoiHalfHeight, pRoiHalfWidth);
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
@@ -1003,7 +1124,7 @@ RppStatus warp_perspective_nn_f16_f16_host_tensor(Rpp16f *srcPtr,
                 int vectorLoopCount = 0;
                 Rpp32f locX, locY, locW, srcX, srcY;
                 compute_warp_perspective_src_loc_params(i, vectorLoopCount, locW, locY, locX, perspectiveMatrix_f9, roiHalfHeight, roiHalfWidth);
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m256 plocX, plocY, plocW, pSrcX, pSrcY;
                 compute_warp_perspective_src_loc_first_term_avx(locX, locY, locW, plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6, pPerspectiveMatrixTerm3, pPerspectiveMatrixTerm0, pRoiHalfHeight, pRoiHalfWidth);
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
@@ -1080,7 +1201,7 @@ RppStatus warp_perspective_bilinear_u8_u8_host_tensor(Rpp8u *srcPtr,
         Rpp32u bufferLength = roi.xywhROI.roiWidth;
         Rpp32u alignedLength = bufferLength & ~7;   // Align dst width to process 16 dst pixels per iteration
 
-#if __AVX2__
+#if defined(__AVX2__)
         __m256 pBilinearCoeffs[4];
         __m256 pSrcStrideH = _mm256_set1_ps(srcDescPtr->strides.hStride);
         __m256 pPerspectiveMatrixTerm0 = _mm256_setr_ps(0, perspectiveMatrix_f9->data[0], perspectiveMatrix_f9->data[0] * 2, perspectiveMatrix_f9->data[0] * 3, perspectiveMatrix_f9->data[0] * 4, perspectiveMatrix_f9->data[0] * 5, perspectiveMatrix_f9->data[0] * 6, perspectiveMatrix_f9->data[0] * 7);
@@ -1102,6 +1223,28 @@ RppStatus warp_perspective_bilinear_u8_u8_host_tensor(Rpp8u *srcPtr,
         pxSrcStridesCHW[1] = _mm256_set1_epi32(srcDescPtr->strides.hStride);
         pxSrcStridesCHW[2] = _mm256_set1_epi32(srcDescPtr->strides.wStride);
         RpptBilinearNbhoodLocsVecLen8 srcLocs;
+#elif defined(__loongarch_asx)
+        __m256 pBilinearCoeffs[4];
+        __m256 pSrcStrideH = lasx_set1_f32(srcDescPtr->strides.hStride);
+        __m256 pPerspectiveMatrixTerm0 = lasx_setr_f32(0, perspectiveMatrix_f9->data[0], perspectiveMatrix_f9->data[0] * 2, perspectiveMatrix_f9->data[0] * 3, perspectiveMatrix_f9->data[0] * 4, perspectiveMatrix_f9->data[0] * 5, perspectiveMatrix_f9->data[0] * 6, perspectiveMatrix_f9->data[0] * 7);
+        __m256 pPerspectiveMatrixTerm3 = lasx_setr_f32(0, perspectiveMatrix_f9->data[3], perspectiveMatrix_f9->data[3] * 2, perspectiveMatrix_f9->data[3] * 3, perspectiveMatrix_f9->data[3] * 4, perspectiveMatrix_f9->data[3] * 5, perspectiveMatrix_f9->data[3] * 6, perspectiveMatrix_f9->data[3] * 7);
+        __m256 pPerspectiveMatrixTerm6 = lasx_setr_f32(0, perspectiveMatrix_f9->data[6], perspectiveMatrix_f9->data[6] * 2, perspectiveMatrix_f9->data[6] * 3, perspectiveMatrix_f9->data[6] * 4, perspectiveMatrix_f9->data[6] * 5, perspectiveMatrix_f9->data[6] * 6, perspectiveMatrix_f9->data[6] * 7);
+        __m256 pPerspectiveMatrixTerm0Incr = lasx_set1_f32(perspectiveMatrix_f9->data[0] * 8);
+        __m256 pPerspectiveMatrixTerm3Incr = lasx_set1_f32(perspectiveMatrix_f9->data[3] * 8);
+        __m256 pPerspectiveMatrixTerm6Incr = lasx_set1_f32(perspectiveMatrix_f9->data[6] * 8);
+        __m256 pRoiHalfHeight = lasx_set1_f32(roiHalfHeight);
+        __m256 pRoiHalfWidth = lasx_set1_f32(roiHalfWidth);
+        __m256 pRoiLTRB[4];
+        pRoiLTRB[0] = lasx_set1_f32(roiLTRB.ltrbROI.lt.x);
+        pRoiLTRB[1] = lasx_set1_f32(roiLTRB.ltrbROI.lt.y);
+        pRoiLTRB[2] = lasx_set1_f32(roiLTRB.ltrbROI.rb.x);
+        pRoiLTRB[3] = lasx_set1_f32(roiLTRB.ltrbROI.rb.y);
+
+        __m256i pxSrcStridesCHW[3];
+        pxSrcStridesCHW[0] = __lasx_xvreplgr2vr_w(srcDescPtr->strides.cStride);
+        pxSrcStridesCHW[1] = __lasx_xvreplgr2vr_w(srcDescPtr->strides.hStride);
+        pxSrcStridesCHW[2] = __lasx_xvreplgr2vr_w(srcDescPtr->strides.wStride);
+        RpptBilinearNbhoodLocsVecLen8 srcLocs;
 #endif
 
         // Warp perspective with fused output-layout toggle (NHWC -> NCHW)
@@ -1121,7 +1264,7 @@ RppStatus warp_perspective_bilinear_u8_u8_host_tensor(Rpp8u *srcPtr,
                 int vectorLoopCount = 0;
                 Rpp32f locX, locY, locW, srcX, srcY;
                 compute_warp_perspective_src_loc_params(i, vectorLoopCount, locW, locY, locX, perspectiveMatrix_f9, roiHalfHeight, roiHalfWidth);
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m256 plocX, plocY, plocW, pSrcX, pSrcY;
                 compute_warp_perspective_src_loc_first_term_avx(locX, locY, locW, plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6, pPerspectiveMatrixTerm3, pPerspectiveMatrixTerm0, pRoiHalfHeight, pRoiHalfWidth);
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
@@ -1165,7 +1308,7 @@ RppStatus warp_perspective_bilinear_u8_u8_host_tensor(Rpp8u *srcPtr,
                 int vectorLoopCount = 0;
                 Rpp32f locX, locY, locW, srcX, srcY;
                 compute_warp_perspective_src_loc_params(i, vectorLoopCount, locW, locY, locX, perspectiveMatrix_f9, roiHalfHeight, roiHalfWidth);
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m256 plocX, plocY, plocW, pSrcX, pSrcY;
                 compute_warp_perspective_src_loc_first_term_avx(locX, locY, locW, plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6, pPerspectiveMatrixTerm3, pPerspectiveMatrixTerm0, pRoiHalfHeight, pRoiHalfWidth);
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
@@ -1206,7 +1349,7 @@ RppStatus warp_perspective_bilinear_u8_u8_host_tensor(Rpp8u *srcPtr,
                 int vectorLoopCount = 0;
                 Rpp32f locX, locY, locW, srcX, srcY;
                 compute_warp_perspective_src_loc_params(i, vectorLoopCount, locW, locY, locX, perspectiveMatrix_f9, roiHalfHeight, roiHalfWidth);
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m256 plocX, plocY, plocW, pSrcX, pSrcY;
                 compute_warp_perspective_src_loc_first_term_avx(locX, locY, locW, plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6, pPerspectiveMatrixTerm3, pPerspectiveMatrixTerm0, pRoiHalfHeight, pRoiHalfWidth);
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
@@ -1252,7 +1395,7 @@ RppStatus warp_perspective_bilinear_u8_u8_host_tensor(Rpp8u *srcPtr,
                 int vectorLoopCount = 0;
                 Rpp32f locX, locY, locW, srcX, srcY;
                 compute_warp_perspective_src_loc_params(i, vectorLoopCount, locW, locY, locX, perspectiveMatrix_f9, roiHalfHeight, roiHalfWidth);
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m256 plocX, plocY, plocW, pSrcX, pSrcY;
                 compute_warp_perspective_src_loc_first_term_avx(locX, locY, locW, plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6, pPerspectiveMatrixTerm3, pPerspectiveMatrixTerm0, pRoiHalfHeight, pRoiHalfWidth);
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
@@ -1297,7 +1440,7 @@ RppStatus warp_perspective_bilinear_u8_u8_host_tensor(Rpp8u *srcPtr,
                 int vectorLoopCount = 0;
                 Rpp32f locX, locY, locW, srcX, srcY;
                 compute_warp_perspective_src_loc_params(i, vectorLoopCount, locW, locY, locX, perspectiveMatrix_f9, roiHalfHeight, roiHalfWidth);
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m256 plocX, plocY, plocW, pSrcX, pSrcY;
                 compute_warp_perspective_src_loc_first_term_avx(locX, locY, locW, plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6, pPerspectiveMatrixTerm3, pPerspectiveMatrixTerm0, pRoiHalfHeight, pRoiHalfWidth);
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
@@ -1367,7 +1510,7 @@ RppStatus warp_perspective_bilinear_f32_f32_host_tensor(Rpp32f *srcPtr,
         Rpp32u bufferLength = roi.xywhROI.roiWidth;
         Rpp32u alignedLength = bufferLength & ~7;   // Align dst width to process 16 dst pixels per iteration
 
-#if __AVX2__
+#if defined(__AVX2__)
         __m256 pBilinearCoeffs[4];
         __m256 pSrcStrideH = _mm256_set1_ps(srcDescPtr->strides.hStride);
         __m256 pPerspectiveMatrixTerm0 = _mm256_setr_ps(0, perspectiveMatrix_f9->data[0], perspectiveMatrix_f9->data[0] * 2, perspectiveMatrix_f9->data[0] * 3, perspectiveMatrix_f9->data[0] * 4, perspectiveMatrix_f9->data[0] * 5, perspectiveMatrix_f9->data[0] * 6, perspectiveMatrix_f9->data[0] * 7);
@@ -1389,6 +1532,28 @@ RppStatus warp_perspective_bilinear_f32_f32_host_tensor(Rpp32f *srcPtr,
         pxSrcStridesCHW[1] = _mm256_set1_epi32(srcDescPtr->strides.hStride);
         pxSrcStridesCHW[2] = _mm256_set1_epi32(srcDescPtr->strides.wStride);
         RpptBilinearNbhoodLocsVecLen8 srcLocs;
+#elif defined(__loongarch_asx)
+        __m256 pBilinearCoeffs[4];
+        __m256 pSrcStrideH = lasx_set1_f32(srcDescPtr->strides.hStride);
+        __m256 pPerspectiveMatrixTerm0 = lasx_setr_f32(0, perspectiveMatrix_f9->data[0], perspectiveMatrix_f9->data[0] * 2, perspectiveMatrix_f9->data[0] * 3, perspectiveMatrix_f9->data[0] * 4, perspectiveMatrix_f9->data[0] * 5, perspectiveMatrix_f9->data[0] * 6, perspectiveMatrix_f9->data[0] * 7);
+        __m256 pPerspectiveMatrixTerm3 = lasx_setr_f32(0, perspectiveMatrix_f9->data[3], perspectiveMatrix_f9->data[3] * 2, perspectiveMatrix_f9->data[3] * 3, perspectiveMatrix_f9->data[3] * 4, perspectiveMatrix_f9->data[3] * 5, perspectiveMatrix_f9->data[3] * 6, perspectiveMatrix_f9->data[3] * 7);
+        __m256 pPerspectiveMatrixTerm6 = lasx_setr_f32(0, perspectiveMatrix_f9->data[6], perspectiveMatrix_f9->data[6] * 2, perspectiveMatrix_f9->data[6] * 3, perspectiveMatrix_f9->data[6] * 4, perspectiveMatrix_f9->data[6] * 5, perspectiveMatrix_f9->data[6] * 6, perspectiveMatrix_f9->data[6] * 7);
+        __m256 pPerspectiveMatrixTerm0Incr = lasx_set1_f32(perspectiveMatrix_f9->data[0] * 8);
+        __m256 pPerspectiveMatrixTerm3Incr = lasx_set1_f32(perspectiveMatrix_f9->data[3] * 8);
+        __m256 pPerspectiveMatrixTerm6Incr = lasx_set1_f32(perspectiveMatrix_f9->data[6] * 8);
+        __m256 pRoiHalfHeight = lasx_set1_f32(roiHalfHeight);
+        __m256 pRoiHalfWidth = lasx_set1_f32(roiHalfWidth);
+        __m256 pRoiLTRB[4];
+        pRoiLTRB[0] = lasx_set1_f32(roiLTRB.ltrbROI.lt.x);
+        pRoiLTRB[1] = lasx_set1_f32(roiLTRB.ltrbROI.lt.y);
+        pRoiLTRB[2] = lasx_set1_f32(roiLTRB.ltrbROI.rb.x);
+        pRoiLTRB[3] = lasx_set1_f32(roiLTRB.ltrbROI.rb.y);
+
+        __m256i pxSrcStridesCHW[3];
+        pxSrcStridesCHW[0] = __lasx_xvreplgr2vr_w(srcDescPtr->strides.cStride);
+        pxSrcStridesCHW[1] = __lasx_xvreplgr2vr_w(srcDescPtr->strides.hStride);
+        pxSrcStridesCHW[2] = __lasx_xvreplgr2vr_w(srcDescPtr->strides.wStride);
+        RpptBilinearNbhoodLocsVecLen8 srcLocs;
 #endif
 
         // Warp perspective with fused output-layout toggle (NHWC -> NCHW)
@@ -1408,7 +1573,7 @@ RppStatus warp_perspective_bilinear_f32_f32_host_tensor(Rpp32f *srcPtr,
                 int vectorLoopCount = 0;
                 Rpp32f locX, locY, locW, srcX, srcY;
                 compute_warp_perspective_src_loc_params(i, vectorLoopCount, locW, locY, locX, perspectiveMatrix_f9, roiHalfHeight, roiHalfWidth);
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m256 plocX, plocY, plocW, pSrcX, pSrcY;
                 compute_warp_perspective_src_loc_first_term_avx(locX, locY, locW, plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6, pPerspectiveMatrixTerm3, pPerspectiveMatrixTerm0, pRoiHalfHeight, pRoiHalfWidth);
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
@@ -1452,7 +1617,7 @@ RppStatus warp_perspective_bilinear_f32_f32_host_tensor(Rpp32f *srcPtr,
                 int vectorLoopCount = 0;
                 Rpp32f locX, locY, locW, srcX, srcY;
                 compute_warp_perspective_src_loc_params(i, vectorLoopCount, locW, locY, locX, perspectiveMatrix_f9, roiHalfHeight, roiHalfWidth);
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m256 plocX, plocY, plocW, pSrcX, pSrcY;
                 compute_warp_perspective_src_loc_first_term_avx(locX, locY, locW, plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6, pPerspectiveMatrixTerm3, pPerspectiveMatrixTerm0, pRoiHalfHeight, pRoiHalfWidth);
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
@@ -1493,7 +1658,7 @@ RppStatus warp_perspective_bilinear_f32_f32_host_tensor(Rpp32f *srcPtr,
                 int vectorLoopCount = 0;
                 Rpp32f locX, locY, locW, srcX, srcY;
                 compute_warp_perspective_src_loc_params(i, vectorLoopCount, locW, locY, locX, perspectiveMatrix_f9, roiHalfHeight, roiHalfWidth);
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m256 plocX, plocY, plocW, pSrcX, pSrcY;
                 compute_warp_perspective_src_loc_first_term_avx(locX, locY, locW, plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6, pPerspectiveMatrixTerm3, pPerspectiveMatrixTerm0, pRoiHalfHeight, pRoiHalfWidth);
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
@@ -1539,7 +1704,7 @@ RppStatus warp_perspective_bilinear_f32_f32_host_tensor(Rpp32f *srcPtr,
                 int vectorLoopCount = 0;
                 Rpp32f locX, locY, locW, srcX, srcY;
                 compute_warp_perspective_src_loc_params(i, vectorLoopCount, locW, locY, locX, perspectiveMatrix_f9, roiHalfHeight, roiHalfWidth);
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m256 plocX, plocY, plocW, pSrcX, pSrcY;
                 compute_warp_perspective_src_loc_first_term_avx(locX, locY, locW, plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6, pPerspectiveMatrixTerm3, pPerspectiveMatrixTerm0, pRoiHalfHeight, pRoiHalfWidth);
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
@@ -1584,7 +1749,7 @@ RppStatus warp_perspective_bilinear_f32_f32_host_tensor(Rpp32f *srcPtr,
                 int vectorLoopCount = 0;
                 Rpp32f locX, locY, locW, srcX, srcY;
                 compute_warp_perspective_src_loc_params(i, vectorLoopCount, locW, locY, locX, perspectiveMatrix_f9, roiHalfHeight, roiHalfWidth);
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m256 plocX, plocY, plocW, pSrcX, pSrcY;
                 compute_warp_perspective_src_loc_first_term_avx(locX, locY, locW, plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6, pPerspectiveMatrixTerm3, pPerspectiveMatrixTerm0, pRoiHalfHeight, pRoiHalfWidth);
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
@@ -1654,7 +1819,7 @@ RppStatus warp_perspective_bilinear_i8_i8_host_tensor(Rpp8s *srcPtr,
         Rpp32u bufferLength = roi.xywhROI.roiWidth;
         Rpp32u alignedLength = bufferLength & ~7;   // Align dst width to process 16 dst pixels per iteration
 
-#if __AVX2__
+#if defined(__AVX2__)
         __m256 pBilinearCoeffs[4];
         __m256 pSrcStrideH = _mm256_set1_ps(srcDescPtr->strides.hStride);
         __m256 pPerspectiveMatrixTerm0 = _mm256_setr_ps(0, perspectiveMatrix_f9->data[0], perspectiveMatrix_f9->data[0] * 2, perspectiveMatrix_f9->data[0] * 3, perspectiveMatrix_f9->data[0] * 4, perspectiveMatrix_f9->data[0] * 5, perspectiveMatrix_f9->data[0] * 6, perspectiveMatrix_f9->data[0] * 7);
@@ -1676,6 +1841,28 @@ RppStatus warp_perspective_bilinear_i8_i8_host_tensor(Rpp8s *srcPtr,
         pxSrcStridesCHW[1] = _mm256_set1_epi32(srcDescPtr->strides.hStride);
         pxSrcStridesCHW[2] = _mm256_set1_epi32(srcDescPtr->strides.wStride);
         RpptBilinearNbhoodLocsVecLen8 srcLocs;
+#elif defined(__loongarch_asx)
+        __m256 pBilinearCoeffs[4];
+        __m256 pSrcStrideH = lasx_set1_f32(srcDescPtr->strides.hStride);
+        __m256 pPerspectiveMatrixTerm0 = lasx_setr_f32(0, perspectiveMatrix_f9->data[0], perspectiveMatrix_f9->data[0] * 2, perspectiveMatrix_f9->data[0] * 3, perspectiveMatrix_f9->data[0] * 4, perspectiveMatrix_f9->data[0] * 5, perspectiveMatrix_f9->data[0] * 6, perspectiveMatrix_f9->data[0] * 7);
+        __m256 pPerspectiveMatrixTerm3 = lasx_setr_f32(0, perspectiveMatrix_f9->data[3], perspectiveMatrix_f9->data[3] * 2, perspectiveMatrix_f9->data[3] * 3, perspectiveMatrix_f9->data[3] * 4, perspectiveMatrix_f9->data[3] * 5, perspectiveMatrix_f9->data[3] * 6, perspectiveMatrix_f9->data[3] * 7);
+        __m256 pPerspectiveMatrixTerm6 = lasx_setr_f32(0, perspectiveMatrix_f9->data[6], perspectiveMatrix_f9->data[6] * 2, perspectiveMatrix_f9->data[6] * 3, perspectiveMatrix_f9->data[6] * 4, perspectiveMatrix_f9->data[6] * 5, perspectiveMatrix_f9->data[6] * 6, perspectiveMatrix_f9->data[6] * 7);
+        __m256 pPerspectiveMatrixTerm0Incr = lasx_set1_f32(perspectiveMatrix_f9->data[0] * 8);
+        __m256 pPerspectiveMatrixTerm3Incr = lasx_set1_f32(perspectiveMatrix_f9->data[3] * 8);
+        __m256 pPerspectiveMatrixTerm6Incr = lasx_set1_f32(perspectiveMatrix_f9->data[6] * 8);
+        __m256 pRoiHalfHeight = lasx_set1_f32(roiHalfHeight);
+        __m256 pRoiHalfWidth = lasx_set1_f32(roiHalfWidth);
+        __m256 pRoiLTRB[4];
+        pRoiLTRB[0] = lasx_set1_f32(roiLTRB.ltrbROI.lt.x);
+        pRoiLTRB[1] = lasx_set1_f32(roiLTRB.ltrbROI.lt.y);
+        pRoiLTRB[2] = lasx_set1_f32(roiLTRB.ltrbROI.rb.x);
+        pRoiLTRB[3] = lasx_set1_f32(roiLTRB.ltrbROI.rb.y);
+
+        __m256i pxSrcStridesCHW[3];
+        pxSrcStridesCHW[0] = __lasx_xvreplgr2vr_w(srcDescPtr->strides.cStride);
+        pxSrcStridesCHW[1] = __lasx_xvreplgr2vr_w(srcDescPtr->strides.hStride);
+        pxSrcStridesCHW[2] = __lasx_xvreplgr2vr_w(srcDescPtr->strides.wStride);
+        RpptBilinearNbhoodLocsVecLen8 srcLocs;
 #endif
 
         // Warp perspective with fused output-layout toggle (NHWC -> NCHW)
@@ -1695,7 +1882,7 @@ RppStatus warp_perspective_bilinear_i8_i8_host_tensor(Rpp8s *srcPtr,
                 int vectorLoopCount = 0;
                 Rpp32f locX, locY, locW, srcX, srcY;
                 compute_warp_perspective_src_loc_params(i, vectorLoopCount, locW, locY, locX, perspectiveMatrix_f9, roiHalfHeight, roiHalfWidth);
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m256 plocX, plocY, plocW, pSrcX, pSrcY;
                 compute_warp_perspective_src_loc_first_term_avx(locX, locY, locW, plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6, pPerspectiveMatrixTerm3, pPerspectiveMatrixTerm0, pRoiHalfHeight, pRoiHalfWidth);
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
@@ -1740,7 +1927,7 @@ RppStatus warp_perspective_bilinear_i8_i8_host_tensor(Rpp8s *srcPtr,
                 int vectorLoopCount = 0;
                 Rpp32f locX, locY, locW, srcX, srcY;
                 compute_warp_perspective_src_loc_params(i, vectorLoopCount, locW, locY, locX, perspectiveMatrix_f9, roiHalfHeight, roiHalfWidth);
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m256 plocX, plocY, plocW, pSrcX, pSrcY;
                 compute_warp_perspective_src_loc_first_term_avx(locX, locY, locW, plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6, pPerspectiveMatrixTerm3, pPerspectiveMatrixTerm0, pRoiHalfHeight, pRoiHalfWidth);
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
@@ -1782,7 +1969,7 @@ RppStatus warp_perspective_bilinear_i8_i8_host_tensor(Rpp8s *srcPtr,
                 int vectorLoopCount = 0;
                 Rpp32f locX, locY, locW, srcX, srcY;
                 compute_warp_perspective_src_loc_params(i, vectorLoopCount, locW, locY, locX, perspectiveMatrix_f9, roiHalfHeight, roiHalfWidth);
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m256 plocX, plocY, plocW, pSrcX, pSrcY;
                 compute_warp_perspective_src_loc_first_term_avx(locX, locY, locW, plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6, pPerspectiveMatrixTerm3, pPerspectiveMatrixTerm0, pRoiHalfHeight, pRoiHalfWidth);
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
@@ -1829,7 +2016,7 @@ RppStatus warp_perspective_bilinear_i8_i8_host_tensor(Rpp8s *srcPtr,
                 int vectorLoopCount = 0;
                 Rpp32f locX, locY, locW, srcX, srcY;
                 compute_warp_perspective_src_loc_params(i, vectorLoopCount, locW, locY, locX, perspectiveMatrix_f9, roiHalfHeight, roiHalfWidth);
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m256 plocX, plocY, plocW, pSrcX, pSrcY;
                 compute_warp_perspective_src_loc_first_term_avx(locX, locY, locW, plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6, pPerspectiveMatrixTerm3, pPerspectiveMatrixTerm0, pRoiHalfHeight, pRoiHalfWidth);
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
@@ -1875,7 +2062,7 @@ RppStatus warp_perspective_bilinear_i8_i8_host_tensor(Rpp8s *srcPtr,
                 int vectorLoopCount = 0;
                 Rpp32f locX, locY, locW, srcX, srcY;
                 compute_warp_perspective_src_loc_params(i, vectorLoopCount, locW, locY, locX, perspectiveMatrix_f9, roiHalfHeight, roiHalfWidth);
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m256 plocX, plocY, plocW, pSrcX, pSrcY;
                 compute_warp_perspective_src_loc_first_term_avx(locX, locY, locW, plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6, pPerspectiveMatrixTerm3, pPerspectiveMatrixTerm0, pRoiHalfHeight, pRoiHalfWidth);
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
@@ -1946,7 +2133,7 @@ RppStatus warp_perspective_bilinear_f16_f16_host_tensor(Rpp16f *srcPtr,
         Rpp32u bufferLength = roi.xywhROI.roiWidth;
         Rpp32u alignedLength = bufferLength & ~7;   // Align dst width to process 16 dst pixels per iteration
 
-#if __AVX2__
+#if defined(__AVX2__)
         __m256 pBilinearCoeffs[4];
         __m256 pSrcStrideH = _mm256_set1_ps(srcDescPtr->strides.hStride);
         __m256 pPerspectiveMatrixTerm0 = _mm256_setr_ps(0, perspectiveMatrix_f9->data[0], perspectiveMatrix_f9->data[0] * 2, perspectiveMatrix_f9->data[0] * 3, perspectiveMatrix_f9->data[0] * 4, perspectiveMatrix_f9->data[0] * 5, perspectiveMatrix_f9->data[0] * 6, perspectiveMatrix_f9->data[0] * 7);
@@ -1968,6 +2155,28 @@ RppStatus warp_perspective_bilinear_f16_f16_host_tensor(Rpp16f *srcPtr,
         pxSrcStridesCHW[1] = _mm256_set1_epi32(srcDescPtr->strides.hStride);
         pxSrcStridesCHW[2] = _mm256_set1_epi32(srcDescPtr->strides.wStride);
         RpptBilinearNbhoodLocsVecLen8 srcLocs;
+#elif defined(__loongarch_asx)
+        __m256 pBilinearCoeffs[4];
+        __m256 pSrcStrideH = lasx_set1_f32(srcDescPtr->strides.hStride);
+        __m256 pPerspectiveMatrixTerm0 = lasx_setr_f32(0, perspectiveMatrix_f9->data[0], perspectiveMatrix_f9->data[0] * 2, perspectiveMatrix_f9->data[0] * 3, perspectiveMatrix_f9->data[0] * 4, perspectiveMatrix_f9->data[0] * 5, perspectiveMatrix_f9->data[0] * 6, perspectiveMatrix_f9->data[0] * 7);
+        __m256 pPerspectiveMatrixTerm3 = lasx_setr_f32(0, perspectiveMatrix_f9->data[3], perspectiveMatrix_f9->data[3] * 2, perspectiveMatrix_f9->data[3] * 3, perspectiveMatrix_f9->data[3] * 4, perspectiveMatrix_f9->data[3] * 5, perspectiveMatrix_f9->data[3] * 6, perspectiveMatrix_f9->data[3] * 7);
+        __m256 pPerspectiveMatrixTerm6 = lasx_setr_f32(0, perspectiveMatrix_f9->data[6], perspectiveMatrix_f9->data[6] * 2, perspectiveMatrix_f9->data[6] * 3, perspectiveMatrix_f9->data[6] * 4, perspectiveMatrix_f9->data[6] * 5, perspectiveMatrix_f9->data[6] * 6, perspectiveMatrix_f9->data[6] * 7);
+        __m256 pPerspectiveMatrixTerm0Incr = lasx_set1_f32(perspectiveMatrix_f9->data[0] * 8);
+        __m256 pPerspectiveMatrixTerm3Incr = lasx_set1_f32(perspectiveMatrix_f9->data[3] * 8);
+        __m256 pPerspectiveMatrixTerm6Incr = lasx_set1_f32(perspectiveMatrix_f9->data[6] * 8);
+        __m256 pRoiHalfHeight = lasx_set1_f32(roiHalfHeight);
+        __m256 pRoiHalfWidth = lasx_set1_f32(roiHalfWidth);
+        __m256 pRoiLTRB[4];
+        pRoiLTRB[0] = lasx_set1_f32(roiLTRB.ltrbROI.lt.x);
+        pRoiLTRB[1] = lasx_set1_f32(roiLTRB.ltrbROI.lt.y);
+        pRoiLTRB[2] = lasx_set1_f32(roiLTRB.ltrbROI.rb.x);
+        pRoiLTRB[3] = lasx_set1_f32(roiLTRB.ltrbROI.rb.y);
+
+        __m256i pxSrcStridesCHW[3];
+        pxSrcStridesCHW[0] = __lasx_xvreplgr2vr_w(srcDescPtr->strides.cStride);
+        pxSrcStridesCHW[1] = __lasx_xvreplgr2vr_w(srcDescPtr->strides.hStride);
+        pxSrcStridesCHW[2] = __lasx_xvreplgr2vr_w(srcDescPtr->strides.wStride);
+        RpptBilinearNbhoodLocsVecLen8 srcLocs;
 #endif
 
         // Warp perspective with fused output-layout toggle (NHWC -> NCHW)
@@ -1987,7 +2196,7 @@ RppStatus warp_perspective_bilinear_f16_f16_host_tensor(Rpp16f *srcPtr,
                 int vectorLoopCount = 0;
                 Rpp32f locX, locY, locW, srcX, srcY;
                 compute_warp_perspective_src_loc_params(i, vectorLoopCount, locW, locY, locX, perspectiveMatrix_f9, roiHalfHeight, roiHalfWidth);
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m256 plocX, plocY, plocW, pSrcX, pSrcY;
                 compute_warp_perspective_src_loc_first_term_avx(locX, locY, locW, plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6, pPerspectiveMatrixTerm3, pPerspectiveMatrixTerm0, pRoiHalfHeight, pRoiHalfWidth);
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
@@ -2031,7 +2240,7 @@ RppStatus warp_perspective_bilinear_f16_f16_host_tensor(Rpp16f *srcPtr,
                 int vectorLoopCount = 0;
                 Rpp32f locX, locY, locW, srcX, srcY;
                 compute_warp_perspective_src_loc_params(i, vectorLoopCount, locW, locY, locX, perspectiveMatrix_f9, roiHalfHeight, roiHalfWidth);
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m256 plocX, plocY, plocW, pSrcX, pSrcY;
                 compute_warp_perspective_src_loc_first_term_avx(locX, locY, locW, plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6, pPerspectiveMatrixTerm3, pPerspectiveMatrixTerm0, pRoiHalfHeight, pRoiHalfWidth);
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
@@ -2072,7 +2281,7 @@ RppStatus warp_perspective_bilinear_f16_f16_host_tensor(Rpp16f *srcPtr,
                 int vectorLoopCount = 0;
                 Rpp32f locX, locY, locW, srcX, srcY;
                 compute_warp_perspective_src_loc_params(i, vectorLoopCount, locW, locY, locX, perspectiveMatrix_f9, roiHalfHeight, roiHalfWidth);
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m256 plocX, plocY, plocW, pSrcX, pSrcY;
                 compute_warp_perspective_src_loc_first_term_avx(locX, locY, locW, plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6, pPerspectiveMatrixTerm3, pPerspectiveMatrixTerm0, pRoiHalfHeight, pRoiHalfWidth);
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
@@ -2118,7 +2327,7 @@ RppStatus warp_perspective_bilinear_f16_f16_host_tensor(Rpp16f *srcPtr,
                 int vectorLoopCount = 0;
                 Rpp32f locX, locY, locW, srcX, srcY;
                 compute_warp_perspective_src_loc_params(i, vectorLoopCount, locW, locY, locX, perspectiveMatrix_f9, roiHalfHeight, roiHalfWidth);
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m256 plocX, plocY, plocW, pSrcX, pSrcY;
                 compute_warp_perspective_src_loc_first_term_avx(locX, locY, locW, plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6, pPerspectiveMatrixTerm3, pPerspectiveMatrixTerm0, pRoiHalfHeight, pRoiHalfWidth);
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
@@ -2163,7 +2372,7 @@ RppStatus warp_perspective_bilinear_f16_f16_host_tensor(Rpp16f *srcPtr,
                 int vectorLoopCount = 0;
                 Rpp32f locX, locY, locW, srcX, srcY;
                 compute_warp_perspective_src_loc_params(i, vectorLoopCount, locW, locY, locX, perspectiveMatrix_f9, roiHalfHeight, roiHalfWidth);
-#if __AVX2__
+#if defined(__AVX2__) || defined(__loongarch_asx)
                 __m256 plocX, plocY, plocW, pSrcX, pSrcY;
                 compute_warp_perspective_src_loc_first_term_avx(locX, locY, locW, plocW, plocY, plocX, pSrcY, pSrcX, pPerspectiveMatrixTerm6, pPerspectiveMatrixTerm3, pPerspectiveMatrixTerm0, pRoiHalfHeight, pRoiHalfWidth);
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
diff --git a/src/modules/cpu/kernel/water.hpp b/src/modules/cpu/kernel/water.hpp
index 4aa43602..bb1e5a85 100644
--- a/src/modules/cpu/kernel/water.hpp
+++ b/src/modules/cpu/kernel/water.hpp
@@ -23,16 +23,21 @@ SOFTWARE.
 */
 
 #include "rppdefs.h"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_simd.hpp"
 #include "rpp_cpu_common.hpp"
+#endif
 
 inline void compute_water_src_loc_avx(__m256 &pDstY, __m256 &pDstX, __m256 &pSrcY, __m256 &pSrcX, __m256 *pWaterParams,
                                       __m256 &pSinFactor, __m256 &pCosFactor, __m256 &pRowLimit, __m256 &pColLimit,
                                       __m256 &pSrcStrideH, Rpp32s *srcLocArray, bool hasRGBChannels = false)
 {
-    pSrcY = _mm256_fmadd_ps(pWaterParams[1], pCosFactor, pDstY);
-    pSrcX = _mm256_fmadd_ps(pWaterParams[0], pSinFactor, pDstX);
-    pDstX = _mm256_add_ps(pDstX, avx_p8);
+    pSrcY = __lasx_xvfmadd_s(pWaterParams[1], pCosFactor, pDstY);
+    pSrcX = __lasx_xvfmadd_s(pWaterParams[0], pSinFactor, pDstX);
+    pDstX = __lasx_xvfadd_s(pDstX, avx_p8);
 }
 
 inline void compute_water_src_loc(Rpp32f dstY, Rpp32f dstX, Rpp32f &srcY, Rpp32f &srcX, Rpp32f amplY, Rpp32f amplX,
@@ -91,20 +96,20 @@ RppStatus water_u8_u8_host_tensor(Rpp8u *srcPtr,
         Rpp32s srcLocArray[8] = {0};                // Since 8 dst pixels are processed per iteration
         Rpp32s invalidLoad[8] = {0};                // Since 8 dst pixels are processed per iteration
 
-        __m256 pSrcStrideH = _mm256_set1_ps(srcDescPtr->strides.hStride);
+        __m256 pSrcStrideH = lasx_set1_f32(srcDescPtr->strides.hStride);
         __m256 pRoiLTRB[4];
-        pRoiLTRB[0] = _mm256_set1_ps(roiLTRB.ltrbROI.lt.x);
-        pRoiLTRB[1] = _mm256_set1_ps(roiLTRB.ltrbROI.lt.y);
-        pRoiLTRB[2] = _mm256_set1_ps(roiLTRB.ltrbROI.rb.x);
-        pRoiLTRB[3] = _mm256_set1_ps(roiLTRB.ltrbROI.rb.y);
+        pRoiLTRB[0] = lasx_set1_f32(roiLTRB.ltrbROI.lt.x);
+        pRoiLTRB[1] = lasx_set1_f32(roiLTRB.ltrbROI.lt.y);
+        pRoiLTRB[2] = lasx_set1_f32(roiLTRB.ltrbROI.rb.x);
+        pRoiLTRB[3] = lasx_set1_f32(roiLTRB.ltrbROI.rb.y);
 
         __m256 pWaterParams[6];
-        pWaterParams[0] = _mm256_set1_ps(amplX);
-        pWaterParams[1] = _mm256_set1_ps(amplY);
-        pWaterParams[2] = _mm256_set1_ps(freqX);
-        pWaterParams[3] = _mm256_set1_ps(freqY);
-        pWaterParams[4] = _mm256_set1_ps(phaseX);
-        pWaterParams[5] = _mm256_set1_ps(phaseY);
+        pWaterParams[0] = lasx_set1_f32(amplX);
+        pWaterParams[1] = lasx_set1_f32(amplY);
+        pWaterParams[2] = lasx_set1_f32(freqX);
+        pWaterParams[3] = lasx_set1_f32(freqY);
+        pWaterParams[4] = lasx_set1_f32(phaseX);
+        pWaterParams[5] = lasx_set1_f32(phaseY);
 
         // Water with fused output-layout toggle (NHWC -> NCHW)
         if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
@@ -126,14 +131,14 @@ RppStatus water_u8_u8_host_tensor(Rpp8u *srcPtr,
                 dstY = static_cast<Rpp32f>(i);
                 sinFactor = std::sin((freqX * dstY) + phaseX);
                 pDstX = avx_pDstLocInit;
-                pDstY = _mm256_set1_ps(dstY);
-                pSinFactor = _mm256_set1_ps(sinFactor);
+                pDstY = lasx_set1_f32(dstY);
+                pSinFactor = lasx_set1_f32(sinFactor);
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pCosFactor, pDummy, pSrcX, pSrcY;
                     __m256i pRow;
-                    sincos_ps(_mm256_fmadd_ps(pWaterParams[3], pDstX, pWaterParams[5]), &pDummy, &pCosFactor);
+                    sincos_ps(__lasx_xvfmadd_s(pWaterParams[3], pDstX, pWaterParams[5]), &pDummy, &pCosFactor);
                     compute_water_src_loc_avx(pDstY, pDstX, pSrcY, pSrcX, pWaterParams, pSinFactor, pCosFactor, pRoiLTRB[3], pRoiLTRB[2], pSrcStrideH, srcLocArray, true);
                     compute_generic_nn_srclocs_and_validate_avx(pSrcY, pSrcX, pRoiLTRB, pSrcStrideH, srcLocArray, invalidLoad, true);
                     rpp_simd_load(rpp_generic_nn_load_u8pkd3_avx, srcPtrChannel, srcLocArray, invalidLoad, pRow);
@@ -176,14 +181,14 @@ RppStatus water_u8_u8_host_tensor(Rpp8u *srcPtr,
                 dstY = static_cast<Rpp32f>(i);
                 sinFactor = std::sin((freqX * dstY) + phaseX);
                 pDstX = avx_pDstLocInit;
-                pDstY = _mm256_set1_ps(dstY);
-                pSinFactor = _mm256_set1_ps(sinFactor);
+                pDstY = lasx_set1_f32(dstY);
+                pSinFactor = lasx_set1_f32(sinFactor);
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pCosFactor, pDummy, pSrcX, pSrcY;
                     __m256i pRow[4];
-                    sincos_ps(_mm256_fmadd_ps(pWaterParams[3], pDstX, pWaterParams[5]), &pDummy, &pCosFactor);
+                    sincos_ps(__lasx_xvfmadd_s(pWaterParams[3], pDstX, pWaterParams[5]), &pDummy, &pCosFactor);
                     compute_water_src_loc_avx(pDstY, pDstX, pSrcY, pSrcX, pWaterParams, pSinFactor, pCosFactor, pRoiLTRB[3], pRoiLTRB[2], pSrcStrideH, srcLocArray);
                     compute_generic_nn_srclocs_and_validate_avx(pSrcY, pSrcX, pRoiLTRB, pSrcStrideH, srcLocArray, invalidLoad);
                     rpp_simd_load(rpp_generic_nn_load_u8pln1_avx, srcPtrChannelR, srcLocArray, invalidLoad, pRow[0]);
@@ -221,15 +226,15 @@ RppStatus water_u8_u8_host_tensor(Rpp8u *srcPtr,
                 dstY = static_cast<Rpp32f>(i);
                 sinFactor = std::sin((freqX * dstY) + phaseX);
                 pDstX = avx_pDstLocInit;
-                pDstY = _mm256_set1_ps(dstY);
-                pSinFactor = _mm256_set1_ps(sinFactor);
+                pDstY = lasx_set1_f32(dstY);
+                pSinFactor = lasx_set1_f32(sinFactor);
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pCosFactor, pDummy, pSrcX, pSrcY;
                     __m256i pRow;
-                    sincos_ps(_mm256_fmadd_ps(pWaterParams[3], pDstX, pWaterParams[5]), &pDummy, &pCosFactor);
+                    sincos_ps(__lasx_xvfmadd_s(pWaterParams[3], pDstX, pWaterParams[5]), &pDummy, &pCosFactor);
                     compute_water_src_loc_avx(pDstY, pDstX, pSrcY, pSrcX, pWaterParams, pSinFactor, pCosFactor, pRoiLTRB[3], pRoiLTRB[2], pSrcStrideH, srcLocArray, true);
                     compute_generic_nn_srclocs_and_validate_avx(pSrcY, pSrcX, pRoiLTRB, pSrcStrideH, srcLocArray, invalidLoad, true);
                     rpp_simd_load(rpp_generic_nn_load_u8pkd3_avx, srcPtrChannel, srcLocArray, invalidLoad, pRow);
@@ -265,13 +270,13 @@ RppStatus water_u8_u8_host_tensor(Rpp8u *srcPtr,
                 dstY = static_cast<Rpp32f>(i);
                 sinFactor = std::sin((freqX * dstY) + phaseX);
                 pDstX = avx_pDstLocInit;
-                pDstY = _mm256_set1_ps(dstY);
-                pSinFactor = _mm256_set1_ps(sinFactor);
+                pDstY = lasx_set1_f32(dstY);
+                pSinFactor = lasx_set1_f32(sinFactor);
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pCosFactor, pDummy, pSrcX, pSrcY;
-                    sincos_ps(_mm256_fmadd_ps(pWaterParams[3], pDstX, pWaterParams[5]), &pDummy, &pCosFactor);
+                    sincos_ps(__lasx_xvfmadd_s(pWaterParams[3], pDstX, pWaterParams[5]), &pDummy, &pCosFactor);
                     compute_water_src_loc_avx(pDstY, pDstX, pSrcY, pSrcX, pWaterParams, pSinFactor, pCosFactor, pRoiLTRB[3], pRoiLTRB[2], pSrcStrideH, srcLocArray);
                     compute_generic_nn_srclocs_and_validate_avx(pSrcY, pSrcX, pRoiLTRB, pSrcStrideH, srcLocArray, invalidLoad);
                     Rpp8u *dstPtrTempChn, *srcPtrTempChn;
@@ -281,7 +286,7 @@ RppStatus water_u8_u8_host_tensor(Rpp8u *srcPtr,
                     {
                         __m256i pRow;
                         rpp_simd_load(rpp_generic_nn_load_u8pln1_avx, srcPtrTempChn, srcLocArray, invalidLoad, pRow);
-                        rpp_storeu_si64((__m128i *)(dstPtrTempChn), _mm256_castsi256_si128(pRow));
+                        rpp_storeu_si64((__m128i *)(dstPtrTempChn), __lasx_cvt_256_128(pRow));
                         srcPtrTempChn += srcDescPtr->strides.cStride;
                         dstPtrTempChn += dstDescPtr->strides.cStride;
                     }
@@ -352,20 +357,20 @@ RppStatus water_f32_f32_host_tensor(Rpp32f *srcPtr,
         Rpp32s srcLocArray[8] = {0};                // Since 8 dst pixels are processed per iteration
         Rpp32s invalidLoad[8] = {0};                // Since 8 dst pixels are processed per iteration
 
-        __m256 pSrcStrideH = _mm256_set1_ps(srcDescPtr->strides.hStride);
+        __m256 pSrcStrideH = lasx_set1_f32(srcDescPtr->strides.hStride);
         __m256 pRoiLTRB[4];
-        pRoiLTRB[0] = _mm256_set1_ps(roiLTRB.ltrbROI.lt.x);
-        pRoiLTRB[1] = _mm256_set1_ps(roiLTRB.ltrbROI.lt.y);
-        pRoiLTRB[2] = _mm256_set1_ps(roiLTRB.ltrbROI.rb.x);
-        pRoiLTRB[3] = _mm256_set1_ps(roiLTRB.ltrbROI.rb.y);
+        pRoiLTRB[0] = lasx_set1_f32(roiLTRB.ltrbROI.lt.x);
+        pRoiLTRB[1] = lasx_set1_f32(roiLTRB.ltrbROI.lt.y);
+        pRoiLTRB[2] = lasx_set1_f32(roiLTRB.ltrbROI.rb.x);
+        pRoiLTRB[3] = lasx_set1_f32(roiLTRB.ltrbROI.rb.y);
 
         __m256 pWaterParams[6];
-        pWaterParams[0] = _mm256_set1_ps(amplX);
-        pWaterParams[1] = _mm256_set1_ps(amplY);
-        pWaterParams[2] = _mm256_set1_ps(freqX);
-        pWaterParams[3] = _mm256_set1_ps(freqY);
-        pWaterParams[4] = _mm256_set1_ps(phaseX);
-        pWaterParams[5] = _mm256_set1_ps(phaseY);
+        pWaterParams[0] = lasx_set1_f32(amplX);
+        pWaterParams[1] = lasx_set1_f32(amplY);
+        pWaterParams[2] = lasx_set1_f32(freqX);
+        pWaterParams[3] = lasx_set1_f32(freqY);
+        pWaterParams[4] = lasx_set1_f32(phaseX);
+        pWaterParams[5] = lasx_set1_f32(phaseY);
 
         // Water with fused output-layout toggle (NHWC -> NCHW)
         if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
@@ -387,14 +392,14 @@ RppStatus water_f32_f32_host_tensor(Rpp32f *srcPtr,
                 dstY = static_cast<Rpp32f>(i);
                 sinFactor = std::sin((freqX * dstY) + phaseX);
                 pDstX = avx_pDstLocInit;
-                pDstY = _mm256_set1_ps(dstY);
-                pSinFactor = _mm256_set1_ps(sinFactor);
+                pDstY = lasx_set1_f32(dstY);
+                pSinFactor = lasx_set1_f32(sinFactor);
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pCosFactor, pDummy, pSrcX, pSrcY;
                     __m256 pRow[3];
-                    sincos_ps(_mm256_fmadd_ps(pWaterParams[3], pDstX, pWaterParams[5]), &pDummy, &pCosFactor);
+                    sincos_ps(__lasx_xvfmadd_s(pWaterParams[3], pDstX, pWaterParams[5]), &pDummy, &pCosFactor);
                     compute_water_src_loc_avx(pDstY, pDstX, pSrcY, pSrcX, pWaterParams, pSinFactor, pCosFactor, pRoiLTRB[3], pRoiLTRB[2], pSrcStrideH, srcLocArray, true);
                     compute_generic_nn_srclocs_and_validate_avx(pSrcY, pSrcX, pRoiLTRB, pSrcStrideH, srcLocArray, invalidLoad, true);
                     rpp_simd_load(rpp_generic_nn_load_f32pkd3_to_f32pln3_avx, srcPtrChannel, srcLocArray, invalidLoad, pRow);
@@ -437,14 +442,14 @@ RppStatus water_f32_f32_host_tensor(Rpp32f *srcPtr,
                 dstY = static_cast<Rpp32f>(i);
                 sinFactor = std::sin((freqX * dstY) + phaseX);
                 pDstX = avx_pDstLocInit;
-                pDstY = _mm256_set1_ps(dstY);
-                pSinFactor = _mm256_set1_ps(sinFactor);
+                pDstY = lasx_set1_f32(dstY);
+                pSinFactor = lasx_set1_f32(sinFactor);
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pCosFactor, pDummy, pSrcX, pSrcY;
                     __m256 pRow[4];
-                    sincos_ps(_mm256_fmadd_ps(pWaterParams[3], pDstX, pWaterParams[5]), &pDummy, &pCosFactor);
+                    sincos_ps(__lasx_xvfmadd_s(pWaterParams[3], pDstX, pWaterParams[5]), &pDummy, &pCosFactor);
                     compute_water_src_loc_avx(pDstY, pDstX, pSrcY, pSrcX, pWaterParams, pSinFactor, pCosFactor, pRoiLTRB[3], pRoiLTRB[2], pSrcStrideH, srcLocArray);
                     compute_generic_nn_srclocs_and_validate_avx(pSrcY, pSrcX, pRoiLTRB, pSrcStrideH, srcLocArray, invalidLoad);
                     rpp_simd_load(rpp_generic_nn_load_f32pln1_avx, srcPtrChannelR, srcLocArray, invalidLoad, pRow[0]);
@@ -482,14 +487,14 @@ RppStatus water_f32_f32_host_tensor(Rpp32f *srcPtr,
                 dstY = static_cast<Rpp32f>(i);
                 sinFactor = std::sin((freqX * dstY) + phaseX);
                 pDstX = avx_pDstLocInit;
-                pDstY = _mm256_set1_ps(dstY);
-                pSinFactor = _mm256_set1_ps(sinFactor);
+                pDstY = lasx_set1_f32(dstY);
+                pSinFactor = lasx_set1_f32(sinFactor);
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pCosFactor, pDummy, pSrcX, pSrcY;
                     __m256 pRow[4];
-                    sincos_ps(_mm256_fmadd_ps(pWaterParams[3], pDstX, pWaterParams[5]), &pDummy, &pCosFactor);
+                    sincos_ps(__lasx_xvfmadd_s(pWaterParams[3], pDstX, pWaterParams[5]), &pDummy, &pCosFactor);
                     compute_water_src_loc_avx(pDstY, pDstX, pSrcY, pSrcX, pWaterParams, pSinFactor, pCosFactor, pRoiLTRB[3], pRoiLTRB[2], pSrcStrideH, srcLocArray);
                     compute_generic_nn_srclocs_and_validate_avx(pSrcY, pSrcX, pRoiLTRB, pSrcStrideH, srcLocArray, invalidLoad, true);
                     rpp_simd_load(rpp_generic_nn_load_f32pkd3_to_f32pkd3_avx, srcPtrChannel, srcLocArray, invalidLoad, pRow);
@@ -525,13 +530,13 @@ RppStatus water_f32_f32_host_tensor(Rpp32f *srcPtr,
                 dstY = static_cast<Rpp32f>(i);
                 sinFactor = std::sin((freqX * dstY) + phaseX);
                 pDstX = avx_pDstLocInit;
-                pDstY = _mm256_set1_ps(dstY);
-                pSinFactor = _mm256_set1_ps(sinFactor);
+                pDstY = lasx_set1_f32(dstY);
+                pSinFactor = lasx_set1_f32(sinFactor);
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pCosFactor, pDummy, pSrcX, pSrcY;
-                    sincos_ps(_mm256_fmadd_ps(pWaterParams[3], pDstX, pWaterParams[5]), &pDummy, &pCosFactor);
+                    sincos_ps(__lasx_xvfmadd_s(pWaterParams[3], pDstX, pWaterParams[5]), &pDummy, &pCosFactor);
                     compute_water_src_loc_avx(pDstY, pDstX, pSrcY, pSrcX, pWaterParams, pSinFactor, pCosFactor, pRoiLTRB[3], pRoiLTRB[2], pSrcStrideH, srcLocArray);
                     compute_generic_nn_srclocs_and_validate_avx(pSrcY, pSrcX, pRoiLTRB, pSrcStrideH, srcLocArray, invalidLoad);
 
@@ -776,20 +781,20 @@ RppStatus water_i8_i8_host_tensor(Rpp8s *srcPtr,
         Rpp32s srcLocArray[8] = {0};                // Since 8 dst pixels are processed per iteration
         Rpp32s invalidLoad[8] = {0};                // Since 8 dst pixels are processed per iteration
 
-        __m256 pSrcStrideH = _mm256_set1_ps(srcDescPtr->strides.hStride);
+        __m256 pSrcStrideH = lasx_set1_f32(srcDescPtr->strides.hStride);
         __m256 pRoiLTRB[4];
-        pRoiLTRB[0] = _mm256_set1_ps(roiLTRB.ltrbROI.lt.x);
-        pRoiLTRB[1] = _mm256_set1_ps(roiLTRB.ltrbROI.lt.y);
-        pRoiLTRB[2] = _mm256_set1_ps(roiLTRB.ltrbROI.rb.x);
-        pRoiLTRB[3] = _mm256_set1_ps(roiLTRB.ltrbROI.rb.y);
+        pRoiLTRB[0] = lasx_set1_f32(roiLTRB.ltrbROI.lt.x);
+        pRoiLTRB[1] = lasx_set1_f32(roiLTRB.ltrbROI.lt.y);
+        pRoiLTRB[2] = lasx_set1_f32(roiLTRB.ltrbROI.rb.x);
+        pRoiLTRB[3] = lasx_set1_f32(roiLTRB.ltrbROI.rb.y);
 
         __m256 pWaterParams[6];
-        pWaterParams[0] = _mm256_set1_ps(amplX);
-        pWaterParams[1] = _mm256_set1_ps(amplY);
-        pWaterParams[2] = _mm256_set1_ps(freqX);
-        pWaterParams[3] = _mm256_set1_ps(freqY);
-        pWaterParams[4] = _mm256_set1_ps(phaseX);
-        pWaterParams[5] = _mm256_set1_ps(phaseY);
+        pWaterParams[0] = lasx_set1_f32(amplX);
+        pWaterParams[1] = lasx_set1_f32(amplY);
+        pWaterParams[2] = lasx_set1_f32(freqX);
+        pWaterParams[3] = lasx_set1_f32(freqY);
+        pWaterParams[4] = lasx_set1_f32(phaseX);
+        pWaterParams[5] = lasx_set1_f32(phaseY);
 
         // Water with fused output-layout toggle (NHWC -> NCHW)
         if ((srcDescPtr->c == 3) && (srcDescPtr->layout == RpptLayout::NHWC) && (dstDescPtr->layout == RpptLayout::NCHW))
@@ -811,14 +816,14 @@ RppStatus water_i8_i8_host_tensor(Rpp8s *srcPtr,
                 dstY = static_cast<Rpp32f>(i);
                 sinFactor = std::sin((freqX * dstY) + phaseX);
                 pDstX = avx_pDstLocInit;
-                pDstY = _mm256_set1_ps(dstY);
-                pSinFactor = _mm256_set1_ps(sinFactor);
+                pDstY = lasx_set1_f32(dstY);
+                pSinFactor = lasx_set1_f32(sinFactor);
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pCosFactor, pDummy, pSrcX, pSrcY;
                     __m256i pRow;
-                    sincos_ps(_mm256_fmadd_ps(pWaterParams[3], pDstX, pWaterParams[5]), &pDummy, &pCosFactor);
+                    sincos_ps(__lasx_xvfmadd_s(pWaterParams[3], pDstX, pWaterParams[5]), &pDummy, &pCosFactor);
                     compute_water_src_loc_avx(pDstY, pDstX, pSrcY, pSrcX, pWaterParams, pSinFactor, pCosFactor, pRoiLTRB[3], pRoiLTRB[2], pSrcStrideH, srcLocArray, true);
                     compute_generic_nn_srclocs_and_validate_avx(pSrcY, pSrcX, pRoiLTRB, pSrcStrideH, srcLocArray, invalidLoad, true);
                     rpp_simd_load(rpp_generic_nn_load_i8pkd3_avx, srcPtrChannel, srcLocArray, invalidLoad, pRow);
@@ -861,14 +866,14 @@ RppStatus water_i8_i8_host_tensor(Rpp8s *srcPtr,
                 dstY = static_cast<Rpp32f>(i);
                 sinFactor = std::sin((freqX * dstY) + phaseX);
                 pDstX = avx_pDstLocInit;
-                pDstY = _mm256_set1_ps(dstY);
-                pSinFactor = _mm256_set1_ps(sinFactor);
+                pDstY = lasx_set1_f32(dstY);
+                pSinFactor = lasx_set1_f32(sinFactor);
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pCosFactor, pDummy, pSrcX, pSrcY;
                     __m256i pRow[3];
-                    sincos_ps(_mm256_fmadd_ps(pWaterParams[3], pDstX, pWaterParams[5]), &pDummy, &pCosFactor);
+                    sincos_ps(__lasx_xvfmadd_s(pWaterParams[3], pDstX, pWaterParams[5]), &pDummy, &pCosFactor);
                     compute_water_src_loc_avx(pDstY, pDstX, pSrcY, pSrcX, pWaterParams, pSinFactor, pCosFactor, pRoiLTRB[3], pRoiLTRB[2], pSrcStrideH, srcLocArray);
                     compute_generic_nn_srclocs_and_validate_avx(pSrcY, pSrcX, pRoiLTRB, pSrcStrideH, srcLocArray, invalidLoad);
                     rpp_simd_load(rpp_generic_nn_load_i8pln1_avx, srcPtrChannelR, srcLocArray, invalidLoad, pRow[0]);
@@ -905,15 +910,15 @@ RppStatus water_i8_i8_host_tensor(Rpp8s *srcPtr,
                 dstY = static_cast<Rpp32f>(i);
                 sinFactor = std::sin((freqX * dstY) + phaseX);
                 pDstX = avx_pDstLocInit;
-                pDstY = _mm256_set1_ps(dstY);
-                pSinFactor = _mm256_set1_ps(sinFactor);
+                pDstY = lasx_set1_f32(dstY);
+                pSinFactor = lasx_set1_f32(sinFactor);
 
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pCosFactor, pDummy, pSrcX, pSrcY;
                     __m256i pRow;
-                    sincos_ps(_mm256_fmadd_ps(pWaterParams[3], pDstX, pWaterParams[5]), &pDummy, &pCosFactor);
+                    sincos_ps(__lasx_xvfmadd_s(pWaterParams[3], pDstX, pWaterParams[5]), &pDummy, &pCosFactor);
                     compute_water_src_loc_avx(pDstY, pDstX, pSrcY, pSrcX, pWaterParams, pSinFactor, pCosFactor, pRoiLTRB[3], pRoiLTRB[2], pSrcStrideH, srcLocArray, true);
                     compute_generic_nn_srclocs_and_validate_avx(pSrcY, pSrcX, pRoiLTRB, pSrcStrideH, srcLocArray, invalidLoad, true);
                     rpp_simd_load(rpp_generic_nn_load_i8pkd3_avx, srcPtrChannel, srcLocArray, invalidLoad, pRow);
@@ -949,13 +954,13 @@ RppStatus water_i8_i8_host_tensor(Rpp8s *srcPtr,
                 dstY = static_cast<Rpp32f>(i);
                 sinFactor = std::sin((freqX * dstY) + phaseX);
                 pDstX = avx_pDstLocInit;
-                pDstY = _mm256_set1_ps(dstY);
-                pSinFactor = _mm256_set1_ps(sinFactor);
+                pDstY = lasx_set1_f32(dstY);
+                pSinFactor = lasx_set1_f32(sinFactor);
                 int vectorLoopCount = 0;
                 for (; vectorLoopCount < alignedLength; vectorLoopCount += vectorIncrementPerChannel)
                 {
                     __m256 pCosFactor, pDummy, pSrcX, pSrcY;
-                    sincos_ps(_mm256_fmadd_ps(pWaterParams[3], pDstX, pWaterParams[5]), &pDummy, &pCosFactor);
+                    sincos_ps(__lasx_xvfmadd_s(pWaterParams[3], pDstX, pWaterParams[5]), &pDummy, &pCosFactor);
                     compute_water_src_loc_avx(pDstY, pDstX, pSrcY, pSrcX, pWaterParams, pSinFactor, pCosFactor, pRoiLTRB[3], pRoiLTRB[2], pSrcStrideH, srcLocArray);
                     compute_generic_nn_srclocs_and_validate_avx(pSrcY, pSrcX, pRoiLTRB, pSrcStrideH, srcLocArray, invalidLoad);
                     Rpp8s *dstPtrTempChn, *srcPtrTempChn;
@@ -965,7 +970,7 @@ RppStatus water_i8_i8_host_tensor(Rpp8s *srcPtr,
                     {
                         __m256i pRow;
                         rpp_simd_load(rpp_generic_nn_load_i8pln1_avx, srcPtrTempChn, srcLocArray, invalidLoad, pRow);
-                        rpp_storeu_si64((__m128i *)(dstPtrTempChn), _mm256_castsi256_si128(pRow));
+                        rpp_storeu_si64((__m128i *)(dstPtrTempChn), __lasx_cvt_256_128(pRow));
                         srcPtrTempChn += srcDescPtr->strides.cStride;
                         dstPtrTempChn += dstDescPtr->strides.cStride;
                     }
diff --git a/src/modules/cpu/loongarch_advanced_augmentations.hpp b/src/modules/cpu/loongarch_advanced_augmentations.hpp
new file mode 100644
index 00000000..08422e2b
--- /dev/null
+++ b/src/modules/cpu/loongarch_advanced_augmentations.hpp
@@ -0,0 +1,2657 @@
+/*
+MIT License
+
+Copyright (c) 2019 - 2024 Advanced Micro Devices, Inc.
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
+*/
+
+#ifndef HOST_ADVANCED_AUGMENTATIONS_H
+#define HOST_ADVANCED_AUGMENTATIONS_H
+
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+
+/**************** water ***************/
+
+template <typename T>
+RppStatus water_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                         Rpp32f *batch_ampl_x, Rpp32f *batch_ampl_y,
+                         Rpp32f *batch_freq_x, Rpp32f *batch_freq_y,
+                         Rpp32f *batch_phase_x, Rpp32f *batch_phase_y,
+                         Rpp32u outputFormatToggle, Rpp32u nbatchSize,
+                         RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32f ampl_x = batch_ampl_x[batchCount];
+            Rpp32f ampl_y = batch_ampl_y[batchCount];
+            Rpp32f freq_x = batch_freq_x[batchCount];
+            Rpp32f freq_y = batch_freq_y[batchCount];
+            Rpp32f phase_x = batch_phase_x[batchCount];
+            Rpp32f phase_y = batch_phase_y[batchCount];
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            T *srcPtrImageR, *srcPtrImageG, *srcPtrImageB;
+            srcPtrImageR = srcPtrImage;
+            srcPtrImageG = srcPtrImageR + imageDimMax;
+            srcPtrImageB = srcPtrImageG + imageDimMax;
+
+            Rpp32u elementsInRowMax = batch_srcSizeMax[batchCount].width;
+
+            for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+            {
+                T *dstPtrTempR, *dstPtrTempG, *dstPtrTempB;
+                dstPtrTempR = dstPtrImage + (i * elementsInRowMax);
+                dstPtrTempG = dstPtrTempR + imageDimMax;
+                dstPtrTempB = dstPtrTempG + imageDimMax;
+
+                Rpp32u bufferLength = batch_srcSize[batchCount].width;
+                Rpp32u alignedLength = bufferLength & ~3;
+
+                __m128 pI = lsx_set1_f32((Rpp32f)i);
+                __m128 pJ, pWaterI, pWaterJ;
+                __m128 pAmplX = lsx_set1_f32(ampl_x);
+                __m128 pAmplY = lsx_set1_f32(ampl_y);
+                __m128 pFreqX = lsx_set1_f32(freq_x);
+                __m128 pFreqY = lsx_set1_f32(freq_y);
+                __m128 pPhaseX = lsx_set1_f32(phase_x);
+                __m128 pPhaseY = lsx_set1_f32(phase_y);
+                __m128 p0, p1, p2, p3;
+
+                Rpp32f waterI[4], waterJ[4];
+
+                int vectorLoopCount = 0;
+                for (; vectorLoopCount < alignedLength; vectorLoopCount+=4)
+                {
+                    pJ = lsx_setr_f32((Rpp32f)(vectorLoopCount), (Rpp32f)(vectorLoopCount + 1), (Rpp32f)(vectorLoopCount + 2), (Rpp32f)(vectorLoopCount + 3));
+                    p0 = __lsx_vfmul_s(pFreqX, pI);
+                    p0 = __lsx_vfadd_s(p0, pPhaseX);
+                    sincos_ps(p0, &p1, &p2);
+                    p1 = __lsx_vfmul_s(p1, pAmplX);
+                    pWaterJ = __lsx_vfadd_s(pJ, p1);
+                    p0 = __lsx_vfmul_s(pFreqY, pJ);
+                    p0 = __lsx_vfadd_s(p0, pPhaseY);
+                    sincos_ps(p0, &p1, &p2);
+                    p2 = __lsx_vfmul_s(p2, pAmplY);
+                    pWaterI = __lsx_vfadd_s(pI, p2);
+
+                    __lsx_vst(pWaterI, waterI, 0);
+                    __lsx_vst(pWaterJ, waterJ, 0);
+
+                    for (int count = 0; count < 4; count++)
+                    {
+                        Rpp32u waterIint, waterJint;
+                        waterIint = (Rpp32u) RPPPRANGECHECK(waterI[count], 0, batch_srcSize[batchCount].height - 1);
+                        waterJint = (Rpp32u) RPPPRANGECHECK(waterJ[count], 0, batch_srcSize[batchCount].width - 1);
+
+                        *dstPtrTempR = *(srcPtrImageR + (waterIint * elementsInRowMax) + waterJint);
+                        dstPtrTempR++;
+
+                        if (channel == 3)
+                        {
+                            *dstPtrTempG = *(srcPtrImageG + (waterIint * elementsInRowMax) + waterJint);
+                            *dstPtrTempB = *(srcPtrImageB + (waterIint * elementsInRowMax) + waterJint);
+                            dstPtrTempG++;
+                            dstPtrTempB++;
+                        }
+                    }
+                }
+                for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                {
+                    Rpp32f locI = (Rpp32f) i;
+                    Rpp32f locJ = (Rpp32f) vectorLoopCount;
+
+                    Rpp32f waterLocJ = locJ + ampl_x * sin((freq_x * locI) + phase_x);
+                    Rpp32f waterLocI = locI + ampl_y * cos((freq_y * locJ) + phase_y);
+
+                    Rpp32u waterLocIint, waterLocJint;
+                    waterLocIint = (Rpp32u) RPPPRANGECHECK(waterLocI, 0, batch_srcSize[batchCount].height);
+                    waterLocJint = (Rpp32u) RPPPRANGECHECK(waterLocJ, 0, batch_srcSize[batchCount].width);
+
+                    *dstPtrTempR = *(srcPtrImageR + (waterLocIint * elementsInRowMax) + waterLocJint);
+                    dstPtrTempR++;
+
+                    if (channel == 3)
+                    {
+                        *dstPtrTempG = *(srcPtrImageG + (waterLocIint * elementsInRowMax) + waterLocJint);
+                        *dstPtrTempB = *(srcPtrImageB + (waterLocIint * elementsInRowMax) + waterLocJint);
+                        dstPtrTempG++;
+                        dstPtrTempB++;
+                    }
+                }
+            }
+            if (outputFormatToggle == 1)
+            {
+                T *dstPtrImageUnpadded = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+                T *dstPtrImageUnpaddedCopy = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width * sizeof(T));
+
+                compute_planar_to_packed_host(dstPtrImageUnpaddedCopy, batch_srcSize[batchCount], dstPtrImageUnpadded, channel);
+
+                memset(dstPtrImage, (T) 0, imageDimMax * channel * sizeof(T));
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImage, RPPI_CHN_PACKED, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32f ampl_x = batch_ampl_x[batchCount];
+            Rpp32f ampl_y = batch_ampl_y[batchCount];
+            Rpp32f freq_x = batch_freq_x[batchCount];
+            Rpp32f freq_y = batch_freq_y[batchCount];
+            Rpp32f phase_x = batch_phase_x[batchCount];
+            Rpp32f phase_y = batch_phase_y[batchCount];
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            Rpp32u elementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+
+            for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+            {
+                T *dstPtrTemp;
+                dstPtrTemp = dstPtrImage + (i * elementsInRowMax);
+
+                Rpp32u bufferLength = batch_srcSize[batchCount].width;
+                Rpp32u alignedLength = bufferLength & ~3;
+
+                __m128 pI = lsx_set1_f32((Rpp32f)i);
+                __m128 pJ, pWaterI, pWaterJ;
+                __m128 pAmplX = lsx_set1_f32(ampl_x);
+                __m128 pAmplY = lsx_set1_f32(ampl_y);
+                __m128 pFreqX = lsx_set1_f32(freq_x);
+                __m128 pFreqY = lsx_set1_f32(freq_y);
+                __m128 pPhaseX = lsx_set1_f32(phase_x);
+                __m128 pPhaseY = lsx_set1_f32(phase_y);
+                __m128 p0, p1, p2, p3;
+
+                Rpp32f waterI[4], waterJ[4];
+
+                int vectorLoopCount = 0;
+                for (; vectorLoopCount < alignedLength; vectorLoopCount+=4)
+                {
+                    pJ = lsx_setr_f32((Rpp32f)(vectorLoopCount), (Rpp32f)(vectorLoopCount + 1), (Rpp32f)(vectorLoopCount + 2), (Rpp32f)(vectorLoopCount + 3));
+                    p0 = __lsx_vfmul_s(pFreqX, pI);
+                    p0 = __lsx_vfadd_s(p0, pPhaseX);
+                    sincos_ps(p0, &p1, &p2);
+                    p1 = __lsx_vfmul_s(p1, pAmplX);
+                    pWaterJ = __lsx_vfadd_s(pJ, p1);
+                    p0 = __lsx_vfmul_s(pFreqY, pJ);
+                    p0 = __lsx_vfadd_s(p0, pPhaseY);
+                    sincos_ps(p0, &p1, &p2);
+                    p2 = __lsx_vfmul_s(p2, pAmplY);
+                    pWaterI = __lsx_vfadd_s(pI, p2);
+
+                    __lsx_vst(pWaterI, waterI, 0);
+                    __lsx_vst(pWaterJ, waterJ, 0);
+
+                    for (int count = 0; count < 4; count++)
+                    {
+                        Rpp32u waterIint, waterJint;
+                        waterIint = (Rpp32u) RPPPRANGECHECK(waterI[count], 0, batch_srcSize[batchCount].height - 1);
+                        waterJint = (Rpp32u) RPPPRANGECHECK(waterJ[count], 0, batch_srcSize[batchCount].width - 1);
+
+                        for (int c = 0; c < channel; c++)
+                        {
+                            *dstPtrTemp = *(srcPtrImage + (waterIint * elementsInRowMax) + (waterJint * channel) + c);
+                            dstPtrTemp++;
+                        }
+                    }
+                }
+                for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                {
+                    Rpp32f locI = (Rpp32f) i;
+                    Rpp32f locJ = (Rpp32f) vectorLoopCount;
+
+                    Rpp32f waterLocJ = locJ + ampl_x * sin((freq_x * locI) + phase_x);
+                    Rpp32f waterLocI = locI + ampl_y * cos((freq_y * locJ) + phase_y);
+
+                    Rpp32u waterLocIint, waterLocJint;
+                    waterLocIint = (Rpp32u) RPPPRANGECHECK(waterLocI, 0, batch_srcSize[batchCount].height);
+                    waterLocJint = (Rpp32u) RPPPRANGECHECK(waterLocJ, 0, batch_srcSize[batchCount].width);
+
+                    for (int c = 0; c < channel; c++)
+                    {
+                        *dstPtrTemp = *(srcPtrImage + (waterLocIint * elementsInRowMax) + (waterLocJint * channel) + c);
+                        dstPtrTemp++;
+                    }
+                }
+            }
+            if (outputFormatToggle == 1)
+            {
+                T *dstPtrImageUnpadded = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+                T *dstPtrImageUnpaddedCopy = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width * sizeof(T));
+
+                compute_packed_to_planar_host(dstPtrImageUnpaddedCopy, batch_srcSize[batchCount], dstPtrImageUnpadded, channel);
+
+                memset(dstPtrImage, (T) 0, imageDimMax * channel * sizeof(T));
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImage, RPPI_CHN_PLANAR, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+/**************** non_linear_blend ***************/
+
+template <typename T>
+RppStatus non_linear_blend_host_batch(T* srcPtr1, T* srcPtr2, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                         Rpp32f *batch_std_dev,
+                         Rpp32u outputFormatToggle, Rpp32u nbatchSize,
+                         RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32f std_dev = batch_std_dev[batchCount];
+            Rpp32f multiplier = - 1.0 / (2.0 * std_dev * std_dev);
+
+            T *srcPtr1Image, *srcPtr2Image, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtr1Image = srcPtr1 + loc;
+            srcPtr2Image = srcPtr2 + loc;
+            dstPtrImage = dstPtr + loc;
+
+            Rpp32u elementsInRowMax = batch_srcSizeMax[batchCount].width;
+            Rpp32s subtrahendI = (Rpp32s) (batch_srcSize[batchCount].height >> 1);
+            Rpp32s subtrahendJ = (Rpp32s) (batch_srcSize[batchCount].width >> 1);
+
+            for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+            {
+                T *srcPtr1Temp, *srcPtr2Temp, *dstPtrTemp;
+                srcPtr1Temp = srcPtr1Image + (i * elementsInRowMax);
+                srcPtr2Temp = srcPtr2Image + (i * elementsInRowMax);
+                dstPtrTemp = dstPtrImage + (i * elementsInRowMax);
+
+                Rpp32f locI = (Rpp32f) (i - subtrahendI);
+                Rpp32f relativeGaussian[4];
+
+                Rpp32u bufferLength = batch_srcSize[batchCount].width;
+                Rpp32u alignedLength = (bufferLength & ~3) - 4;
+
+                __m128i const zero = __lsx_vldi(0);
+                __m128 pMultiplier = lsx_set1_f32(multiplier);
+                __m128 pZero = lsx_set1_f32(0.0);
+                __m128 pOne = lsx_set1_f32(1.0);
+                __m128 pExpI = lsx_set1_f32(locI * locI * multiplier);
+                __m128 pJ, pExpJ, pRelGaussian;
+                __m128i px0, px1, px2, px3;
+                __m128 p0, p1;
+
+                int vectorLoopCount = 0;
+                for (; vectorLoopCount < alignedLength; vectorLoopCount+=4)
+                {
+                    pJ = lsx_setr_f32(
+                        (Rpp32f)(vectorLoopCount - subtrahendJ),
+                        (Rpp32f)(vectorLoopCount + 1 - subtrahendJ),
+                        (Rpp32f)(vectorLoopCount + 2 - subtrahendJ),
+                        (Rpp32f)(vectorLoopCount + 3 - subtrahendJ)
+                    );
+                    pExpJ = __lsx_vfmul_s(pJ, pJ);
+                    pExpJ = __lsx_vfmul_s(pExpJ, pMultiplier);
+                    pRelGaussian = __lsx_vfadd_s(pExpI, pExpJ);
+                    pRelGaussian = fast_exp_sse(pRelGaussian);
+
+                    for (int c = 0; c < channel; c++)
+                    {
+                        T *srcPtr1TempChannel, *srcPtr2TempChannel, *dstPtrTempChannel;
+                        srcPtr1TempChannel = srcPtr1Temp + (c * imageDimMax);
+                        srcPtr2TempChannel = srcPtr2Temp + (c * imageDimMax);
+                        dstPtrTempChannel = dstPtrTemp + (c * imageDimMax);
+
+                        px0 =  __lsx_vld((__m128i *)srcPtr1TempChannel, 0);
+                        px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                        p0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+
+                        px0 =  __lsx_vld((__m128i *)srcPtr2TempChannel, 0);
+                        px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                        p1 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+
+                        p0 = __lsx_vfmul_s(pRelGaussian, p0);
+                        p1 = __lsx_vfmul_s(__lsx_vfsub_s(pOne, pRelGaussian), p1);
+                        p0 = __lsx_vfadd_s(p0, p1);
+
+                        px0 = __lsx_vftint_w_s(p0);
+                        px1 = __lsx_vftint_w_s(pZero);
+                        px2 = __lsx_vftint_w_s(pZero);
+                        px3 = __lsx_vftint_w_s(pZero);
+
+                        px0 = lsx_packus_i32(px0, px1);
+                        px1 = lsx_packus_i32(px2, px3);
+
+                        px0 = lsx_packus_i16(px0, px1);    // pixels 0-15
+
+                        __lsx_vst(px0, (__m128i *)dstPtrTempChannel, 0);
+                    }
+
+                    srcPtr1Temp += 4;
+                    srcPtr2Temp += 4;
+                    dstPtrTemp += 4;
+                }
+                for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                {
+                    Rpp32s locI = i - subtrahendI;
+                    Rpp32s locJ = vectorLoopCount - subtrahendJ;
+
+                    Rpp32f gaussianValue = gaussian_2d_relative(locI, locJ, std_dev);
+
+                    for (int c = 0; c < channel; c++)
+                    {
+                        T *srcPtr1TempChannel, *srcPtr2TempChannel, *dstPtrTempChannel;
+                        srcPtr1TempChannel = srcPtr1Temp + (c * imageDimMax);
+                        srcPtr2TempChannel = srcPtr2Temp + (c * imageDimMax);
+                        dstPtrTempChannel = dstPtrTemp + (c * imageDimMax);
+
+                        *dstPtrTempChannel = (T) RPPPIXELCHECK((gaussianValue * ((Rpp32f) *srcPtr1TempChannel)) + ((1 - gaussianValue) * ((Rpp32f) *srcPtr2TempChannel)));
+                    }
+
+                    srcPtr1Temp++;
+                    srcPtr2Temp++;
+                    dstPtrTemp++;
+                }
+            }
+            if (outputFormatToggle == 1)
+            {
+                T *dstPtrImageUnpadded = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+                T *dstPtrImageUnpaddedCopy = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width * sizeof(T));
+
+                compute_planar_to_packed_host(dstPtrImageUnpaddedCopy, batch_srcSize[batchCount], dstPtrImageUnpadded, channel);
+
+                memset(dstPtrImage, (T) 0, imageDimMax * channel * sizeof(T));
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImage, RPPI_CHN_PACKED, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32f std_dev = batch_std_dev[batchCount];
+            Rpp32f multiplier = - 1.0 / (2.0 * std_dev * std_dev);
+
+            T *srcPtr1Image, *srcPtr2Image, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtr1Image = srcPtr1 + loc;
+            srcPtr2Image = srcPtr2 + loc;
+            dstPtrImage = dstPtr + loc;
+
+            Rpp32u elementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+            Rpp32s subtrahendI = (Rpp32s) (batch_srcSize[batchCount].height >> 1);
+            Rpp32s subtrahendJ = (Rpp32s) (batch_srcSize[batchCount].width >> 1);
+
+            for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+            {
+                T *srcPtr1Temp, *srcPtr2Temp, *dstPtrTemp;
+                srcPtr1Temp = srcPtr1Image + (i * elementsInRowMax);
+                srcPtr2Temp = srcPtr2Image + (i * elementsInRowMax);
+                dstPtrTemp = dstPtrImage + (i * elementsInRowMax);
+
+                Rpp32f locI = (Rpp32f) (i - subtrahendI);
+                Rpp32f relativeGaussian[4];
+
+                Rpp32u bufferLength = batch_srcSize[batchCount].width;
+                Rpp32u alignedLength = bufferLength & ~3;
+
+                __m128i const zero = __lsx_vldi(0);
+                __m128 pMultiplier = lsx_set1_f32(multiplier);
+                __m128 pZero = lsx_set1_f32(0.0);
+                __m128 pOne = lsx_set1_f32(1.0);
+                __m128 pExpI = lsx_set1_f32(locI * locI * multiplier);
+                __m128 pJ, pExpJ, pRelGaussian;
+                __m128i px0, px1, px2, px3;
+                __m128 p0, p1, p2, p3, p4, p5, p6, p7;
+
+                int vectorLoopCount = 0;
+                for (; vectorLoopCount < alignedLength; vectorLoopCount+=4)
+                {
+                    pJ = lsx_setr_f32(
+                        (Rpp32f)(vectorLoopCount - subtrahendJ),
+                        (Rpp32f)(vectorLoopCount + 1 - subtrahendJ),
+                        (Rpp32f)(vectorLoopCount + 2 - subtrahendJ),
+                        (Rpp32f)(vectorLoopCount + 3 - subtrahendJ)
+                    );
+                    pExpJ = __lsx_vfmul_s(pJ, pJ);
+                    pExpJ = __lsx_vfmul_s(pExpJ, pMultiplier);
+                    pRelGaussian = __lsx_vfadd_s(pExpI, pExpJ);
+                    pRelGaussian = fast_exp_sse(pRelGaussian);
+
+                    __lsx_vst(pRelGaussian, relativeGaussian, 0);
+
+                    __m128 pRelGaussian0 = lsx_setr_f32(relativeGaussian[0], relativeGaussian[0], relativeGaussian[0], relativeGaussian[1]);
+                    __m128 pRelGaussian1 = lsx_setr_f32(relativeGaussian[1], relativeGaussian[1], relativeGaussian[2], relativeGaussian[2]);
+                    __m128 pRelGaussian2 = lsx_setr_f32(relativeGaussian[2], relativeGaussian[3], relativeGaussian[3], relativeGaussian[3]);
+
+                    px0 =  __lsx_vld((__m128i *)srcPtr1Temp, 0);
+                    px1 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+                    px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                    p0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+                    p1 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px0));    // pixels 4-7
+                    p2 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 8-11
+                    p3 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px1));    // pixels 12-15
+
+                    px0 =  __lsx_vld((__m128i *)srcPtr2Temp, 0);
+                    px1 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+                    px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                    p4 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+                    p5 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px0));    // pixels 4-7
+                    p6 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 8-11
+                    p7 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px1));    // pixels 12-15
+
+                    p0 = __lsx_vfmul_s(pRelGaussian0, p0);
+                    p4 = __lsx_vfmul_s(__lsx_vfsub_s(pOne, pRelGaussian0), p4);
+                    p0 = __lsx_vfadd_s(p0, p4);
+
+                    p1 = __lsx_vfmul_s(pRelGaussian1, p1);
+                    p5 = __lsx_vfmul_s(__lsx_vfsub_s(pOne, pRelGaussian1), p5);
+                    p1 = __lsx_vfadd_s(p1, p5);
+
+                    p2 = __lsx_vfmul_s(pRelGaussian2, p2);
+                    p6 = __lsx_vfmul_s(__lsx_vfsub_s(pOne, pRelGaussian2), p6);
+                    p2 = __lsx_vfadd_s(p2, p6);
+
+                    px0 = __lsx_vftint_w_s(p0);
+                    px1 = __lsx_vftint_w_s(p1);
+                    px2 = __lsx_vftint_w_s(p2);
+                    px3 = __lsx_vftint_w_s(pZero);
+
+                    px0 = lsx_packus_i32(px0, px1);
+                    px1 = lsx_packus_i32(px2, px3);
+
+                    px0 = lsx_packus_i16(px0, px1);    // pixels 0-15
+
+                    __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+
+                    srcPtr1Temp += 12;
+                    srcPtr2Temp += 12;
+                    dstPtrTemp += 12;
+                }
+                for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                {
+                    Rpp32s locI = i - subtrahendI;
+                    Rpp32s locJ = vectorLoopCount - subtrahendJ;
+
+                    Rpp32f gaussianValue = gaussian_2d_relative(locI, locJ, std_dev);
+
+                    for (int c = 0; c < channel; c++)
+                    {
+                        *dstPtrTemp = (T) RPPPIXELCHECK((gaussianValue * ((Rpp32f) *srcPtr1Temp)) + ((1 - gaussianValue) * ((Rpp32f) *srcPtr2Temp)));
+                        srcPtr1Temp++;
+                        srcPtr2Temp++;
+                        dstPtrTemp++;
+                    }
+                }
+            }
+            if (outputFormatToggle == 1)
+            {
+                T *dstPtrImageUnpadded = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+                T *dstPtrImageUnpaddedCopy = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width * sizeof(T));
+
+                compute_packed_to_planar_host(dstPtrImageUnpaddedCopy, batch_srcSize[batchCount], dstPtrImageUnpadded, channel);
+
+                memset(dstPtrImage, (T) 0, imageDimMax * channel * sizeof(T));
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImage, RPPI_CHN_PLANAR, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+RppStatus non_linear_blend_f32_host_batch(Rpp32f* srcPtr1, Rpp32f* srcPtr2, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, Rpp32f* dstPtr,
+                         Rpp32f *batch_std_dev,
+                         Rpp32u outputFormatToggle, Rpp32u nbatchSize,
+                         RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32f std_dev = batch_std_dev[batchCount];
+            Rpp32f multiplier = - 1.0 / (2.0 * std_dev * std_dev);
+
+            Rpp32f *srcPtr1Image, *srcPtr2Image, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtr1Image = srcPtr1 + loc;
+            srcPtr2Image = srcPtr2 + loc;
+            dstPtrImage = dstPtr + loc;
+
+            Rpp32u elementsInRowMax = batch_srcSizeMax[batchCount].width;
+            Rpp32s subtrahendI = (Rpp32s) (batch_srcSize[batchCount].height >> 1);
+            Rpp32s subtrahendJ = (Rpp32s) (batch_srcSize[batchCount].width >> 1);
+
+            for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+            {
+                Rpp32f *srcPtr1Temp, *srcPtr2Temp, *dstPtrTemp;
+                srcPtr1Temp = srcPtr1Image + (i * elementsInRowMax);
+                srcPtr2Temp = srcPtr2Image + (i * elementsInRowMax);
+                dstPtrTemp = dstPtrImage + (i * elementsInRowMax);
+
+                Rpp32f locI = (Rpp32f) (i - subtrahendI);
+                Rpp32f relativeGaussian[4];
+
+                Rpp32u bufferLength = batch_srcSize[batchCount].width;
+                Rpp32u alignedLength = bufferLength & ~3;
+
+                __m128 pMultiplier = lsx_set1_f32(multiplier);
+                __m128 pOne = lsx_set1_f32(1.0);
+                __m128 pExpI = lsx_set1_f32(locI * locI * multiplier);
+                __m128 pJ, pExpJ, pRelGaussian;
+                __m128 p0, p1;
+
+                int vectorLoopCount = 0;
+                for (; vectorLoopCount < alignedLength; vectorLoopCount+=4)
+                {
+                    pJ = lsx_setr_f32(
+                        (Rpp32f)(vectorLoopCount - subtrahendJ),
+                        (Rpp32f)(vectorLoopCount + 1 - subtrahendJ),
+                        (Rpp32f)(vectorLoopCount + 2 - subtrahendJ),
+                        (Rpp32f)(vectorLoopCount + 3 - subtrahendJ)
+                    );
+                    pExpJ = __lsx_vfmul_s(pJ, pJ);
+                    pExpJ = __lsx_vfmul_s(pExpJ, pMultiplier);
+                    pRelGaussian = __lsx_vfadd_s(pExpI, pExpJ);
+                    pRelGaussian = fast_exp_sse(pRelGaussian);
+
+                    for (int c = 0; c < channel; c++)
+                    {
+                        Rpp32f *srcPtr1TempChannel, *srcPtr2TempChannel, *dstPtrTempChannel;
+                        srcPtr1TempChannel = srcPtr1Temp + (c * imageDimMax);
+                        srcPtr2TempChannel = srcPtr2Temp + (c * imageDimMax);
+                        dstPtrTempChannel = dstPtrTemp + (c * imageDimMax);
+
+                        p0 = (__m128)__lsx_vld(srcPtr1TempChannel, 0);
+                        p1 = (__m128)__lsx_vld(srcPtr2TempChannel, 0);
+
+                        p0 = __lsx_vfmul_s(pRelGaussian, p0);
+                        p1 = __lsx_vfmul_s(__lsx_vfsub_s(pOne, pRelGaussian), p1);
+                        p0 = __lsx_vfadd_s(p0, p1);
+
+                        __lsx_vst(p0, dstPtrTempChannel, 0);
+                    }
+
+                    srcPtr1Temp += 4;
+                    srcPtr2Temp += 4;
+                    dstPtrTemp += 4;
+                }
+                for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                {
+                    Rpp32s locI = i - subtrahendI;
+                    Rpp32s locJ = vectorLoopCount - subtrahendJ;
+
+                    Rpp32f gaussianValue = gaussian_2d_relative(locI, locJ, std_dev);
+
+                    for (int c = 0; c < channel; c++)
+                    {
+                        Rpp32f *srcPtr1TempChannel, *srcPtr2TempChannel, *dstPtrTempChannel;
+                        srcPtr1TempChannel = srcPtr1Temp + (c * imageDimMax);
+                        srcPtr2TempChannel = srcPtr2Temp + (c * imageDimMax);
+                        dstPtrTempChannel = dstPtrTemp + (c * imageDimMax);
+
+                        *dstPtrTempChannel = (gaussianValue * *srcPtr1TempChannel) + ((1 - gaussianValue) * *srcPtr2TempChannel);
+                    }
+
+                    srcPtr1Temp++;
+                    srcPtr2Temp++;
+                    dstPtrTemp++;
+                }
+            }
+            if (outputFormatToggle == 1)
+            {
+                Rpp32f *dstPtrImageUnpadded = (Rpp32f*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(Rpp32f));
+                Rpp32f *dstPtrImageUnpaddedCopy = (Rpp32f*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(Rpp32f));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width * sizeof(Rpp32f));
+
+                compute_planar_to_packed_host(dstPtrImageUnpaddedCopy, batch_srcSize[batchCount], dstPtrImageUnpadded, channel);
+
+                memset(dstPtrImage, (Rpp32f) 0, imageDimMax * channel * sizeof(Rpp32f));
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImage, RPPI_CHN_PACKED, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32f std_dev = batch_std_dev[batchCount];
+            Rpp32f multiplier = - 1.0 / (2.0 * std_dev * std_dev);
+
+            Rpp32f *srcPtr1Image, *srcPtr2Image, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtr1Image = srcPtr1 + loc;
+            srcPtr2Image = srcPtr2 + loc;
+            dstPtrImage = dstPtr + loc;
+
+            Rpp32u elementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+            Rpp32s subtrahendI = (Rpp32s) (batch_srcSize[batchCount].height >> 1);
+            Rpp32s subtrahendJ = (Rpp32s) (batch_srcSize[batchCount].width >> 1);
+
+            for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+            {
+                Rpp32f *srcPtr1Temp, *srcPtr2Temp, *dstPtrTemp;
+                srcPtr1Temp = srcPtr1Image + (i * elementsInRowMax);
+                srcPtr2Temp = srcPtr2Image + (i * elementsInRowMax);
+                dstPtrTemp = dstPtrImage + (i * elementsInRowMax);
+
+                Rpp32f locI = (Rpp32f) (i - subtrahendI);
+                Rpp32f relativeGaussian[4];
+
+                Rpp32u bufferLength = batch_srcSize[batchCount].width;
+                Rpp32u alignedLength = bufferLength & ~3;
+
+                __m128 pMultiplier = lsx_set1_f32(multiplier);
+                __m128 pOne = lsx_set1_f32(1.0);
+                __m128 pExpI = lsx_set1_f32(locI * locI * multiplier);
+                __m128 pJ, pExpJ, pRelGaussian;
+                __m128 p0, p1, p2, p4, p5, p6;
+
+                int vectorLoopCount = 0;
+                for (; vectorLoopCount < alignedLength; vectorLoopCount+=4)
+                {
+                    pJ = lsx_setr_f32(
+                        (Rpp32f)(vectorLoopCount - subtrahendJ),
+                        (Rpp32f)(vectorLoopCount + 1 - subtrahendJ),
+                        (Rpp32f)(vectorLoopCount + 2 - subtrahendJ),
+                        (Rpp32f)(vectorLoopCount + 3 - subtrahendJ)
+                    );
+                    pExpJ = __lsx_vfmul_s(pJ, pJ);
+                    pExpJ = __lsx_vfmul_s(pExpJ, pMultiplier);
+                    pRelGaussian = __lsx_vfadd_s(pExpI, pExpJ);
+                    pRelGaussian = fast_exp_sse(pRelGaussian);
+
+                    __lsx_vst(pRelGaussian, relativeGaussian, 0);
+
+                    __m128 pRelGaussian0 = lsx_setr_f32(relativeGaussian[0], relativeGaussian[0], relativeGaussian[0], relativeGaussian[1]);
+                    __m128 pRelGaussian1 = lsx_setr_f32(relativeGaussian[1], relativeGaussian[1], relativeGaussian[2], relativeGaussian[2]);
+                    __m128 pRelGaussian2 = lsx_setr_f32(relativeGaussian[2], relativeGaussian[3], relativeGaussian[3], relativeGaussian[3]);
+
+                    p0 = (__m128)__lsx_vld(srcPtr1Temp, 0);
+                    p1 = (__m128)__lsx_vld(srcPtr1Temp + 4, 0);
+                    p2 = (__m128)__lsx_vld(srcPtr1Temp + 8, 0);
+
+                    p4 = (__m128)__lsx_vld(srcPtr2Temp, 0);
+                    p5 = (__m128)__lsx_vld(srcPtr2Temp + 4, 0);
+                    p6 = (__m128)__lsx_vld(srcPtr2Temp + 8, 0);
+
+                    p0 = __lsx_vfmul_s(pRelGaussian0, p0);
+                    p4 = __lsx_vfmul_s(__lsx_vfsub_s(pOne, pRelGaussian0), p4);
+                    p0 = __lsx_vfadd_s(p0, p4);
+
+                    p1 = __lsx_vfmul_s(pRelGaussian1, p1);
+                    p5 = __lsx_vfmul_s(__lsx_vfsub_s(pOne, pRelGaussian1), p5);
+                    p1 = __lsx_vfadd_s(p1, p5);
+
+                    p2 = __lsx_vfmul_s(pRelGaussian2, p2);
+                    p6 = __lsx_vfmul_s(__lsx_vfsub_s(pOne, pRelGaussian2), p6);
+                    p2 = __lsx_vfadd_s(p2, p6);
+
+                    __lsx_vst(p0, dstPtrTemp, 0);
+                    __lsx_vst(p1, dstPtrTemp + 4, 0);
+                    __lsx_vst(p2, dstPtrTemp + 8, 0);
+
+                    srcPtr1Temp += 12;
+                    srcPtr2Temp += 12;
+                    dstPtrTemp += 12;
+                }
+                for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                {
+                    Rpp32s locI = i - subtrahendI;
+                    Rpp32s locJ = vectorLoopCount - subtrahendJ;
+
+                    Rpp32f gaussianValue = gaussian_2d_relative(locI, locJ, std_dev);
+
+                    for (int c = 0; c < channel; c++)
+                    {
+                        *dstPtrTemp = (gaussianValue * *srcPtr1Temp) + ((1 - gaussianValue) * *srcPtr2Temp);
+                        srcPtr1Temp++;
+                        srcPtr2Temp++;
+                        dstPtrTemp++;
+                    }
+                }
+            }
+            if (outputFormatToggle == 1)
+            {
+                Rpp32f *dstPtrImageUnpadded = (Rpp32f*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(Rpp32f));
+                Rpp32f *dstPtrImageUnpaddedCopy = (Rpp32f*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(Rpp32f));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width * sizeof(Rpp32f));
+
+                compute_packed_to_planar_host(dstPtrImageUnpaddedCopy, batch_srcSize[batchCount], dstPtrImageUnpadded, channel);
+
+                memset(dstPtrImage, (Rpp32f) 0, imageDimMax * channel * sizeof(Rpp32f));
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImage, RPPI_CHN_PLANAR, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+RppStatus non_linear_blend_f16_host_batch(Rpp16f* srcPtr1, Rpp16f* srcPtr2, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, Rpp16f* dstPtr,
+                         Rpp32f *batch_std_dev,
+                         Rpp32u outputFormatToggle, Rpp32u nbatchSize,
+                         RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32f std_dev = batch_std_dev[batchCount];
+            Rpp32f multiplier = - 1.0 / (2.0 * std_dev * std_dev);
+
+            Rpp16f *srcPtr1Image, *srcPtr2Image, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtr1Image = srcPtr1 + loc;
+            srcPtr2Image = srcPtr2 + loc;
+            dstPtrImage = dstPtr + loc;
+
+            Rpp32u elementsInRowMax = batch_srcSizeMax[batchCount].width;
+            Rpp32s subtrahendI = (Rpp32s) (batch_srcSize[batchCount].height >> 1);
+            Rpp32s subtrahendJ = (Rpp32s) (batch_srcSize[batchCount].width >> 1);
+
+            Rpp32f srcPtr1TempChannelps[4], srcPtr2TempChannelps[4], dstPtrTempChannelps[4];
+
+            for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+            {
+                Rpp16f *srcPtr1Temp, *srcPtr2Temp, *dstPtrTemp;
+                srcPtr1Temp = srcPtr1Image + (i * elementsInRowMax);
+                srcPtr2Temp = srcPtr2Image + (i * elementsInRowMax);
+                dstPtrTemp = dstPtrImage + (i * elementsInRowMax);
+
+                Rpp32f locI = (Rpp32f) (i - subtrahendI);
+                Rpp32f relativeGaussian[4];
+
+                Rpp32u bufferLength = batch_srcSize[batchCount].width;
+                Rpp32u alignedLength = bufferLength & ~3;
+
+                __m128 pMultiplier = lsx_set1_f32(multiplier);
+                __m128 pOne = lsx_set1_f32(1.0);
+                __m128 pExpI = lsx_set1_f32(locI * locI * multiplier);
+                __m128 pJ, pExpJ, pRelGaussian;
+                __m128 p0, p1;
+
+                int vectorLoopCount = 0;
+                for (; vectorLoopCount < alignedLength; vectorLoopCount+=4)
+                {
+                    pJ = lsx_setr_f32(
+                        (Rpp32f)(vectorLoopCount - subtrahendJ),
+                        (Rpp32f)(vectorLoopCount + 1 - subtrahendJ),
+                        (Rpp32f)(vectorLoopCount + 2 - subtrahendJ),
+                        (Rpp32f)(vectorLoopCount + 3 - subtrahendJ)
+                    );
+                    pExpJ = __lsx_vfmul_s(pJ, pJ);
+                    pExpJ = __lsx_vfmul_s(pExpJ, pMultiplier);
+                    pRelGaussian = __lsx_vfadd_s(pExpI, pExpJ);
+                    pRelGaussian = fast_exp_sse(pRelGaussian);
+
+                    for (int c = 0; c < channel; c++)
+                    {
+                        Rpp16f *srcPtr1TempChannel, *srcPtr2TempChannel, *dstPtrTempChannel;
+                        srcPtr1TempChannel = srcPtr1Temp + (c * imageDimMax);
+                        srcPtr2TempChannel = srcPtr2Temp + (c * imageDimMax);
+                        dstPtrTempChannel = dstPtrTemp + (c * imageDimMax);
+
+                        for(int cnt = 0; cnt < 4; cnt++)
+                        {
+                            *(srcPtr1TempChannelps + cnt) = (Rpp32f) (*(srcPtr1TempChannel + cnt));
+                            *(srcPtr2TempChannelps + cnt) = (Rpp32f) (*(srcPtr2TempChannel + cnt));
+                        }
+
+                        p0 = (__m128)__lsx_vld(srcPtr1TempChannelps, 0);
+                        p1 = (__m128)__lsx_vld(srcPtr2TempChannelps, 0);
+
+                        p0 = __lsx_vfmul_s(pRelGaussian, p0);
+                        p1 = __lsx_vfmul_s(__lsx_vfsub_s(pOne, pRelGaussian), p1);
+                        p0 = __lsx_vfadd_s(p0, p1);
+
+                        __lsx_vst(p0, dstPtrTempChannelps, 0);
+
+                        for(int cnt = 0; cnt < 4; cnt++)
+                        {
+                            *(dstPtrTempChannel + cnt) = (Rpp16f) (*(dstPtrTempChannelps + cnt));
+                        }
+                    }
+
+                    srcPtr1Temp += 4;
+                    srcPtr2Temp += 4;
+                    dstPtrTemp += 4;
+                }
+                for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                {
+                    Rpp32s locI = i - subtrahendI;
+                    Rpp32s locJ = vectorLoopCount - subtrahendJ;
+
+                    Rpp32f gaussianValue = gaussian_2d_relative(locI, locJ, std_dev);
+
+                    for (int c = 0; c < channel; c++)
+                    {
+                        Rpp16f *srcPtr1TempChannel, *srcPtr2TempChannel, *dstPtrTempChannel;
+                        srcPtr1TempChannel = srcPtr1Temp + (c * imageDimMax);
+                        srcPtr2TempChannel = srcPtr2Temp + (c * imageDimMax);
+                        dstPtrTempChannel = dstPtrTemp + (c * imageDimMax);
+
+                        *dstPtrTempChannel = (Rpp16f) ((gaussianValue * *srcPtr1TempChannel) + ((1 - gaussianValue) * *srcPtr2TempChannel));
+                    }
+
+                    srcPtr1Temp++;
+                    srcPtr2Temp++;
+                    dstPtrTemp++;
+                }
+            }
+            if (outputFormatToggle == 1)
+            {
+                Rpp16f *dstPtrImageUnpadded = (Rpp16f*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(Rpp16f));
+                Rpp16f *dstPtrImageUnpaddedCopy = (Rpp16f*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(Rpp16f));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width * sizeof(Rpp16f));
+
+                compute_planar_to_packed_host(dstPtrImageUnpaddedCopy, batch_srcSize[batchCount], dstPtrImageUnpadded, channel);
+
+                memset(dstPtrImage, (Rpp16f) 0, imageDimMax * channel * sizeof(Rpp16f));
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImage, RPPI_CHN_PACKED, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32f std_dev = batch_std_dev[batchCount];
+            Rpp32f multiplier = - 1.0 / (2.0 * std_dev * std_dev);
+
+            Rpp16f *srcPtr1Image, *srcPtr2Image, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtr1Image = srcPtr1 + loc;
+            srcPtr2Image = srcPtr2 + loc;
+            dstPtrImage = dstPtr + loc;
+
+            Rpp32u elementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+            Rpp32s subtrahendI = (Rpp32s) (batch_srcSize[batchCount].height >> 1);
+            Rpp32s subtrahendJ = (Rpp32s) (batch_srcSize[batchCount].width >> 1);
+
+            Rpp32f srcPtr1Tempps[12], srcPtr2Tempps[12], dstPtrTempps[12];
+
+            for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+            {
+                Rpp16f *srcPtr1Temp, *srcPtr2Temp, *dstPtrTemp;
+                srcPtr1Temp = srcPtr1Image + (i * elementsInRowMax);
+                srcPtr2Temp = srcPtr2Image + (i * elementsInRowMax);
+                dstPtrTemp = dstPtrImage + (i * elementsInRowMax);
+
+                Rpp32f locI = (Rpp32f) (i - subtrahendI);
+                Rpp32f relativeGaussian[4];
+
+                Rpp32u bufferLength = batch_srcSize[batchCount].width;
+                Rpp32u alignedLength = bufferLength & ~3;
+
+                __m128 pMultiplier = lsx_set1_f32(multiplier);
+                __m128 pOne = lsx_set1_f32(1.0);
+                __m128 pExpI = lsx_set1_f32(locI * locI * multiplier);
+                __m128 pJ, pExpJ, pRelGaussian;
+                __m128 p0, p1, p2, p4, p5, p6;
+
+                int vectorLoopCount = 0;
+                for (; vectorLoopCount < alignedLength; vectorLoopCount+=4)
+                {
+                    pJ = lsx_setr_f32(
+                        (Rpp32f)(vectorLoopCount - subtrahendJ),
+                        (Rpp32f)(vectorLoopCount + 1 - subtrahendJ),
+                        (Rpp32f)(vectorLoopCount + 2 - subtrahendJ),
+                        (Rpp16f)(vectorLoopCount + 3 - subtrahendJ)
+                    );
+                    pExpJ = __lsx_vfmul_s(pJ, pJ);
+                    pExpJ = __lsx_vfmul_s(pExpJ, pMultiplier);
+                    pRelGaussian = __lsx_vfadd_s(pExpI, pExpJ);
+                    pRelGaussian = fast_exp_sse(pRelGaussian);
+
+                    __lsx_vst(pRelGaussian, relativeGaussian, 0);
+
+                    __m128 pRelGaussian0 = lsx_setr_f32(relativeGaussian[0], relativeGaussian[0], relativeGaussian[0], relativeGaussian[1]);
+                    __m128 pRelGaussian1 = lsx_setr_f32(relativeGaussian[1], relativeGaussian[1], relativeGaussian[2], relativeGaussian[2]);
+                    __m128 pRelGaussian2 = lsx_setr_f32(relativeGaussian[2], relativeGaussian[3], relativeGaussian[3], relativeGaussian[3]);
+
+                    for(int cnt = 0; cnt < 12; cnt++)
+                    {
+                        *(srcPtr1Tempps + cnt) = (Rpp32f) (*(srcPtr1Temp + cnt));
+                        *(srcPtr2Tempps + cnt) = (Rpp32f) (*(srcPtr2Temp + cnt));
+                    }
+
+                    p0 = (__m128)__lsx_vld(srcPtr1Tempps, 0);
+                    p1 = (__m128)__lsx_vld(srcPtr1Tempps + 4, 0);
+                    p2 = (__m128)__lsx_vld(srcPtr1Tempps + 8, 0);
+
+                    p4 = (__m128)__lsx_vld(srcPtr2Tempps, 0);
+                    p5 = (__m128)__lsx_vld(srcPtr2Tempps + 4, 0);
+                    p6 = (__m128)__lsx_vld(srcPtr2Tempps + 8, 0);
+
+                    p0 = __lsx_vfmul_s(pRelGaussian0, p0);
+                    p4 = __lsx_vfmul_s(__lsx_vfsub_s(pOne, pRelGaussian0), p4);
+                    p0 = __lsx_vfadd_s(p0, p4);
+
+                    p1 = __lsx_vfmul_s(pRelGaussian1, p1);
+                    p5 = __lsx_vfmul_s(__lsx_vfsub_s(pOne, pRelGaussian1), p5);
+                    p1 = __lsx_vfadd_s(p1, p5);
+
+                    p2 = __lsx_vfmul_s(pRelGaussian2, p2);
+                    p6 = __lsx_vfmul_s(__lsx_vfsub_s(pOne, pRelGaussian2), p6);
+                    p2 = __lsx_vfadd_s(p2, p6);
+
+                    __lsx_vst(p0, dstPtrTempps, 0);
+                    __lsx_vst(p1, dstPtrTempps + 4, 0);
+                    __lsx_vst(p2, dstPtrTempps + 8, 0);
+
+                    for(int cnt = 0; cnt < 12; cnt++)
+                    {
+                        *(dstPtrTemp + cnt) = (Rpp16f) (*(dstPtrTempps + cnt));
+                    }
+
+                    srcPtr1Temp += 12;
+                    srcPtr2Temp += 12;
+                    dstPtrTemp += 12;
+                }
+                for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                {
+                    Rpp32s locI = i - subtrahendI;
+                    Rpp32s locJ = vectorLoopCount - subtrahendJ;
+
+                    Rpp32f gaussianValue = gaussian_2d_relative(locI, locJ, std_dev);
+
+                    for (int c = 0; c < channel; c++)
+                    {
+                        *dstPtrTemp = (Rpp16f) ((gaussianValue * *srcPtr1Temp) + ((1 - gaussianValue) * *srcPtr2Temp));
+                        srcPtr1Temp++;
+                        srcPtr2Temp++;
+                        dstPtrTemp++;
+                    }
+                }
+            }
+            if (outputFormatToggle == 1)
+            {
+                Rpp16f *dstPtrImageUnpadded = (Rpp16f*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(Rpp16f));
+                Rpp16f *dstPtrImageUnpaddedCopy = (Rpp16f*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(Rpp16f));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width * sizeof(Rpp16f));
+
+                compute_packed_to_planar_host(dstPtrImageUnpaddedCopy, batch_srcSize[batchCount], dstPtrImageUnpadded, channel);
+
+                memset(dstPtrImage, (Rpp16f) 0, imageDimMax * channel * sizeof(Rpp16f));
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImage, RPPI_CHN_PLANAR, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+RppStatus non_linear_blend_i8_host_batch(Rpp8s* srcPtr1, Rpp8s* srcPtr2, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, Rpp8s* dstPtr,
+                         Rpp32f *batch_std_dev,
+                         Rpp32u outputFormatToggle, Rpp32u nbatchSize,
+                         RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp64u bufferLength = batch_srcSizeMax[0].height * batch_srcSizeMax[0].width * channel * nbatchSize;
+
+    Rpp8u *srcPtr1_8u = (Rpp8u*) calloc(bufferLength, sizeof(Rpp8u));
+    Rpp8u *srcPtr2_8u = (Rpp8u*) calloc(bufferLength, sizeof(Rpp8u));
+    Rpp8u *dstPtr_8u = (Rpp8u*) calloc(bufferLength, sizeof(Rpp8u));
+
+    Rpp8s *srcPtr1Temp, *srcPtr2Temp;
+    srcPtr1Temp = srcPtr1;
+    srcPtr2Temp = srcPtr2;
+
+    Rpp8u *srcPtr1_8uTemp, *srcPtr2_8uTemp;
+    srcPtr1_8uTemp = srcPtr1_8u;
+    srcPtr2_8uTemp = srcPtr2_8u;
+
+    for (int i = 0; i < bufferLength; i++)
+    {
+        *srcPtr1_8uTemp = (Rpp8u) RPPPIXELCHECK(((Rpp32s) *srcPtr1Temp) + 128);
+        *srcPtr2_8uTemp = (Rpp8u) RPPPIXELCHECK(((Rpp32s) *srcPtr2Temp) + 128);
+        srcPtr1Temp++;
+        srcPtr2Temp++;
+        srcPtr1_8uTemp++;
+        srcPtr2_8uTemp++;
+    }
+
+    non_linear_blend_host_batch<Rpp8u>(srcPtr1_8u, srcPtr2_8u, batch_srcSize, batch_srcSizeMax, dstPtr_8u, batch_std_dev, outputFormatToggle, nbatchSize, chnFormat, channel, handle);
+
+    Rpp8s *dstPtrTemp;
+    dstPtrTemp = dstPtr;
+
+    Rpp8u *dstPtr_8uTemp;
+    dstPtr_8uTemp = dstPtr_8u;
+
+    for (int i = 0; i < bufferLength; i++)
+    {
+        *dstPtrTemp = (Rpp8s) (((Rpp32s) *dstPtr_8uTemp) - 128);
+        dstPtrTemp++;
+        dstPtr_8uTemp++;
+    }
+
+    free(srcPtr1_8u);
+    free(srcPtr2_8u);
+    free(dstPtr_8u);
+
+    return RPP_SUCCESS;
+}
+
+/**************** color_cast ***************/
+
+template <typename T>
+RppStatus color_cast_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                         Rpp8u *batch_r, Rpp8u *batch_g, Rpp8u *batch_b, Rpp32f *batch_alpha,
+                         Rpp32u outputFormatToggle, Rpp32u nbatchSize,
+                         RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp8u r = batch_r[batchCount];
+            Rpp8u g = batch_g[batchCount];
+            Rpp8u b = batch_b[batchCount];
+            Rpp32f alpha = batch_alpha[batchCount];
+
+            Rpp32f userPixel[3] = {(Rpp32f) b, (Rpp32f) g, (Rpp32f) r};
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            Rpp32u elementsInRowMax = batch_srcSizeMax[batchCount].width;
+
+            for (int c = 0; c < channel; c++)
+            {
+                T *srcPtrChannel, *dstPtrChannel;
+                srcPtrChannel = srcPtrImage + (c * imageDimMax);
+                dstPtrChannel = dstPtrImage + (c * imageDimMax);
+
+                __m128i const zero = __lsx_vldi(0);
+                __m128 pZero = lsx_set1_f32(0.0);
+                __m128 pOne = lsx_set1_f32(1.0);
+                __m128 pUserPixel = lsx_set1_f32(userPixel[c]);
+                __m128 pAlpha = lsx_set1_f32(alpha);
+
+                for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+                {
+                    T *srcPtrTemp, *dstPtrTemp;
+                    srcPtrTemp = srcPtrChannel + (i * elementsInRowMax);
+                    dstPtrTemp = dstPtrChannel + (i * elementsInRowMax);
+
+                    Rpp32u bufferLength = batch_srcSize[batchCount].width;
+                    Rpp32u alignedLength = (bufferLength & ~3) - 4;
+
+                    __m128i px0, px1, px2, px3;
+                    __m128 p0, p1;
+
+                    int vectorLoopCount = 0;
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount+=4)
+                    {
+                        px0 =  __lsx_vld((__m128i *)srcPtrTemp, 0);
+                        px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                        p0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+
+                        p0 = __lsx_vfmul_s(pAlpha, p0);
+                        p1 = __lsx_vfmul_s(__lsx_vfsub_s(pOne, pAlpha), pUserPixel);
+                        p0 = __lsx_vfadd_s(p0, p1);
+
+                        px0 = __lsx_vftint_w_s(p0);
+                        px1 = __lsx_vftint_w_s(pZero);
+                        px2 = __lsx_vftint_w_s(pZero);
+                        px3 = __lsx_vftint_w_s(pZero);
+
+                        px0 = lsx_packus_i32(px0, px1);
+                        px1 = lsx_packus_i32(px2, px3);
+
+                        px0 = lsx_packus_i16(px0, px1);    // pixels 0-15
+
+                        __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+
+                        srcPtrTemp += 4;
+                        dstPtrTemp += 4;
+                    }
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                    {
+                        *dstPtrTemp = (T) RPPPIXELCHECK((alpha * ((Rpp32f) *srcPtrTemp)) + ((1 - alpha) * userPixel[c]));
+
+                        srcPtrTemp++;
+                        dstPtrTemp++;
+                    }
+                }
+            }
+            if (outputFormatToggle == 1)
+            {
+                T *dstPtrImageUnpadded = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+                T *dstPtrImageUnpaddedCopy = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width * sizeof(T));
+
+                compute_planar_to_packed_host(dstPtrImageUnpaddedCopy, batch_srcSize[batchCount], dstPtrImageUnpadded, channel);
+
+                memset(dstPtrImage, (T) 0, imageDimMax * channel * sizeof(T));
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImage, RPPI_CHN_PACKED, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp8u r = batch_r[batchCount];
+            Rpp8u g = batch_g[batchCount];
+            Rpp8u b = batch_b[batchCount];
+            Rpp32f alpha = batch_alpha[batchCount];
+
+            Rpp32f userPixel[3] = {(Rpp32f) b, (Rpp32f) g, (Rpp32f) r};
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            Rpp32u elementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+
+            for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+            {
+                T *srcPtrTemp, *dstPtrTemp;
+                srcPtrTemp = srcPtrImage + (i * elementsInRowMax);
+                dstPtrTemp = dstPtrImage + (i * elementsInRowMax);
+
+                Rpp32u bufferLength = batch_srcSize[batchCount].width;
+                Rpp32u alignedLength = bufferLength & ~3;
+
+                __m128i const zero = __lsx_vldi(0);
+                __m128 pZero = lsx_set1_f32(0.0);
+                __m128 pOne = lsx_set1_f32(1.0);
+                __m128 pAlpha = lsx_set1_f32(alpha);
+                __m128 pUserPixelPartialVector1 = lsx_setr_f32(userPixel[0], userPixel[1], userPixel[2], userPixel[0]);
+                __m128 pUserPixelPartialVector2 = lsx_setr_f32(userPixel[1], userPixel[2], userPixel[0], userPixel[1]);
+                __m128 pUserPixelPartialVector3 = lsx_setr_f32(userPixel[2], userPixel[0], userPixel[1], userPixel[2]);
+                __m128i px0, px1, px2, px3;
+                __m128 p0, p1, p2, p3, p4, p5, p6, p7;
+
+                int vectorLoopCount = 0;
+                for (; vectorLoopCount < alignedLength; vectorLoopCount+=4)
+                {
+                    px0 =  __lsx_vld((__m128i *)srcPtrTemp, 0);
+                    px1 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+                    px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                    p0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+                    p1 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px0));    // pixels 4-7
+                    p2 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 8-11
+                    p3 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px1));    // pixels 12-15
+
+                    p0 = __lsx_vfmul_s(pAlpha, p0);
+                    p4 = __lsx_vfmul_s(__lsx_vfsub_s(pOne, pAlpha), pUserPixelPartialVector1);
+                    p0 = __lsx_vfadd_s(p0, p4);
+
+                    p1 = __lsx_vfmul_s(pAlpha, p1);
+                    p5 = __lsx_vfmul_s(__lsx_vfsub_s(pOne, pAlpha), pUserPixelPartialVector2);
+                    p1 = __lsx_vfadd_s(p1, p5);
+
+                    p2 = __lsx_vfmul_s(pAlpha, p2);
+                    p6 = __lsx_vfmul_s(__lsx_vfsub_s(pOne, pAlpha), pUserPixelPartialVector3);
+                    p2 = __lsx_vfadd_s(p2, p6);
+
+                    px0 = __lsx_vftint_w_s(p0);
+                    px1 = __lsx_vftint_w_s(p1);
+                    px2 = __lsx_vftint_w_s(p2);
+                    px3 = __lsx_vftint_w_s(pZero);
+
+                    px0 = lsx_packus_i32(px0, px1);
+                    px1 = lsx_packus_i32(px2, px3);
+
+                    px0 = lsx_packus_i16(px0, px1);    // pixels 0-15
+
+                    __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+
+                    srcPtrTemp += 12;
+                    dstPtrTemp += 12;
+                }
+                for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                {
+                    for (int c = 0; c < channel; c++)
+                    {
+                        *dstPtrTemp = (T) RPPPIXELCHECK((alpha * ((Rpp32f) *srcPtrTemp)) + ((1 - alpha) * userPixel[c]));
+                        srcPtrTemp++;
+                        dstPtrTemp++;
+                    }
+                }
+            }
+            if (outputFormatToggle == 1)
+            {
+                T *dstPtrImageUnpadded = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+                T *dstPtrImageUnpaddedCopy = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width * sizeof(T));
+
+                compute_packed_to_planar_host(dstPtrImageUnpaddedCopy, batch_srcSize[batchCount], dstPtrImageUnpadded, channel);
+
+                memset(dstPtrImage, (T) 0, imageDimMax * channel * sizeof(T));
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImage, RPPI_CHN_PLANAR, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+RppStatus color_cast_f32_host_batch(Rpp32f* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, Rpp32f* dstPtr,
+                         Rpp8u *batch_r, Rpp8u *batch_g, Rpp8u *batch_b, Rpp32f *batch_alpha,
+                         Rpp32u outputFormatToggle, Rpp32u nbatchSize,
+                         RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp8u r = batch_r[batchCount];
+            Rpp8u g = batch_g[batchCount];
+            Rpp8u b = batch_b[batchCount];
+            Rpp32f alpha = batch_alpha[batchCount];
+
+            Rpp32f userPixel[3] = {((Rpp32f) b) / 255, ((Rpp32f) g) / 255, ((Rpp32f) r) / 255};
+
+            Rpp32f *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            Rpp32u elementsInRowMax = batch_srcSizeMax[batchCount].width;
+
+            for (int c = 0; c < channel; c++)
+            {
+                Rpp32f *srcPtrChannel, *dstPtrChannel;
+                srcPtrChannel = srcPtrImage + (c * imageDimMax);
+                dstPtrChannel = dstPtrImage + (c * imageDimMax);
+
+                __m128 pOne = lsx_set1_f32(1.0);
+                __m128 pUserPixel = lsx_set1_f32(userPixel[c]);
+                __m128 pAlpha = lsx_set1_f32(alpha);
+
+                for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+                {
+                    Rpp32f *srcPtrTemp, *dstPtrTemp;
+                    srcPtrTemp = srcPtrChannel + (i * elementsInRowMax);
+                    dstPtrTemp = dstPtrChannel + (i * elementsInRowMax);
+
+                    Rpp32u bufferLength = batch_srcSize[batchCount].width;
+                    Rpp32u alignedLength = (bufferLength & ~3) - 4;
+
+                    __m128i px0, px1, px2, px3;
+                    __m128 p0, p1;
+
+                    int vectorLoopCount = 0;
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount+=4)
+                    {
+                        p0 = (__m128)__lsx_vld(srcPtrTemp, 0);
+
+                        p0 = __lsx_vfmul_s(pAlpha, p0);
+                        p1 = __lsx_vfmul_s(__lsx_vfsub_s(pOne, pAlpha), pUserPixel);
+                        p0 = __lsx_vfadd_s(p0, p1);
+
+                        __lsx_vst(p0, dstPtrTemp, 0);
+
+                        srcPtrTemp += 4;
+                        dstPtrTemp += 4;
+                    }
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                    {
+                        *dstPtrTemp = (Rpp32f) ((alpha * ((Rpp32f) *srcPtrTemp)) + ((1 - alpha) * userPixel[c]));
+
+                        srcPtrTemp++;
+                        dstPtrTemp++;
+                    }
+                }
+            }
+            if (outputFormatToggle == 1)
+            {
+                Rpp32f *dstPtrImageUnpadded = (Rpp32f*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(Rpp32f));
+                Rpp32f *dstPtrImageUnpaddedCopy = (Rpp32f*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(Rpp32f));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width * sizeof(Rpp32f));
+
+                compute_planar_to_packed_host(dstPtrImageUnpaddedCopy, batch_srcSize[batchCount], dstPtrImageUnpadded, channel);
+
+                memset(dstPtrImage, (Rpp32f) 0, imageDimMax * channel * sizeof(Rpp32f));
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImage, RPPI_CHN_PACKED, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp8u r = batch_r[batchCount];
+            Rpp8u g = batch_g[batchCount];
+            Rpp8u b = batch_b[batchCount];
+            Rpp32f alpha = batch_alpha[batchCount];
+
+            Rpp32f userPixel[3] = {((Rpp32f) b) / 255, ((Rpp32f) g) / 255, ((Rpp32f) r) / 255};
+
+            Rpp32f *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            Rpp32u elementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+
+            for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+            {
+                Rpp32f *srcPtrTemp, *dstPtrTemp;
+                srcPtrTemp = srcPtrImage + (i * elementsInRowMax);
+                dstPtrTemp = dstPtrImage + (i * elementsInRowMax);
+
+                Rpp32u bufferLength = batch_srcSize[batchCount].width;
+                Rpp32u alignedLength = bufferLength & ~3;
+
+                __m128 pOne = lsx_set1_f32(1.0);
+                __m128 pAlpha = lsx_set1_f32(alpha);
+                __m128 pUserPixelPartialVector1 = lsx_setr_f32(userPixel[0], userPixel[1], userPixel[2], userPixel[0]);
+                __m128 pUserPixelPartialVector2 = lsx_setr_f32(userPixel[1], userPixel[2], userPixel[0], userPixel[1]);
+                __m128 pUserPixelPartialVector3 = lsx_setr_f32(userPixel[2], userPixel[0], userPixel[1], userPixel[2]);
+                __m128i px0, px1, px2, px3;
+                __m128 p0, p1, p2, p4, p5, p6;
+
+                int vectorLoopCount = 0;
+                for (; vectorLoopCount < alignedLength; vectorLoopCount+=4)
+                {
+                    p0 = (__m128)__lsx_vld(srcPtrTemp, 0);
+                    p1 = (__m128)__lsx_vld(srcPtrTemp + 4, 0);
+                    p2 = (__m128)__lsx_vld(srcPtrTemp + 8, 0);
+
+                    p0 = __lsx_vfmul_s(pAlpha, p0);
+                    p4 = __lsx_vfmul_s(__lsx_vfsub_s(pOne, pAlpha), pUserPixelPartialVector1);
+                    p0 = __lsx_vfadd_s(p0, p4);
+
+                    p1 = __lsx_vfmul_s(pAlpha, p1);
+                    p5 = __lsx_vfmul_s(__lsx_vfsub_s(pOne, pAlpha), pUserPixelPartialVector2);
+                    p1 = __lsx_vfadd_s(p1, p5);
+
+                    p2 = __lsx_vfmul_s(pAlpha, p2);
+                    p6 = __lsx_vfmul_s(__lsx_vfsub_s(pOne, pAlpha), pUserPixelPartialVector3);
+                    p2 = __lsx_vfadd_s(p2, p6);
+
+                    __lsx_vst(p0, dstPtrTemp, 0);
+                    __lsx_vst(p1, dstPtrTemp + 4, 0);
+                    __lsx_vst(p2, dstPtrTemp + 8, 0);
+
+                    srcPtrTemp += 12;
+                    dstPtrTemp += 12;
+                }
+                for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                {
+                    for (int c = 0; c < channel; c++)
+                    {
+                        *dstPtrTemp = (Rpp32f) ((alpha * ((Rpp32f) *srcPtrTemp)) + ((1 - alpha) * userPixel[c]));
+                        srcPtrTemp++;
+                        dstPtrTemp++;
+                    }
+                }
+            }
+            if (outputFormatToggle == 1)
+            {
+                Rpp32f *dstPtrImageUnpadded = (Rpp32f*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(Rpp32f));
+                Rpp32f *dstPtrImageUnpaddedCopy = (Rpp32f*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(Rpp32f));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width * sizeof(Rpp32f));
+
+                compute_packed_to_planar_host(dstPtrImageUnpaddedCopy, batch_srcSize[batchCount], dstPtrImageUnpadded, channel);
+
+                memset(dstPtrImage, (Rpp32f) 0, imageDimMax * channel * sizeof(Rpp32f));
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImage, RPPI_CHN_PLANAR, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+RppStatus color_cast_f16_host_batch(Rpp16f* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, Rpp16f* dstPtr,
+                         Rpp8u *batch_r, Rpp8u *batch_g, Rpp8u *batch_b, Rpp32f *batch_alpha,
+                         Rpp32u outputFormatToggle, Rpp32u nbatchSize,
+                         RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp8u r = batch_r[batchCount];
+            Rpp8u g = batch_g[batchCount];
+            Rpp8u b = batch_b[batchCount];
+            Rpp32f alpha = batch_alpha[batchCount];
+
+            Rpp32f userPixel[3] = {((Rpp32f) b) / 255, ((Rpp32f) g) / 255, ((Rpp32f) r) / 255};
+
+            Rpp16f *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            Rpp32u elementsInRowMax = batch_srcSizeMax[batchCount].width;
+
+            Rpp32f srcPtrTempps[4], dstPtrTempps[4];
+
+            for (int c = 0; c < channel; c++)
+            {
+                Rpp16f *srcPtrChannel, *dstPtrChannel;
+                srcPtrChannel = srcPtrImage + (c * imageDimMax);
+                dstPtrChannel = dstPtrImage + (c * imageDimMax);
+
+                __m128 pOne = lsx_set1_f32(1.0);
+                __m128 pUserPixel = lsx_set1_f32(userPixel[c]);
+                __m128 pAlpha = lsx_set1_f32(alpha);
+
+                for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+                {
+                    Rpp16f *srcPtrTemp, *dstPtrTemp;
+                    srcPtrTemp = srcPtrChannel + (i * elementsInRowMax);
+                    dstPtrTemp = dstPtrChannel + (i * elementsInRowMax);
+
+                    Rpp32u bufferLength = batch_srcSize[batchCount].width;
+                    Rpp32u alignedLength = (bufferLength & ~3) - 4;
+
+                    __m128i px0, px1, px2, px3;
+                    __m128 p0, p1;
+
+                    int vectorLoopCount = 0;
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount+=4)
+                    {
+                        for(int cnt = 0; cnt < 4; cnt++)
+                        {
+                            *(srcPtrTempps + cnt) = (Rpp32f) (*(srcPtrTemp + cnt));
+                        }
+
+                        p0 = (__m128)__lsx_vld(srcPtrTempps, 0);
+
+                        p0 = __lsx_vfmul_s(pAlpha, p0);
+                        p1 = __lsx_vfmul_s(__lsx_vfsub_s(pOne, pAlpha), pUserPixel);
+                        p0 = __lsx_vfadd_s(p0, p1);
+
+                        __lsx_vst(p0, dstPtrTempps, 0);
+
+                        for(int cnt = 0; cnt < 4; cnt++)
+                        {
+                            *(dstPtrTemp + cnt) = (Rpp16f) (*(dstPtrTempps + cnt));
+                        }
+
+                        srcPtrTemp += 4;
+                        dstPtrTemp += 4;
+                    }
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                    {
+                        *dstPtrTemp = (Rpp16f) ((alpha * ((Rpp32f) *srcPtrTemp)) + ((1 - alpha) * userPixel[c]));
+
+                        srcPtrTemp++;
+                        dstPtrTemp++;
+                    }
+                }
+            }
+            if (outputFormatToggle == 1)
+            {
+                Rpp16f *dstPtrImageUnpadded = (Rpp16f*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(Rpp16f));
+                Rpp16f *dstPtrImageUnpaddedCopy = (Rpp16f*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(Rpp16f));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width * sizeof(Rpp16f));
+
+                compute_planar_to_packed_host(dstPtrImageUnpaddedCopy, batch_srcSize[batchCount], dstPtrImageUnpadded, channel);
+
+                memset(dstPtrImage, (Rpp16f) 0, imageDimMax * channel * sizeof(Rpp16f));
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImage, RPPI_CHN_PACKED, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp8u r = batch_r[batchCount];
+            Rpp8u g = batch_g[batchCount];
+            Rpp8u b = batch_b[batchCount];
+            Rpp32f alpha = batch_alpha[batchCount];
+
+            Rpp32f userPixel[3] = {((Rpp32f) b) / 255, ((Rpp32f) g) / 255, ((Rpp32f) r) / 255};
+
+            Rpp16f *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            Rpp32u elementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+
+            Rpp32f srcPtrTempps[12], dstPtrTempps[12];
+
+            for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+            {
+                Rpp16f *srcPtrTemp, *dstPtrTemp;
+                srcPtrTemp = srcPtrImage + (i * elementsInRowMax);
+                dstPtrTemp = dstPtrImage + (i * elementsInRowMax);
+
+                Rpp32u bufferLength = batch_srcSize[batchCount].width;
+                Rpp32u alignedLength = bufferLength & ~3;
+
+                __m128 pOne = lsx_set1_f32(1.0);
+                __m128 pAlpha = lsx_set1_f32(alpha);
+                __m128 pUserPixelPartialVector1 = lsx_setr_f32(userPixel[0], userPixel[1], userPixel[2], userPixel[0]);
+                __m128 pUserPixelPartialVector2 = lsx_setr_f32(userPixel[1], userPixel[2], userPixel[0], userPixel[1]);
+                __m128 pUserPixelPartialVector3 = lsx_setr_f32(userPixel[2], userPixel[0], userPixel[1], userPixel[2]);
+                __m128i px0, px1, px2, px3;
+                __m128 p0, p1, p2, p4, p5, p6;
+
+                int vectorLoopCount = 0;
+                for (; vectorLoopCount < alignedLength; vectorLoopCount+=4)
+                {
+                    for(int cnt = 0; cnt < 12; cnt++)
+                    {
+                        *(srcPtrTempps + cnt) = (Rpp32f) (*(srcPtrTemp + cnt));
+                    }
+
+                    p0 = (__m128)__lsx_vld(srcPtrTempps, 0);
+                    p1 = (__m128)__lsx_vld(srcPtrTempps + 4, 0);
+                    p2 = (__m128)__lsx_vld(srcPtrTempps + 8, 0);
+
+                    p0 = __lsx_vfmul_s(pAlpha, p0);
+                    p4 = __lsx_vfmul_s(__lsx_vfsub_s(pOne, pAlpha), pUserPixelPartialVector1);
+                    p0 = __lsx_vfadd_s(p0, p4);
+
+                    p1 = __lsx_vfmul_s(pAlpha, p1);
+                    p5 = __lsx_vfmul_s(__lsx_vfsub_s(pOne, pAlpha), pUserPixelPartialVector2);
+                    p1 = __lsx_vfadd_s(p1, p5);
+
+                    p2 = __lsx_vfmul_s(pAlpha, p2);
+                    p6 = __lsx_vfmul_s(__lsx_vfsub_s(pOne, pAlpha), pUserPixelPartialVector3);
+                    p2 = __lsx_vfadd_s(p2, p6);
+
+                    __lsx_vst(p0, dstPtrTempps, 0);
+                    __lsx_vst(p0, dstPtrTempps + 4, 0);
+                    __lsx_vst(p0, dstPtrTempps + 8, 0);
+
+                    for(int cnt = 0; cnt < 12; cnt++)
+                    {
+                        *(dstPtrTemp + cnt) = (Rpp16f) (*(dstPtrTempps + cnt));
+                    }
+
+                    srcPtrTemp += 12;
+                    dstPtrTemp += 12;
+                }
+                for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                {
+                    for (int c = 0; c < channel; c++)
+                    {
+                        *dstPtrTemp = (Rpp16f) ((alpha * ((Rpp32f) *srcPtrTemp)) + ((1 - alpha) * userPixel[c]));
+                        srcPtrTemp++;
+                        dstPtrTemp++;
+                    }
+                }
+            }
+            if (outputFormatToggle == 1)
+            {
+                Rpp16f *dstPtrImageUnpadded = (Rpp16f*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(Rpp16f));
+                Rpp16f *dstPtrImageUnpaddedCopy = (Rpp16f*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(Rpp16f));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width * sizeof(Rpp16f));
+
+                compute_packed_to_planar_host(dstPtrImageUnpaddedCopy, batch_srcSize[batchCount], dstPtrImageUnpadded, channel);
+
+                memset(dstPtrImage, (Rpp16f) 0, imageDimMax * channel * sizeof(Rpp16f));
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImage, RPPI_CHN_PLANAR, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+RppStatus color_cast_i8_host_batch(Rpp8s* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, Rpp8s* dstPtr,
+                         Rpp8u *batch_r, Rpp8u *batch_g, Rpp8u *batch_b, Rpp32f *batch_alpha,
+                         Rpp32u outputFormatToggle, Rpp32u nbatchSize,
+                         RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp64u bufferLength = batch_srcSizeMax[0].height * batch_srcSizeMax[0].width * channel * nbatchSize;
+
+    Rpp8u *srcPtr_8u = (Rpp8u*) calloc(bufferLength, sizeof(Rpp8u));
+    Rpp8u *dstPtr_8u = (Rpp8u*) calloc(bufferLength, sizeof(Rpp8u));
+
+    Rpp8s *srcPtrTemp;
+    srcPtrTemp = srcPtr;
+
+    Rpp8u *srcPtr_8uTemp;
+    srcPtr_8uTemp = srcPtr_8u;
+
+    for (int i = 0; i < bufferLength; i++)
+    {
+        *srcPtr_8uTemp = (Rpp8u) RPPPIXELCHECK(((Rpp32s) *srcPtrTemp) + 128);
+        srcPtrTemp++;
+        srcPtr_8uTemp++;
+    }
+
+    color_cast_host_batch<Rpp8u>(srcPtr_8u, batch_srcSize, batch_srcSizeMax, dstPtr_8u, batch_r, batch_g, batch_b, batch_alpha, outputFormatToggle, nbatchSize, chnFormat, channel, handle);
+
+    Rpp8s *dstPtrTemp;
+    dstPtrTemp = dstPtr;
+
+    Rpp8u *dstPtr_8uTemp;
+    dstPtr_8uTemp = dstPtr_8u;
+
+    for (int i = 0; i < bufferLength; i++)
+    {
+        *dstPtrTemp = (Rpp8s) (((Rpp32s) *dstPtr_8uTemp) - 128);
+        dstPtrTemp++;
+        dstPtr_8uTemp++;
+    }
+
+    free(srcPtr_8u);
+    free(dstPtr_8u);
+
+    return RPP_SUCCESS;
+}
+
+/**************** erase ***************/
+
+template <typename T>
+RppStatus erase_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                         Rpp32u *batch_anchor_box_info, T *batch_colors, Rpp32u *batch_box_offset, Rpp32u *batch_num_of_boxes,
+                         Rpp32u outputFormatToggle, Rpp32u nbatchSize,
+                         RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32u num_of_boxes = batch_num_of_boxes[batchCount];
+            Rpp32u box_offset = batch_box_offset[batchCount];
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            memcpy(dstPtrImage, srcPtrImage, imageDimMax * channel * sizeof(T));
+
+            Rpp32u elementsInRowMax = batch_srcSizeMax[batchCount].width;
+
+            for(int count = 0; count < num_of_boxes; count++)
+            {
+                Rpp32u boxLoc = (box_offset * 4) + (count * 4);
+
+                Rpp32u x1 = (Rpp32u) RPPPRANGECHECK(batch_anchor_box_info[boxLoc], 0, batch_srcSize[batchCount].width);
+                Rpp32u y1 = (Rpp32u) RPPPRANGECHECK(batch_anchor_box_info[boxLoc + 1], 0, batch_srcSize[batchCount].height);
+                Rpp32u x2 = (Rpp32u) RPPPRANGECHECK(batch_anchor_box_info[boxLoc + 2], 0, batch_srcSize[batchCount].width);
+                Rpp32u y2 = (Rpp32u) RPPPRANGECHECK(batch_anchor_box_info[boxLoc + 3], 0, batch_srcSize[batchCount].height);
+
+                Rpp32u locationInEachChannel = (y1 * elementsInRowMax) + x1;
+                Rpp32u vtBufferLength = y2 - y1 + 1;
+                Rpp32u hrBufferLength = x2 - x1 + 1;
+
+                for (int c = 0; c < channel; c++)
+                {
+                    T userPixel = batch_colors[(box_offset * channel) + (count * channel) + c];
+
+                    T *dstPtrTemp;
+                    dstPtrTemp = dstPtrImage + (c * imageDimMax) + locationInEachChannel;
+
+                    for (int i = 0; i < vtBufferLength; i++)
+                    {
+                        T * dstPtrTemp2;
+                        dstPtrTemp2 = dstPtrTemp;
+
+                        for(int j = 0; j < hrBufferLength; j++)
+                        {
+                            *dstPtrTemp2 = userPixel;
+                            dstPtrTemp2++;
+                        }
+
+                        // memset(dstPtrTemp, userPixel, hrBufferLength * sizeof(T));
+                        dstPtrTemp += elementsInRowMax;
+                    }
+                }
+            }
+            if (outputFormatToggle == 1)
+            {
+                T *dstPtrImageUnpadded = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+                T *dstPtrImageUnpaddedCopy = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width * sizeof(T));
+
+                compute_planar_to_packed_host(dstPtrImageUnpaddedCopy, batch_srcSize[batchCount], dstPtrImageUnpadded, channel);
+
+                memset(dstPtrImage, (T) 0, imageDimMax * channel * sizeof(T));
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImage, RPPI_CHN_PACKED, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32u num_of_boxes = batch_num_of_boxes[batchCount];
+            Rpp32u box_offset = batch_box_offset[batchCount];
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            memcpy(dstPtrImage, srcPtrImage, imageDimMax * channel * sizeof(T));
+
+            Rpp32u elementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+
+            for(int count = 0; count < num_of_boxes; count++)
+            {
+                Rpp32u boxLoc = (box_offset * 4) + (count * 4);
+
+                Rpp32u x1 = (Rpp32u) RPPPRANGECHECK(batch_anchor_box_info[boxLoc], 0, batch_srcSize[batchCount].width);
+                Rpp32u y1 = (Rpp32u) RPPPRANGECHECK(batch_anchor_box_info[boxLoc + 1], 0, batch_srcSize[batchCount].height);
+                Rpp32u x2 = (Rpp32u) RPPPRANGECHECK(batch_anchor_box_info[boxLoc + 2], 0, batch_srcSize[batchCount].width);
+                Rpp32u y2 = (Rpp32u) RPPPRANGECHECK(batch_anchor_box_info[boxLoc + 3], 0, batch_srcSize[batchCount].height);
+
+                Rpp32u locationInImage = (y1 * elementsInRowMax) + (x1 * channel);
+                Rpp32u vtBufferLength = y2 - y1 + 1;
+                Rpp32u hrBufferLength = x2 - x1 + 1;
+
+                for (int c = 0; c < channel; c++)
+                {
+                    T userPixel = batch_colors[(box_offset * channel) + (count * channel) + c];
+
+                    T *dstPtrTemp;
+                    dstPtrTemp = dstPtrImage + locationInImage + c;
+
+                    for (int i = 0; i < vtBufferLength; i++)
+                    {
+                        T * dstPtrTemp2;
+                        dstPtrTemp2 = dstPtrTemp;
+
+                        for(int j = 0; j < hrBufferLength; j++)
+                        {
+                            *dstPtrTemp2 = userPixel;
+                            dstPtrTemp2 += channel;
+                        }
+
+                        // memset(dstPtrTemp, userPixel, hrBufferLength * sizeof(T));
+                        dstPtrTemp += elementsInRowMax;
+                    }
+                }
+            }
+            if (outputFormatToggle == 1)
+            {
+                T *dstPtrImageUnpadded = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+                T *dstPtrImageUnpaddedCopy = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width * sizeof(T));
+
+                compute_packed_to_planar_host(dstPtrImageUnpaddedCopy, batch_srcSize[batchCount], dstPtrImageUnpadded, channel);
+
+                memset(dstPtrImage, (T) 0, imageDimMax * channel * sizeof(T));
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImage, RPPI_CHN_PLANAR, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus crop_and_patch_host_batch(T* srcPtr1, RppiSize *batch_srcSize1, RppiSize *batch_srcSizeMax1,
+                                    T* srcPtr2, RppiSize *batch_srcSize2, RppiSize *batch_srcSizeMax2, T* dstPtr,
+                                    Rpp32u *batch_src1x1, Rpp32u *batch_src1y1, Rpp32u *batch_src1x2, Rpp32u *batch_src1y2,
+                                    Rpp32u *batch_src2x1, Rpp32u *batch_src2y1, Rpp32u *batch_src2x2, Rpp32u *batch_src2y2,
+                                    Rpp32u outputFormatToggle, Rpp32u nbatchSize,
+                                    RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u dstImageDimMax = batch_srcSizeMax1[batchCount].height * batch_srcSizeMax1[batchCount].width;
+            Rpp32u src2ImageDimMax = batch_srcSizeMax2[batchCount].height * batch_srcSizeMax2[batchCount].width;
+
+            Rpp32u src1x1 = batch_src1x1[batchCount];
+            Rpp32u src1y1 = batch_src1y1[batchCount];
+            Rpp32u src1x2 = batch_src1x2[batchCount];
+            Rpp32u src1y2 = batch_src1y2[batchCount];
+            Rpp32u src2x1 = batch_src2x1[batchCount];
+            Rpp32u src2y1 = batch_src2y1[batchCount];
+            Rpp32u src2x2 = batch_src2x2[batchCount];
+            Rpp32u src2y2 = batch_src2y2[batchCount];
+
+            T *src1PtrImage, *src2PtrImage, *dstPtrImage;
+            Rpp32u loc1 = 0;
+            Rpp32u loc2 = 0;
+            compute_image_location_host(batch_srcSizeMax1, batchCount, &loc1, channel);
+            compute_image_location_host(batch_srcSizeMax2, batchCount, &loc2, channel);
+            src1PtrImage = srcPtr1 + loc1;
+            dstPtrImage = dstPtr + loc1;
+            src2PtrImage = srcPtr2 + loc2;
+
+            Rpp32u elementsInRow1 = batch_srcSize1[batchCount].width;
+            Rpp32u elementsInRowMax1 = batch_srcSizeMax1[batchCount].width;
+
+            Rpp32u elementsInRow2 = batch_srcSize2[batchCount].width;
+            Rpp32u elementsInRowMax2 = batch_srcSizeMax2[batchCount].width;
+
+            for (int c = 0; c < channel; c++)
+            {
+                T *srcPtrChannel, *dstPtrChannel;
+                srcPtrChannel = src1PtrImage + (c * dstImageDimMax);
+                dstPtrChannel = dstPtrImage + (c * dstImageDimMax);
+
+                for(int i = 0; i < batch_srcSize1[batchCount].height; i++)
+                {
+                    T *srcPtrTemp, *dstPtrTemp;
+                    srcPtrTemp = srcPtrChannel + (i * elementsInRowMax1);
+                    dstPtrTemp = dstPtrChannel + (i * elementsInRowMax1);
+                    memcpy(dstPtrTemp, srcPtrTemp, elementsInRow1 * sizeof(T));
+                }
+            }
+
+            RppiSize srcSize2SubImage, dstSizeSubImage;
+            T *srcPtr2SubImage, *dstPtrSubImage;
+            srcSize2SubImage.height = RPPABS(src2y2 - src2y1) + 1;
+            srcSize2SubImage.width = RPPABS(src2x2 - src2x1) + 1;
+            srcPtr2SubImage = src2PtrImage + (src2y1 * elementsInRowMax2) + (src2x1);
+            dstSizeSubImage.height = RPPABS(src1y2 - src1y1) + 1;
+            dstSizeSubImage.width = RPPABS(src1x2 - src1x1) + 1;
+            dstPtrSubImage = dstPtrImage + (src1y1 * elementsInRowMax1) + (src1x1);
+
+            Rpp32f hRatio = (((Rpp32f) (dstSizeSubImage.height - 1)) / ((Rpp32f) (srcSize2SubImage.height - 1)));
+            Rpp32f wRatio = (((Rpp32f) (dstSizeSubImage.width - 1)) / ((Rpp32f) (srcSize2SubImage.width - 1)));
+            Rpp32f srcLocationRow, srcLocationColumn, pixel;
+            Rpp32s srcLocationRowFloor, srcLocationColumnFloor;
+            T *srcPtrTemp, *dstPtrTemp, *srcPtrTopRow, *srcPtrBottomRow;
+            Rpp32u remainingElementsInRowDst = (batch_srcSizeMax1[batchCount].width - dstSizeSubImage.width);
+            for (int c = 0; c < channel; c++)
+            {
+                srcPtrTemp = srcPtr2SubImage + (c * src2ImageDimMax);
+                dstPtrTemp = dstPtrSubImage + (c * dstImageDimMax);
+
+                for (int i = 0; i < dstSizeSubImage.height; i++)
+                {
+                    srcLocationRow = ((Rpp32f) i) / hRatio;
+                    srcLocationRowFloor = (Rpp32s) RPPFLOOR(srcLocationRow);
+                    Rpp32f weightedHeight = srcLocationRow - srcLocationRowFloor;
+
+                    if (srcLocationRowFloor > (srcSize2SubImage.height - 2))
+                    {
+                        srcLocationRowFloor = srcSize2SubImage.height - 2;
+                    }
+
+                    srcPtrTopRow = srcPtrTemp + srcLocationRowFloor * elementsInRowMax1;
+                    srcPtrBottomRow  = srcPtrTopRow + elementsInRowMax1;
+
+                    for (int j = 0; j < dstSizeSubImage.width; j++)
+                    {
+                        srcLocationColumn = ((Rpp32f) j) / wRatio;
+                        srcLocationColumnFloor = (Rpp32s) RPPFLOOR(srcLocationColumn);
+                        Rpp32f weightedWidth = srcLocationColumn - srcLocationColumnFloor;
+
+                        if (srcLocationColumnFloor > (srcSize2SubImage.width - 2))
+                        {
+                            srcLocationColumnFloor = srcSize2SubImage.width - 2;
+                        }
+
+                        pixel = ((*(srcPtrTopRow + srcLocationColumnFloor)) * (1 - weightedHeight) * (1 - weightedWidth))
+                                + ((*(srcPtrTopRow + srcLocationColumnFloor + 1)) * (1 - weightedHeight) * (weightedWidth))
+                                + ((*(srcPtrBottomRow + srcLocationColumnFloor)) * (weightedHeight) * (1 - weightedWidth))
+                                + ((*(srcPtrBottomRow + srcLocationColumnFloor + 1)) * (weightedHeight) * (weightedWidth));
+
+                        *dstPtrTemp = (T) pixel;
+                        dstPtrTemp ++;
+                    }
+                    dstPtrTemp = dstPtrTemp + remainingElementsInRowDst;
+                }
+            }
+            if (outputFormatToggle == 1)
+            {
+                T *dstPtrImageUnpadded = (T*) calloc(channel * batch_srcSize1[batchCount].height * batch_srcSize1[batchCount].width, sizeof(T));
+                T *dstPtrImageUnpaddedCopy = (T*) calloc(channel * batch_srcSize1[batchCount].height * batch_srcSize1[batchCount].width, sizeof(T));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_srcSize1[batchCount], batch_srcSizeMax1[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_srcSize1[batchCount].height * batch_srcSize1[batchCount].width * sizeof(T));
+
+                compute_planar_to_packed_host(dstPtrImageUnpaddedCopy, batch_srcSize1[batchCount], dstPtrImageUnpadded, channel);
+
+                memset(dstPtrImage, (T) 0, dstImageDimMax * channel * sizeof(T));
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_srcSize1[batchCount], batch_srcSizeMax1[batchCount], dstPtrImage, RPPI_CHN_PACKED, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u dstImageDimMax = batch_srcSizeMax1[batchCount].height * batch_srcSizeMax1[batchCount].width;
+
+            Rpp32u src1x1 = batch_src1x1[batchCount];
+            Rpp32u src1y1 = batch_src1y1[batchCount];
+            Rpp32u src1x2 = batch_src1x2[batchCount];
+            Rpp32u src1y2 = batch_src1y2[batchCount];
+            Rpp32u src2x1 = batch_src2x1[batchCount];
+            Rpp32u src2y1 = batch_src2y1[batchCount];
+            Rpp32u src2x2 = batch_src2x2[batchCount];
+            Rpp32u src2y2 = batch_src2y2[batchCount];
+
+            T *src1PtrImage, *src2PtrImage, *dstPtrImage;
+            Rpp32u loc1 = 0;
+            Rpp32u loc2 = 0;
+            compute_image_location_host(batch_srcSizeMax1, batchCount, &loc1, channel);
+            compute_image_location_host(batch_srcSizeMax2, batchCount, &loc2, channel);
+            src1PtrImage = srcPtr1 + loc1;
+            dstPtrImage = dstPtr + loc1;
+            src2PtrImage = srcPtr2 + loc2;
+
+            Rpp32u elementsInRow1 = channel * batch_srcSize1[batchCount].width;
+            Rpp32u elementsInRowMax1 = channel * batch_srcSizeMax1[batchCount].width;
+
+            Rpp32u elementsInRow2 = channel * batch_srcSize2[batchCount].width;
+            Rpp32u elementsInRowMax2 = channel * batch_srcSizeMax2[batchCount].width;
+
+            for(int i = 0; i < batch_srcSize1[batchCount].height; i++)
+            {
+                T *srcPtrTemp, *dstPtrTemp;
+                srcPtrTemp = src1PtrImage + (i * elementsInRowMax1);
+                dstPtrTemp = dstPtrImage + (i * elementsInRowMax1);
+                memcpy(dstPtrTemp, srcPtrTemp, elementsInRow1 * sizeof(T));
+            }
+
+            RppiSize srcSize2SubImage, dstSizeSubImage;
+            T *srcPtr2SubImage, *dstPtrSubImage;
+            srcSize2SubImage.height = RPPABS(src2y2 - src2y1) + 1;
+            srcSize2SubImage.width = RPPABS(src2x2 - src2x1) + 1;
+            srcPtr2SubImage = src2PtrImage + (src2y1 * elementsInRowMax2) + (src2x1 * channel);
+            dstSizeSubImage.height = RPPABS(src1y2 - src1y1) + 1;
+            dstSizeSubImage.width = RPPABS(src1x2 - src1x1) + 1;
+            dstPtrSubImage = dstPtrImage + (src1y1 * elementsInRowMax1) + (src1x1 * channel);
+
+            Rpp32f hRatio = (((Rpp32f) (dstSizeSubImage.height - 1)) / ((Rpp32f) (srcSize2SubImage.height - 1)));
+            Rpp32f wRatio = (((Rpp32f) (dstSizeSubImage.width - 1)) / ((Rpp32f) (srcSize2SubImage.width - 1)));
+            Rpp32f srcLocationRow, srcLocationColumn, pixel;
+            Rpp32s srcLocationRowFloor, srcLocationColumnFloor;
+            T *srcPtrTemp, *dstPtrTemp;
+            srcPtrTemp = srcPtr2SubImage;
+            dstPtrTemp = dstPtrSubImage;
+            Rpp32u remainingElementsInRowDst = (batch_srcSizeMax1[batchCount].width - dstSizeSubImage.width) * channel;
+            for (int i = 0; i < dstSizeSubImage.height; i++)
+            {
+                srcLocationRow = ((Rpp32f) i) / hRatio;
+                srcLocationRowFloor = (Rpp32s) RPPFLOOR(srcLocationRow);
+                Rpp32f weightedHeight = srcLocationRow - srcLocationRowFloor;
+
+                if (srcLocationRowFloor > (srcSize2SubImage.height - 2))
+                {
+                    srcLocationRowFloor = srcSize2SubImage.height - 2;
+                }
+
+                T *srcPtrTopRow, *srcPtrBottomRow;
+                srcPtrTopRow = srcPtrTemp + srcLocationRowFloor * elementsInRowMax1;
+                srcPtrBottomRow  = srcPtrTopRow + elementsInRowMax1;
+
+                for (int j = 0; j < dstSizeSubImage.width; j++)
+                {
+                    srcLocationColumn = ((Rpp32f) j) / wRatio;
+                    srcLocationColumnFloor = (Rpp32s) RPPFLOOR(srcLocationColumn);
+                    Rpp32f weightedWidth = srcLocationColumn - srcLocationColumnFloor;
+
+                    if (srcLocationColumnFloor > (srcSize2SubImage.width - 2))
+                    {
+                        srcLocationColumnFloor = srcSize2SubImage.width - 2;
+                    }
+
+                    Rpp32s srcLocColFloorChanneled = channel * srcLocationColumnFloor;
+
+                    for (int c = 0; c < channel; c++)
+                    {
+                        pixel = ((*(srcPtrTopRow + c + srcLocColFloorChanneled)) * (1 - weightedHeight) * (1 - weightedWidth))
+                                + ((*(srcPtrTopRow + c + srcLocColFloorChanneled + channel)) * (1 - weightedHeight) * (weightedWidth))
+                                + ((*(srcPtrBottomRow + c + srcLocColFloorChanneled)) * (weightedHeight) * (1 - weightedWidth))
+                                + ((*(srcPtrBottomRow + c + srcLocColFloorChanneled + channel)) * (weightedHeight) * (weightedWidth));
+
+                        *dstPtrTemp = (T) pixel;
+                        dstPtrTemp ++;
+                    }
+                }
+                dstPtrTemp = dstPtrTemp + remainingElementsInRowDst;
+            }
+            if (outputFormatToggle == 1)
+            {
+                T *dstPtrImageUnpadded = (T*) calloc(channel * batch_srcSize1[batchCount].height * batch_srcSize1[batchCount].width, sizeof(T));
+                T *dstPtrImageUnpaddedCopy = (T*) calloc(channel * batch_srcSize1[batchCount].height * batch_srcSize1[batchCount].width, sizeof(T));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_srcSize1[batchCount], batch_srcSizeMax1[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_srcSize1[batchCount].height * batch_srcSize1[batchCount].width * sizeof(T));
+
+                compute_packed_to_planar_host(dstPtrImageUnpaddedCopy, batch_srcSize1[batchCount], dstPtrImageUnpadded, channel);
+
+                memset(dstPtrImage, (T) 0, dstImageDimMax * channel * sizeof(T));
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_srcSize1[batchCount], batch_srcSizeMax1[batchCount], dstPtrImage, RPPI_CHN_PLANAR, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus lut_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                        T *batch_lutPtr,
+                        Rpp32u outputFormatToggle, Rpp32u nbatchSize,
+                        RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u lutSize = 256;
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            T* lutPtr = (T*) calloc(lutSize, sizeof(T));
+            memcpy(lutPtr, (batch_lutPtr + (batchCount * lutSize)), lutSize * sizeof(T));
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            for(int c = 0; c < channel; c++)
+            {
+                T *srcPtrChannel, *dstPtrChannel;
+                srcPtrChannel = srcPtrImage + (c * imageDimMax);
+                dstPtrChannel = dstPtrImage + (c * imageDimMax);
+
+                for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+                {
+                    T *srcPtrTemp, *dstPtrTemp;
+                    srcPtrTemp = srcPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+                    dstPtrTemp = dstPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+
+                    if (typeid(Rpp8u) == typeid(T))
+                    {
+                        for(int j = 0; j < batch_srcSize[batchCount].width; j++)
+                        {
+                            *dstPtrTemp = *(lutPtr + (Rpp32s)(*srcPtrTemp));
+
+                            srcPtrTemp++;
+                            dstPtrTemp++;
+                        }
+                    }
+                    else if (typeid(Rpp8s) == typeid(T))
+                    {
+                        for(int j = 0; j < batch_srcSize[batchCount].width; j++)
+                        {
+                            *dstPtrTemp = *(lutPtr + (((Rpp32s) (*srcPtrTemp)) + 128));
+
+                            srcPtrTemp++;
+                            dstPtrTemp++;
+                        }
+                    }
+                }
+            }
+            free(lutPtr);
+            if (outputFormatToggle == 1)
+            {
+                T *dstPtrImageUnpadded = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+                T *dstPtrImageUnpaddedCopy = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width * sizeof(T));
+
+                compute_planar_to_packed_host(dstPtrImageUnpaddedCopy, batch_srcSize[batchCount], dstPtrImageUnpadded, channel);
+
+                memset(dstPtrImage, (T) 0, imageDimMax * channel * sizeof(T));
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImage, RPPI_CHN_PACKED, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+    else if(chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            T* lutPtr = (T*) calloc(lutSize, sizeof(T));
+            memcpy(lutPtr, (batch_lutPtr + (batchCount * lutSize)), lutSize * sizeof(T));
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            Rpp32u elementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+            Rpp32u elementsInRow = channel * batch_srcSize[batchCount].width;
+
+            if (typeid(Rpp8u) == typeid(T))
+            {
+                for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+                {
+                    T *srcPtrTemp, *dstPtrTemp;
+                    srcPtrTemp = srcPtrImage + (i * elementsInRowMax);
+                    dstPtrTemp = dstPtrImage + (i * elementsInRowMax);
+
+                    for(int j = 0; j < batch_srcSize[batchCount].width; j++)
+                    {
+                        for(int c = 0; c < channel; c++)
+                        {
+                            *dstPtrTemp = *(lutPtr + (Rpp32s) *srcPtrTemp);
+
+                            srcPtrTemp++;
+                            dstPtrTemp++;
+                        }
+                    }
+
+                    srcPtrTemp += (elementsInRowMax - elementsInRow);
+                    dstPtrTemp += (elementsInRowMax - elementsInRow);
+                }
+            }
+            else if (typeid(Rpp8s) == typeid(T))
+            {
+                for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+                {
+                    T *srcPtrTemp, *dstPtrTemp;
+                    srcPtrTemp = srcPtrImage + (i * elementsInRowMax);
+                    dstPtrTemp = dstPtrImage + (i * elementsInRowMax);
+
+                    for(int j = 0; j < batch_srcSize[batchCount].width; j++)
+                    {
+                        for(int c = 0; c < channel; c++)
+                        {
+                            *dstPtrTemp = *(lutPtr + (Rpp32s) *srcPtrTemp + 128);
+
+                            srcPtrTemp++;
+                            dstPtrTemp++;
+                        }
+                    }
+
+                    srcPtrTemp += (elementsInRowMax - elementsInRow);
+                    dstPtrTemp += (elementsInRowMax - elementsInRow);
+                }
+            }
+            free(lutPtr);
+            if (outputFormatToggle == 1)
+            {
+                T *dstPtrImageUnpadded = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+                T *dstPtrImageUnpaddedCopy = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width * sizeof(T));
+
+                compute_packed_to_planar_host(dstPtrImageUnpaddedCopy, batch_srcSize[batchCount], dstPtrImageUnpadded, channel);
+
+                memset(dstPtrImage, (T) 0, imageDimMax * channel * sizeof(T));
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImage, RPPI_CHN_PLANAR, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+/**************** glitch ***************/
+
+template <typename T>
+RppStatus glitch_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                         Rpp32u *batch_x_offset_r, Rpp32u *batch_y_offset_r,
+                         Rpp32u *batch_x_offset_g, Rpp32u *batch_y_offset_g,
+                         Rpp32u *batch_x_offset_b, Rpp32u *batch_y_offset_b,
+                         Rpp32u outputFormatToggle, Rpp32u nbatchSize,
+                         RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32u x_offset_r = batch_x_offset_r[batchCount];
+            Rpp32u y_offset_r = batch_y_offset_r[batchCount];
+            Rpp32u x_offset_g = batch_x_offset_g[batchCount];
+            Rpp32u y_offset_g = batch_y_offset_g[batchCount];
+            Rpp32u x_offset_b = batch_x_offset_b[batchCount];
+            Rpp32u y_offset_b = batch_y_offset_b[batchCount];
+
+            Rpp32u elementsInRowMax = batch_srcSizeMax[batchCount].width;
+            Rpp32u elementsInRow = batch_srcSize[batchCount].width;
+            Rpp32u increment = elementsInRowMax - elementsInRow;
+
+            Rpp32u xOffsets[3] = {
+                x_offset_r,
+                x_offset_g,
+                x_offset_b
+            };
+
+            Rpp32u yOffsets[3] = {
+                y_offset_r,
+                y_offset_g,
+                y_offset_b
+            };
+
+            Rpp32u xOffsetsLoc[3] = {
+                x_offset_r,
+                x_offset_g,
+                x_offset_b
+            };
+
+            Rpp32u yOffsetsLoc[3] = {
+                y_offset_r * elementsInRowMax,
+                y_offset_g * elementsInRowMax,
+                y_offset_b * elementsInRowMax
+            };
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            memcpy(dstPtrImage, srcPtrImage, imageDimMax * channel * sizeof(T));
+
+            Rpp32u currentRow, currentCol;
+
+            for (int c = 0; c < channel; c++)
+            {
+                T *srcPtrImageTemp, *dstPtrImageTemp;
+                srcPtrImageTemp = srcPtrImage + (c * imageDimMax) + yOffsetsLoc[c] + xOffsetsLoc[c];
+                dstPtrImageTemp = dstPtrImage + (c * imageDimMax);
+
+                currentRow = yOffsets[c];
+                for (int i = 0; i < batch_srcSize[batchCount].height; i++)
+                {
+                    currentCol = xOffsets[c];
+                    for (int j = 0; j < batch_srcSize[batchCount].width; j++)
+                    {
+                        if (
+                            ((currentRow >= 0) && (currentRow < batch_srcSize[batchCount].height)) &&
+                            ((currentCol >= 0) && (currentCol < batch_srcSize[batchCount].width))
+                        )
+                        {
+                            *dstPtrImageTemp = *srcPtrImageTemp;
+                        }
+                        dstPtrImageTemp++;
+                        srcPtrImageTemp++;
+                        currentCol++;
+                    }
+                    dstPtrImageTemp += increment;
+                    srcPtrImageTemp += increment;
+                    currentRow++;
+                }
+            }
+            if (outputFormatToggle == 1)
+            {
+                T *dstPtrImageUnpadded = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+                T *dstPtrImageUnpaddedCopy = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width * sizeof(T));
+
+                compute_planar_to_packed_host(dstPtrImageUnpaddedCopy, batch_srcSize[batchCount], dstPtrImageUnpadded, channel);
+
+                memset(dstPtrImage, (T) 0, imageDimMax * channel * sizeof(T));
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImage, RPPI_CHN_PACKED, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32u x_offset_r = batch_x_offset_r[batchCount];
+            Rpp32u y_offset_r = batch_y_offset_r[batchCount];
+            Rpp32u x_offset_g = batch_x_offset_g[batchCount];
+            Rpp32u y_offset_g = batch_y_offset_g[batchCount];
+            Rpp32u x_offset_b = batch_x_offset_b[batchCount];
+            Rpp32u y_offset_b = batch_y_offset_b[batchCount];
+
+            Rpp32u elementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+            Rpp32u elementsInRow = channel * batch_srcSize[batchCount].width;
+            Rpp32u increment = elementsInRowMax - elementsInRow;
+
+            Rpp32u xOffsets[3] = {
+                x_offset_r,
+                x_offset_g,
+                x_offset_b
+            };
+
+            Rpp32u yOffsets[3] = {
+                y_offset_r,
+                y_offset_g,
+                y_offset_b
+            };
+
+            Rpp32u xOffsetsLoc[3] = {
+                x_offset_r * channel,
+                x_offset_g * channel,
+                x_offset_b * channel
+            };
+
+            Rpp32u yOffsetsLoc[3] = {
+                y_offset_r * elementsInRowMax,
+                y_offset_g * elementsInRowMax,
+                y_offset_b * elementsInRowMax
+            };
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            memcpy(dstPtrImage, srcPtrImage, imageDimMax * channel * sizeof(T));
+
+            Rpp32u currentRow, currentCol;
+
+            for (int c = 0; c < channel; c++)
+            {
+                T *srcPtrImageTemp, *dstPtrImageTemp;
+                srcPtrImageTemp = srcPtrImage + c + yOffsetsLoc[c] + xOffsetsLoc[c];
+                dstPtrImageTemp = dstPtrImage + c;
+
+                currentRow = yOffsets[c];
+                for (int i = 0; i < batch_srcSize[batchCount].height; i++)
+                {
+                    currentCol = xOffsets[c];
+                    for (int j = 0; j < batch_srcSize[batchCount].width; j++)
+                    {
+
+                        if (
+                            ((currentRow >= 0) && (currentRow < batch_srcSize[batchCount].height)) &&
+                            ((currentCol >= 0) && (currentCol < batch_srcSize[batchCount].width))
+                        )
+                        {
+                            *dstPtrImageTemp = *srcPtrImageTemp;
+                        }
+                        dstPtrImageTemp += channel;
+                        srcPtrImageTemp += channel;
+                        currentCol++;
+                    }
+                    dstPtrImageTemp += increment;
+                    srcPtrImageTemp += increment;
+                    currentRow++;
+                }
+            }
+            if (outputFormatToggle == 1)
+            {
+                T *dstPtrImageUnpadded = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+                T *dstPtrImageUnpaddedCopy = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width * sizeof(T));
+
+                compute_packed_to_planar_host(dstPtrImageUnpaddedCopy, batch_srcSize[batchCount], dstPtrImageUnpadded, channel);
+
+                memset(dstPtrImage, (T) 0, imageDimMax * channel * sizeof(T));
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImage, RPPI_CHN_PLANAR, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+#endif
diff --git a/src/modules/cpu/loongarch_color_model_conversions.hpp b/src/modules/cpu/loongarch_color_model_conversions.hpp
new file mode 100644
index 00000000..d7c7aebd
--- /dev/null
+++ b/src/modules/cpu/loongarch_color_model_conversions.hpp
@@ -0,0 +1,3585 @@
+/*
+MIT License
+
+Copyright (c) 2019 - 2024 Advanced Micro Devices, Inc.
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
+*/
+
+#ifndef HOST_COLOR_MODER_CONVERSIONS_HPP
+#define HOST_COLOR_MODER_CONVERSIONS_HPP
+
+#include "rpp_loongarch_common.hpp"
+
+/**************** channel_extract ***************/
+
+template <typename T>
+RppStatus channel_extract_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                                     Rpp32u *batch_extractChannelNumber,
+                                     Rpp32u nbatchSize,
+                                     RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32u extractChannelNumber = batch_extractChannelNumber[batchCount];
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+            compute_image_location_host(batch_srcSizeMax, batchCount, &dstLoc, 1);
+            srcPtrImage = srcPtr + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+
+            T *srcPtrChannel;
+            srcPtrChannel = srcPtrImage + (extractChannelNumber * imageDimMax);
+
+            for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+            {
+                T *srcPtrTemp, *dstPtrTemp;
+                srcPtrTemp = srcPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+                dstPtrTemp = dstPtrImage + (i * batch_srcSizeMax[batchCount].width);
+
+                memcpy(dstPtrTemp, srcPtrTemp, batch_srcSize[batchCount].width * sizeof(T));
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32u extractChannelNumber = batch_extractChannelNumber[batchCount];
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+            compute_image_location_host(batch_srcSizeMax, batchCount, &dstLoc, 1);
+            srcPtrImage = srcPtr + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+
+            T *srcPtrChannel;
+            srcPtrChannel = srcPtrImage + extractChannelNumber;
+
+            Rpp32u elementsInRowSrc = channel * batch_srcSize[batchCount].width;
+            Rpp32u elementsInRowMaxSrc = channel * batch_srcSizeMax[batchCount].width;
+            Rpp32u elementsInRowDst = 1 * batch_srcSize[batchCount].width;
+            Rpp32u elementsInRowMaxDst = 1 * batch_srcSizeMax[batchCount].width;
+
+            for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+            {
+                T *srcPtrTemp, *dstPtrTemp;
+                srcPtrTemp = srcPtrChannel + (i * elementsInRowMaxSrc);
+                dstPtrTemp = dstPtrImage + (i * elementsInRowMaxDst);
+
+                for(int j = 0; j < batch_srcSize[batchCount].width; j++)
+                {
+                    *dstPtrTemp = *srcPtrTemp;
+                    srcPtrTemp = srcPtrTemp + channel;
+                    dstPtrTemp++;
+                }
+                srcPtrTemp += (elementsInRowMaxSrc - elementsInRowSrc);
+                dstPtrTemp += (elementsInRowMaxDst - elementsInRowDst);
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus channel_extract_host(T* srcPtr, RppiSize srcSize, T* dstPtr,
+                    Rpp32u extractChannelNumber,
+                    RppiChnFormat chnFormat, Rpp32u channel)
+{
+    if (extractChannelNumber != 0 && extractChannelNumber != 1 && extractChannelNumber != 2)
+    {
+        return RPP_ERROR;
+    }
+
+    T *srcPtrTemp, *dstPtrTemp;
+    dstPtrTemp = dstPtr;
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        srcPtrTemp = srcPtr + (extractChannelNumber * srcSize.height * srcSize.width);
+        memcpy(dstPtrTemp, srcPtrTemp, srcSize.height * srcSize.width * sizeof(T));
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        srcPtrTemp = srcPtr + extractChannelNumber;
+        for (int i = 0; i < srcSize.height * srcSize.width; i++)
+        {
+            *dstPtrTemp = *srcPtrTemp;
+            srcPtrTemp = srcPtrTemp + channel;
+            dstPtrTemp++;
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+/**************** channel_combine ***************/
+
+template <typename T>
+RppStatus channel_combine_host_batch(T* srcPtr1, T* srcPtr2, T* srcPtr3, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                                     Rpp32u nbatchSize,
+                                     RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            T *srcPtr1Image, *srcPtr2Image, *srcPtr3Image, *dstPtrImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, 1);
+            compute_image_location_host(batch_srcSizeMax, batchCount, &dstLoc, channel);
+            srcPtr1Image = srcPtr1 + srcLoc;
+            srcPtr2Image = srcPtr2 + srcLoc;
+            srcPtr3Image = srcPtr3 + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+
+            T *dstPtrChannel;
+            dstPtrChannel = dstPtrImage;
+
+            for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+            {
+                T *srcPtrTemp, *dstPtrTemp;
+                srcPtrTemp = srcPtr1Image + (i * batch_srcSizeMax[batchCount].width);
+                dstPtrTemp = dstPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+
+                memcpy(dstPtrTemp, srcPtrTemp, batch_srcSize[batchCount].width * sizeof(T));
+            }
+
+            dstPtrChannel = dstPtrImage + (imageDimMax);
+
+
+            for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+            {
+                T *srcPtrTemp, *dstPtrTemp;
+                srcPtrTemp = srcPtr2Image + (i * batch_srcSizeMax[batchCount].width);
+                dstPtrTemp = dstPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+
+                memcpy(dstPtrTemp, srcPtrTemp, batch_srcSize[batchCount].width * sizeof(T));
+            }
+
+            dstPtrChannel = dstPtrImage + (2 * imageDimMax);
+
+
+            for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+            {
+                T *srcPtrTemp, *dstPtrTemp;
+                srcPtrTemp = srcPtr3Image + (i * batch_srcSizeMax[batchCount].width);
+                dstPtrTemp = dstPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+
+                memcpy(dstPtrTemp, srcPtrTemp, batch_srcSize[batchCount].width * sizeof(T));
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            T *srcPtr1Image, *srcPtr2Image, *srcPtr3Image, *dstPtrImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, 1);
+            compute_image_location_host(batch_srcSizeMax, batchCount, &dstLoc, channel);
+            srcPtr1Image = srcPtr1 + srcLoc;
+            srcPtr2Image = srcPtr2 + srcLoc;
+            srcPtr3Image = srcPtr3 + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+
+            Rpp32u elementsInRowSrc = 1 * batch_srcSize[batchCount].width;
+            Rpp32u elementsInRowMaxSrc = 1 * batch_srcSizeMax[batchCount].width;
+            Rpp32u elementsInRowDst = channel * batch_srcSize[batchCount].width;
+            Rpp32u elementsInRowMaxDst = channel * batch_srcSizeMax[batchCount].width;
+
+
+            for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+            {
+                T *srcPtr1Temp, *srcPtr2Temp, *srcPtr3Temp, *dstPtrTemp;
+                srcPtr1Temp = srcPtr1Image + (i * elementsInRowMaxSrc);
+                srcPtr2Temp = srcPtr2Image + (i * elementsInRowMaxSrc);
+                srcPtr3Temp = srcPtr3Image + (i * elementsInRowMaxSrc);
+                dstPtrTemp = dstPtrImage + (i * elementsInRowMaxDst);
+
+                for(int j = 0; j < batch_srcSize[batchCount].width; j++)
+                {
+                    *dstPtrTemp = *srcPtr1Temp;
+                    dstPtrTemp++;
+                    srcPtr1Temp++;
+                    *dstPtrTemp = *srcPtr2Temp;
+                    dstPtrTemp++;
+                    srcPtr2Temp++;
+                    *dstPtrTemp = *srcPtr3Temp;
+                    dstPtrTemp++;
+                    srcPtr3Temp++;
+                }
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus channel_combine_host(T* srcPtr1, T* srcPtr2, T* srcPtr3, RppiSize srcSize, T* dstPtr,
+                               RppiChnFormat chnFormat, Rpp32u channel)
+{
+
+    T *srcPtr1Temp, *srcPtr2Temp, *srcPtr3Temp, *dstPtrTemp;
+    srcPtr1Temp = srcPtr1;
+    srcPtr2Temp = srcPtr2;
+    srcPtr3Temp = srcPtr3;
+    dstPtrTemp = dstPtr;
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        Rpp32u increment = srcSize.height * srcSize.width;
+        memcpy(dstPtrTemp, srcPtr1Temp, srcSize.height * srcSize.width * sizeof(T));
+        dstPtrTemp += increment;
+        memcpy(dstPtrTemp, srcPtr2Temp, srcSize.height * srcSize.width * sizeof(T));
+        dstPtrTemp += increment;
+        memcpy(dstPtrTemp, srcPtr3Temp, srcSize.height * srcSize.width * sizeof(T));
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        for (int i = 0; i < srcSize.height * srcSize.width; i++)
+        {
+            *dstPtrTemp = *srcPtr1Temp;
+            dstPtrTemp++;
+            srcPtr1Temp++;
+            *dstPtrTemp = *srcPtr2Temp;
+            dstPtrTemp++;
+            srcPtr2Temp++;
+            *dstPtrTemp = *srcPtr3Temp;
+            dstPtrTemp++;
+            srcPtr3Temp++;
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+/**************** look_up_table ***************/
+
+// template <typename T>
+// RppStatus look_up_table_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+//                                    T* batch_lutPtr,
+//                                    RppiROI *roiPoints, Rpp32u nbatchSize,
+//                                    RppiChnFormat chnFormat, Rpp32u channel)
+// {
+//     Rpp32u lutSize = channel * 256;
+//     if(chnFormat == RPPI_CHN_PLANAR)
+//     {
+//         omp_set_dynamic(0);
+// #pragma omp parallel for num_threads(numThreads)
+//         for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+//         {
+//             Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+//             Rpp32f x1 = roiPoints[batchCount].x;
+//             Rpp32f y1 = roiPoints[batchCount].y;
+//             Rpp32f x2 = x1 + roiPoints[batchCount].roiWidth - 1;
+//             Rpp32f y2 = y1 + roiPoints[batchCount].roiHeight - 1;
+//             if (x2 == -1)
+//             {
+//                 x2 = batch_srcSize[batchCount].width - 1;
+//                 roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+//             }
+//             if (y2 == -1)
+//             {
+//                 y2 = batch_srcSize[batchCount].height - 1;
+//                 roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+//             }
+
+//             T* lutPtr = (T*) calloc(lutSize, sizeof(T));
+//             memcpy(lutPtr, (batch_lutPtr + (batchCount * lutSize)), lutSize * sizeof(T));
+
+//             T *srcPtrImage, *dstPtrImage;
+//             Rpp32u loc = 0;
+//             compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+//             srcPtrImage = srcPtr + loc;
+//             dstPtrImage = dstPtr + loc;
+
+//             for(int c = 0; c < channel; c++)
+//             {
+//                 T *srcPtrChannel, *dstPtrChannel;
+//                 srcPtrChannel = srcPtrImage + (c * imageDimMax);
+//                 dstPtrChannel = dstPtrImage + (c * imageDimMax);
+
+//                 T *lutPtrTemp;
+//                 lutPtrTemp = lutPtr + (c * 256);
+
+
+//                 for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+//                 {
+//                     T *srcPtrTemp, *dstPtrTemp;
+//                     srcPtrTemp = srcPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+//                     dstPtrTemp = dstPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+
+//                     if (!((y1 <= i) && (i <= y2)))
+//                     {
+//                         memcpy(dstPtrTemp, srcPtrTemp, batch_srcSize[batchCount].width * sizeof(T));
+//                         dstPtrTemp += batch_srcSizeMax[batchCount].width;
+//                         srcPtrTemp += batch_srcSizeMax[batchCount].width;
+//                     }
+//                     else
+//                     {
+//                         for(int j = 0; j < batch_srcSize[batchCount].width; j++)
+//                         {
+//                             if((x1 <= j) && (j <= x2 ))
+//                             {
+//                                 *dstPtrTemp = *(lutPtrTemp + (Rpp32u)(*srcPtrTemp));
+
+//                                 srcPtrTemp++;
+//                                 dstPtrTemp++;
+//                             }
+//                             else
+//                             {
+//                                 *dstPtrTemp = *srcPtrTemp;
+//                                 srcPtrTemp++;
+//                                 dstPtrTemp++;
+//                             }
+//                         }
+//                         srcPtrTemp += batch_srcSizeMax[batchCount].width - batch_srcSize[batchCount].width;
+//                         dstPtrTemp += batch_srcSizeMax[batchCount].width - batch_srcSize[batchCount].width;
+//                     }
+//                 }
+//             }
+
+//             free(lutPtr);
+//         }
+//     }
+//     else if (chnFormat == RPPI_CHN_PACKED)
+//     {
+//         omp_set_dynamic(0);
+// #pragma omp parallel for num_threads(numThreads)
+//         for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+//         {
+//             Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+//             Rpp32f x1 = roiPoints[batchCount].x;
+//             Rpp32f y1 = roiPoints[batchCount].y;
+//             Rpp32f x2 = x1 + roiPoints[batchCount].roiWidth - 1;
+//             Rpp32f y2 = y1 + roiPoints[batchCount].roiHeight - 1;
+//             if (x2 == -1)
+//             {
+//                 x2 = batch_srcSize[batchCount].width - 1;
+//                 roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+//             }
+//             if (y2 == -1)
+//             {
+//                 y2 = batch_srcSize[batchCount].height - 1;
+//                 roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+//             }
+
+//             T* lutPtr = (T*) calloc(lutSize, sizeof(T));
+//             memcpy(lutPtr, (batch_lutPtr + (batchCount * lutSize)), lutSize * sizeof(T));
+
+//             T *srcPtrImage, *dstPtrImage;
+//             Rpp32u loc = 0;
+//             compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+//             srcPtrImage = srcPtr + loc;
+//             dstPtrImage = dstPtr + loc;
+
+//             Rpp32u elementsInRow = channel * batch_srcSize[batchCount].width;
+//             Rpp32u elementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+
+
+//             for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+//             {
+//                 T *srcPtrTemp, *dstPtrTemp;
+//                 srcPtrTemp = srcPtrImage + (i * elementsInRowMax);
+//                 dstPtrTemp = dstPtrImage + (i * elementsInRowMax);
+
+//                 if (!((y1 <= i) && (i <= y2)))
+//                 {
+//                     memcpy(dstPtrTemp, srcPtrTemp, elementsInRow * sizeof(T));
+//                     dstPtrTemp += elementsInRowMax;
+//                     srcPtrTemp += elementsInRowMax;
+//                 }
+//                 else
+//                 {
+//                     for(int j = 0; j < batch_srcSize[batchCount].width; j++)
+//                     {
+//                         if (!((x1 <= j) && (j <= x2 )))
+//                         {
+//                             memcpy(dstPtrTemp, srcPtrTemp, channel * sizeof(T));
+//                             dstPtrTemp += channel;
+//                             srcPtrTemp += channel;
+//                         }
+//                         else
+//                         {
+//                             for(int c = 0; c < channel; c++)
+//                             {
+//                                 *dstPtrTemp = *(lutPtr + c + (channel * (Rpp32u)(*srcPtrTemp)));
+
+//                                 srcPtrTemp++;
+//                                 dstPtrTemp++;
+//                             }
+//                         }
+//                     }
+//                     srcPtrTemp += (elementsInRowMax - elementsInRow);
+//                     dstPtrTemp += (elementsInRowMax - elementsInRow);
+//                 }
+//             }
+
+//             free(lutPtr);
+//         }
+//     }
+
+//     return RPP_SUCCESS;
+// }
+
+template <typename T>
+RppStatus look_up_table_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                        T *batch_lutPtr,
+                        RppiROI *roiPoints, Rpp32u nbatchSize,
+                        RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u lutSize = 256;
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            T* lutPtr = (T*) calloc(lutSize, sizeof(T));
+            memcpy(lutPtr, (batch_lutPtr + (batchCount * lutSize)), lutSize * sizeof(T));
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            for(int c = 0; c < channel; c++)
+            {
+                T *srcPtrChannel, *dstPtrChannel;
+                srcPtrChannel = srcPtrImage + (c * imageDimMax);
+                dstPtrChannel = dstPtrImage + (c * imageDimMax);
+
+                for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+                {
+                    T *srcPtrTemp, *dstPtrTemp;
+                    srcPtrTemp = srcPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+                    dstPtrTemp = dstPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+
+                    for(int j = 0; j < batch_srcSize[batchCount].width; j++)
+                    {
+                        *dstPtrTemp = *(lutPtr + (Rpp32s)(*srcPtrTemp));
+
+                        srcPtrTemp++;
+                        dstPtrTemp++;
+                    }
+                }
+            }
+            free(lutPtr);
+        }
+    }
+    else if(chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            T* lutPtr = (T*) calloc(lutSize, sizeof(T));
+            memcpy(lutPtr, (batch_lutPtr + (batchCount * lutSize)), lutSize * sizeof(T));
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            Rpp32u elementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+            Rpp32u elementsInRow = channel * batch_srcSize[batchCount].width;
+
+            for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+            {
+                T *srcPtrTemp, *dstPtrTemp;
+                srcPtrTemp = srcPtrImage + (i * elementsInRowMax);
+                dstPtrTemp = dstPtrImage + (i * elementsInRowMax);
+
+                for(int j = 0; j < batch_srcSize[batchCount].width; j++)
+                {
+                    for(int c = 0; c < channel; c++)
+                    {
+                        *dstPtrTemp = *(lutPtr + (Rpp32s) *srcPtrTemp);
+
+                        srcPtrTemp++;
+                        dstPtrTemp++;
+                    }
+                }
+
+                srcPtrTemp += (elementsInRowMax - elementsInRow);
+                dstPtrTemp += (elementsInRowMax - elementsInRow);
+            }
+            free(lutPtr);
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T, typename U>
+RppStatus look_up_table_host(T* srcPtr, RppiSize srcSize, U* dstPtr,
+                    U *lutPtr,
+                    RppiChnFormat chnFormat, Rpp32u channel)
+{
+    T *srcPtrTemp, *dstPtrTemp;
+    srcPtrTemp = srcPtr;
+    dstPtrTemp = dstPtr;
+    U *lutPtrTemp;
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        for (int c = 0; c < channel; c++)
+        {
+            lutPtrTemp = lutPtr + (c * 256);
+            for (int i = 0; i < (srcSize.height * srcSize.width); i++)
+            {
+                *dstPtrTemp = *(lutPtrTemp + (Rpp32u)(*srcPtrTemp));
+                srcPtrTemp++;
+                dstPtrTemp++;
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        lutPtrTemp = lutPtr;
+        for (int i = 0; i < (srcSize.height * srcSize.width); i++)
+        {
+            for (int c = 0; c < channel; c++)
+            {
+                *dstPtrTemp = *(lutPtrTemp + c + (channel * (Rpp32u)(*srcPtrTemp)));
+                srcPtrTemp++;
+                dstPtrTemp++;
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+/**************** color_temperature ***************/
+
+template <typename T>
+RppStatus color_temperature_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                                Rpp32s *batch_adjustmentValue,
+                                RppiROI *roiPoints, Rpp32u nbatchSize,
+                                RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if (channel == 1)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32u x1 = roiPoints[batchCount].x;
+            Rpp32u y1 = roiPoints[batchCount].y;
+            Rpp32u x2 = x1 + roiPoints[batchCount].roiWidth - 1;
+            Rpp32u y2 = y1 + roiPoints[batchCount].roiHeight - 1;
+            if (x2 == -1)
+            {
+                x2 = batch_srcSize[batchCount].width - 1;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == -1)
+            {
+                y2 = batch_srcSize[batchCount].height - 1;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp32u remainingElementsAfterROI = (batch_srcSize[batchCount].width - (roiPoints[batchCount].x + roiPoints[batchCount].roiWidth));
+
+            Rpp32s adjustmentValue = batch_adjustmentValue[batchCount];
+            Rpp16s adjustmentValueShort = (Rpp16s) adjustmentValue;
+            Rpp8s adjustmentValueChar = (Rpp8s) adjustmentValue;
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            for(int c = 0; c < channel; c++)
+            {
+                T *srcPtrChannel, *dstPtrChannel;
+                srcPtrChannel = srcPtrImage + (c * imageDimMax);
+                dstPtrChannel = dstPtrImage + (c * imageDimMax);
+
+
+                for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+                {
+                    Rpp32s pixel;
+
+                    T *srcPtrTemp, *dstPtrTemp;
+                    srcPtrTemp = srcPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+                    dstPtrTemp = dstPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+
+                    if (!((y1 <= i) && (i <= y2)))
+                    {
+                        memcpy(dstPtrTemp, srcPtrTemp, batch_srcSize[batchCount].width * sizeof(T));
+
+                        dstPtrTemp += batch_srcSizeMax[batchCount].width;
+                        srcPtrTemp += batch_srcSizeMax[batchCount].width;
+                    }
+                    else
+                    {
+                        memcpy(dstPtrTemp, srcPtrTemp, x1 * sizeof(T));
+                        srcPtrTemp += x1;
+                        dstPtrTemp += x1;
+
+                        int bufferLength = roiPoints[batchCount].roiWidth;
+                        int alignedLength = bufferLength & ~15;
+
+                        __m128i const zero = __lsx_vldi(0);
+                        __m128i pAdd = __lsx_vreplgr2vr_b(adjustmentValueChar);
+                        __m128i px0;
+
+                        int vectorLoopCount = 0;
+                        for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+                        {
+                            px0 =  __lsx_vld((__m128i *)srcPtrTemp, 0);
+                            px0 = __lsx_vsadd_bu(px0, pAdd);
+                            __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+
+                            srcPtrTemp +=16;
+                            dstPtrTemp +=16;
+                        }
+                        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                        {
+                            *dstPtrTemp++ = (T) RPPPIXELCHECK((Rpp16s) *srcPtrTemp++ + adjustmentValueShort);
+                        }
+
+                        memcpy(dstPtrTemp, srcPtrTemp, remainingElementsAfterROI * sizeof(T));
+                        srcPtrTemp += remainingElementsAfterROI;
+                        dstPtrTemp += remainingElementsAfterROI;
+                    }
+                }
+            }
+        }
+    }
+    else if (channel == 3)
+    {
+        if(chnFormat == RPPI_CHN_PLANAR)
+        {
+            omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+            for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+            {
+                Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+                Rpp32u x1 = roiPoints[batchCount].x;
+                Rpp32u y1 = roiPoints[batchCount].y;
+                Rpp32u x2 = x1 + roiPoints[batchCount].roiWidth - 1;
+                Rpp32u y2 = y1 + roiPoints[batchCount].roiHeight - 1;
+                if (x2 == -1)
+                {
+                    x2 = batch_srcSize[batchCount].width - 1;
+                    roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+                }
+                if (y2 == -1)
+                {
+                    y2 = batch_srcSize[batchCount].height - 1;
+                    roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+                }
+
+                Rpp32u remainingElementsAfterROI = (batch_srcSize[batchCount].width - (roiPoints[batchCount].x + roiPoints[batchCount].roiWidth));
+
+                Rpp32s adjustmentValue = batch_adjustmentValue[batchCount];
+                Rpp16s adjustmentValueShort = (Rpp16s) adjustmentValue;
+                Rpp8s adjustmentValueChar = (Rpp8s) adjustmentValue;
+
+                T *srcPtrImage, *dstPtrImage;
+                Rpp32u loc = 0;
+                compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+                srcPtrImage = srcPtr + loc;
+                dstPtrImage = dstPtr + loc;
+
+                T *srcPtrChannel, *dstPtrChannel;
+
+                srcPtrChannel = srcPtrImage;
+                dstPtrChannel = dstPtrImage;
+
+
+                for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+                {
+                    Rpp32s pixel;
+
+                    T *srcPtrTemp, *dstPtrTemp;
+                    srcPtrTemp = srcPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+                    dstPtrTemp = dstPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+
+                    if (!((y1 <= i) && (i <= y2)))
+                    {
+                        memcpy(dstPtrTemp, srcPtrTemp, batch_srcSize[batchCount].width * sizeof(T));
+
+                        dstPtrTemp += batch_srcSizeMax[batchCount].width;
+                        srcPtrTemp += batch_srcSizeMax[batchCount].width;
+                    }
+                    else
+                    {
+                        memcpy(dstPtrTemp, srcPtrTemp, x1 * sizeof(T));
+                        srcPtrTemp += x1;
+                        dstPtrTemp += x1;
+
+                        int bufferLength = roiPoints[batchCount].roiWidth;
+                        int alignedLength = bufferLength & ~15;
+
+                        __m128i const zero = __lsx_vldi(0);
+                        __m128i pAdd = __lsx_vreplgr2vr_b(adjustmentValueChar);
+                        __m128i px0;
+
+                        int vectorLoopCount = 0;
+                        for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+                        {
+                            px0 =  __lsx_vld((__m128i *)srcPtrTemp, 0);
+                            px0 = __lsx_vssub_bu(px0, pAdd);
+                            __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+
+                            srcPtrTemp +=16;
+                            dstPtrTemp +=16;
+                        }
+                        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                        {
+                            *dstPtrTemp++ = (T) RPPPIXELCHECK((Rpp16s) *srcPtrTemp++ - adjustmentValueShort);
+                        }
+
+                        memcpy(dstPtrTemp, srcPtrTemp, remainingElementsAfterROI * sizeof(T));
+                        srcPtrTemp += remainingElementsAfterROI;
+                        dstPtrTemp += remainingElementsAfterROI;
+                    }
+                }
+
+                srcPtrChannel = srcPtrImage + (imageDimMax);
+                dstPtrChannel = dstPtrImage + (imageDimMax);
+
+
+                for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+                {
+                    T *srcPtrTemp, *dstPtrTemp;
+                    srcPtrTemp = srcPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+                    dstPtrTemp = dstPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+
+                    memcpy(dstPtrTemp, srcPtrTemp, batch_srcSize[batchCount].width * sizeof(T));
+                }
+
+                srcPtrChannel = srcPtrImage + (2 * imageDimMax);
+                dstPtrChannel = dstPtrImage + (2 * imageDimMax);
+
+
+                for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+                {
+                    Rpp32s pixel;
+
+                    T *srcPtrTemp, *dstPtrTemp;
+                    srcPtrTemp = srcPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+                    dstPtrTemp = dstPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+
+                    if (!((y1 <= i) && (i <= y2)))
+                    {
+                        memcpy(dstPtrTemp, srcPtrTemp, batch_srcSize[batchCount].width * sizeof(T));
+
+                        dstPtrTemp += batch_srcSizeMax[batchCount].width;
+                        srcPtrTemp += batch_srcSizeMax[batchCount].width;
+                    }
+                    else
+                    {
+                        memcpy(dstPtrTemp, srcPtrTemp, x1 * sizeof(T));
+                        srcPtrTemp += x1;
+                        dstPtrTemp += x1;
+
+                        int bufferLength = roiPoints[batchCount].roiWidth;
+                        int alignedLength = bufferLength & ~15;
+
+                        __m128i const zero = __lsx_vldi(0);
+                        __m128i pAdd = __lsx_vreplgr2vr_b(adjustmentValueChar);
+                        __m128i px0;
+
+                        int vectorLoopCount = 0;
+                        for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+                        {
+                            px0 =  __lsx_vld((__m128i *)srcPtrTemp, 0);
+                            px0 = __lsx_vsadd_bu(px0, pAdd);
+                            __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+
+                            srcPtrTemp +=16;
+                            dstPtrTemp +=16;
+                        }
+                        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                        {
+                            *dstPtrTemp++ = (T) RPPPIXELCHECK((Rpp16s) *srcPtrTemp++ + adjustmentValueShort);
+                        }
+
+                        memcpy(dstPtrTemp, srcPtrTemp, remainingElementsAfterROI * sizeof(T));
+                        srcPtrTemp += remainingElementsAfterROI;
+                        dstPtrTemp += remainingElementsAfterROI;
+                    }
+                }
+            }
+        }
+        else if (chnFormat == RPPI_CHN_PACKED)
+        {
+            omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+            for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+            {
+                Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+                Rpp32f x1 = roiPoints[batchCount].x;
+                Rpp32f y1 = roiPoints[batchCount].y;
+                Rpp32f x2 = x1 + roiPoints[batchCount].roiWidth - 1;
+                Rpp32f y2 = y1 + roiPoints[batchCount].roiHeight - 1;
+                if (x2 == -1)
+                {
+                    x2 = batch_srcSize[batchCount].width - 1;
+                    roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+                }
+                if (y2 == -1)
+                {
+                    y2 = batch_srcSize[batchCount].height - 1;
+                    roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+                }
+
+                Rpp32u elementsBeforeROI = channel * x1;
+                Rpp32u remainingElementsAfterROI = channel * (batch_srcSize[batchCount].width - (roiPoints[batchCount].x + roiPoints[batchCount].roiWidth));
+
+                Rpp32s adjustmentValue = batch_adjustmentValue[batchCount];
+                Rpp16s adjustmentValueShort = (Rpp16s) adjustmentValue;
+                Rpp8s adjustmentValueChar = (Rpp8s) adjustmentValue;
+
+                T *srcPtrImage, *dstPtrImage;
+                Rpp32u loc = 0;
+                compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+                srcPtrImage = srcPtr + loc;
+                dstPtrImage = dstPtr + loc;
+
+                Rpp32u elementsInRow = channel * batch_srcSize[batchCount].width;
+                Rpp32u elementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+
+                for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+                {
+                    Rpp32s pixel;
+
+                    T *srcPtrTemp, *dstPtrTemp;
+                    srcPtrTemp = srcPtrImage + (i * elementsInRowMax);
+                    dstPtrTemp = dstPtrImage + (i * elementsInRowMax);
+
+                    if (!((y1 <= i) && (i <= y2)))
+                    {
+                        memcpy(dstPtrTemp, srcPtrTemp, elementsInRow * sizeof(T));
+
+                        dstPtrTemp += elementsInRowMax;
+                        srcPtrTemp += elementsInRowMax;
+                    }
+                    else
+                    {
+                        memcpy(dstPtrTemp, srcPtrTemp, elementsBeforeROI * sizeof(T));
+                        srcPtrTemp += elementsBeforeROI;
+                        dstPtrTemp += elementsBeforeROI;
+
+                        Rpp32u bufferLength = channel * roiPoints[batchCount].roiWidth;
+                        int alignedLength = bufferLength & ~14;
+
+                        __m128i const zero = __lsx_vldi(0);
+                        __m128i pAdd1 = __lsx_vreplgr2vr_h(adjustmentValueShort);
+                        __m128i pAdd2 = __lsx_vreplgr2vr_h(adjustmentValueShort);
+                        __m128i pMask1 = lsx_setr_i16(-1, 0, 1, -1, 0, 1, -1, 0);
+                        __m128i pMask2 = lsx_setr_i16(1, -1, 0, 1, -1, 0, 1, 0);
+                        pAdd1 = __lsx_vmul_h(pAdd1, pMask1);
+                        pAdd2 = __lsx_vmul_h(pAdd2, pMask2);
+                        __m128i px0, px1;
+
+                        int vectorLoopCount = 0;
+                        for (; vectorLoopCount < alignedLength; vectorLoopCount+=15)
+                        {
+                            px0 =  __lsx_vld((__m128i *)srcPtrTemp, 0);
+                            px1 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+                            px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+
+                            px0 = __lsx_vsadd_h(px0, pAdd1);
+                            px1 = __lsx_vsadd_h(px1, pAdd2);
+
+                            px0 = lsx_packus_i16(px0, px1);    // pixels 0-15
+                            __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+
+                            srcPtrTemp +=15;
+                            dstPtrTemp +=15;
+                        }
+                        for (; vectorLoopCount < bufferLength; vectorLoopCount+=3)
+                        {
+                            *dstPtrTemp++ = (T) RPPPIXELCHECK((Rpp16s) *srcPtrTemp++ + adjustmentValueShort);
+                            *dstPtrTemp++ = *srcPtrTemp++;
+                            *dstPtrTemp++ = (T) RPPPIXELCHECK((Rpp16s) *srcPtrTemp++ - adjustmentValueShort);
+                        }
+
+                        memcpy(dstPtrTemp, srcPtrTemp, remainingElementsAfterROI * sizeof(T));
+                        srcPtrTemp += remainingElementsAfterROI;
+                        dstPtrTemp += remainingElementsAfterROI;
+                    }
+                }
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus color_temperature_host(T* srcPtr, RppiSize srcSize, T* dstPtr,
+                    Rpp32s adjustmentValue,
+                    RppiChnFormat chnFormat, Rpp32u channel)
+{
+    if (channel != 1 && channel !=  3)
+    {
+        return RPP_ERROR;
+    }
+    if (adjustmentValue < -100 || adjustmentValue > 100)
+    {
+        return RPP_ERROR;
+    }
+
+    Rpp16s adjustmentValueShort = (Rpp16s) adjustmentValue;
+    Rpp8s adjustmentValueChar = (Rpp8s) adjustmentValue;
+
+    T *srcPtrTemp, *dstPtrTemp;
+    srcPtrTemp = srcPtr;
+    dstPtrTemp = dstPtr;
+
+    Rpp32s pixel;
+
+    if (channel == 1)
+    {
+        int bufferLength = srcSize.height * srcSize.width;
+        int alignedLength = bufferLength & ~15;
+
+        __m128i const zero = __lsx_vldi(0);
+        __m128i pAdd = __lsx_vreplgr2vr_b(adjustmentValueChar);
+        __m128i px0;
+
+        int vectorLoopCount = 0;
+        for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+        {
+            px0 =  __lsx_vld((__m128i *)srcPtrTemp, 0);
+            px0 = __lsx_vsadd_bu(px0, pAdd);
+            __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+
+            srcPtrTemp +=16;
+            dstPtrTemp +=16;
+        }
+        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+        {
+            *dstPtrTemp++ = (T) RPPPIXELCHECK((Rpp16s) *srcPtrTemp++ + adjustmentValueShort);
+        }
+    }
+    else if (channel == 3)
+    {
+        if (chnFormat == RPPI_CHN_PLANAR)
+        {
+            int bufferLength = srcSize.height * srcSize.width;
+            int alignedLength = bufferLength & ~15;
+
+            __m128i const zero = __lsx_vldi(0);
+            __m128i pAdd = __lsx_vreplgr2vr_b(adjustmentValueChar);
+            __m128i px0;
+
+            int vectorLoopCount = 0;
+            for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+            {
+                px0 =  __lsx_vld((__m128i *)srcPtrTemp, 0);
+                px0 = __lsx_vsadd_bu(px0, pAdd);
+                __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+
+                srcPtrTemp +=16;
+                dstPtrTemp +=16;
+            }
+            for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+            {
+                *dstPtrTemp++ = (T) RPPPIXELCHECK((Rpp16s) *srcPtrTemp++ + adjustmentValueShort);
+            }
+
+            memcpy(dstPtrTemp, srcPtrTemp, bufferLength * sizeof(T));
+            dstPtrTemp += bufferLength;
+            srcPtrTemp += bufferLength;
+
+            vectorLoopCount = 0;
+            for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+            {
+                px0 =  __lsx_vld((__m128i *)srcPtrTemp, 0);
+                px0 = __lsx_vssub_bu(px0, pAdd);
+                __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+
+                srcPtrTemp +=16;
+                dstPtrTemp +=16;
+            }
+            for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+            {
+                *dstPtrTemp++ = (T) RPPPIXELCHECK((Rpp16s) *srcPtrTemp++ - adjustmentValueShort);
+            }
+
+        }
+        else if (chnFormat == RPPI_CHN_PACKED)
+        {
+            int bufferLength = srcSize.height * srcSize.width * channel;
+            int alignedLength = bufferLength & ~14;
+
+            __m128i const zero = __lsx_vldi(0);
+            __m128i pAdd = __lsx_vreplgr2vr_b(adjustmentValueChar);
+            __m128i pMask = lsx_setr_i8(1, 0, -1, 1, 0, -1, 1, 0, -1, 1, 0, -1, 1, 0, -1, 0);
+            pAdd = _mm_mullo_epi8(pAdd, pMask);
+            __m128i px0;
+
+            int vectorLoopCount = 0;
+            for (; vectorLoopCount < alignedLength; vectorLoopCount+=15)
+            {
+                px0 =  __lsx_vld((__m128i *)srcPtrTemp, 0);
+                px0 = __lsx_vsadd_bu(px0, pAdd);
+                __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+
+                srcPtrTemp +=15;
+                dstPtrTemp +=15;
+            }
+            for (; vectorLoopCount < bufferLength; vectorLoopCount+=3)
+            {
+                *dstPtrTemp++ = (T) RPPPIXELCHECK((Rpp16s) *srcPtrTemp++ + adjustmentValueShort);
+                dstPtrTemp++;
+                srcPtrTemp++;
+                *dstPtrTemp++ = (T) RPPPIXELCHECK((Rpp16s) *srcPtrTemp++ - adjustmentValueShort);
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+/**************** vignette ***************/
+
+template <typename T>
+RppStatus vignette_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                              Rpp32f *batch_stdDev,
+                              RppiROI *roiPoints, Rpp32u nbatchSize,
+                              RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32u x1 = roiPoints[batchCount].x;
+            Rpp32u y1 = roiPoints[batchCount].y;
+            Rpp32u x2 = x1 + roiPoints[batchCount].roiWidth;
+            Rpp32u y2 = y1 + roiPoints[batchCount].roiHeight;
+            if (x2 == 0)
+            {
+                x2 = batch_srcSize[batchCount].width;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == 0)
+            {
+                y2 = batch_srcSize[batchCount].height;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp32u remainingElementsAfterROI = (batch_srcSize[batchCount].width - (roiPoints[batchCount].x + roiPoints[batchCount].roiWidth));
+
+            Rpp32f stdDev = batch_stdDev[batchCount];
+            Rpp32f multiplier = -1 / (2 * stdDev * stdDev);
+
+            Rpp32u halfHeight = batch_srcSize[batchCount].height / 2;
+            Rpp32u halfWidth = batch_srcSize[batchCount].width / 2;
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            for(int c = 0; c < channel; c++)
+            {
+                T *srcPtrChannel, *dstPtrChannel;
+                srcPtrChannel = srcPtrImage + (c * imageDimMax);
+                dstPtrChannel = dstPtrImage + (c * imageDimMax);
+
+
+                for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+                {
+                    T *srcPtrTemp, *dstPtrTemp;
+                    srcPtrTemp = srcPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+                    dstPtrTemp = dstPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+
+                    if (!((y1 <= i) && (i <= y2)))
+                    {
+                        memcpy(dstPtrTemp, srcPtrTemp, batch_srcSize[batchCount].width * sizeof(T));
+
+                        dstPtrTemp += batch_srcSizeMax[batchCount].width;
+                        srcPtrTemp += batch_srcSizeMax[batchCount].width;
+                    }
+                    else
+                    {
+                        memcpy(dstPtrTemp, srcPtrTemp, x1 * sizeof(T));
+                        srcPtrTemp += x1;
+                        dstPtrTemp += x1;
+
+                        int iCenterRef = i - halfHeight;
+                        Rpp32f iCenterRefSqr = iCenterRef * iCenterRef;
+
+                        Rpp32u bufferLength = roiPoints[batchCount].roiWidth;
+                        Rpp32u alignedLength = bufferLength & ~15;
+
+                        __m128i const zero = __lsx_vldi(0);
+                        __m128i pMask0 = lsx_setr_i32(0, 1, 2, 3);
+                        __m128i pMask1 = lsx_setr_i32(4, 5, 6, 7);
+                        __m128i pMask2 = lsx_setr_i32(8, 9, 10, 11);
+                        __m128i pMask3 = lsx_setr_i32(12, 13, 14, 15);
+                        __m128 pICenterRefSqr = lsx_set1_f32((Rpp32f) (iCenterRef * iCenterRef));
+                        __m128 pHalfWidth = lsx_set1_f32((Rpp32f) halfWidth);
+                        __m128 pMul = lsx_set1_f32(multiplier);
+                        __m128 p0, p1, p2, p3, q0, q1, q2, q3;
+                        __m128i px0, px1, px2, px3, qx0, qx1, qx2, qx3;
+
+                        int vectorLoopCount = 0;
+                        for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+                        {
+                            px0 =  __lsx_vld((__m128i *)srcPtrTemp, 0);
+
+                            px1 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+                            px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                            p0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+                            p1 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px0));    // pixels 4-7
+                            p2 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 8-11
+                            p3 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px1));    // pixels 12-15
+
+                            qx3 = __lsx_vreplgr2vr_w(vectorLoopCount);
+
+                            qx0 = __lsx_vadd_w(qx3, pMask0);
+                            qx1 = __lsx_vadd_w(qx3, pMask1);
+                            qx2 = __lsx_vadd_w(qx3, pMask2);
+                            qx3 = __lsx_vadd_w(qx3, pMask3);
+
+                            q0 = __lsx_vffint_s_w(qx0);
+                            q1 = __lsx_vffint_s_w(qx1);
+                            q2 = __lsx_vffint_s_w(qx2);
+                            q3 = __lsx_vffint_s_w(qx3);
+
+                            q0 = __lsx_vfsub_s(q0, pHalfWidth);
+                            q1 = __lsx_vfsub_s(q1, pHalfWidth);
+                            q2 = __lsx_vfsub_s(q2, pHalfWidth);
+                            q3 = __lsx_vfsub_s(q3, pHalfWidth);
+
+                            q0 = __lsx_vfmul_s(q0, q0);
+                            q1 = __lsx_vfmul_s(q1, q1);
+                            q2 = __lsx_vfmul_s(q2, q2);
+                            q3 = __lsx_vfmul_s(q3, q3);
+
+                            q0 = __lsx_vfadd_s(pICenterRefSqr, q0);
+                            q1 = __lsx_vfadd_s(pICenterRefSqr, q1);
+                            q2 = __lsx_vfadd_s(pICenterRefSqr, q2);
+                            q3 = __lsx_vfadd_s(pICenterRefSqr, q3);
+
+                            q0 = __lsx_vfmul_s(q0, pMul);
+                            q1 = __lsx_vfmul_s(q1, pMul);
+                            q2 = __lsx_vfmul_s(q2, pMul);
+                            q3 = __lsx_vfmul_s(q3, pMul);
+
+                            q0 = fast_exp_sse(q0);
+                            q1 = fast_exp_sse(q1);
+                            q2 = fast_exp_sse(q2);
+                            q3 = fast_exp_sse(q3);
+
+                            p0 = __lsx_vfmul_s(p0, q0);
+                            p1 = __lsx_vfmul_s(p1, q1);
+                            p2 = __lsx_vfmul_s(p2, q2);
+                            p3 = __lsx_vfmul_s(p3, q3);
+
+                            px0 = __lsx_vftint_w_s(p0);
+                            px1 = __lsx_vftint_w_s(p1);
+                            px2 = __lsx_vftint_w_s(p2);
+                            px3 = __lsx_vftint_w_s(p3);
+
+                            px0 = lsx_packus_i32(px0, px1);
+                            px1 = lsx_packus_i32(px2, px3);
+                            px0 = lsx_packus_i16(px0, px1);    // pixels 0-15
+
+                            __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+
+                            srcPtrTemp +=16;
+                            dstPtrTemp +=16;
+                        }
+                        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                        {
+                            int jCenterRef = vectorLoopCount - halfWidth;
+                            Rpp32f jCenterRefSqr = jCenterRef * jCenterRef;
+
+                            *dstPtrTemp++ = (T) RPPPIXELCHECK(((Rpp32f) (*srcPtrTemp++)) * (multiplier * (iCenterRefSqr + jCenterRefSqr)));
+                        }
+
+                        memcpy(dstPtrTemp, srcPtrTemp, remainingElementsAfterROI * sizeof(T));
+                        srcPtrTemp += remainingElementsAfterROI;
+                        dstPtrTemp += remainingElementsAfterROI;
+                    }
+                }
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32u x1 = roiPoints[batchCount].x;
+            Rpp32u y1 = roiPoints[batchCount].y;
+            Rpp32u x2 = x1 + roiPoints[batchCount].roiWidth;
+            Rpp32u y2 = y1 + roiPoints[batchCount].roiHeight;
+            if (x2 == 0)
+            {
+                x2 = batch_srcSize[batchCount].width;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == 0)
+            {
+                y2 = batch_srcSize[batchCount].height;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp32u elementsBeforeROI = channel * x1;
+            Rpp32u remainingElementsAfterROI = channel * (batch_srcSize[batchCount].width - (roiPoints[batchCount].x + roiPoints[batchCount].roiWidth));
+
+            Rpp32f stdDev = batch_stdDev[batchCount];
+            Rpp32f multiplier = -1 / (2 * stdDev * stdDev);
+
+            Rpp32u halfHeight = batch_srcSize[batchCount].height / 2;
+            Rpp32u halfWidth = batch_srcSize[batchCount].width / 2;
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            Rpp32u elementsInRow = channel * batch_srcSize[batchCount].width;
+            Rpp32u elementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+
+
+            for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+            {
+                T *srcPtrTemp, *dstPtrTemp;
+                srcPtrTemp = srcPtrImage + (i * elementsInRowMax);
+                dstPtrTemp = dstPtrImage + (i * elementsInRowMax);
+
+                if (!((y1 <= i) && (i <= y2)))
+                {
+                    memcpy(dstPtrTemp, srcPtrTemp, elementsInRow * sizeof(T));
+
+                    dstPtrTemp += elementsInRowMax;
+                    srcPtrTemp += elementsInRowMax;
+                }
+                else
+                {
+                    memcpy(dstPtrTemp, srcPtrTemp, elementsBeforeROI * sizeof(T));
+                    srcPtrTemp += elementsBeforeROI;
+                    dstPtrTemp += elementsBeforeROI;
+
+                    int iCenterRef = i - halfHeight;
+                    Rpp32f iCenterRefSqr = iCenterRef * iCenterRef;
+
+                    Rpp32u bufferLength = channel * roiPoints[batchCount].roiWidth;
+                    Rpp32u alignedLength = bufferLength & ~14;
+
+                    __m128i const zero = __lsx_vldi(0);
+                    __m128i pMask0 = lsx_setr_i32(0, 0, 0, 1);
+                    __m128i pMask1 = lsx_setr_i32(1, 1, 2, 2);
+                    __m128i pMask2 = lsx_setr_i32(2, 3, 3, 3);
+                    __m128i pMask3 = lsx_setr_i32(4, 4, 4, 0);
+                    __m128 pICenterRefSqr = lsx_set1_f32((Rpp32f) (iCenterRef * iCenterRef));
+                    __m128 pHalfWidth = lsx_set1_f32((Rpp32f) halfWidth);
+                    __m128 pMul = lsx_set1_f32(multiplier);
+                    __m128 p0, p1, p2, p3, q0, q1, q2, q3;
+                    __m128i px0, px1, px2, px3, qx0, qx1, qx2, qx3;
+
+                    int vectorLoopCount = 0;
+                    int jPos = 0;
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount+=15, jPos += 5)
+                    {
+                        px0 =  __lsx_vld((__m128i *)srcPtrTemp, 0);
+
+                        px1 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+                        px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                        p0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+                        p1 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px0));    // pixels 4-7
+                        p2 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 8-11
+                        p3 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px1));    // pixels 12-15
+
+                        qx3 = __lsx_vreplgr2vr_w(jPos);
+
+                        qx0 = __lsx_vadd_w(qx3, pMask0);
+                        qx1 = __lsx_vadd_w(qx3, pMask1);
+                        qx2 = __lsx_vadd_w(qx3, pMask2);
+                        qx3 = __lsx_vadd_w(qx3, pMask3);
+
+                        q0 = __lsx_vffint_s_w(qx0);
+                        q1 = __lsx_vffint_s_w(qx1);
+                        q2 = __lsx_vffint_s_w(qx2);
+                        q3 = __lsx_vffint_s_w(qx3);
+
+                        q0 = __lsx_vfsub_s(q0, pHalfWidth);
+                        q1 = __lsx_vfsub_s(q1, pHalfWidth);
+                        q2 = __lsx_vfsub_s(q2, pHalfWidth);
+                        q3 = __lsx_vfsub_s(q3, pHalfWidth);
+
+                        q0 = __lsx_vfmul_s(q0, q0);
+                        q1 = __lsx_vfmul_s(q1, q1);
+                        q2 = __lsx_vfmul_s(q2, q2);
+                        q3 = __lsx_vfmul_s(q3, q3);
+
+                        q0 = __lsx_vfadd_s(pICenterRefSqr, q0);
+                        q1 = __lsx_vfadd_s(pICenterRefSqr, q1);
+                        q2 = __lsx_vfadd_s(pICenterRefSqr, q2);
+                        q3 = __lsx_vfadd_s(pICenterRefSqr, q3);
+
+                        q0 = __lsx_vfmul_s(q0, pMul);
+                        q1 = __lsx_vfmul_s(q1, pMul);
+                        q2 = __lsx_vfmul_s(q2, pMul);
+                        q3 = __lsx_vfmul_s(q3, pMul);
+
+                        q0 = fast_exp_sse(q0);
+                        q1 = fast_exp_sse(q1);
+                        q2 = fast_exp_sse(q2);
+                        q3 = fast_exp_sse(q3);
+
+                        p0 = __lsx_vfmul_s(p0, q0);
+                        p1 = __lsx_vfmul_s(p1, q1);
+                        p2 = __lsx_vfmul_s(p2, q2);
+                        p3 = __lsx_vfmul_s(p3, q3);
+
+                        px0 = __lsx_vftint_w_s(p0);
+                        px1 = __lsx_vftint_w_s(p1);
+                        px2 = __lsx_vftint_w_s(p2);
+                        px3 = __lsx_vftint_w_s(p3);
+
+                        px0 = lsx_packus_i32(px0, px1);
+                        px1 = lsx_packus_i32(px2, px3);
+                        px0 = lsx_packus_i16(px0, px1);    // pixels 0-15
+
+                        __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+
+                        srcPtrTemp +=15;
+                        dstPtrTemp +=15;
+                    }
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount+=channel, jPos++)
+                    {
+                        int jCenterRef = jPos - halfWidth;
+                        Rpp32f jCenterRefSqr = jCenterRef * jCenterRef;
+
+                        for (int c = 0; c < channel; c++)
+                        {
+                            *dstPtrTemp++ = (T) RPPPIXELCHECK(((Rpp32f) (*srcPtrTemp++)) * (multiplier * (iCenterRefSqr + jCenterRefSqr)));
+                        }
+                    }
+
+                    memcpy(dstPtrTemp, srcPtrTemp, remainingElementsAfterROI * sizeof(T));
+                    srcPtrTemp += remainingElementsAfterROI;
+                    dstPtrTemp += remainingElementsAfterROI;
+                }
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus vignette_host(T* srcPtr, RppiSize srcSize, T* dstPtr,
+                          Rpp32f stdDev,
+                          RppiChnFormat chnFormat, Rpp32u channel)
+{
+    T *srcPtrTemp, *dstPtrTemp;
+
+    srcPtrTemp = srcPtr;
+    dstPtrTemp = dstPtr;
+
+    Rpp32f multiplier = -1 / (2 * stdDev * stdDev);
+
+    Rpp32u halfHeight = srcSize.height / 2;
+    Rpp32u halfWidth = srcSize.width / 2;
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        for (int c = 0; c < channel; c++)
+        {
+            srcPtrTemp = srcPtr + (c * srcSize.height * srcSize.width);
+
+            for (int i = 0; i < srcSize.height; i++)
+            {
+                int iCenterRef = i - halfHeight;
+                Rpp32f iCenterRefSqr = iCenterRef * iCenterRef;
+
+                int bufferLength = srcSize.width;
+                int alignedLength = bufferLength & ~15;
+
+                __m128i const zero = __lsx_vldi(0);
+                __m128i pMask0 = lsx_setr_i32(0, 1, 2, 3);
+                __m128i pMask1 = lsx_setr_i32(4, 5, 6, 7);
+                __m128i pMask2 = lsx_setr_i32(8, 9, 10, 11);
+                __m128i pMask3 = lsx_setr_i32(12, 13, 14, 15);
+                __m128 pICenterRefSqr = lsx_set1_f32((Rpp32f) (iCenterRef * iCenterRef));
+                __m128 pHalfWidth = lsx_set1_f32((Rpp32f) halfWidth);
+                __m128 pMul = lsx_set1_f32(multiplier);
+                __m128 p0, p1, p2, p3, q0, q1, q2, q3;
+                __m128i px0, px1, px2, px3, qx0, qx1, qx2, qx3;
+
+                int vectorLoopCount = 0;
+                for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+                {
+                    px0 =  __lsx_vld((__m128i *)srcPtrTemp, 0);
+
+                    px1 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+                    px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                    p0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+                    p1 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px0));    // pixels 4-7
+                    p2 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 8-11
+                    p3 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px1));    // pixels 12-15
+
+                    qx3 = __lsx_vreplgr2vr_w(vectorLoopCount);
+
+                    qx0 = __lsx_vadd_w(qx3, pMask0);
+                    qx1 = __lsx_vadd_w(qx3, pMask1);
+                    qx2 = __lsx_vadd_w(qx3, pMask2);
+                    qx3 = __lsx_vadd_w(qx3, pMask3);
+
+                    q0 = __lsx_vffint_s_w(qx0);
+                    q1 = __lsx_vffint_s_w(qx1);
+                    q2 = __lsx_vffint_s_w(qx2);
+                    q3 = __lsx_vffint_s_w(qx3);
+
+                    q0 = __lsx_vfsub_s(q0, pHalfWidth);
+                    q1 = __lsx_vfsub_s(q1, pHalfWidth);
+                    q2 = __lsx_vfsub_s(q2, pHalfWidth);
+                    q3 = __lsx_vfsub_s(q3, pHalfWidth);
+
+                    q0 = __lsx_vfmul_s(q0, q0);
+                    q1 = __lsx_vfmul_s(q1, q1);
+                    q2 = __lsx_vfmul_s(q2, q2);
+                    q3 = __lsx_vfmul_s(q3, q3);
+
+                    q0 = __lsx_vfadd_s(pICenterRefSqr, q0);
+                    q1 = __lsx_vfadd_s(pICenterRefSqr, q1);
+                    q2 = __lsx_vfadd_s(pICenterRefSqr, q2);
+                    q3 = __lsx_vfadd_s(pICenterRefSqr, q3);
+
+                    q0 = __lsx_vfmul_s(q0, pMul);
+                    q1 = __lsx_vfmul_s(q1, pMul);
+                    q2 = __lsx_vfmul_s(q2, pMul);
+                    q3 = __lsx_vfmul_s(q3, pMul);
+
+                    q0 = fast_exp_sse(q0);
+                    q1 = fast_exp_sse(q1);
+                    q2 = fast_exp_sse(q2);
+                    q3 = fast_exp_sse(q3);
+
+                    p0 = __lsx_vfmul_s(p0, q0);
+                    p1 = __lsx_vfmul_s(p1, q1);
+                    p2 = __lsx_vfmul_s(p2, q2);
+                    p3 = __lsx_vfmul_s(p3, q3);
+
+                    px0 = __lsx_vftint_w_s(p0);
+                    px1 = __lsx_vftint_w_s(p1);
+                    px2 = __lsx_vftint_w_s(p2);
+                    px3 = __lsx_vftint_w_s(p3);
+
+                    px0 = lsx_packus_i32(px0, px1);
+                    px1 = lsx_packus_i32(px2, px3);
+                    px0 = lsx_packus_i16(px0, px1);    // pixels 0-15
+
+                    __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+
+                    srcPtrTemp +=16;
+                    dstPtrTemp +=16;
+                }
+                for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                {
+                    int jCenterRef = vectorLoopCount - halfWidth;
+                    Rpp32f jCenterRefSqr = jCenterRef * jCenterRef;
+
+                    *dstPtrTemp++ = (T) RPPPIXELCHECK(((Rpp32f) (*srcPtrTemp++)) * (multiplier * (iCenterRefSqr + jCenterRefSqr)));
+                }
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        Rpp32u elementsInRow = channel * srcSize.width;
+
+        for (int i = 0; i < srcSize.height; i++)
+        {
+            int iCenterRef = i - halfHeight;
+            Rpp32f iCenterRefSqr = iCenterRef * iCenterRef;
+
+            int bufferLength = elementsInRow;
+            int alignedLength = bufferLength & ~14;
+
+            __m128i const zero = __lsx_vldi(0);
+            __m128i pMask0 = lsx_setr_i32(0, 0, 0, 1);
+            __m128i pMask1 = lsx_setr_i32(1, 1, 2, 2);
+            __m128i pMask2 = lsx_setr_i32(2, 3, 3, 3);
+            __m128i pMask3 = lsx_setr_i32(4, 4, 4, 0);
+            __m128 pICenterRefSqr = lsx_set1_f32((Rpp32f) (iCenterRef * iCenterRef));
+            __m128 pHalfWidth = lsx_set1_f32((Rpp32f) halfWidth);
+            __m128 pMul = lsx_set1_f32(multiplier);
+            __m128 p0, p1, p2, p3, q0, q1, q2, q3;
+            __m128i px0, px1, px2, px3, qx0, qx1, qx2, qx3;
+
+            int vectorLoopCount = 0;
+            int jPos = 0;
+            for (; vectorLoopCount < alignedLength; vectorLoopCount+=15, jPos += 5)
+            {
+                px0 =  __lsx_vld((__m128i *)srcPtrTemp, 0);
+
+                px1 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+                px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                p0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+                p1 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px0));    // pixels 4-7
+                p2 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 8-11
+                p3 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px1));    // pixels 12-15
+
+                qx3 = __lsx_vreplgr2vr_w(jPos);
+
+                qx0 = __lsx_vadd_w(qx3, pMask0);
+                qx1 = __lsx_vadd_w(qx3, pMask1);
+                qx2 = __lsx_vadd_w(qx3, pMask2);
+                qx3 = __lsx_vadd_w(qx3, pMask3);
+
+                q0 = __lsx_vffint_s_w(qx0);
+                q1 = __lsx_vffint_s_w(qx1);
+                q2 = __lsx_vffint_s_w(qx2);
+                q3 = __lsx_vffint_s_w(qx3);
+
+                q0 = __lsx_vfsub_s(q0, pHalfWidth);
+                q1 = __lsx_vfsub_s(q1, pHalfWidth);
+                q2 = __lsx_vfsub_s(q2, pHalfWidth);
+                q3 = __lsx_vfsub_s(q3, pHalfWidth);
+
+                q0 = __lsx_vfmul_s(q0, q0);
+                q1 = __lsx_vfmul_s(q1, q1);
+                q2 = __lsx_vfmul_s(q2, q2);
+                q3 = __lsx_vfmul_s(q3, q3);
+
+                q0 = __lsx_vfadd_s(pICenterRefSqr, q0);
+                q1 = __lsx_vfadd_s(pICenterRefSqr, q1);
+                q2 = __lsx_vfadd_s(pICenterRefSqr, q2);
+                q3 = __lsx_vfadd_s(pICenterRefSqr, q3);
+
+                q0 = __lsx_vfmul_s(q0, pMul);
+                q1 = __lsx_vfmul_s(q1, pMul);
+                q2 = __lsx_vfmul_s(q2, pMul);
+                q3 = __lsx_vfmul_s(q3, pMul);
+
+                q0 = fast_exp_sse(q0);
+                q1 = fast_exp_sse(q1);
+                q2 = fast_exp_sse(q2);
+                q3 = fast_exp_sse(q3);
+
+                p0 = __lsx_vfmul_s(p0, q0);
+                p1 = __lsx_vfmul_s(p1, q1);
+                p2 = __lsx_vfmul_s(p2, q2);
+                p3 = __lsx_vfmul_s(p3, q3);
+
+                px0 = __lsx_vftint_w_s(p0);
+                px1 = __lsx_vftint_w_s(p1);
+                px2 = __lsx_vftint_w_s(p2);
+                px3 = __lsx_vftint_w_s(p3);
+
+                px0 = lsx_packus_i32(px0, px1);
+                px1 = lsx_packus_i32(px2, px3);
+                px0 = lsx_packus_i16(px0, px1);    // pixels 0-15
+
+                __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+
+                srcPtrTemp +=15;
+                dstPtrTemp +=15;
+            }
+            for (; vectorLoopCount < bufferLength; vectorLoopCount+=channel, jPos++)
+            {
+                int jCenterRef = jPos - halfWidth;
+                Rpp32f jCenterRefSqr = jCenterRef * jCenterRef;
+
+                for (int c = 0; c < channel; c++)
+                {
+                    *dstPtrTemp++ = (T) RPPPIXELCHECK(((Rpp32f) (*srcPtrTemp++)) * (multiplier * (iCenterRefSqr + jCenterRefSqr)));
+                }
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+/**************** hue ***************/
+
+template <typename T>
+RppStatus hueRGB_processBuffer_host(T* srcPtr, RppiSize srcSize, T* dstPtr,
+                                    Rpp32f hueShift, Rpp32f hueShiftAngle, Rpp64u bufferLength,
+                                    RppiChnFormat chnFormat, Rpp32u channel)
+{
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        T *srcPtrTempR, *srcPtrTempG, *srcPtrTempB;
+        T *dstPtrTempR, *dstPtrTempG, *dstPtrTempB;
+
+        Rpp64u imageDim = srcSize.height * srcSize.width;
+        srcPtrTempR = srcPtr;
+        srcPtrTempG = srcPtr + (imageDim);
+        srcPtrTempB = srcPtr + (2 * imageDim);
+        dstPtrTempR = dstPtr;
+        dstPtrTempG = dstPtr + (imageDim);
+        dstPtrTempB = dstPtr + (2 * imageDim);
+
+        Rpp64u alignedLength = bufferLength & ~3;
+
+        __m128i const zero = __lsx_vldi(0);
+        __m128 pZeros = lsx_set1_f32(0.0);
+        __m128 pOnes = lsx_set1_f32(1.0);
+        __m128 pFactor = lsx_set1_f32(255.0);
+        __m128 pHueShift = lsx_set1_f32(hueShift);
+        __m128i px0, px1, px2;
+        __m128 xR, xG, xB;
+        __m128 xH, xS, xV, xC;
+        __m128 xX, xY, xZ;
+        __m128 h0, h1, h2, h3;
+        __m128 x0, x1, x2, x3;
+        __m128 a0, a1;
+        h0 = lsx_set1_f32(1.0);
+
+        Rpp8u arrayR[4];
+        Rpp8u arrayG[4];
+        Rpp8u arrayB[4];
+
+        Rpp64u vectorLoopCount = 0;
+        for (; vectorLoopCount < alignedLength; vectorLoopCount+=4)
+        {
+            px0 =  __lsx_vld((__m128i *)srcPtrTempR, 0);
+            px1 =  __lsx_vld((__m128i *)srcPtrTempG, 0);
+            px2 =  __lsx_vld((__m128i *)srcPtrTempB, 0);
+
+            px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+            px1 = __lsx_vilvl_b(zero, px1);    // pixels 0-7
+            px2 = __lsx_vilvl_b(zero, px2);    // pixels 0-7
+
+            xR = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+            xG = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 0-3
+            xB = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px2));    // pixels 0-3
+
+            xR = __lsx_vfdiv_s(xR, pFactor);
+            xG = __lsx_vfdiv_s(xG, pFactor);
+            xB = __lsx_vfdiv_s(xB, pFactor);
+
+            // Calculate Saturation, Value, Chroma
+            xS = __lsx_vfmax_s(xG, xB);                               // xS <- [max(G, B)]
+            xC = __lsx_vfmin_s(xG, xB);                               // xC <- [min(G, B)]
+
+            xS = __lsx_vfmax_s(xS, xR);                               // xS <- [max(G, B, R)]
+            xC = __lsx_vfmin_s(xC, xR);                               // xC <- [min(G, B, R)]
+
+            xV = xS;                                               // xV <- [V    ]
+            xS = __lsx_vfsub_s(xS, xC);                               // xS <- [V - m]
+            xS = __lsx_vfdiv_s(xS, xV);                               // xS <- [S    ]
+
+            xC = __lsx_vfsub_s(xC, xV);                               // xC <- [V + m]
+
+            // Calculate Hue
+            xZ = (__m128)__lsx_vfcmp_ceq_s(xV, xG);                             // xZ <- [V==G]
+            xX = (__m128)__lsx_vfcmp_cune_s(xV, xR);                            // xX <- [V!=R]
+
+            xY = (__m128)__lsx_vand_v((__m128i)xZ, (__m128i)xX);                               // xY <- [V!=R && V==G]
+            xZ = (__m128)__lsx_vandn_v((__m128i)xZ, (__m128i)xX);                            // xZ <- [V!=R && V!=G]
+
+            xY = (__m128)__lsx_vxor_v((__m128i)xY, (__m128i)SIMD_GET_PS(full));                // xY <- [V==R || V!=G]
+            xZ = (__m128)__lsx_vxor_v((__m128i)xZ, (__m128i)SIMD_GET_PS(full));                // xZ <- [V==R || V==G]
+
+            xR = (__m128)__lsx_vand_v((__m128i)xR, (__m128i)xX);                               // xR <- [X!=0 ? R : 0]
+            xB = (__m128)__lsx_vand_v((__m128i)xB, (__m128i)xZ);                               // xB <- [Z!=0 ? B : 0]
+            xG = (__m128)__lsx_vand_v((__m128i)xG, (__m128i)xY);                               // xG <- [Y!=0 ? G : 0]
+
+            xZ = (__m128)__lsx_vandn_v((__m128i)xZ, (__m128i)SIMD_GET_PS(sn));               // xZ <- [sign(!Z)]
+            xY = (__m128)__lsx_vandn_v((__m128i)xY, (__m128i)SIMD_GET_PS(sn));               // xY <- [sign(!Y)]
+
+            xG = (__m128)__lsx_vxor_v((__m128i)xG, (__m128i)xZ);                               // xG <- [Y!=0 ? (Z==0 ? G : -G) : 0]
+            xR = (__m128)__lsx_vxor_v((__m128i)xR, (__m128i)xY);                               // xR <- [X!=0 ? (Y==0 ? R : -R) : 0]
+
+            // G is the accumulator
+            xG = __lsx_vfadd_s(xG, xR);                               // xG <- [Rx + Gx]
+            xB = (__m128)__lsx_vxor_v((__m128i)xB, (__m128i)xY);                               // xB <- [Z!=0 ? (Y==0 ? B : -B) : 0]
+
+            xC = __lsx_vfmul_s(xC, SIMD_GET_PS(m6_m6_m6_m6));         // xC <- [C*6     ]
+            xG = __lsx_vfsub_s(xG, xB);                               // xG <- [Rx+Gx+Bx]
+
+            xH = (__m128)__lsx_vand_v((__m128i)xX, (__m128i)SIMD_GET_PS(m4o6_m4o6_m4o6_m4o6)); // xH <- [V==R ?0 :-4/6]
+            xG = __lsx_vfdiv_s(xG, xC);                               // xG <- [(Rx+Gx+Bx)/6C]
+
+            // Correct achromatic cases (H/S may be infinite due to zero division)
+            xH = (__m128)__lsx_vxor_v((__m128i)xH, (__m128i)xZ);                               // xH <- [V==R ? 0 : V==G ? -4/6 : 4/6]
+            xC = (__m128)__lsx_vfcmp_cle_s(SIMD_GET_PS(eps), xC);
+            xH = __lsx_vfadd_s(xH, SIMD_GET_PS(p1));                  // xH <- [V==R ? 1 : V==G ?  2/6 :10/6]
+
+            xG = __lsx_vfadd_s(xG, xH);
+
+            // Normalize H to fraction
+            xH = (__m128)__lsx_vfcmp_cle_s(SIMD_GET_PS(p1), xG);
+
+            xH = (__m128)__lsx_vand_v((__m128i)xH, (__m128i)SIMD_GET_PS(p1));
+            xS = (__m128)__lsx_vand_v((__m128i)xS, (__m128i)xC);
+            xG = (__m128)__lsx_vand_v((__m128i)xG, (__m128i)xC);
+            xG = __lsx_vfsub_s(xG, xH);
+
+            // Modify Hue Values and re-normalize H to fraction
+            xG = __lsx_vfadd_s(xG, pHueShift);
+
+            xH = (__m128)__lsx_vfcmp_cle_s(SIMD_GET_PS(p1), xG);
+            xH = (__m128)__lsx_vand_v((__m128i)xH, (__m128i)SIMD_GET_PS(p1));
+
+            xG = __lsx_vfsub_s(xG, xH);
+
+            _MM_TRANSPOSE4_PS (h0, xG, xS, xV);
+
+            __m128 h1, h2, h3;
+
+            h1 = xG;
+            h2 = xS;
+            h3 = xV;
+
+            // Prepare HUE for RGB components (per pixel).
+            x0 = SIMD_SHUFFLE_PS(h0, _MM_SHUFFLE(1, 1, 1, 3));     // x0 <- [H           |H           |H           |V          ]
+            x1 = SIMD_SHUFFLE_PS(h1, _MM_SHUFFLE(1, 1, 1, 3));     // x1 <- [H           |H           |H           |V          ]
+            x2 = SIMD_SHUFFLE_PS(h2, _MM_SHUFFLE(1, 1, 1, 3));     // x2 <- [H           |H           |H           |V          ]
+            x3 = SIMD_SHUFFLE_PS(h3, _MM_SHUFFLE(1, 1, 1, 3));     // x3 <- [H           |H           |H           |V          ]
+
+            // Calculate intervals from HUE.
+            x0 = __lsx_vfsub_s(x0, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x0 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+            x1 = __lsx_vfsub_s(x1, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x1 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+            x2 = __lsx_vfsub_s(x2, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x2 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+            x3 = __lsx_vfsub_s(x3, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x3 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+
+            x0 = (__m128)__lsx_vand_v((__m128i)x0, (__m128i)SIMD_GET_PS(abs));                 // x0 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+            x1 = (__m128)__lsx_vand_v((__m128i)x1, (__m128i)SIMD_GET_PS(abs));                 // x1 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+            x2 = (__m128)__lsx_vand_v((__m128i)x2, (__m128i)SIMD_GET_PS(abs));                 // x2 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+            x3 = (__m128)__lsx_vand_v((__m128i)x3, (__m128i)SIMD_GET_PS(abs));                 // x3 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+
+            x0 = __lsx_vfmul_s(x0, SIMD_GET_PS(m6_m6_p6_p0));         // x0 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+            x1 = __lsx_vfmul_s(x1, SIMD_GET_PS(m6_m6_p6_p0));         // x1 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+            x2 = __lsx_vfmul_s(x2, SIMD_GET_PS(m6_m6_p6_p0));         // x2 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+            x3 = __lsx_vfmul_s(x3, SIMD_GET_PS(m6_m6_p6_p0));         // x3 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+
+            x0 = __lsx_vfadd_s(x0, SIMD_GET_PS(p1_p1_m2_p0));         // x0 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+            x1 = __lsx_vfadd_s(x1, SIMD_GET_PS(p1_p1_m2_p0));         // x1 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+            x2 = __lsx_vfadd_s(x2, SIMD_GET_PS(p1_p1_m2_p0));         // x2 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+            x3 = __lsx_vfadd_s(x3, SIMD_GET_PS(p1_p1_m2_p0));         // x3 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+
+            // Bound intervals.
+            x0 = __lsx_vfmax_s(x0, SIMD_GET_PS(m1_m1_m1_p1));
+            x1 = __lsx_vfmax_s(x1, SIMD_GET_PS(m1_m1_m1_p1));
+            x2 = __lsx_vfmax_s(x2, SIMD_GET_PS(m1_m1_m1_p1));
+            x3 = __lsx_vfmax_s(x3, SIMD_GET_PS(m1_m1_m1_p1));
+
+            x0 = __lsx_vfmin_s(x0, SIMD_GET_PS(p0));                  // x0 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+            x1 = __lsx_vfmin_s(x1, SIMD_GET_PS(p0));                  // x1 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+            x2 = __lsx_vfmin_s(x2, SIMD_GET_PS(p0));                  // x2 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+            x3 = __lsx_vfmin_s(x3, SIMD_GET_PS(p0));                  // x3 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+
+            // Prepare S/V vectors.
+            a0 = SIMD_SHUFFLE_PS(h0, _MM_SHUFFLE(2, 2, 2, 2));     // a0 <- [S           |S           |S           |S          ]
+            a1 = SIMD_SHUFFLE_PS(h1, _MM_SHUFFLE(2, 2, 2, 2));     // a1 <- [S           |S           |S           |S          ]
+            h0 = SIMD_SHUFFLE_PS(h0, _MM_SHUFFLE(3, 3, 3, 0));     // h0 <- [V           |V           |V           |A          ]
+            h1 = SIMD_SHUFFLE_PS(h1, _MM_SHUFFLE(3, 3, 3, 0));     // h1 <- [V           |V           |V           |A          ]
+
+            // Multiply with 'S*V' and add 'V'.
+            x0 = __lsx_vfmul_s(x0, a0);                               // x0 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            x1 = __lsx_vfmul_s(x1, a1);                               // x1 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            a0 = SIMD_SHUFFLE_PS(h2, _MM_SHUFFLE(2, 2, 2, 2));     // a0 <- [S           |S           |S           |S          ]
+            a1 = SIMD_SHUFFLE_PS(h3, _MM_SHUFFLE(2, 2, 2, 2));     // a1 <- [S           |S           |S           |S          ]
+
+            x0 = __lsx_vfmul_s(x0, h0);                               // x0 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            x1 = __lsx_vfmul_s(x1, h1);                               // x1 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            h2 = SIMD_SHUFFLE_PS(h2, _MM_SHUFFLE(3, 3, 3, 0));     // h2 <- [V           |V           |V           |A          ]
+            h3 = SIMD_SHUFFLE_PS(h3, _MM_SHUFFLE(3, 3, 3, 0));     // h3 <- [V           |V           |V           |A          ]
+
+            x2 = __lsx_vfmul_s(x2, a0);                               // x2 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            x3 = __lsx_vfmul_s(x3, a1);                               // x3 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            x0 = __lsx_vfadd_s(x0, h0);                               // x0 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |A          ]
+
+            x2 = __lsx_vfmul_s(x2, h2);                               // x2 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            x3 = __lsx_vfmul_s(x3, h3);                               // x3 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            x1 = __lsx_vfadd_s(x1, h1);                               // x1 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |A          ]
+
+            x2 = __lsx_vfadd_s(x2, h2);                               // x2 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |A          ]
+            x3 = __lsx_vfadd_s(x3, h3);                               // x3 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |A          ]
+
+            // Store
+            _MM_TRANSPOSE4_PS (x0, x1, x2, x3);
+
+            x1 = __lsx_vfmul_s(x1, pFactor);
+            x2 = __lsx_vfmul_s(x2, pFactor);
+            x3 = __lsx_vfmul_s(x3, pFactor);
+
+            px0 = __lsx_vftint_w_s(x1);
+            px1 = __lsx_vftint_w_s(x2);
+            px2 = __lsx_vftint_w_s(x3);
+
+            px0 = lsx_packs_i32(px0, px0);
+            px0 = lsx_packus_i16(px0, px0);
+            *((int*)arrayR) = __lsx_vpickve2gr_w(px0, 0);
+
+            px1 = lsx_packs_i32(px1, px1);
+            px1 = lsx_packus_i16(px1, px1);
+            *((int*)arrayG) = __lsx_vpickve2gr_w(px1, 0);
+
+            px2 = lsx_packs_i32(px2, px2);
+            px2 = lsx_packus_i16(px2, px2);
+            *((int*)arrayB) = __lsx_vpickve2gr_w(px2, 0);
+
+            memcpy(dstPtrTempR, arrayR, 4 * sizeof(Rpp8u));
+            memcpy(dstPtrTempG, arrayG, 4 * sizeof(Rpp8u));
+            memcpy(dstPtrTempB, arrayB, 4 * sizeof(Rpp8u));
+
+            srcPtrTempR += 4;
+            srcPtrTempG += 4;
+            srcPtrTempB += 4;
+            dstPtrTempR += 4;
+            dstPtrTempG += 4;
+            dstPtrTempB += 4;
+        }
+        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+        {
+            // RGB to HSV
+
+            Rpp32f hue, sat, val;
+            Rpp32f rf, gf, bf, cmax, cmin, delta;
+            rf = ((Rpp32f) *srcPtrTempR) / 255;
+            gf = ((Rpp32f) *srcPtrTempG) / 255;
+            bf = ((Rpp32f) *srcPtrTempB) / 255;
+            cmax = RPPMAX3(rf, gf, bf);
+            cmin = RPPMIN3(rf, gf, bf);
+            delta = cmax - cmin;
+
+            if (delta == 0)
+            {
+                hue = 0;
+            }
+            else if (cmax == rf)
+            {
+                hue = round(60 * fmod(((gf - bf) / delta),6));
+            }
+            else if (cmax == gf)
+            {
+                hue = round(60 * (((bf - rf) / delta) + 2));
+            }
+            else if (cmax == bf)
+            {
+                hue = round(60 * (((rf - gf) / delta) + 4));
+            }
+
+            while (hue >= 360)
+            {
+                hue = hue - 360;
+            }
+            while (hue < 0)
+            {
+                hue = 360 + hue;
+            }
+
+            if (cmax == 0)
+            {
+                sat = 0;
+            }
+            else
+            {
+                sat = delta / cmax;
+            }
+
+            val = cmax;
+
+            // Modify Hue
+
+            hue = hue + hueShiftAngle;
+            while (hue >= 360)
+            {
+                hue = hue - 360;
+            }
+            while (hue < 0)
+            {
+                hue = 360 + hue;
+            }
+
+            // HSV to RGB
+
+            Rpp32f c, x, m;
+            c = val * sat;
+            x = c * (1 - abs(int(fmod((hue / 60), 2)) - 1));
+            m = val - c;
+
+            if ((0 <= hue) && (hue < 60))
+            {
+                rf = c;
+                gf = x;
+                bf = 0;
+            }
+            else if ((60 <= hue) && (hue < 120))
+            {
+                rf = x;
+                gf = c;
+                bf = 0;
+            }
+            else if ((120 <= hue) && (hue < 180))
+            {
+                rf = 0;
+                gf = c;
+                bf = x;
+            }
+            else if ((180 <= hue) && (hue < 240))
+            {
+                rf = 0;
+                gf = x;
+                bf = c;
+            }
+            else if ((240 <= hue) && (hue < 300))
+            {
+                rf = x;
+                gf = 0;
+                bf = c;
+            }
+            else if ((300 <= hue) && (hue < 360))
+            {
+                rf = c;
+                gf = 0;
+                bf = x;
+            }
+
+            *dstPtrTempR = (Rpp8u) round((rf + m) * 255);
+            *dstPtrTempG = (Rpp8u) round((gf + m) * 255);
+            *dstPtrTempB = (Rpp8u) round((bf + m) * 255);
+
+            srcPtrTempR++;
+            srcPtrTempG++;
+            srcPtrTempB++;
+            dstPtrTempR++;
+            dstPtrTempG++;
+            dstPtrTempB++;
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        T *srcPtrTempPx0, *srcPtrTempPx1, *srcPtrTempPx2, *srcPtrTempPx3;
+        T *dstPtrTempPx0, *dstPtrTempPx1, *dstPtrTempPx2, *dstPtrTempPx3;
+
+        srcPtrTempPx0 = srcPtr;
+        srcPtrTempPx1 = srcPtr + 3;
+        srcPtrTempPx2 = srcPtr + 6;
+        srcPtrTempPx3 = srcPtr + 9;
+        dstPtrTempPx0 = dstPtr;
+        dstPtrTempPx1 = dstPtr + 3;
+        dstPtrTempPx2 = dstPtr + 6;
+        dstPtrTempPx3 = dstPtr + 9;
+
+        Rpp64u alignedLength = (bufferLength / 12) * 12;
+
+        __m128i const zero = __lsx_vldi(0);
+        __m128 pZeros = lsx_set1_f32(0.0);
+        __m128 pOnes = lsx_set1_f32(1.0);
+        __m128 pFactor = lsx_set1_f32(255.0);
+        __m128 pHueShift = lsx_set1_f32(hueShift);
+        __m128i px0, px1, px2, px3;
+        __m128 xR, xG, xB, xA;
+        __m128 xH, xS, xV, xC;
+        __m128 xX, xY, xZ;
+        __m128 h0, h1, h2, h3;
+        __m128 x0, x1, x2, x3;
+        __m128 a0, a1;
+        h0 = lsx_set1_f32(1.0);
+
+        Rpp8u arrayPx0[4];
+        Rpp8u arrayPx1[4];
+        Rpp8u arrayPx2[4];
+        Rpp8u arrayPx3[4];
+
+        Rpp64u vectorLoopCount = 0;
+        for (; vectorLoopCount < alignedLength; vectorLoopCount+=12)
+        {
+            px0 = __lsx_vld((__m128i *)srcPtrTempPx0, 0);
+            px1 = __lsx_vld((__m128i *)srcPtrTempPx1, 0);
+            px2 = __lsx_vld((__m128i *)srcPtrTempPx2, 0);
+            px3 = __lsx_vld((__m128i *)srcPtrTempPx3, 0);
+
+            px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+            px1 = __lsx_vilvl_b(zero, px1);    // pixels 0-7
+            px2 = __lsx_vilvl_b(zero, px2);    // pixels 0-7
+            px3 = __lsx_vilvl_b(zero, px3);    // pixels 0-7
+
+            xR = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+            xG = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 0-3
+            xB = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px2));    // pixels 0-3
+            xA = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px3));    // pixels 0-3
+
+            _MM_TRANSPOSE4_PS (xR, xG, xB, xA);
+
+            xR = __lsx_vfdiv_s(xR, pFactor);
+            xG = __lsx_vfdiv_s(xG, pFactor);
+            xB = __lsx_vfdiv_s(xB, pFactor);
+
+            // Calculate Saturation, Value, Chroma
+            xS = __lsx_vfmax_s(xG, xB);                               // xS <- [max(G, B)]
+            xC = __lsx_vfmin_s(xG, xB);                               // xC <- [min(G, B)]
+
+            xS = __lsx_vfmax_s(xS, xR);                               // xS <- [max(G, B, R)]
+            xC = __lsx_vfmin_s(xC, xR);                               // xC <- [min(G, B, R)]
+
+            xV = xS;                                               // xV <- [V    ]
+            xS = __lsx_vfsub_s(xS, xC);                               // xS <- [V - m]
+            xS = __lsx_vfdiv_s(xS, xV);                               // xS <- [S    ]
+
+            xC = __lsx_vfsub_s(xC, xV);                               // xC <- [V + m]
+
+            // Calculate Hue
+            xZ = (__m128)__lsx_vfcmp_ceq_s(xV, xG);                             // xZ <- [V==G]
+            xX = (__m128)__lsx_vfcmp_cune_s(xV, xR);                            // xX <- [V!=R]
+
+            xY = (__m128)__lsx_vand_v((__m128i)xZ, (__m128i)xX);                               // xY <- [V!=R && V==G]
+            xZ = (__m128)__lsx_vandn_v((__m128i)xZ, (__m128i)xX);                            // xZ <- [V!=R && V!=G]
+
+            xY = (__m128)__lsx_vxor_v((__m128i)xY, (__m128i)SIMD_GET_PS(full));                // xY <- [V==R || V!=G]
+            xZ = (__m128)__lsx_vxor_v((__m128i)xZ, (__m128i)SIMD_GET_PS(full));                // xZ <- [V==R || V==G]
+
+            xR = (__m128)__lsx_vand_v((__m128i)xR, (__m128i)xX);                               // xR <- [X!=0 ? R : 0]
+            xB = (__m128)__lsx_vand_v((__m128i)xB, (__m128i)xZ);                               // xB <- [Z!=0 ? B : 0]
+            xG = (__m128)__lsx_vand_v((__m128i)xG, (__m128i)xY);                               // xG <- [Y!=0 ? G : 0]
+
+            xZ = (__m128)__lsx_vandn_v((__m128i)xZ, (__m128i)SIMD_GET_PS(sn));               // xZ <- [sign(!Z)]
+            xY = (__m128)__lsx_vandn_v((__m128i)xY, (__m128i)SIMD_GET_PS(sn));               // xY <- [sign(!Y)]
+
+            xG = (__m128)__lsx_vxor_v((__m128i)xG, (__m128i)xZ);                               // xG <- [Y!=0 ? (Z==0 ? G : -G) : 0]
+            xR = (__m128)__lsx_vxor_v((__m128i)xR, (__m128i)xY);                               // xR <- [X!=0 ? (Y==0 ? R : -R) : 0]
+
+            // G is the accumulator
+            xG = __lsx_vfadd_s(xG, xR);                               // xG <- [Rx + Gx]
+            xB = (__m128)__lsx_vxor_v((__m128i)xB, (__m128i)xY);                               // xB <- [Z!=0 ? (Y==0 ? B : -B) : 0]
+
+            xC = __lsx_vfmul_s(xC, SIMD_GET_PS(m6_m6_m6_m6));         // xC <- [C*6     ]
+            xG = __lsx_vfsub_s(xG, xB);                               // xG <- [Rx+Gx+Bx]
+
+            xH = (__m128)__lsx_vand_v((__m128i)xX, (__m128i)SIMD_GET_PS(m4o6_m4o6_m4o6_m4o6)); // xH <- [V==R ?0 :-4/6]
+            xG = __lsx_vfdiv_s(xG, xC);                               // xG <- [(Rx+Gx+Bx)/6C]
+
+            // Correct achromatic cases (H/S may be infinite due to zero division)
+            xH = (__m128)__lsx_vxor_v((__m128i)xH, (__m128i)xZ);                               // xH <- [V==R ? 0 : V==G ? -4/6 : 4/6]
+            xC = (__m128)__lsx_vfcmp_cle_s(SIMD_GET_PS(eps), xC);
+            xH = __lsx_vfadd_s(xH, SIMD_GET_PS(p1));                  // xH <- [V==R ? 1 : V==G ?  2/6 :10/6]
+
+            xG = __lsx_vfadd_s(xG, xH);
+
+            // Normalize H to fraction
+            xH = (__m128)__lsx_vfcmp_cle_s(SIMD_GET_PS(p1), xG);
+
+            xH = (__m128)__lsx_vand_v((__m128i)xH, (__m128i)SIMD_GET_PS(p1));
+            xS = (__m128)__lsx_vand_v((__m128i)xS, (__m128i)xC);
+            xG = (__m128)__lsx_vand_v((__m128i)xG, (__m128i)xC);
+            xG = __lsx_vfsub_s(xG, xH);
+
+            // Modify Hue Values and re-normalize H to fraction
+            xG = __lsx_vfadd_s(xG, pHueShift);
+
+            xH = (__m128)__lsx_vfcmp_cle_s(SIMD_GET_PS(p1), xG);
+            xH = (__m128)__lsx_vand_v((__m128i)xH, (__m128i)SIMD_GET_PS(p1));
+
+            xG = __lsx_vfsub_s(xG, xH);
+
+            _MM_TRANSPOSE4_PS (h0, xG, xS, xV);
+
+            __m128 h1, h2, h3;
+
+            h1 = xG;
+            h2 = xS;
+            h3 = xV;
+
+            // Prepare HUE for RGB components (per pixel).
+            x0 = SIMD_SHUFFLE_PS(h0, _MM_SHUFFLE(1, 1, 1, 3));     // x0 <- [H           |H           |H           |V          ]
+            x1 = SIMD_SHUFFLE_PS(h1, _MM_SHUFFLE(1, 1, 1, 3));     // x1 <- [H           |H           |H           |V          ]
+            x2 = SIMD_SHUFFLE_PS(h2, _MM_SHUFFLE(1, 1, 1, 3));     // x2 <- [H           |H           |H           |V          ]
+            x3 = SIMD_SHUFFLE_PS(h3, _MM_SHUFFLE(1, 1, 1, 3));     // x3 <- [H           |H           |H           |V          ]
+
+            // Calculate intervals from HUE.
+            x0 = __lsx_vfsub_s(x0, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x0 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+            x1 = __lsx_vfsub_s(x1, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x1 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+            x2 = __lsx_vfsub_s(x2, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x2 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+            x3 = __lsx_vfsub_s(x3, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x3 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+
+            x0 = (__m128)__lsx_vand_v((__m128i)x0, (__m128i)SIMD_GET_PS(abs));                 // x0 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+            x1 = (__m128)__lsx_vand_v((__m128i)x1, (__m128i)SIMD_GET_PS(abs));                 // x1 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+            x2 = (__m128)__lsx_vand_v((__m128i)x2, (__m128i)SIMD_GET_PS(abs));                 // x2 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+            x3 = (__m128)__lsx_vand_v((__m128i)x3, (__m128i)SIMD_GET_PS(abs));                 // x3 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+
+            x0 = __lsx_vfmul_s(x0, SIMD_GET_PS(m6_m6_p6_p0));         // x0 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+            x1 = __lsx_vfmul_s(x1, SIMD_GET_PS(m6_m6_p6_p0));         // x1 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+            x2 = __lsx_vfmul_s(x2, SIMD_GET_PS(m6_m6_p6_p0));         // x2 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+            x3 = __lsx_vfmul_s(x3, SIMD_GET_PS(m6_m6_p6_p0));         // x3 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+
+            x0 = __lsx_vfadd_s(x0, SIMD_GET_PS(p1_p1_m2_p0));         // x0 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+            x1 = __lsx_vfadd_s(x1, SIMD_GET_PS(p1_p1_m2_p0));         // x1 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+            x2 = __lsx_vfadd_s(x2, SIMD_GET_PS(p1_p1_m2_p0));         // x2 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+            x3 = __lsx_vfadd_s(x3, SIMD_GET_PS(p1_p1_m2_p0));         // x3 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+
+            // Bound intervals.
+            x0 = __lsx_vfmax_s(x0, SIMD_GET_PS(m1_m1_m1_p1));
+            x1 = __lsx_vfmax_s(x1, SIMD_GET_PS(m1_m1_m1_p1));
+            x2 = __lsx_vfmax_s(x2, SIMD_GET_PS(m1_m1_m1_p1));
+            x3 = __lsx_vfmax_s(x3, SIMD_GET_PS(m1_m1_m1_p1));
+
+            x0 = __lsx_vfmin_s(x0, SIMD_GET_PS(p0));                  // x0 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+            x1 = __lsx_vfmin_s(x1, SIMD_GET_PS(p0));                  // x1 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+            x2 = __lsx_vfmin_s(x2, SIMD_GET_PS(p0));                  // x2 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+            x3 = __lsx_vfmin_s(x3, SIMD_GET_PS(p0));                  // x3 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+
+            // Prepare S/V vectors.
+            a0 = SIMD_SHUFFLE_PS(h0, _MM_SHUFFLE(2, 2, 2, 2));     // a0 <- [S           |S           |S           |S          ]
+            a1 = SIMD_SHUFFLE_PS(h1, _MM_SHUFFLE(2, 2, 2, 2));     // a1 <- [S           |S           |S           |S          ]
+            h0 = SIMD_SHUFFLE_PS(h0, _MM_SHUFFLE(3, 3, 3, 0));     // h0 <- [V           |V           |V           |A          ]
+            h1 = SIMD_SHUFFLE_PS(h1, _MM_SHUFFLE(3, 3, 3, 0));     // h1 <- [V           |V           |V           |A          ]
+
+            // Multiply with 'S*V' and add 'V'.
+            x0 = __lsx_vfmul_s(x0, a0);                               // x0 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            x1 = __lsx_vfmul_s(x1, a1);                               // x1 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            a0 = SIMD_SHUFFLE_PS(h2, _MM_SHUFFLE(2, 2, 2, 2));     // a0 <- [S           |S           |S           |S          ]
+            a1 = SIMD_SHUFFLE_PS(h3, _MM_SHUFFLE(2, 2, 2, 2));     // a1 <- [S           |S           |S           |S          ]
+
+            x0 = __lsx_vfmul_s(x0, h0);                               // x0 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            x1 = __lsx_vfmul_s(x1, h1);                               // x1 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            h2 = SIMD_SHUFFLE_PS(h2, _MM_SHUFFLE(3, 3, 3, 0));     // h2 <- [V           |V           |V           |A          ]
+            h3 = SIMD_SHUFFLE_PS(h3, _MM_SHUFFLE(3, 3, 3, 0));     // h3 <- [V           |V           |V           |A          ]
+
+            x2 = __lsx_vfmul_s(x2, a0);                               // x2 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            x3 = __lsx_vfmul_s(x3, a1);                               // x3 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            x0 = __lsx_vfadd_s(x0, h0);                               // x0 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |A          ]
+
+            x2 = __lsx_vfmul_s(x2, h2);                               // x2 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            x3 = __lsx_vfmul_s(x3, h3);                               // x3 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            x1 = __lsx_vfadd_s(x1, h1);                               // x1 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |A          ]
+
+            x2 = __lsx_vfadd_s(x2, h2);                               // x2 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |A          ]
+            x3 = __lsx_vfadd_s(x3, h3);                               // x3 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |A          ]
+
+            // Store
+            x0 = (__m128)__lsx_vpermi_w((__m128i)x0, (__m128i)x0, _MM_SHUFFLE(0,3,2,1));
+            x1 = (__m128)__lsx_vpermi_w((__m128i)x1, (__m128i)x1, _MM_SHUFFLE(0,3,2,1));
+            x2 = (__m128)__lsx_vpermi_w((__m128i)x2, (__m128i)x2, _MM_SHUFFLE(0,3,2,1));
+            x3 = (__m128)__lsx_vpermi_w((__m128i)x3, (__m128i)x3, _MM_SHUFFLE(0,3,2,1));
+
+            x0 = __lsx_vfmul_s(x0, pFactor);
+            x1 = __lsx_vfmul_s(x1, pFactor);
+            x2 = __lsx_vfmul_s(x2, pFactor);
+            x3 = __lsx_vfmul_s(x3, pFactor);
+
+            px0 = __lsx_vftint_w_s(x0);
+            px1 = __lsx_vftint_w_s(x1);
+            px2 = __lsx_vftint_w_s(x2);
+            px3 = __lsx_vftint_w_s(x3);
+
+            px0 = lsx_packs_i32(px0, px0);
+            px0 = lsx_packus_i16(px0, px0);
+            *((int*)arrayPx0) = __lsx_vpickve2gr_w(px0, 0);
+
+            px1 = lsx_packs_i32(px1, px1);
+            px1 = lsx_packus_i16(px1, px1);
+            *((int*)arrayPx1) = __lsx_vpickve2gr_w(px1, 0);
+
+            px2 = lsx_packs_i32(px2, px2);
+            px2 = lsx_packus_i16(px2, px2);
+            *((int*)arrayPx2) = __lsx_vpickve2gr_w(px2, 0);
+
+            px3 = lsx_packs_i32(px3, px3);
+            px3 = lsx_packus_i16(px3, px3);
+            *((int*)arrayPx3) = __lsx_vpickve2gr_w(px3, 0);
+
+            memcpy(dstPtrTempPx0, arrayPx0, 4 * sizeof(Rpp8u));
+            memcpy(dstPtrTempPx1, arrayPx1, 4 * sizeof(Rpp8u));
+            memcpy(dstPtrTempPx2, arrayPx2, 4 * sizeof(Rpp8u));
+            memcpy(dstPtrTempPx3, arrayPx3, 4 * sizeof(Rpp8u));
+
+            srcPtrTempPx0 += 12;
+            srcPtrTempPx1 += 12;
+            srcPtrTempPx2 += 12;
+            srcPtrTempPx3 += 12;
+            dstPtrTempPx0 += 12;
+            dstPtrTempPx1 += 12;
+            dstPtrTempPx2 += 12;
+            dstPtrTempPx3 += 12;
+        }
+        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+        {
+            T *srcPtrTempR, *srcPtrTempG, *srcPtrTempB;
+            T *dstPtrTempR, *dstPtrTempG, *dstPtrTempB;
+
+            srcPtrTempR = srcPtrTempPx0;
+            srcPtrTempG = srcPtrTempPx0 + 1;
+            srcPtrTempB = srcPtrTempPx0 + 2;
+            dstPtrTempR = dstPtrTempPx0;
+            dstPtrTempG = dstPtrTempPx0 + 1;
+            dstPtrTempB = dstPtrTempPx0 + 2;
+
+            // RGB to HSV
+
+            Rpp32f hue, sat, val;
+            Rpp32f rf, gf, bf, cmax, cmin, delta;
+            rf = ((Rpp32f) *srcPtrTempR) / 255;
+            gf = ((Rpp32f) *srcPtrTempG) / 255;
+            bf = ((Rpp32f) *srcPtrTempB) / 255;
+            cmax = RPPMAX3(rf, gf, bf);
+            cmin = RPPMIN3(rf, gf, bf);
+            delta = cmax - cmin;
+
+            if (delta == 0)
+            {
+                hue = 0;
+            }
+            else if (cmax == rf)
+            {
+                hue = round(60 * fmod(((gf - bf) / delta),6));
+            }
+            else if (cmax == gf)
+            {
+                hue = round(60 * (((bf - rf) / delta) + 2));
+            }
+            else if (cmax == bf)
+            {
+                hue = round(60 * (((rf - gf) / delta) + 4));
+            }
+
+            while (hue >= 360)
+            {
+                hue = hue - 360;
+            }
+            while (hue < 0)
+            {
+                hue = 360 + hue;
+            }
+
+            if (cmax == 0)
+            {
+                sat = 0;
+            }
+            else
+            {
+                sat = delta / cmax;
+            }
+
+            val = cmax;
+
+            // Modify Hue
+
+            hue = hue + hueShiftAngle;
+            while (hue >= 360)
+            {
+                hue = hue - 360;
+            }
+            while (hue < 0)
+            {
+                hue = 360 + hue;
+            }
+
+            // HSV to RGB
+
+            Rpp32f c, x, m;
+            c = val * sat;
+            x = c * (1 - abs(int(fmod((hue / 60), 2)) - 1));
+            m = val - c;
+
+            if ((0 <= hue) && (hue < 60))
+            {
+                rf = c;
+                gf = x;
+                bf = 0;
+            }
+            else if ((60 <= hue) && (hue < 120))
+            {
+                rf = x;
+                gf = c;
+                bf = 0;
+            }
+            else if ((120 <= hue) && (hue < 180))
+            {
+                rf = 0;
+                gf = c;
+                bf = x;
+            }
+            else if ((180 <= hue) && (hue < 240))
+            {
+                rf = 0;
+                gf = x;
+                bf = c;
+            }
+            else if ((240 <= hue) && (hue < 300))
+            {
+                rf = x;
+                gf = 0;
+                bf = c;
+            }
+            else if ((300 <= hue) && (hue < 360))
+            {
+                rf = c;
+                gf = 0;
+                bf = x;
+            }
+
+            *dstPtrTempR = (Rpp8u) round((rf + m) * 255);
+            *dstPtrTempG = (Rpp8u) round((gf + m) * 255);
+            *dstPtrTempB = (Rpp8u) round((bf + m) * 255);
+
+            srcPtrTempPx0 += 3;
+            dstPtrTempPx0 += 3;
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus hueRGB_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                         Rpp32f *batch_hueShift,
+                         RppiROI *roiPoints, Rpp32u nbatchSize,
+                         RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32u x1 = roiPoints[batchCount].x;
+            Rpp32u y1 = roiPoints[batchCount].y;
+            Rpp32u x2 = x1 + roiPoints[batchCount].roiWidth;
+            Rpp32u y2 = y1 + roiPoints[batchCount].roiHeight;
+            if (x2 == 0)
+            {
+                x2 = batch_srcSize[batchCount].width;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == 0)
+            {
+                y2 = batch_srcSize[batchCount].height;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp32u remainingElementsAfterROI = (batch_srcSize[batchCount].width - (roiPoints[batchCount].x + roiPoints[batchCount].roiWidth));
+
+            Rpp32f hueShift = batch_hueShift[batchCount];
+            Rpp32f hueShiftAngle = hueShift;
+
+            while (hueShiftAngle > 360)
+            {
+                hueShiftAngle = hueShiftAngle - 360;
+            }
+            while (hueShiftAngle < 0)
+            {
+                hueShiftAngle = 360 + hueShiftAngle;
+            }
+
+            hueShift = hueShiftAngle / 360;
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+            {
+                T *srcPtrTemp, *dstPtrTemp;
+                srcPtrTemp = srcPtrImage + (i * batch_srcSizeMax[batchCount].width);
+                dstPtrTemp = dstPtrImage + (i * batch_srcSizeMax[batchCount].width);
+
+                if (!((y1 <= i) && (i <= y2)))
+                {
+                    memcpy(dstPtrTemp, srcPtrTemp, batch_srcSize[batchCount].width * sizeof(T));
+
+                    dstPtrTemp += batch_srcSizeMax[batchCount].width;
+                    srcPtrTemp += batch_srcSizeMax[batchCount].width;
+                }
+                else
+                {
+                    memcpy(dstPtrTemp, srcPtrTemp, x1 * sizeof(T));
+                    srcPtrTemp += x1;
+                    dstPtrTemp += x1;
+
+                    Rpp32u bufferLength = roiPoints[batchCount].roiWidth;
+
+                    hueRGB_processBuffer_host(srcPtrTemp, batch_srcSizeMax[batchCount], dstPtrTemp, hueShift, hueShiftAngle, bufferLength, chnFormat, channel);
+
+                    srcPtrTemp += bufferLength;
+                    dstPtrTemp += bufferLength;
+
+                    memcpy(dstPtrTemp, srcPtrTemp, remainingElementsAfterROI * sizeof(T));
+                    srcPtrTemp += remainingElementsAfterROI;
+                    dstPtrTemp += remainingElementsAfterROI;
+                }
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32u x1 = roiPoints[batchCount].x;
+            Rpp32u y1 = roiPoints[batchCount].y;
+            Rpp32u x2 = x1 + roiPoints[batchCount].roiWidth;
+            Rpp32u y2 = y1 + roiPoints[batchCount].roiHeight;
+            if (x2 == 0)
+            {
+                x2 = batch_srcSize[batchCount].width;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == 0)
+            {
+                y2 = batch_srcSize[batchCount].height;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp32u elementsBeforeROI = channel * x1;
+            Rpp32u remainingElementsAfterROI = channel * (batch_srcSize[batchCount].width - (roiPoints[batchCount].x + roiPoints[batchCount].roiWidth));
+
+            Rpp32f hueShift = batch_hueShift[batchCount];
+            Rpp32f hueShiftAngle = hueShift;
+
+            while (hueShiftAngle > 360)
+            {
+                hueShiftAngle = hueShiftAngle - 360;
+            }
+            while (hueShiftAngle < 0)
+            {
+                hueShiftAngle = 360 + hueShiftAngle;
+            }
+
+            hueShift = hueShiftAngle / 360;
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            Rpp32u elementsInRow = channel * batch_srcSize[batchCount].width;
+            Rpp32u elementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+
+
+            for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+            {
+                Rpp32f pixel;
+
+                T *srcPtrTemp, *dstPtrTemp;
+                srcPtrTemp = srcPtrImage + (i * elementsInRowMax);
+                dstPtrTemp = dstPtrImage + (i * elementsInRowMax);
+
+                if (!((y1 <= i) && (i <= y2)))
+                {
+                    memcpy(dstPtrTemp, srcPtrTemp, elementsInRow * sizeof(T));
+
+                    dstPtrTemp += elementsInRowMax;
+                    srcPtrTemp += elementsInRowMax;
+                }
+                else
+                {
+                    memcpy(dstPtrTemp, srcPtrTemp, elementsBeforeROI * sizeof(T));
+                    srcPtrTemp += elementsBeforeROI;
+                    dstPtrTemp += elementsBeforeROI;
+
+                    Rpp32u bufferLength = channel * roiPoints[batchCount].roiWidth;
+
+                    hueRGB_processBuffer_host(srcPtrTemp, batch_srcSizeMax[batchCount], dstPtrTemp, hueShift, hueShiftAngle, bufferLength, chnFormat, channel);
+
+                    srcPtrTemp += bufferLength;
+                    dstPtrTemp += bufferLength;
+                    memcpy(dstPtrTemp, srcPtrTemp, remainingElementsAfterROI * sizeof(T));
+                    srcPtrTemp += remainingElementsAfterROI;
+                    dstPtrTemp += remainingElementsAfterROI;
+                }
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus hueRGB_host(T* srcPtr, RppiSize srcSize, T* dstPtr,
+                    Rpp32f hueShift,
+                    RppiChnFormat chnFormat, Rpp32u channel)
+{
+    Rpp32f hueShiftAngle = hueShift;
+    while (hueShiftAngle > 360)
+    {
+        hueShiftAngle = hueShiftAngle - 360;
+    }
+    while (hueShiftAngle < 0)
+    {
+        hueShiftAngle = 360 + hueShiftAngle;
+    }
+
+    hueShift = hueShiftAngle / 360;
+
+    Rpp64u bufferLength;
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        bufferLength = srcSize.height * srcSize.width;
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        bufferLength = channel * srcSize.height * srcSize.width;
+    }
+
+    hueRGB_processBuffer_host(srcPtr, srcSize, dstPtr, hueShift, hueShiftAngle, bufferLength, chnFormat, channel);
+
+    return RPP_SUCCESS;
+}
+
+/**************** saturation ***************/
+
+template <typename T>
+RppStatus saturationRGB_processBuffer_host(T* srcPtr, RppiSize srcSize, T* dstPtr,
+                                           Rpp32f saturationFactor, Rpp64u bufferLength,
+                                           RppiChnFormat chnFormat, Rpp32u channel)
+{
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        T *srcPtrTempR, *srcPtrTempG, *srcPtrTempB;
+        T *dstPtrTempR, *dstPtrTempG, *dstPtrTempB;
+
+        Rpp64u imageDim = srcSize.height * srcSize.width;
+        srcPtrTempR = srcPtr;
+        srcPtrTempG = srcPtr + (imageDim);
+        srcPtrTempB = srcPtr + (2 * imageDim);
+        dstPtrTempR = dstPtr;
+        dstPtrTempG = dstPtr + (imageDim);
+        dstPtrTempB = dstPtr + (2 * imageDim);
+
+        Rpp64u alignedLength = (bufferLength / 4) * 4;
+
+        __m128i const zero = __lsx_vldi(0);
+        __m128 pZeros = lsx_set1_f32(0.0);
+        __m128 pOnes = lsx_set1_f32(1.0);
+        __m128 pFactor = lsx_set1_f32(255.0);
+        __m128 pSaturationFactor = lsx_set1_f32(saturationFactor);
+        __m128i px0, px1, px2;
+        __m128 xR, xG, xB;
+        __m128 xH, xS, xV, xC;
+        __m128 xX, xY, xZ;
+        __m128 h0, h1, h2, h3;
+        __m128 x0, x1, x2, x3;
+        __m128 a0, a1;
+        h0 = lsx_set1_f32(1.0);
+
+        Rpp8u arrayR[4];
+        Rpp8u arrayG[4];
+        Rpp8u arrayB[4];
+
+        Rpp64u vectorLoopCount = 0;
+        for (; vectorLoopCount < alignedLength; vectorLoopCount+=4)
+        {
+            px0 =  __lsx_vld((__m128i *)srcPtrTempR, 0);
+            px1 =  __lsx_vld((__m128i *)srcPtrTempG, 0);
+            px2 =  __lsx_vld((__m128i *)srcPtrTempB, 0);
+
+            px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+            px1 = __lsx_vilvl_b(zero, px1);    // pixels 0-7
+            px2 = __lsx_vilvl_b(zero, px2);    // pixels 0-7
+
+            xR = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+            xG = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 0-3
+            xB = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px2));    // pixels 0-3
+
+            xR = __lsx_vfdiv_s(xR, pFactor);
+            xG = __lsx_vfdiv_s(xG, pFactor);
+            xB = __lsx_vfdiv_s(xB, pFactor);
+
+            // Calculate Saturation, Value, Chroma
+            xS = __lsx_vfmax_s(xG, xB);                               // xS <- [max(G, B)]
+            xC = __lsx_vfmin_s(xG, xB);                               // xC <- [min(G, B)]
+
+            xS = __lsx_vfmax_s(xS, xR);                               // xS <- [max(G, B, R)]
+            xC = __lsx_vfmin_s(xC, xR);                               // xC <- [min(G, B, R)]
+
+            xV = xS;                                               // xV <- [V    ]
+            xS = __lsx_vfsub_s(xS, xC);                               // xS <- [V - m]
+            xS = __lsx_vfdiv_s(xS, xV);                               // xS <- [S    ]
+
+            xC = __lsx_vfsub_s(xC, xV);                               // xC <- [V + m]
+
+            // Calculate Hue
+            xZ = (__m128)__lsx_vfcmp_ceq_s(xV, xG);                             // xZ <- [V==G]
+            xX = (__m128)__lsx_vfcmp_cune_s(xV, xR);                            // xX <- [V!=R]
+
+            xY = (__m128)__lsx_vand_v((__m128i)xZ, (__m128i)xX);                               // xY <- [V!=R && V==G]
+            xZ = (__m128)__lsx_vandn_v((__m128i)xZ, (__m128i)xX);                            // xZ <- [V!=R && V!=G]
+
+            xY = (__m128)__lsx_vxor_v((__m128i)xY, (__m128i)SIMD_GET_PS(full));                // xY <- [V==R || V!=G]
+            xZ = (__m128)__lsx_vxor_v((__m128i)xZ, (__m128i)SIMD_GET_PS(full));                // xZ <- [V==R || V==G]
+
+            xR = (__m128)__lsx_vand_v((__m128i)xR, (__m128i)xX);                               // xR <- [X!=0 ? R : 0]
+            xB = (__m128)__lsx_vand_v((__m128i)xB, (__m128i)xZ);                               // xB <- [Z!=0 ? B : 0]
+            xG = (__m128)__lsx_vand_v((__m128i)xG, (__m128i)xY);                               // xG <- [Y!=0 ? G : 0]
+
+            xZ = (__m128)__lsx_vandn_v((__m128i)xZ, (__m128i)SIMD_GET_PS(sn));               // xZ <- [sign(!Z)]
+            xY = (__m128)__lsx_vandn_v((__m128i)xY, (__m128i)SIMD_GET_PS(sn));               // xY <- [sign(!Y)]
+
+            xG = (__m128)__lsx_vxor_v((__m128i)xG, (__m128i)xZ);                               // xG <- [Y!=0 ? (Z==0 ? G : -G) : 0]
+            xR = (__m128)__lsx_vxor_v((__m128i)xR, (__m128i)xY);                               // xR <- [X!=0 ? (Y==0 ? R : -R) : 0]
+
+            // G is the accumulator
+            xG = __lsx_vfadd_s(xG, xR);                               // xG <- [Rx + Gx]
+            xB = (__m128)__lsx_vxor_v((__m128i)xB, (__m128i)xY);                               // xB <- [Z!=0 ? (Y==0 ? B : -B) : 0]
+
+            xC = __lsx_vfmul_s(xC, SIMD_GET_PS(m6_m6_m6_m6));         // xC <- [C*6     ]
+            xG = __lsx_vfsub_s(xG, xB);                               // xG <- [Rx+Gx+Bx]
+
+            xH = (__m128)__lsx_vand_v((__m128i)xX, (__m128i)SIMD_GET_PS(m4o6_m4o6_m4o6_m4o6)); // xH <- [V==R ?0 :-4/6]
+            xG = __lsx_vfdiv_s(xG, xC);                               // xG <- [(Rx+Gx+Bx)/6C]
+
+            // Correct achromatic cases (H/S may be infinite due to zero division)
+            xH = (__m128)__lsx_vxor_v((__m128i)xH, (__m128i)xZ);                               // xH <- [V==R ? 0 : V==G ? -4/6 : 4/6]
+            xC = (__m128)__lsx_vfcmp_cle_s(SIMD_GET_PS(eps), xC);
+            xH = __lsx_vfadd_s(xH, SIMD_GET_PS(p1));                  // xH <- [V==R ? 1 : V==G ?  2/6 :10/6]
+
+            xG = __lsx_vfadd_s(xG, xH);
+
+            // Normalize H to fraction
+            xH = (__m128)__lsx_vfcmp_cle_s(SIMD_GET_PS(p1), xG);
+
+            xH = (__m128)__lsx_vand_v((__m128i)xH, (__m128i)SIMD_GET_PS(p1));
+            xS = (__m128)__lsx_vand_v((__m128i)xS, (__m128i)xC);
+            xG = (__m128)__lsx_vand_v((__m128i)xG, (__m128i)xC);
+            xG = __lsx_vfsub_s(xG, xH);
+
+            // Modify Saturation Values
+            xS = __lsx_vfmul_s(xS, pSaturationFactor);
+            xS = __lsx_vfmin_s(xS, pOnes);
+            xS = __lsx_vfmax_s(xS, pZeros);
+
+            _MM_TRANSPOSE4_PS (h0, xG, xS, xV);
+
+            __m128 h1, h2, h3;
+
+            h1 = xG;
+            h2 = xS;
+            h3 = xV;
+
+            // Prepare HUE for RGB components (per pixel).
+            x0 = SIMD_SHUFFLE_PS(h0, _MM_SHUFFLE(1, 1, 1, 3));     // x0 <- [H           |H           |H           |V          ]
+            x1 = SIMD_SHUFFLE_PS(h1, _MM_SHUFFLE(1, 1, 1, 3));     // x1 <- [H           |H           |H           |V          ]
+            x2 = SIMD_SHUFFLE_PS(h2, _MM_SHUFFLE(1, 1, 1, 3));     // x2 <- [H           |H           |H           |V          ]
+            x3 = SIMD_SHUFFLE_PS(h3, _MM_SHUFFLE(1, 1, 1, 3));     // x3 <- [H           |H           |H           |V          ]
+
+            // Calculate intervals from HUE.
+            x0 = __lsx_vfsub_s(x0, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x0 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+            x1 = __lsx_vfsub_s(x1, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x1 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+            x2 = __lsx_vfsub_s(x2, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x2 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+            x3 = __lsx_vfsub_s(x3, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x3 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+
+            x0 = (__m128)__lsx_vand_v((__m128i)x0, (__m128i)SIMD_GET_PS(abs));                 // x0 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+            x1 = (__m128)__lsx_vand_v((__m128i)x1, (__m128i)SIMD_GET_PS(abs));                 // x1 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+            x2 = (__m128)__lsx_vand_v((__m128i)x2, (__m128i)SIMD_GET_PS(abs));                 // x2 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+            x3 = (__m128)__lsx_vand_v((__m128i)x3, (__m128i)SIMD_GET_PS(abs));                 // x3 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+
+            x0 = __lsx_vfmul_s(x0, SIMD_GET_PS(m6_m6_p6_p0));         // x0 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+            x1 = __lsx_vfmul_s(x1, SIMD_GET_PS(m6_m6_p6_p0));         // x1 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+            x2 = __lsx_vfmul_s(x2, SIMD_GET_PS(m6_m6_p6_p0));         // x2 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+            x3 = __lsx_vfmul_s(x3, SIMD_GET_PS(m6_m6_p6_p0));         // x3 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+
+            x0 = __lsx_vfadd_s(x0, SIMD_GET_PS(p1_p1_m2_p0));         // x0 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+            x1 = __lsx_vfadd_s(x1, SIMD_GET_PS(p1_p1_m2_p0));         // x1 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+            x2 = __lsx_vfadd_s(x2, SIMD_GET_PS(p1_p1_m2_p0));         // x2 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+            x3 = __lsx_vfadd_s(x3, SIMD_GET_PS(p1_p1_m2_p0));         // x3 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+
+            // Bound intervals.
+            x0 = __lsx_vfmax_s(x0, SIMD_GET_PS(m1_m1_m1_p1));
+            x1 = __lsx_vfmax_s(x1, SIMD_GET_PS(m1_m1_m1_p1));
+            x2 = __lsx_vfmax_s(x2, SIMD_GET_PS(m1_m1_m1_p1));
+            x3 = __lsx_vfmax_s(x3, SIMD_GET_PS(m1_m1_m1_p1));
+
+            x0 = __lsx_vfmin_s(x0, SIMD_GET_PS(p0));                  // x0 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+            x1 = __lsx_vfmin_s(x1, SIMD_GET_PS(p0));                  // x1 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+            x2 = __lsx_vfmin_s(x2, SIMD_GET_PS(p0));                  // x2 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+            x3 = __lsx_vfmin_s(x3, SIMD_GET_PS(p0));                  // x3 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+
+            // Prepare S/V vectors.
+            a0 = SIMD_SHUFFLE_PS(h0, _MM_SHUFFLE(2, 2, 2, 2));     // a0 <- [S           |S           |S           |S          ]
+            a1 = SIMD_SHUFFLE_PS(h1, _MM_SHUFFLE(2, 2, 2, 2));     // a1 <- [S           |S           |S           |S          ]
+            h0 = SIMD_SHUFFLE_PS(h0, _MM_SHUFFLE(3, 3, 3, 0));     // h0 <- [V           |V           |V           |A          ]
+            h1 = SIMD_SHUFFLE_PS(h1, _MM_SHUFFLE(3, 3, 3, 0));     // h1 <- [V           |V           |V           |A          ]
+
+            // Multiply with 'S*V' and add 'V'.
+            x0 = __lsx_vfmul_s(x0, a0);                               // x0 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            x1 = __lsx_vfmul_s(x1, a1);                               // x1 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            a0 = SIMD_SHUFFLE_PS(h2, _MM_SHUFFLE(2, 2, 2, 2));     // a0 <- [S           |S           |S           |S          ]
+            a1 = SIMD_SHUFFLE_PS(h3, _MM_SHUFFLE(2, 2, 2, 2));     // a1 <- [S           |S           |S           |S          ]
+
+            x0 = __lsx_vfmul_s(x0, h0);                               // x0 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            x1 = __lsx_vfmul_s(x1, h1);                               // x1 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            h2 = SIMD_SHUFFLE_PS(h2, _MM_SHUFFLE(3, 3, 3, 0));     // h2 <- [V           |V           |V           |A          ]
+            h3 = SIMD_SHUFFLE_PS(h3, _MM_SHUFFLE(3, 3, 3, 0));     // h3 <- [V           |V           |V           |A          ]
+
+            x2 = __lsx_vfmul_s(x2, a0);                               // x2 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            x3 = __lsx_vfmul_s(x3, a1);                               // x3 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            x0 = __lsx_vfadd_s(x0, h0);                               // x0 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |A          ]
+
+            x2 = __lsx_vfmul_s(x2, h2);                               // x2 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            x3 = __lsx_vfmul_s(x3, h3);                               // x3 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            x1 = __lsx_vfadd_s(x1, h1);                               // x1 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |A          ]
+
+            x2 = __lsx_vfadd_s(x2, h2);                               // x2 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |A          ]
+            x3 = __lsx_vfadd_s(x3, h3);                               // x3 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |A          ]
+
+            // Store
+            _MM_TRANSPOSE4_PS (x0, x1, x2, x3);
+
+            x1 = __lsx_vfmul_s(x1, pFactor);
+            x2 = __lsx_vfmul_s(x2, pFactor);
+            x3 = __lsx_vfmul_s(x3, pFactor);
+
+            px0 = __lsx_vftint_w_s(x1);
+            px1 = __lsx_vftint_w_s(x2);
+            px2 = __lsx_vftint_w_s(x3);
+
+            px0 = lsx_packs_i32(px0, px0);
+            px0 = lsx_packus_i16(px0, px0);
+            *((int*)arrayR) = __lsx_vpickve2gr_w(px0, 0);
+
+            px1 = lsx_packs_i32(px1, px1);
+            px1 = lsx_packus_i16(px1, px1);
+            *((int*)arrayG) = __lsx_vpickve2gr_w(px1, 0);
+
+            px2 = lsx_packs_i32(px2, px2);
+            px2 = lsx_packus_i16(px2, px2);
+            *((int*)arrayB) = __lsx_vpickve2gr_w(px2, 0);
+
+            memcpy(dstPtrTempR, arrayR, 4 * sizeof(Rpp8u));
+            memcpy(dstPtrTempG, arrayG, 4 * sizeof(Rpp8u));
+            memcpy(dstPtrTempB, arrayB, 4 * sizeof(Rpp8u));
+
+            srcPtrTempR += 4;
+            srcPtrTempG += 4;
+            srcPtrTempB += 4;
+            dstPtrTempR += 4;
+            dstPtrTempG += 4;
+            dstPtrTempB += 4;
+        }
+        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+        {
+            // RGB to HSV
+
+            Rpp32f hue, sat, val;
+            Rpp32f rf, gf, bf, cmax, cmin, delta;
+            rf = ((Rpp32f) *srcPtrTempR) / 255;
+            gf = ((Rpp32f) *srcPtrTempG) / 255;
+            bf = ((Rpp32f) *srcPtrTempB) / 255;
+            cmax = RPPMAX3(rf, gf, bf);
+            cmin = RPPMIN3(rf, gf, bf);
+            delta = cmax - cmin;
+
+            if (delta == 0)
+            {
+                hue = 0;
+            }
+            else if (cmax == rf)
+            {
+                hue = round(60 * fmod(((gf - bf) / delta),6));
+            }
+            else if (cmax == gf)
+            {
+                hue = round(60 * (((bf - rf) / delta) + 2));
+            }
+            else if (cmax == bf)
+            {
+                hue = round(60 * (((rf - gf) / delta) + 4));
+            }
+
+            while (hue >= 360)
+            {
+                hue = hue - 360;
+            }
+            while (hue < 0)
+            {
+                hue = 360 + hue;
+            }
+
+            if (cmax == 0)
+            {
+                sat = 0;
+            }
+            else
+            {
+                sat = delta / cmax;
+            }
+
+            val = cmax;
+
+            // Modify Saturation
+
+            sat *= saturationFactor;
+            sat = (sat < (Rpp32f) 1) ? sat : ((Rpp32f) 1);
+            sat = (sat > (Rpp32f) 0) ? sat : ((Rpp32f) 0);
+
+            // HSV to RGB
+
+            Rpp32f c, x, m;
+            c = val * sat;
+            x = c * (1 - abs(int(fmod((hue / 60), 2)) - 1));
+            m = val - c;
+
+            if ((0 <= hue) && (hue < 60))
+            {
+                rf = c;
+                gf = x;
+                bf = 0;
+            }
+            else if ((60 <= hue) && (hue < 120))
+            {
+                rf = x;
+                gf = c;
+                bf = 0;
+            }
+            else if ((120 <= hue) && (hue < 180))
+            {
+                rf = 0;
+                gf = c;
+                bf = x;
+            }
+            else if ((180 <= hue) && (hue < 240))
+            {
+                rf = 0;
+                gf = x;
+                bf = c;
+            }
+            else if ((240 <= hue) && (hue < 300))
+            {
+                rf = x;
+                gf = 0;
+                bf = c;
+            }
+            else if ((300 <= hue) && (hue < 360))
+            {
+                rf = c;
+                gf = 0;
+                bf = x;
+            }
+
+            *dstPtrTempR = (Rpp8u) round((rf + m) * 255);
+            *dstPtrTempG = (Rpp8u) round((gf + m) * 255);
+            *dstPtrTempB = (Rpp8u) round((bf + m) * 255);
+
+            srcPtrTempR++;
+            srcPtrTempG++;
+            srcPtrTempB++;
+            dstPtrTempR++;
+            dstPtrTempG++;
+            dstPtrTempB++;
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        T *srcPtrTempPx0, *srcPtrTempPx1, *srcPtrTempPx2, *srcPtrTempPx3;
+        T *dstPtrTempPx0, *dstPtrTempPx1, *dstPtrTempPx2, *dstPtrTempPx3;
+
+        srcPtrTempPx0 = srcPtr;
+        srcPtrTempPx1 = srcPtr + 3;
+        srcPtrTempPx2 = srcPtr + 6;
+        srcPtrTempPx3 = srcPtr + 9;
+        dstPtrTempPx0 = dstPtr;
+        dstPtrTempPx1 = dstPtr + 3;
+        dstPtrTempPx2 = dstPtr + 6;
+        dstPtrTempPx3 = dstPtr + 9;
+
+        Rpp64u alignedLength = (bufferLength / 12) * 12;
+
+        __m128i const zero = __lsx_vldi(0);
+        __m128 pZeros = lsx_set1_f32(0.0);
+        __m128 pOnes = lsx_set1_f32(1.0);
+        __m128 pFactor = lsx_set1_f32(255.0);
+        __m128 pSaturationFactor = lsx_set1_f32(saturationFactor);
+        __m128i px0, px1, px2, px3;
+        __m128 xR, xG, xB, xA;
+        __m128 xH, xS, xV, xC;
+        __m128 xX, xY, xZ;
+        __m128 h0, h1, h2, h3;
+        __m128 x0, x1, x2, x3;
+        __m128 a0, a1;
+        h0 = lsx_set1_f32(1.0);
+
+        Rpp8u arrayPx0[4];
+        Rpp8u arrayPx1[4];
+        Rpp8u arrayPx2[4];
+        Rpp8u arrayPx3[4];
+
+        Rpp64u vectorLoopCount = 0;
+        for (; vectorLoopCount < alignedLength; vectorLoopCount+=12)
+        {
+            px0 = __lsx_vld((__m128i *)srcPtrTempPx0, 0);
+            px1 = __lsx_vld((__m128i *)srcPtrTempPx1, 0);
+            px2 = __lsx_vld((__m128i *)srcPtrTempPx2, 0);
+            px3 = __lsx_vld((__m128i *)srcPtrTempPx3, 0);
+
+            px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+            px1 = __lsx_vilvl_b(zero, px1);    // pixels 0-7
+            px2 = __lsx_vilvl_b(zero, px2);    // pixels 0-7
+            px3 = __lsx_vilvl_b(zero, px3);    // pixels 0-7
+
+            xR = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+            xG = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 0-3
+            xB = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px2));    // pixels 0-3
+            xA = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px3));    // pixels 0-3
+
+            _MM_TRANSPOSE4_PS (xR, xG, xB, xA);
+
+            xR = __lsx_vfdiv_s(xR, pFactor);
+            xG = __lsx_vfdiv_s(xG, pFactor);
+            xB = __lsx_vfdiv_s(xB, pFactor);
+
+            // Calculate Saturation, Value, Chroma
+            xS = __lsx_vfmax_s(xG, xB);                               // xS <- [max(G, B)]
+            xC = __lsx_vfmin_s(xG, xB);                               // xC <- [min(G, B)]
+
+            xS = __lsx_vfmax_s(xS, xR);                               // xS <- [max(G, B, R)]
+            xC = __lsx_vfmin_s(xC, xR);                               // xC <- [min(G, B, R)]
+
+            xV = xS;                                               // xV <- [V    ]
+            xS = __lsx_vfsub_s(xS, xC);                               // xS <- [V - m]
+            xS = __lsx_vfdiv_s(xS, xV);                               // xS <- [S    ]
+
+            xC = __lsx_vfsub_s(xC, xV);                               // xC <- [V + m]
+
+            // Calculate Hue
+            xZ = (__m128)__lsx_vfcmp_ceq_s(xV, xG);                             // xZ <- [V==G]
+            xX = (__m128)__lsx_vfcmp_cune_s(xV, xR);                            // xX <- [V!=R]
+
+            xY = (__m128)__lsx_vand_v((__m128i)xZ, (__m128i)xX);                               // xY <- [V!=R && V==G]
+            xZ = (__m128)__lsx_vandn_v((__m128i)xZ, (__m128i)xX);                            // xZ <- [V!=R && V!=G]
+
+            xY = (__m128)__lsx_vxor_v((__m128i)xY, (__m128i)SIMD_GET_PS(full));                // xY <- [V==R || V!=G]
+            xZ = (__m128)__lsx_vxor_v((__m128i)xZ, (__m128i)SIMD_GET_PS(full));                // xZ <- [V==R || V==G]
+
+            xR = (__m128)__lsx_vand_v((__m128i)xR, (__m128i)xX);                               // xR <- [X!=0 ? R : 0]
+            xB = (__m128)__lsx_vand_v((__m128i)xB, (__m128i)xZ);                               // xB <- [Z!=0 ? B : 0]
+            xG = (__m128)__lsx_vand_v((__m128i)xG, (__m128i)xY);                               // xG <- [Y!=0 ? G : 0]
+
+            xZ = (__m128)__lsx_vandn_v((__m128i)xZ, (__m128i)SIMD_GET_PS(sn));               // xZ <- [sign(!Z)]
+            xY = (__m128)__lsx_vandn_v((__m128i)xY, (__m128i)SIMD_GET_PS(sn));               // xY <- [sign(!Y)]
+
+            xG = (__m128)__lsx_vxor_v((__m128i)xG, (__m128i)xZ);                               // xG <- [Y!=0 ? (Z==0 ? G : -G) : 0]
+            xR = (__m128)__lsx_vxor_v((__m128i)xR, (__m128i)xY);                               // xR <- [X!=0 ? (Y==0 ? R : -R) : 0]
+
+            // G is the accumulator
+            xG = __lsx_vfadd_s(xG, xR);                               // xG <- [Rx + Gx]
+            xB = (__m128)__lsx_vxor_v((__m128i)xB, (__m128i)xY);                               // xB <- [Z!=0 ? (Y==0 ? B : -B) : 0]
+
+            xC = __lsx_vfmul_s(xC, SIMD_GET_PS(m6_m6_m6_m6));         // xC <- [C*6     ]
+            xG = __lsx_vfsub_s(xG, xB);                               // xG <- [Rx+Gx+Bx]
+
+            xH = (__m128)__lsx_vand_v((__m128i)xX, (__m128i)SIMD_GET_PS(m4o6_m4o6_m4o6_m4o6)); // xH <- [V==R ?0 :-4/6]
+            xG = __lsx_vfdiv_s(xG, xC);                               // xG <- [(Rx+Gx+Bx)/6C]
+
+            // Correct achromatic cases (H/S may be infinite due to zero division)
+            xH = (__m128)__lsx_vxor_v((__m128i)xH, (__m128i)xZ);                               // xH <- [V==R ? 0 : V==G ? -4/6 : 4/6]
+            xC = (__m128)__lsx_vfcmp_cle_s(SIMD_GET_PS(eps), xC);
+            xH = __lsx_vfadd_s(xH, SIMD_GET_PS(p1));                  // xH <- [V==R ? 1 : V==G ?  2/6 :10/6]
+
+            xG = __lsx_vfadd_s(xG, xH);
+
+            // Normalize H to fraction
+            xH = (__m128)__lsx_vfcmp_cle_s(SIMD_GET_PS(p1), xG);
+
+            xH = (__m128)__lsx_vand_v((__m128i)xH, (__m128i)SIMD_GET_PS(p1));
+            xS = (__m128)__lsx_vand_v((__m128i)xS, (__m128i)xC);
+            xG = (__m128)__lsx_vand_v((__m128i)xG, (__m128i)xC);
+            xG = __lsx_vfsub_s(xG, xH);
+
+            // Modify Saturation Values
+            xS = __lsx_vfmul_s(xS, pSaturationFactor);
+            xS = __lsx_vfmin_s(xS, pOnes);
+            xS = __lsx_vfmax_s(xS, pZeros);
+
+            _MM_TRANSPOSE4_PS (h0, xG, xS, xV);
+
+            __m128 h1, h2, h3;
+
+            h1 = xG;
+            h2 = xS;
+            h3 = xV;
+
+            // Prepare HUE for RGB components (per pixel).
+            x0 = SIMD_SHUFFLE_PS(h0, _MM_SHUFFLE(1, 1, 1, 3));     // x0 <- [H           |H           |H           |V          ]
+            x1 = SIMD_SHUFFLE_PS(h1, _MM_SHUFFLE(1, 1, 1, 3));     // x1 <- [H           |H           |H           |V          ]
+            x2 = SIMD_SHUFFLE_PS(h2, _MM_SHUFFLE(1, 1, 1, 3));     // x2 <- [H           |H           |H           |V          ]
+            x3 = SIMD_SHUFFLE_PS(h3, _MM_SHUFFLE(1, 1, 1, 3));     // x3 <- [H           |H           |H           |V          ]
+
+            // Calculate intervals from HUE.
+            x0 = __lsx_vfsub_s(x0, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x0 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+            x1 = __lsx_vfsub_s(x1, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x1 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+            x2 = __lsx_vfsub_s(x2, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x2 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+            x3 = __lsx_vfsub_s(x3, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x3 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+
+            x0 = (__m128)__lsx_vand_v((__m128i)x0, (__m128i)SIMD_GET_PS(abs));                 // x0 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+            x1 = (__m128)__lsx_vand_v((__m128i)x1, (__m128i)SIMD_GET_PS(abs));                 // x1 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+            x2 = (__m128)__lsx_vand_v((__m128i)x2, (__m128i)SIMD_GET_PS(abs));                 // x2 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+            x3 = (__m128)__lsx_vand_v((__m128i)x3, (__m128i)SIMD_GET_PS(abs));                 // x3 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+
+            x0 = __lsx_vfmul_s(x0, SIMD_GET_PS(m6_m6_p6_p0));         // x0 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+            x1 = __lsx_vfmul_s(x1, SIMD_GET_PS(m6_m6_p6_p0));         // x1 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+            x2 = __lsx_vfmul_s(x2, SIMD_GET_PS(m6_m6_p6_p0));         // x2 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+            x3 = __lsx_vfmul_s(x3, SIMD_GET_PS(m6_m6_p6_p0));         // x3 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+
+            x0 = __lsx_vfadd_s(x0, SIMD_GET_PS(p1_p1_m2_p0));         // x0 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+            x1 = __lsx_vfadd_s(x1, SIMD_GET_PS(p1_p1_m2_p0));         // x1 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+            x2 = __lsx_vfadd_s(x2, SIMD_GET_PS(p1_p1_m2_p0));         // x2 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+            x3 = __lsx_vfadd_s(x3, SIMD_GET_PS(p1_p1_m2_p0));         // x3 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+
+            // Bound intervals.
+            x0 = __lsx_vfmax_s(x0, SIMD_GET_PS(m1_m1_m1_p1));
+            x1 = __lsx_vfmax_s(x1, SIMD_GET_PS(m1_m1_m1_p1));
+            x2 = __lsx_vfmax_s(x2, SIMD_GET_PS(m1_m1_m1_p1));
+            x3 = __lsx_vfmax_s(x3, SIMD_GET_PS(m1_m1_m1_p1));
+
+            x0 = __lsx_vfmin_s(x0, SIMD_GET_PS(p0));                  // x0 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+            x1 = __lsx_vfmin_s(x1, SIMD_GET_PS(p0));                  // x1 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+            x2 = __lsx_vfmin_s(x2, SIMD_GET_PS(p0));                  // x2 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+            x3 = __lsx_vfmin_s(x3, SIMD_GET_PS(p0));                  // x3 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+
+            // Prepare S/V vectors.
+            a0 = SIMD_SHUFFLE_PS(h0, _MM_SHUFFLE(2, 2, 2, 2));     // a0 <- [S           |S           |S           |S          ]
+            a1 = SIMD_SHUFFLE_PS(h1, _MM_SHUFFLE(2, 2, 2, 2));     // a1 <- [S           |S           |S           |S          ]
+            h0 = SIMD_SHUFFLE_PS(h0, _MM_SHUFFLE(3, 3, 3, 0));     // h0 <- [V           |V           |V           |A          ]
+            h1 = SIMD_SHUFFLE_PS(h1, _MM_SHUFFLE(3, 3, 3, 0));     // h1 <- [V           |V           |V           |A          ]
+
+            // Multiply with 'S*V' and add 'V'.
+            x0 = __lsx_vfmul_s(x0, a0);                               // x0 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            x1 = __lsx_vfmul_s(x1, a1);                               // x1 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            a0 = SIMD_SHUFFLE_PS(h2, _MM_SHUFFLE(2, 2, 2, 2));     // a0 <- [S           |S           |S           |S          ]
+            a1 = SIMD_SHUFFLE_PS(h3, _MM_SHUFFLE(2, 2, 2, 2));     // a1 <- [S           |S           |S           |S          ]
+
+            x0 = __lsx_vfmul_s(x0, h0);                               // x0 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            x1 = __lsx_vfmul_s(x1, h1);                               // x1 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            h2 = SIMD_SHUFFLE_PS(h2, _MM_SHUFFLE(3, 3, 3, 0));     // h2 <- [V           |V           |V           |A          ]
+            h3 = SIMD_SHUFFLE_PS(h3, _MM_SHUFFLE(3, 3, 3, 0));     // h3 <- [V           |V           |V           |A          ]
+
+            x2 = __lsx_vfmul_s(x2, a0);                               // x2 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            x3 = __lsx_vfmul_s(x3, a1);                               // x3 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            x0 = __lsx_vfadd_s(x0, h0);                               // x0 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |A          ]
+
+            x2 = __lsx_vfmul_s(x2, h2);                               // x2 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            x3 = __lsx_vfmul_s(x3, h3);                               // x3 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            x1 = __lsx_vfadd_s(x1, h1);                               // x1 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |A          ]
+
+            x2 = __lsx_vfadd_s(x2, h2);                               // x2 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |A          ]
+            x3 = __lsx_vfadd_s(x3, h3);                               // x3 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |A          ]
+
+            // Store
+            x0 = (__m128)__lsx_vpermi_w((__m128i)x0, (__m128i)x0, _MM_SHUFFLE(0,3,2,1));
+            x1 = (__m128)__lsx_vpermi_w((__m128i)x1, (__m128i)x1, _MM_SHUFFLE(0,3,2,1));
+            x2 = (__m128)__lsx_vpermi_w((__m128i)x2, (__m128i)x2, _MM_SHUFFLE(0,3,2,1));
+            x3 = (__m128)__lsx_vpermi_w((__m128i)x3, (__m128i)x3, _MM_SHUFFLE(0,3,2,1));
+
+            // _MM_TRANSPOSE4_PS (x0, x1, x2, x3);
+
+            x0 = __lsx_vfmul_s(x0, pFactor);
+            x1 = __lsx_vfmul_s(x1, pFactor);
+            x2 = __lsx_vfmul_s(x2, pFactor);
+            x3 = __lsx_vfmul_s(x3, pFactor);
+
+            px0 = __lsx_vftint_w_s(x0);
+            px1 = __lsx_vftint_w_s(x1);
+            px2 = __lsx_vftint_w_s(x2);
+            px3 = __lsx_vftint_w_s(x3);
+
+            px0 = lsx_packs_i32(px0, px0);
+            px0 = lsx_packus_i16(px0, px0);
+            *((int*)arrayPx0) = __lsx_vpickve2gr_w(px0, 0);
+
+            px1 = lsx_packs_i32(px1, px1);
+            px1 = lsx_packus_i16(px1, px1);
+            *((int*)arrayPx1) = __lsx_vpickve2gr_w(px1, 0);
+
+            px2 = lsx_packs_i32(px2, px2);
+            px2 = lsx_packus_i16(px2, px2);
+            *((int*)arrayPx2) = __lsx_vpickve2gr_w(px2, 0);
+
+            px3 = lsx_packs_i32(px3, px3);
+            px3 = lsx_packus_i16(px3, px3);
+            *((int*)arrayPx3) = __lsx_vpickve2gr_w(px3, 0);
+
+            memcpy(dstPtrTempPx0, arrayPx0, 4 * sizeof(Rpp8u));
+            memcpy(dstPtrTempPx1, arrayPx1, 4 * sizeof(Rpp8u));
+            memcpy(dstPtrTempPx2, arrayPx2, 4 * sizeof(Rpp8u));
+            memcpy(dstPtrTempPx3, arrayPx3, 4 * sizeof(Rpp8u));
+
+            srcPtrTempPx0 += 12;
+            srcPtrTempPx1 += 12;
+            srcPtrTempPx2 += 12;
+            srcPtrTempPx3 += 12;
+            dstPtrTempPx0 += 12;
+            dstPtrTempPx1 += 12;
+            dstPtrTempPx2 += 12;
+            dstPtrTempPx3 += 12;
+        }
+        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+        {
+            T *srcPtrTempR, *srcPtrTempG, *srcPtrTempB;
+            T *dstPtrTempR, *dstPtrTempG, *dstPtrTempB;
+
+            srcPtrTempR = srcPtrTempPx0;
+            srcPtrTempG = srcPtrTempPx0 + 1;
+            srcPtrTempB = srcPtrTempPx0 + 2;
+            dstPtrTempR = dstPtrTempPx0;
+            dstPtrTempG = dstPtrTempPx0 + 1;
+            dstPtrTempB = dstPtrTempPx0 + 2;
+
+            // RGB to HSV
+
+            Rpp32f hue, sat, val;
+            Rpp32f rf, gf, bf, cmax, cmin, delta;
+            rf = ((Rpp32f) *srcPtrTempR) / 255;
+            gf = ((Rpp32f) *srcPtrTempG) / 255;
+            bf = ((Rpp32f) *srcPtrTempB) / 255;
+            cmax = RPPMAX3(rf, gf, bf);
+            cmin = RPPMIN3(rf, gf, bf);
+            delta = cmax - cmin;
+
+            if (delta == 0)
+            {
+                hue = 0;
+            }
+            else if (cmax == rf)
+            {
+                hue = round(60 * fmod(((gf - bf) / delta),6));
+            }
+            else if (cmax == gf)
+            {
+                hue = round(60 * (((bf - rf) / delta) + 2));
+            }
+            else if (cmax == bf)
+            {
+                hue = round(60 * (((rf - gf) / delta) + 4));
+            }
+
+            while (hue >= 360)
+            {
+                hue = hue - 360;
+            }
+            while (hue < 0)
+            {
+                hue = 360 + hue;
+            }
+
+            if (cmax == 0)
+            {
+                sat = 0;
+            }
+            else
+            {
+                sat = delta / cmax;
+            }
+
+            val = cmax;
+
+            // Modify Saturation
+
+            sat *= saturationFactor;
+            sat = (sat < (Rpp32f) 1) ? sat : ((Rpp32f) 1);
+            sat = (sat > (Rpp32f) 0) ? sat : ((Rpp32f) 0);
+
+            // HSV to RGB
+
+            Rpp32f c, x, m;
+            c = val * sat;
+            x = c * (1 - abs(int(fmod((hue / 60), 2)) - 1));
+            m = val - c;
+
+            if ((0 <= hue) && (hue < 60))
+            {
+                rf = c;
+                gf = x;
+                bf = 0;
+            }
+            else if ((60 <= hue) && (hue < 120))
+            {
+                rf = x;
+                gf = c;
+                bf = 0;
+            }
+            else if ((120 <= hue) && (hue < 180))
+            {
+                rf = 0;
+                gf = c;
+                bf = x;
+            }
+            else if ((180 <= hue) && (hue < 240))
+            {
+                rf = 0;
+                gf = x;
+                bf = c;
+            }
+            else if ((240 <= hue) && (hue < 300))
+            {
+                rf = x;
+                gf = 0;
+                bf = c;
+            }
+            else if ((300 <= hue) && (hue < 360))
+            {
+                rf = c;
+                gf = 0;
+                bf = x;
+            }
+
+            *dstPtrTempR = (Rpp8u) round((rf + m) * 255);
+            *dstPtrTempG = (Rpp8u) round((gf + m) * 255);
+            *dstPtrTempB = (Rpp8u) round((bf + m) * 255);
+
+            srcPtrTempPx0 += 3;
+            dstPtrTempPx0 += 3;
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus saturationRGB_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                         Rpp32f *batch_saturationFactor,
+                         RppiROI *roiPoints, Rpp32u nbatchSize,
+                         RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32u x1 = roiPoints[batchCount].x;
+            Rpp32u y1 = roiPoints[batchCount].y;
+            Rpp32u x2 = x1 + roiPoints[batchCount].roiWidth;
+            Rpp32u y2 = y1 + roiPoints[batchCount].roiHeight;
+            if (x2 == 0)
+            {
+                x2 = batch_srcSize[batchCount].width;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == 0)
+            {
+                y2 = batch_srcSize[batchCount].height;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp32u remainingElementsAfterROI = (batch_srcSize[batchCount].width - (roiPoints[batchCount].x + roiPoints[batchCount].roiWidth));
+
+            Rpp32f saturationFactor = batch_saturationFactor[batchCount];
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+            {
+                T *srcPtrTemp, *dstPtrTemp;
+                srcPtrTemp = srcPtrImage + (i * batch_srcSizeMax[batchCount].width);
+                dstPtrTemp = dstPtrImage + (i * batch_srcSizeMax[batchCount].width);
+
+                if (!((y1 <= i) && (i <= y2)))
+                {
+                    memcpy(dstPtrTemp, srcPtrTemp, batch_srcSize[batchCount].width * sizeof(T));
+
+                    dstPtrTemp += batch_srcSizeMax[batchCount].width;
+                    srcPtrTemp += batch_srcSizeMax[batchCount].width;
+                }
+                else
+                {
+                    memcpy(dstPtrTemp, srcPtrTemp, x1 * sizeof(T));
+                    srcPtrTemp += x1;
+                    dstPtrTemp += x1;
+
+                    Rpp32u bufferLength = roiPoints[batchCount].roiWidth;
+
+                    saturationRGB_processBuffer_host(srcPtrTemp, batch_srcSizeMax[batchCount], dstPtrTemp, saturationFactor, bufferLength, chnFormat, channel);
+
+                    srcPtrTemp += bufferLength;
+                    dstPtrTemp += bufferLength;
+
+                    memcpy(dstPtrTemp, srcPtrTemp, remainingElementsAfterROI * sizeof(T));
+                    srcPtrTemp += remainingElementsAfterROI;
+                    dstPtrTemp += remainingElementsAfterROI;
+                }
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32u x1 = roiPoints[batchCount].x;
+            Rpp32u y1 = roiPoints[batchCount].y;
+            Rpp32u x2 = x1 + roiPoints[batchCount].roiWidth;
+            Rpp32u y2 = y1 + roiPoints[batchCount].roiHeight;
+            if (x2 == 0)
+            {
+                x2 = batch_srcSize[batchCount].width;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == 0)
+            {
+                y2 = batch_srcSize[batchCount].height;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp32u elementsBeforeROI = channel * x1;
+            Rpp32u remainingElementsAfterROI = channel * (batch_srcSize[batchCount].width - (roiPoints[batchCount].x + roiPoints[batchCount].roiWidth));
+
+            Rpp32f saturationFactor = batch_saturationFactor[batchCount];
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            Rpp32u elementsInRow = channel * batch_srcSize[batchCount].width;
+            Rpp32u elementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+
+
+            for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+            {
+                T *srcPtrTemp, *dstPtrTemp;
+                srcPtrTemp = srcPtrImage + (i * elementsInRowMax);
+                dstPtrTemp = dstPtrImage + (i * elementsInRowMax);
+
+                if (!((y1 <= i) && (i <= y2)))
+                {
+                    memcpy(dstPtrTemp, srcPtrTemp, elementsInRow * sizeof(T));
+
+                    dstPtrTemp += elementsInRowMax;
+                    srcPtrTemp += elementsInRowMax;
+                }
+                else
+                {
+                    memcpy(dstPtrTemp, srcPtrTemp, elementsBeforeROI * sizeof(T));
+                    srcPtrTemp += elementsBeforeROI;
+                    dstPtrTemp += elementsBeforeROI;
+
+                    Rpp32u bufferLength = channel * roiPoints[batchCount].roiWidth;
+
+                    saturationRGB_processBuffer_host(srcPtrTemp, batch_srcSizeMax[batchCount], dstPtrTemp, saturationFactor, bufferLength, chnFormat, channel);
+
+                    memcpy(dstPtrTemp, srcPtrTemp, remainingElementsAfterROI * sizeof(T));
+                    srcPtrTemp += remainingElementsAfterROI;
+                    dstPtrTemp += remainingElementsAfterROI;
+                }
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus saturationRGB_host(T* srcPtr, RppiSize srcSize, T* dstPtr,
+                    Rpp32f saturationFactor,
+                    RppiChnFormat chnFormat, Rpp32u channel)
+{
+    Rpp64u totalImageDim = channel * srcSize.height * srcSize.width;
+
+    Rpp64u bufferLength;
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        bufferLength = srcSize.height * srcSize.width;
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        bufferLength = channel * srcSize.height * srcSize.width;
+    }
+
+    saturationRGB_processBuffer_host(srcPtr, srcSize, dstPtr, saturationFactor, bufferLength, chnFormat, channel);
+
+    return RPP_SUCCESS;
+}
+
+/**************** Tensor Look Up Table ***************/
+
+template <typename T>
+RppStatus tensor_look_up_table_host(T* srcPtr, T* dstPtr, T* lutPtr,
+                          Rpp32u tensorDimension, Rpp32u *tensorDimensionValues)
+{
+    Rpp32u *tensorDimensionValuesTemp;
+    tensorDimensionValuesTemp = tensorDimensionValues;
+
+    Rpp32u tensorSize = 1;
+    for(int i = 0; i < tensorDimension; i++)
+    {
+        tensorSize *= *tensorDimensionValuesTemp;
+        tensorDimensionValuesTemp++;
+    }
+
+    T *srcPtrTemp, *dstPtrTemp;
+    srcPtrTemp = srcPtr;
+    dstPtrTemp = dstPtr;
+
+    for (int i = 0; i < tensorSize; i++)
+    {
+        *dstPtrTemp = *(lutPtr + (Rpp32u)(*srcPtrTemp));
+        srcPtrTemp++;
+        dstPtrTemp++;
+    }
+
+    return RPP_SUCCESS;
+
+}
+
+// /**************** color_convert ***************/
+
+template <typename T, typename U>
+RppStatus color_convert_rgb_to_hsv_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, U* dstPtr,
+    RppiColorConvertMode convertMode, Rpp32u nbatchSize,
+    RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+    for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+    {
+        Rpp32u loc = 0;
+        compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+
+        T *srcPtrImage = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+        U *dstPtrImage = (U*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(U));
+
+        compute_unpadded_from_padded_host(srcPtr + loc, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], srcPtrImage,
+                                            chnFormat, channel);
+
+        compute_rgb_to_hsv_host(srcPtrImage, batch_srcSize[batchCount], dstPtrImage, chnFormat, channel);
+
+        compute_padded_from_unpadded_host(dstPtrImage, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtr + loc,
+                                          chnFormat, channel);
+
+        free(srcPtrImage);
+        free(dstPtrImage);
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T, typename U>
+RppStatus color_convert_hsv_to_rgb_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, U* dstPtr,
+    RppiColorConvertMode convertMode, Rpp32u nbatchSize,
+    RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+    for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+    {
+        Rpp32u loc = 0;
+        compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+
+        T *srcPtrImage = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+        U *dstPtrImage = (U*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(U));
+
+        compute_unpadded_from_padded_host(srcPtr + loc, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], srcPtrImage,
+                                            chnFormat, channel);
+
+        compute_hsv_to_rgb_host(srcPtrImage, batch_srcSize[batchCount], dstPtrImage, chnFormat, channel);
+
+        compute_padded_from_unpadded_host(dstPtrImage, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtr + loc,
+                                          chnFormat, channel);
+
+        free(srcPtrImage);
+        free(dstPtrImage);
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T, typename U>
+RppStatus color_convert_rgb_to_hsv_host(T* srcPtr, RppiSize srcSize, U* dstPtr, RppiColorConvertMode convert_mode, RppiChnFormat chnFormat, Rpp32u channel)
+{
+    compute_rgb_to_hsv_host(srcPtr, srcSize, dstPtr, chnFormat, channel);
+
+    return RPP_SUCCESS;
+}
+
+template <typename T, typename U>
+RppStatus color_convert_hsv_to_rgb_host(T* srcPtr, RppiSize srcSize, U* dstPtr, RppiColorConvertMode convert_mode, RppiChnFormat chnFormat, Rpp32u channel)
+{
+    compute_hsv_to_rgb_host(srcPtr, srcSize, dstPtr, chnFormat, channel);
+
+    return RPP_SUCCESS;
+}
+
+#endif
diff --git a/src/modules/cpu/loongarch_fused_functions.hpp b/src/modules/cpu/loongarch_fused_functions.hpp
new file mode 100644
index 00000000..387b85ec
--- /dev/null
+++ b/src/modules/cpu/loongarch_fused_functions.hpp
@@ -0,0 +1,6290 @@
+/*
+MIT License
+
+Copyright (c) 2019 - 2024 Advanced Micro Devices, Inc.
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
+*/
+
+#ifndef HOST_FUSED_FUNCTIONS_H
+#define HOST_FUSED_FUNCTIONS_H
+
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+
+/**************** color_twist ***************/
+
+template <typename T>
+RppStatus color_twist_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                         Rpp32f *batch_alpha, Rpp32f *batch_beta,
+                         Rpp32f *batch_hueShift, Rpp32f *batch_saturationFactor,
+                         RppiROI *roiPoints, Rpp32u outputFormatToggle, Rpp32u nbatchSize,
+                         RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp64u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32f hueShift = batch_hueShift[batchCount];
+            Rpp32f saturationFactor = batch_saturationFactor[batchCount];
+            Rpp32f alpha = batch_alpha[batchCount];
+            Rpp32f beta = batch_beta[batchCount];
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            color_twist_host(srcPtrImage, batch_srcSizeMax[batchCount], dstPtrImage, alpha, beta, hueShift, saturationFactor, chnFormat, channel);
+
+            if (outputFormatToggle == 1)
+            {
+                T *dstPtrImageUnpadded = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+                T *dstPtrImageUnpaddedCopy = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width * sizeof(T));
+
+                compute_planar_to_packed_host(dstPtrImageUnpaddedCopy, batch_srcSize[batchCount], dstPtrImageUnpadded, channel);
+
+                memset(dstPtrImage, (T) 0, imageDimMax * channel * sizeof(T));
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImage, RPPI_CHN_PACKED, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+    else if(chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp64u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32f hueShift = batch_hueShift[batchCount];
+            Rpp32f saturationFactor = batch_saturationFactor[batchCount];
+            Rpp32f alpha = batch_alpha[batchCount];
+            Rpp32f beta = batch_beta[batchCount];
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            color_twist_host(srcPtrImage, batch_srcSizeMax[batchCount], dstPtrImage, alpha, beta, hueShift, saturationFactor, chnFormat, channel);
+
+            if (outputFormatToggle == 1)
+            {
+                T *dstPtrImageUnpadded = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+                T *dstPtrImageUnpaddedCopy = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width * sizeof(T));
+
+                compute_packed_to_planar_host(dstPtrImageUnpaddedCopy, batch_srcSize[batchCount], dstPtrImageUnpadded, channel);
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImage, RPPI_CHN_PLANAR, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus color_twist_host(T* srcPtr, RppiSize srcSize, T* dstPtr,
+                    Rpp32f alpha, Rpp32f beta,
+                    Rpp32f hueShift, Rpp32f saturationFactor,
+                    RppiChnFormat chnFormat, Rpp32u channel)
+{
+
+    hueShift = (int)hueShift % 360;
+    Rpp32f hueShiftAngle = hueShift;
+    hueShift *= 0.002778f;
+
+    if (hueShift < 0)
+    {
+        hueShift += 1;
+        hueShiftAngle += 360;
+    }
+
+    //Rpp64u totalImageDim = channel * srcSize.height * srcSize.width;
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        T *srcPtrTempR, *srcPtrTempG, *srcPtrTempB;
+        T *dstPtrTempR, *dstPtrTempG, *dstPtrTempB;
+
+        Rpp64u imageDim = srcSize.height * srcSize.width;
+        srcPtrTempR = srcPtr;
+        srcPtrTempG = srcPtr + (imageDim);
+        srcPtrTempB = srcPtr + (2 * imageDim);
+        dstPtrTempR = dstPtr;
+        dstPtrTempG = dstPtr + (imageDim);
+        dstPtrTempB = dstPtr + (2 * imageDim);
+
+        Rpp64u bufferLength = srcSize.height * srcSize.width;
+        Rpp64u alignedLength = bufferLength & ~3;
+
+        __m128i const zero = __lsx_vldi(0);
+        __m128 pZeros = lsx_set1_f32(0.0);
+        __m128 pOnes = lsx_set1_f32(1.0);
+        __m128 pFactor = lsx_set1_f32(255.0);
+        __m128 pHueShift = lsx_set1_f32(hueShift);
+        __m128 pSaturationFactor = lsx_set1_f32(saturationFactor);
+        __m128i px0, px1, px2;
+        __m128 xR, xG, xB;
+        __m128 xH, xS, xV, xC;
+        __m128 xX, xY, xZ;
+        __m128 h0, h1, h2, h3;
+        __m128 x0, x1, x2, x3;
+        __m128 a0, a1;
+        h0 = lsx_set1_f32(1.0);
+
+        __m128 pMul = lsx_set1_f32(alpha);
+        __m128 pAdd = lsx_set1_f32(beta);
+
+        Rpp8u arrayR[4];
+        Rpp8u arrayG[4];
+        Rpp8u arrayB[4];
+
+        Rpp64u vectorLoopCount = 0;
+        for (; vectorLoopCount < alignedLength; vectorLoopCount+=4)
+        {
+            px0 =  __lsx_vld((__m128i *)srcPtrTempR, 0);
+            px1 =  __lsx_vld((__m128i *)srcPtrTempG, 0);
+            px2 =  __lsx_vld((__m128i *)srcPtrTempB, 0);
+
+            px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+            px1 = __lsx_vilvl_b(zero, px1);    // pixels 0-7
+            px2 = __lsx_vilvl_b(zero, px2);    // pixels 0-7
+
+            xR = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+            xG = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 0-3
+            xB = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px2));    // pixels 0-3
+
+            xR = __lsx_vfdiv_s(xR, pFactor);
+            xG = __lsx_vfdiv_s(xG, pFactor);
+            xB = __lsx_vfdiv_s(xB, pFactor);
+
+            // Calculate Saturation, Value, Chroma
+            xS = __lsx_vfmax_s(xG, xB);                               // xS <- [max(G, B)]
+            xC = __lsx_vfmin_s(xG, xB);                               // xC <- [min(G, B)]
+
+            xS = __lsx_vfmax_s(xS, xR);                               // xS <- [max(G, B, R)]
+            xC = __lsx_vfmin_s(xC, xR);                               // xC <- [min(G, B, R)]
+
+            xV = xS;                                               // xV <- [V    ]
+            xS = __lsx_vfsub_s(xS, xC);                               // xS <- [V - m]
+            xS = __lsx_vfdiv_s(xS, xV);                               // xS <- [S    ]
+
+            xC = __lsx_vfsub_s(xC, xV);                               // xC <- [V + m]
+
+            // Calculate Hue
+            xZ = (__m128)__lsx_vfcmp_ceq_s(xV, xG);                             // xZ <- [V==G]
+            xX = (__m128)__lsx_vfcmp_cune_s(xV, xR);                            // xX <- [V!=R]
+
+            xY = (__m128)__lsx_vand_v((__m128i)xZ, (__m128i)xX);                               // xY <- [V!=R && V==G]
+            xZ = (__m128)__lsx_vandn_v((__m128i)xZ, (__m128i)xX);                            // xZ <- [V!=R && V!=G]
+
+            xY = (__m128)__lsx_vxor_v((__m128i)xY, (__m128i)SIMD_GET_PS(full));                // xY <- [V==R || V!=G]
+            xZ = (__m128)__lsx_vxor_v((__m128i)xZ, (__m128i)SIMD_GET_PS(full));                // xZ <- [V==R || V==G]
+
+            xR = (__m128)__lsx_vand_v((__m128i)xR, (__m128i)xX);                               // xR <- [X!=0 ? R : 0]
+            xB = (__m128)__lsx_vand_v((__m128i)xB, (__m128i)xZ);                               // xB <- [Z!=0 ? B : 0]
+            xG = (__m128)__lsx_vand_v((__m128i)xG, (__m128i)xY);                               // xG <- [Y!=0 ? G : 0]
+
+            xZ = (__m128)__lsx_vandn_v((__m128i)xZ, (__m128i)SIMD_GET_PS(sn));               // xZ <- [sign(!Z)]
+            xY = (__m128)__lsx_vandn_v((__m128i)xY, (__m128i)SIMD_GET_PS(sn));               // xY <- [sign(!Y)]
+
+            xG = (__m128)__lsx_vxor_v((__m128i)xG, (__m128i)xZ);                               // xG <- [Y!=0 ? (Z==0 ? G : -G) : 0]
+            xR = (__m128)__lsx_vxor_v((__m128i)xR, (__m128i)xY);                               // xR <- [X!=0 ? (Y==0 ? R : -R) : 0]
+
+            // G is the accumulator
+            xG = __lsx_vfadd_s(xG, xR);                               // xG <- [Rx + Gx]
+            xB = (__m128)__lsx_vxor_v((__m128i)xB, (__m128i)xY);                               // xB <- [Z!=0 ? (Y==0 ? B : -B) : 0]
+
+            xC = __lsx_vfmul_s(xC, SIMD_GET_PS(m6_m6_m6_m6));         // xC <- [C*6     ]
+            xG = __lsx_vfsub_s(xG, xB);                               // xG <- [Rx+Gx+Bx]
+
+            xH = (__m128)__lsx_vand_v((__m128i)xX, (__m128i)SIMD_GET_PS(m4o6_m4o6_m4o6_m4o6)); // xH <- [V==R ?0 :-4/6]
+            xG = __lsx_vfdiv_s(xG, xC);                               // xG <- [(Rx+Gx+Bx)/6C]
+
+            // Correct achromatic cases (H/S may be infinite due to zero division)
+            xH = (__m128)__lsx_vxor_v((__m128i)xH, (__m128i)xZ);                               // xH <- [V==R ? 0 : V==G ? -4/6 : 4/6]
+            xC = (__m128)__lsx_vfcmp_cle_s(SIMD_GET_PS(eps), xC);
+            xH = __lsx_vfadd_s(xH, pOnes);                  // xH <- [V==R ? 1 : V==G ?  2/6 :10/6]
+
+            xG = __lsx_vfadd_s(xG, xH);
+
+            // Normalize H to fraction
+            xH = (__m128)__lsx_vfcmp_cle_s(pOnes, xG);
+
+            xH = (__m128)__lsx_vand_v((__m128i)xH, (__m128i)pOnes);
+            xS = (__m128)__lsx_vand_v((__m128i)xS, (__m128i)xC);
+            xG = (__m128)__lsx_vand_v((__m128i)xG, (__m128i)xC);
+            xG = __lsx_vfsub_s(xG, xH);
+
+            // Modify Hue Values and re-normalize H to fraction
+            xG = __lsx_vfadd_s(xG, pHueShift);
+
+            xH = (__m128)__lsx_vfcmp_cle_s(pOnes, xG);
+            xH = (__m128)__lsx_vand_v((__m128i)xH, (__m128i)pOnes);
+
+            xG = __lsx_vfsub_s(xG, xH);
+
+            // Modify Saturation Values
+            xS = __lsx_vfmul_s(xS, pSaturationFactor);
+            xS = __lsx_vfmin_s(xS, pOnes);
+            xS = __lsx_vfmax_s(xS, pZeros);
+
+            _MM_TRANSPOSE4_PS (h0, xG, xS, xV);
+
+            __m128 h1, h2, h3;
+
+            h1 = xG;
+            h2 = xS;
+            h3 = xV;
+
+            // Prepare HUE for RGB components (per pixel).
+            x0 = SIMD_SHUFFLE_PS(h0, _MM_SHUFFLE(1, 1, 1, 3));     // x0 <- [H           |H           |H           |V          ]
+            x1 = SIMD_SHUFFLE_PS(h1, _MM_SHUFFLE(1, 1, 1, 3));     // x1 <- [H           |H           |H           |V          ]
+            x2 = SIMD_SHUFFLE_PS(h2, _MM_SHUFFLE(1, 1, 1, 3));     // x2 <- [H           |H           |H           |V          ]
+            x3 = SIMD_SHUFFLE_PS(h3, _MM_SHUFFLE(1, 1, 1, 3));     // x3 <- [H           |H           |H           |V          ]
+
+            // Calculate intervals from HUE.
+            x0 = __lsx_vfsub_s(x0, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x0 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+            x1 = __lsx_vfsub_s(x1, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x1 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+            x2 = __lsx_vfsub_s(x2, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x2 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+            x3 = __lsx_vfsub_s(x3, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x3 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+
+            x0 = (__m128)__lsx_vand_v((__m128i)x0, (__m128i)SIMD_GET_PS(abs));                 // x0 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+            x1 = (__m128)__lsx_vand_v((__m128i)x1, (__m128i)SIMD_GET_PS(abs));                 // x1 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+            x2 = (__m128)__lsx_vand_v((__m128i)x2, (__m128i)SIMD_GET_PS(abs));                 // x2 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+            x3 = (__m128)__lsx_vand_v((__m128i)x3, (__m128i)SIMD_GET_PS(abs));                 // x3 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+
+            x0 = __lsx_vfmul_s(x0, SIMD_GET_PS(m6_m6_p6_p0));         // x0 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+            x1 = __lsx_vfmul_s(x1, SIMD_GET_PS(m6_m6_p6_p0));         // x1 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+            x2 = __lsx_vfmul_s(x2, SIMD_GET_PS(m6_m6_p6_p0));         // x2 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+            x3 = __lsx_vfmul_s(x3, SIMD_GET_PS(m6_m6_p6_p0));         // x3 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+
+            x0 = __lsx_vfadd_s(x0, SIMD_GET_PS(p1_p1_m2_p0));         // x0 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+            x1 = __lsx_vfadd_s(x1, SIMD_GET_PS(p1_p1_m2_p0));         // x1 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+            x2 = __lsx_vfadd_s(x2, SIMD_GET_PS(p1_p1_m2_p0));         // x2 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+            x3 = __lsx_vfadd_s(x3, SIMD_GET_PS(p1_p1_m2_p0));         // x3 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+
+            // Bound intervals.
+            x0 = __lsx_vfmax_s(x0, SIMD_GET_PS(m1_m1_m1_p1));
+            x1 = __lsx_vfmax_s(x1, SIMD_GET_PS(m1_m1_m1_p1));
+            x2 = __lsx_vfmax_s(x2, SIMD_GET_PS(m1_m1_m1_p1));
+            x3 = __lsx_vfmax_s(x3, SIMD_GET_PS(m1_m1_m1_p1));
+
+            x0 = __lsx_vfmin_s(x0, SIMD_GET_PS(p0));                  // x0 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+            x1 = __lsx_vfmin_s(x1, SIMD_GET_PS(p0));                  // x1 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+            x2 = __lsx_vfmin_s(x2, SIMD_GET_PS(p0));                  // x2 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+            x3 = __lsx_vfmin_s(x3, SIMD_GET_PS(p0));                  // x3 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+
+            // Prepare S/V vectors.
+            a0 = SIMD_SHUFFLE_PS(h0, _MM_SHUFFLE(2, 2, 2, 2));     // a0 <- [S           |S           |S           |S          ]
+            a1 = SIMD_SHUFFLE_PS(h1, _MM_SHUFFLE(2, 2, 2, 2));     // a1 <- [S           |S           |S           |S          ]
+            h0 = SIMD_SHUFFLE_PS(h0, _MM_SHUFFLE(3, 3, 3, 0));     // h0 <- [V           |V           |V           |A          ]
+            h1 = SIMD_SHUFFLE_PS(h1, _MM_SHUFFLE(3, 3, 3, 0));     // h1 <- [V           |V           |V           |A          ]
+
+            // Multiply with 'S*V' and add 'V'.
+            x0 = __lsx_vfmul_s(x0, a0);                               // x0 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            x1 = __lsx_vfmul_s(x1, a1);                               // x1 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            a0 = SIMD_SHUFFLE_PS(h2, _MM_SHUFFLE(2, 2, 2, 2));     // a0 <- [S           |S           |S           |S          ]
+            a1 = SIMD_SHUFFLE_PS(h3, _MM_SHUFFLE(2, 2, 2, 2));     // a1 <- [S           |S           |S           |S          ]
+
+            x0 = __lsx_vfmul_s(x0, h0);                               // x0 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            x1 = __lsx_vfmul_s(x1, h1);                               // x1 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            h2 = SIMD_SHUFFLE_PS(h2, _MM_SHUFFLE(3, 3, 3, 0));     // h2 <- [V           |V           |V           |A          ]
+            h3 = SIMD_SHUFFLE_PS(h3, _MM_SHUFFLE(3, 3, 3, 0));     // h3 <- [V           |V           |V           |A          ]
+
+            x2 = __lsx_vfmul_s(x2, a0);                               // x2 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            x3 = __lsx_vfmul_s(x3, a1);                               // x3 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            x0 = __lsx_vfadd_s(x0, h0);                               // x0 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |A          ]
+
+            x2 = __lsx_vfmul_s(x2, h2);                               // x2 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            x3 = __lsx_vfmul_s(x3, h3);                               // x3 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            x1 = __lsx_vfadd_s(x1, h1);                               // x1 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |A          ]
+
+            x2 = __lsx_vfadd_s(x2, h2);                               // x2 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |A          ]
+            x3 = __lsx_vfadd_s(x3, h3);                               // x3 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |A          ]
+
+            // Store
+            _MM_TRANSPOSE4_PS (x0, x1, x2, x3);
+
+            x1 = __lsx_vfmul_s(x1, pFactor);
+            x2 = __lsx_vfmul_s(x2, pFactor);
+            x3 = __lsx_vfmul_s(x3, pFactor);
+
+            x1 = __lsx_vfmul_s(x1, pMul);
+            x2 = __lsx_vfmul_s(x2, pMul);
+            x3 = __lsx_vfmul_s(x3, pMul);
+
+            x1 = __lsx_vfadd_s(x1, pAdd);
+            x2 = __lsx_vfadd_s(x2, pAdd);
+            x3 = __lsx_vfadd_s(x3, pAdd);
+
+            px0 = __lsx_vftint_w_s(x1);
+            px1 = __lsx_vftint_w_s(x2);
+            px2 = __lsx_vftint_w_s(x3);
+
+            px0 = lsx_packs_i32(px0, px0);
+            px0 = lsx_packus_i16(px0, px0);
+            *((int*)arrayR) = __lsx_vpickve2gr_w(px0, 0);
+
+            px1 = lsx_packs_i32(px1, px1);
+            px1 = lsx_packus_i16(px1, px1);
+            *((int*)arrayG) = __lsx_vpickve2gr_w(px1, 0);
+
+            px2 = lsx_packs_i32(px2, px2);
+            px2 = lsx_packus_i16(px2, px2);
+            *((int*)arrayB) = __lsx_vpickve2gr_w(px2, 0);
+
+            memcpy(dstPtrTempR, arrayR, 4 * sizeof(Rpp8u));
+            memcpy(dstPtrTempG, arrayG, 4 * sizeof(Rpp8u));
+            memcpy(dstPtrTempB, arrayB, 4 * sizeof(Rpp8u));
+
+            srcPtrTempR += 4;
+            srcPtrTempG += 4;
+            srcPtrTempB += 4;
+            dstPtrTempR += 4;
+            dstPtrTempG += 4;
+            dstPtrTempB += 4;
+        }
+        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+        {
+            // RGB to HSV
+
+            Rpp32f hue, sat, val;
+            Rpp32f rf, gf, bf, cmax, cmin, delta;
+            rf = ((Rpp32f) *srcPtrTempR) / 255;
+            gf = ((Rpp32f) *srcPtrTempG) / 255;
+            bf = ((Rpp32f) *srcPtrTempB) / 255;
+            cmax = RPPMAX3(rf, gf, bf);
+            cmin = RPPMIN3(rf, gf, bf);
+            delta = cmax - cmin;
+
+            if (delta == 0)
+            {
+                hue = 0;
+            }
+            else if (cmax == rf)
+            {
+                hue = round(60 * fmod(((gf - bf) / delta),6));
+            }
+            else if (cmax == gf)
+            {
+                hue = round(60 * (((bf - rf) / delta) + 2));
+            }
+            else if (cmax == bf)
+            {
+                hue = round(60 * (((rf - gf) / delta) + 4));
+            }
+
+            while (hue >= 360)
+            {
+                hue = hue - 360;
+            }
+            while (hue < 0)
+            {
+                hue = 360 + hue;
+            }
+
+            if (cmax == 0)
+            {
+                sat = 0;
+            }
+            else
+            {
+                sat = delta / cmax;
+            }
+
+            val = cmax;
+
+            // Modify Hue and Saturation
+
+            hue = hue + hueShiftAngle;
+            while (hue >= 360)
+            {
+                hue = hue - 360;
+            }
+            while (hue < 0)
+            {
+                hue = 360 + hue;
+            }
+
+            sat *= saturationFactor;
+            sat = (sat < (Rpp32f) 1) ? sat : ((Rpp32f) 1);
+            sat = (sat > (Rpp32f) 0) ? sat : ((Rpp32f) 0);
+
+            // HSV to RGB
+
+            Rpp32f c, x, m;
+            c = val * sat;
+            x = c * (1 - abs(int(fmod((hue / 60), 2)) - 1));
+            m = val - c;
+
+            if ((0 <= hue) && (hue < 60))
+            {
+                rf = c;
+                gf = x;
+                bf = 0;
+            }
+            else if ((60 <= hue) && (hue < 120))
+            {
+                rf = x;
+                gf = c;
+                bf = 0;
+            }
+            else if ((120 <= hue) && (hue < 180))
+            {
+                rf = 0;
+                gf = c;
+                bf = x;
+            }
+            else if ((180 <= hue) && (hue < 240))
+            {
+                rf = 0;
+                gf = x;
+                bf = c;
+            }
+            else if ((240 <= hue) && (hue < 300))
+            {
+                rf = x;
+                gf = 0;
+                bf = c;
+            }
+            else if ((300 <= hue) && (hue < 360))
+            {
+                rf = c;
+                gf = 0;
+                bf = x;
+            }
+
+            *dstPtrTempR = (Rpp8u) round((rf + m) * 255);
+            *dstPtrTempG = (Rpp8u) round((gf + m) * 255);
+            *dstPtrTempB = (Rpp8u) round((bf + m) * 255);
+
+            srcPtrTempR++;
+            srcPtrTempG++;
+            srcPtrTempB++;
+            dstPtrTempR++;
+            dstPtrTempG++;
+            dstPtrTempB++;
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        T *srcPtrTemp, *dstPtrTemp;
+        srcPtrTemp = srcPtr;
+        dstPtrTemp = dstPtr;
+
+        Rpp32u bufferLength = srcSize.height * srcSize.width;
+        Rpp32u alignedLength = bufferLength & ~3;
+        __m128i mask_R = lsx_setr_i8(0, 0x80, 0x80, 0x80, 3, 0x80, 0x80, 0x80, 6, 0x80, 0x80, 0x80, 9, 0x80, 0x80, 0x80);
+        __m128i mask_G = lsx_setr_i8(1, 0x80, 0x80, 0x80, 4, 0x80, 0x80, 0x80, 7, 0x80, 0x80, 0x80, 10, 0x80, 0x80, 0x80);
+        __m128i mask_B = lsx_setr_i8(2, 0x80, 0x80, 0x80, 5, 0x80, 0x80, 0x80, 8, 0x80, 0x80, 0x80, 11, 0x80, 0x80, 0x80);
+        __m128i mask2 = lsx_setr_i8(0, 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 14, 3, 7, 11, 15);
+        __m128 pZeros = lsx_set1_f32(0.0);
+        __m128 pOnes = lsx_set1_f32(1.0);
+        __m128 pMultiplier1 = lsx_set1_f32(1 / 255.0);
+        __m128 pHueShift = lsx_set1_f32(hueShift);
+        __m128 pSaturationFactor = lsx_set1_f32(saturationFactor);
+        __m128 pMul = lsx_set1_f32(alpha*255);
+        __m128 pAdd = lsx_set1_f32(beta);
+
+        Rpp32u vectorLoopCount = 0;
+        for (; vectorLoopCount < alignedLength; vectorLoopCount += 4)
+        {
+          // todo:: tomany registers used: try to reuse and reduce
+          __m128i px0, px1, px2, px3;
+          __m128 xR, xG, xB, xA;
+          __m128 xH, xS, xV, xC;
+          __m128 xX, xY, xZ;
+          __m128 h0, h1, h2, h3;
+          __m128 x0, x1, x2, x3;
+          __m128 a0, a1;
+            // Load -> Shuffle -> Unpack
+            px0 = __lsx_vld((__m128i *)srcPtrTemp, 0);           // load [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|R05|G05|B05|R06] - Need RGB 01-04
+            xR = __lsx_vffint_s_w(lsx_shuffle_i8(px0, mask_R));    // XR - Contains R01-04
+            xG = __lsx_vffint_s_w(lsx_shuffle_i8(px0, mask_G));    // XG - Contains G01-04
+            xB = __lsx_vffint_s_w(lsx_shuffle_i8(px0, mask_B));    // XB - Contains B01-04
+
+            // Normalize 0-255 -> 0-1
+            xR = __lsx_vfmul_s(xR, pMultiplier1);
+            xG = __lsx_vfmul_s(xG, pMultiplier1);
+            xB = __lsx_vfmul_s(xB, pMultiplier1);
+
+            // Calculate Saturation, Value, Chroma
+            xS = __lsx_vfmax_s(xG, xB);                               // xS <- [max(G, B)]
+            xC = __lsx_vfmin_s(xG, xB);                               // xC <- [min(G, B)]
+
+            xS = __lsx_vfmax_s(xS, xR);                               // xS <- [max(G, B, R)]
+            xC = __lsx_vfmin_s(xC, xR);                               // xC <- [min(G, B, R)]
+
+            xV = xS;                                               // xV <- [V    ]
+            xS = __lsx_vfsub_s(xS, xC);                               // xS <- [V - m], delta
+            xC = xS;                                               // Xc <- delta
+            xS = __lsx_vfdiv_s(xS, xV);                               // xS <- [S    ], delta/max
+
+            //xC = __lsx_vfsub_s(xC, xV);                               // xC <- [V + m]
+
+            // Calculate Hue
+            xZ = (__m128)__lsx_vfcmp_ceq_s(xV, xG);                             // xZ <- [V==G]
+            xX = (__m128)__lsx_vfcmp_cune_s(xV, xR);                            // xX <- [V!=R]
+
+            xY = (__m128)__lsx_vand_v((__m128i)xZ, (__m128i)xX);                               // xY <- [V!=R && V==G]
+            xZ = (__m128)__lsx_vandn_v((__m128i)xZ, (__m128i)xX);                            // xZ <- [V!=R && V!=G]
+
+            xY = (__m128)__lsx_vxor_v((__m128i)xY, (__m128i)SIMD_GET_PS(full));                // xY <- [V==R || V!=G]
+            xZ = (__m128)__lsx_vxor_v((__m128i)xZ, (__m128i)SIMD_GET_PS(full));                // xZ <- [V==R || V==G]
+
+            xR = (__m128)__lsx_vand_v((__m128i)xR, (__m128i)xX);                               // xR <- [X!=0 ? R : 0]
+            xB = (__m128)__lsx_vand_v((__m128i)xB, (__m128i)xZ);                               // xB <- [Z!=0 ? B : 0]
+            xG = (__m128)__lsx_vand_v((__m128i)xG, (__m128i)xY);                               // xG <- [Y!=0 ? G : 0]
+            xZ = (__m128)__lsx_vandn_v((__m128i)xZ, (__m128i)SIMD_GET_PS(sn));               // xZ <- [sign(!Z)]
+            xY = (__m128)__lsx_vandn_v((__m128i)xY, (__m128i)SIMD_GET_PS(sn));               // xY <- [sign(!Y)]
+
+            xG = (__m128)__lsx_vxor_v((__m128i)xG, (__m128i)xZ);                               // xG <- [Y!=0 ? (Z==0 ? G : -G) : 0]
+            xR = (__m128)__lsx_vxor_v((__m128i)xR, (__m128i)xY);                               // xR <- [X!=0 ? (Y==0 ? R : -R) : 0]
+
+            // G is the accumulator
+            xG = __lsx_vfadd_s(xG, xR);                               // xG <- [Rx + Gx]
+            xB = (__m128)__lsx_vxor_v((__m128i)xB, (__m128i)xY);                               // xB <- [Z!=0 ? (Y==0 ? B : -B) : 0]
+
+            //xC = __lsx_vfmul_s(xC, SIMD_GET_PS(m6_m6_m6_m6));         // xC <- [C*6     ]
+            xC = __lsx_vfmul_s(xC, xmm_p6);                            // xC <- [C*6     ]
+            xG = __lsx_vfsub_s(xG, xB);                               // xG <- [Rx+Gx+Bx]
+
+            xH = (__m128)__lsx_vand_v((__m128i)xX, (__m128i)SIMD_GET_PS(m4o6_m4o6_m4o6_m4o6)); // xH <- [V==R ?0 :-4/6]
+            xG = __lsx_vfdiv_s(xG, xC);                               // xG <- [(Rx+Gx+Bx)/6C]
+
+            // Correct achromatic cases (H/S may be infinite due to zero division)
+            xH = (__m128)__lsx_vxor_v((__m128i)xH, (__m128i)xZ);                               // xH <- [V==R ? 0 : V==G ? -4/6 : 4/6]
+            xC = (__m128)__lsx_vfcmp_cle_s(SIMD_GET_PS(eps), xC);
+            xH = __lsx_vfadd_s(xH, pOnes);                  // xH <- [V==R ? 1 : V==G ?  2/6 :10/6]
+
+            xG = __lsx_vfadd_s(xG, xH);
+            xG = __lsx_vfadd_s(xG, pHueShift);
+
+            // Normalize H to fraction
+            xH = (__m128)__lsx_vfcmp_cle_s(pOnes, xG);
+
+            xH = (__m128)__lsx_vand_v((__m128i)xH, (__m128i)pOnes);
+            xS = (__m128)__lsx_vand_v((__m128i)xS, (__m128i)xC);
+            xG = (__m128)__lsx_vand_v((__m128i)xG, (__m128i)xC);
+            xG = __lsx_vfsub_s(xG, xH);
+
+            // Modify Saturation Values
+            xS = __lsx_vfmul_s(xS, pSaturationFactor);
+            xS = __lsx_vfmin_s(xS, pOnes);
+            xS = __lsx_vfmax_s(xS, pZeros);
+
+            x0 = SIMD_SHUFFLE_PS(xG, _MM_SHUFFLE(0, 0, 0, 0));    // x0 <- [H           |H           |H           |H          ]
+            x1 = SIMD_SHUFFLE_PS(xG, _MM_SHUFFLE(1, 1, 1, 1));    // x1 <- [H           |H           |H           |H          ]
+            x2 = SIMD_SHUFFLE_PS(xG, _MM_SHUFFLE(2, 2, 2, 2));    // x2 <- [H           |H           |H           |H          ]
+            x3 = SIMD_SHUFFLE_PS(xG, _MM_SHUFFLE(3, 3, 3, 3));    // x3 <- [H           |H           |H           |H          ]
+
+            // Calculate intervals from HUE.
+            x0 = __lsx_vfsub_s(x0, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x0 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+            x1 = __lsx_vfsub_s(x1, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x1 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+            x2 = __lsx_vfsub_s(x2, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x2 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+            x3 = __lsx_vfsub_s(x3, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x3 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+
+            x0 = (__m128)__lsx_vand_v((__m128i)x0, (__m128i)SIMD_GET_PS(abs));                 // x0 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+            x1 = (__m128)__lsx_vand_v((__m128i)x1, (__m128i)SIMD_GET_PS(abs));                 // x1 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+            x2 = (__m128)__lsx_vand_v((__m128i)x2, (__m128i)SIMD_GET_PS(abs));                 // x2 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+            x3 = (__m128)__lsx_vand_v((__m128i)x3, (__m128i)SIMD_GET_PS(abs));                 // x3 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+
+            x0 = __lsx_vfmul_s(x0, SIMD_GET_PS(m6_m6_p6_p0));         // x0 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+            x1 = __lsx_vfmul_s(x1, SIMD_GET_PS(m6_m6_p6_p0));         // x1 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+            x2 = __lsx_vfmul_s(x2, SIMD_GET_PS(m6_m6_p6_p0));         // x2 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+            x3 = __lsx_vfmul_s(x3, SIMD_GET_PS(m6_m6_p6_p0));         // x3 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+
+            x0 = __lsx_vfadd_s(x0, SIMD_GET_PS(p1_p1_m2_p0));         // x0 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+            x1 = __lsx_vfadd_s(x1, SIMD_GET_PS(p1_p1_m2_p0));         // x1 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+            x2 = __lsx_vfadd_s(x2, SIMD_GET_PS(p1_p1_m2_p0));         // x2 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+            x3 = __lsx_vfadd_s(x3, SIMD_GET_PS(p1_p1_m2_p0));         // x3 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+
+            // Bound intervals.
+            x0 = __lsx_vfmax_s(x0, SIMD_GET_PS(m1_m1_m1_p1));
+            x1 = __lsx_vfmax_s(x1, SIMD_GET_PS(m1_m1_m1_p1));
+            x2 = __lsx_vfmax_s(x2, SIMD_GET_PS(m1_m1_m1_p1));
+            x3 = __lsx_vfmax_s(x3, SIMD_GET_PS(m1_m1_m1_p1));
+
+            x0 = __lsx_vfmin_s(x0, SIMD_GET_PS(p0));                  // x0 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+            x1 = __lsx_vfmin_s(x1, SIMD_GET_PS(p0));                  // x1 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+            x2 = __lsx_vfmin_s(x2, SIMD_GET_PS(p0));                  // x2 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+            x3 = __lsx_vfmin_s(x3, SIMD_GET_PS(p0));                  // x3 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+
+            // Prepare S/V vectors.
+            a0 = SIMD_SHUFFLE_PS(xS, _MM_SHUFFLE(0, 0, 0, 0));     // a0 <- [S           |S           |S           |S          ]
+            a1 = SIMD_SHUFFLE_PS(xS, _MM_SHUFFLE(1, 1, 1, 1));     // a1 <- [S           |S           |S           |S          ]
+            h0 = SIMD_SHUFFLE_PS(xV, _MM_SHUFFLE(0, 0, 0, 0));     // h0 <- [V           |V           |V           |V          ]
+            h1 = SIMD_SHUFFLE_PS(xV, _MM_SHUFFLE(1, 1, 1, 1));     // h1 <- [V           |V           |V           |V          ]
+
+            // Multiply with 'S*V' and add 'V'.
+            x0 = __lsx_vfmul_s(x0, a0);                               // x0 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            x1 = __lsx_vfmul_s(x1, a1);                               // x1 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            a0 = SIMD_SHUFFLE_PS(xS, _MM_SHUFFLE(2, 2, 2, 2));     // a0 <- [S           |S           |S           |S          ]
+            a1 = SIMD_SHUFFLE_PS(xS, _MM_SHUFFLE(3, 3, 3, 3));     // a1 <- [S           |S           |S           |S          ]
+
+            x0 = __lsx_vfmul_s(x0, h0);                               // x0 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            x1 = __lsx_vfmul_s(x1, h1);                               // x1 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            h2 = SIMD_SHUFFLE_PS(xV, _MM_SHUFFLE(2, 2, 2, 2));     // h2 <- [V           |V           |V           |V          ]
+            h3 = SIMD_SHUFFLE_PS(xV, _MM_SHUFFLE(3, 3, 3, 3));     // h3 <- [V           |V           |V           |V          ]
+
+            x2 = __lsx_vfmul_s(x2, a0);                               // x2 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            x3 = __lsx_vfmul_s(x3, a1);                               // x3 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            x0 = __lsx_vfadd_s(x0, h0);                               // x0 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |V          ]
+
+            x2 = __lsx_vfmul_s(x2, h2);                               // x2 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            x3 = __lsx_vfmul_s(x3, h3);                               // x3 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            x1 = __lsx_vfadd_s(x1, h1);                               // x1 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |V          ]
+
+            x2 = __lsx_vfadd_s(x2, h2);                               // x2 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |V          ]
+            x3 = __lsx_vfadd_s(x3, h3);                               // x3 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |V          ]
+
+            // Store into ps
+            x0 = (__m128)__lsx_vpermi_w((__m128i)x0, (__m128i)x0, _MM_SHUFFLE(0,3,2,1));
+            x1 = (__m128)__lsx_vpermi_w((__m128i)x1, (__m128i)x1, _MM_SHUFFLE(0,3,2,1));
+            x2 = (__m128)__lsx_vpermi_w((__m128i)x2, (__m128i)x2, _MM_SHUFFLE(0,3,2,1));
+            x3 = (__m128)__lsx_vpermi_w((__m128i)x3, (__m128i)x3, _MM_SHUFFLE(0,3,2,1));
+
+            // Un-normalize + Brightness Change
+            x0 = __lsx_vfmul_s(x0, pMul);
+            x1 = __lsx_vfmul_s(x1, pMul);
+            x2 = __lsx_vfmul_s(x2, pMul);
+            x3 = __lsx_vfmul_s(x3, pMul);
+            x0 = __lsx_vfadd_s(x0, pAdd);
+            x1 = __lsx_vfadd_s(x1, pAdd);
+            x2 = __lsx_vfadd_s(x2, pAdd);
+            x3 = __lsx_vfadd_s(x3, pAdd);
+
+            // Pack -> Shuffle -> Store 0-1 -> 0-255
+            px0 = __lsx_vftint_w_s(x0);
+            px1 = __lsx_vftint_w_s(x1);
+            px2 = __lsx_vftint_w_s(x2);
+            px3 = __lsx_vftint_w_s(x3);
+            px0 = lsx_packus_i32(px0, px1);    // pack pixels 0-7
+            px1 = lsx_packus_i32(px2, px3);    // pack pixels 8-15
+            px0 = lsx_packus_i16(px0, px1);    // pack pixels 0-15 as [R01|G01|B01|A01|R02|G02|B02|A02|R03|G03|B03|A03|R04|G04|B04|A04]
+            px0 = lsx_shuffle_i8(px0, mask2);    // shuffle to get [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|A01|A02|A03|A04]
+            __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);    // store [R01|G01|B01|R02|G02|B02|R03|G03|B03|R04|G04|B04|A01|A02|A03|A04]
+
+            srcPtrTemp += 12;
+            dstPtrTemp += 12;
+        }
+        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+        {
+            // RGB to HSV
+
+            Rpp32f hue, sat, val;
+            Rpp32f rf, gf, bf, cmax, cmin, delta;
+            rf = ((Rpp32f) srcPtrTemp[0]) / 255;
+            gf = ((Rpp32f) srcPtrTemp[1]) / 255;
+            bf = ((Rpp32f) srcPtrTemp[2]) / 255;
+            cmax = RPPMAX3(rf, gf, bf);
+            cmin = RPPMIN3(rf, gf, bf);
+            delta = cmax - cmin;
+            hue = 0.0f;
+            sat = 0.0f;
+            float add = 0.0;
+            if (delta) {
+                if (cmax) sat = delta / cmax;
+                if (cmax == rf)
+                {
+                    hue = gf - bf;
+                    add = 0.0f;
+                }
+                else if (cmax == gf)
+                {
+                    hue = bf - rf;
+                    add = 1.0f/3.0f;
+                }
+                else
+                {
+                    hue = rf - gf;
+                    add = 2.0f/3.0f;
+                }
+                hue /= (6.0*delta);
+                //hue += add;
+            }
+
+
+            // Modify Hue and Saturation
+
+            hue += hueShift + add;
+            //if (hue >= 1.f) hue -= 1.0f;
+            hue = hue - (int)hue;
+            if (hue < 0) hue += 1.0;
+
+            sat *= saturationFactor;
+            sat = (sat < (Rpp32f) 1) ? sat : ((Rpp32f) 1);
+            sat = (sat > (Rpp32f) 0) ? sat : ((Rpp32f) 0);
+
+            // HSV to RGB
+            val = cmax*255;
+            hue *= 6.0f;
+            int index = static_cast<int>(hue);
+
+            float f = (hue - static_cast<float>(index));
+            float p = val * (1.0f - sat);
+            float q = val * (1.0f - sat * f);
+            float t = val * (1.0f - sat * (1.0f - f));
+
+            switch (index) {
+              case 0: dstPtrTemp[0] = val; dstPtrTemp[1] = t; dstPtrTemp[2] = p; break;
+              case 1: dstPtrTemp[0] = q; dstPtrTemp[1] = val; dstPtrTemp[2] = p; break;
+              case 2: dstPtrTemp[0] = p; dstPtrTemp[1] = val; dstPtrTemp[2] = t; break;
+              case 3: dstPtrTemp[0] = p; dstPtrTemp[1] = q; dstPtrTemp[2] = val; break;
+              case 4: dstPtrTemp[0] = t; dstPtrTemp[1] = p; dstPtrTemp[2] = val; break;
+              case 5: dstPtrTemp[0] = val; dstPtrTemp[1] = p; dstPtrTemp[2] = q; break;
+            }
+
+            srcPtrTemp += 3;
+            dstPtrTemp += 3;
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus color_twist_f32_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                         Rpp32f *batch_alpha, Rpp32f *batch_beta,
+                         Rpp32f *batch_hueShift, Rpp32f *batch_saturationFactor,
+                         RppiROI *roiPoints, Rpp32u outputFormatToggle, Rpp32u nbatchSize,
+                         RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp64u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32f hueShift = batch_hueShift[batchCount];
+            Rpp32f saturationFactor = batch_saturationFactor[batchCount];
+            Rpp32f alpha = batch_alpha[batchCount];
+            Rpp32f beta = batch_beta[batchCount];
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            color_twist_f32_host(srcPtrImage, batch_srcSizeMax[batchCount], dstPtrImage, alpha, beta, hueShift, saturationFactor, chnFormat, channel);
+
+            if (outputFormatToggle == 1)
+            {
+                T *dstPtrImageUnpadded = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+                T *dstPtrImageUnpaddedCopy = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width * sizeof(T));
+
+                compute_planar_to_packed_host(dstPtrImageUnpaddedCopy, batch_srcSize[batchCount], dstPtrImageUnpadded, channel);
+
+                memset(dstPtrImage, (T) 0, imageDimMax * channel * sizeof(T));
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImage, RPPI_CHN_PACKED, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+    else if(chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp64u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32f hueShift = batch_hueShift[batchCount];
+            Rpp32f saturationFactor = batch_saturationFactor[batchCount];
+            Rpp32f alpha = batch_alpha[batchCount];
+            Rpp32f beta = batch_beta[batchCount];
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            color_twist_f32_host(srcPtrImage, batch_srcSizeMax[batchCount], dstPtrImage, alpha, beta, hueShift, saturationFactor, chnFormat, channel);
+
+            if (outputFormatToggle == 1)
+            {
+                T *dstPtrImageUnpadded = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+                T *dstPtrImageUnpaddedCopy = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width * sizeof(T));
+
+                compute_packed_to_planar_host(dstPtrImageUnpaddedCopy, batch_srcSize[batchCount], dstPtrImageUnpadded, channel);
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImage, RPPI_CHN_PLANAR, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+RppStatus color_twist_f32_host(Rpp32f* srcPtr, RppiSize srcSize, Rpp32f* dstPtr,
+                    Rpp32f alpha, Rpp32f beta,
+                    Rpp32f hueShift, Rpp32f saturationFactor,
+                    RppiChnFormat chnFormat, Rpp32u channel)
+{
+    hueShift = (int)hueShift % 360;
+    Rpp32f hueShiftAngle = hueShift;
+    hueShift *= 0.002778f;
+
+    if (hueShift < 0)
+    {
+        hueShift += 1;
+        hueShiftAngle += 360;
+    }
+
+    Rpp64u totalImageDim = channel * srcSize.height * srcSize.width;
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        Rpp32f *srcPtrTempR, *srcPtrTempG, *srcPtrTempB;
+        Rpp32f *dstPtrTempR, *dstPtrTempG, *dstPtrTempB;
+
+        Rpp64u imageDim = srcSize.height * srcSize.width;
+        srcPtrTempR = srcPtr;
+        srcPtrTempG = srcPtr + (imageDim);
+        srcPtrTempB = srcPtr + (2 * imageDim);
+        dstPtrTempR = dstPtr;
+        dstPtrTempG = dstPtr + (imageDim);
+        dstPtrTempB = dstPtr + (2 * imageDim);
+
+        Rpp64u bufferLength = srcSize.height * srcSize.width;
+        Rpp64u alignedLength = bufferLength & ~3;
+
+        __m128 pZeros = lsx_set1_f32(0.0);
+        __m128 pOnes = lsx_set1_f32(1.0);
+        __m128 pFactor = lsx_set1_f32(255.0);
+        __m128 pHueShift = lsx_set1_f32(hueShift);
+        __m128 pSaturationFactor = lsx_set1_f32(saturationFactor);
+        __m128 xR, xG, xB;
+        __m128 xH, xS, xV, xC;
+        __m128 xX, xY, xZ;
+        __m128 h0, h1, h2, h3;
+        __m128 x0, x1, x2, x3;
+        __m128 a0, a1;
+        h0 = lsx_set1_f32(1.0);
+
+        __m128 pMul = lsx_set1_f32(alpha);
+        __m128 pAdd = lsx_set1_f32(beta);
+
+        Rpp64u vectorLoopCount = 0;
+        for (; vectorLoopCount < alignedLength; vectorLoopCount+=4)
+        {
+            xR = (__m128)__lsx_vld(srcPtrTempR, 0);
+            xG = (__m128)__lsx_vld(srcPtrTempG, 0);
+            xB = (__m128)__lsx_vld(srcPtrTempB, 0);
+
+            xR = __lsx_vfdiv_s(xR, pFactor);
+            xG = __lsx_vfdiv_s(xG, pFactor);
+            xB = __lsx_vfdiv_s(xB, pFactor);
+
+            // Calculate Saturation, Value, Chroma
+            xS = __lsx_vfmax_s(xG, xB);                               // xS <- [max(G, B)]
+            xC = __lsx_vfmin_s(xG, xB);                               // xC <- [min(G, B)]
+
+            xS = __lsx_vfmax_s(xS, xR);                               // xS <- [max(G, B, R)]
+            xC = __lsx_vfmin_s(xC, xR);                               // xC <- [min(G, B, R)]
+
+            xV = xS;                                               // xV <- [V    ]
+            xS = __lsx_vfsub_s(xS, xC);                               // xS <- [V - m]
+            xS = __lsx_vfdiv_s(xS, xV);                               // xS <- [S    ]
+
+            xC = __lsx_vfsub_s(xC, xV);                               // xC <- [V + m]
+
+            // Calculate Hue
+            xZ = (__m128)__lsx_vfcmp_ceq_s(xV, xG);                             // xZ <- [V==G]
+            xX = (__m128)__lsx_vfcmp_cune_s(xV, xR);                            // xX <- [V!=R]
+
+            xY = (__m128)__lsx_vand_v((__m128i)xZ, (__m128i)xX);                               // xY <- [V!=R && V==G]
+            xZ = (__m128)__lsx_vandn_v((__m128i)xZ, (__m128i)xX);                            // xZ <- [V!=R && V!=G]
+
+            xY = (__m128)__lsx_vxor_v((__m128i)xY, (__m128i)SIMD_GET_PS(full));                // xY <- [V==R || V!=G]
+            xZ = (__m128)__lsx_vxor_v((__m128i)xZ, (__m128i)SIMD_GET_PS(full));                // xZ <- [V==R || V==G]
+
+            xR = (__m128)__lsx_vand_v((__m128i)xR, (__m128i)xX);                               // xR <- [X!=0 ? R : 0]
+            xB = (__m128)__lsx_vand_v((__m128i)xB, (__m128i)xZ);                               // xB <- [Z!=0 ? B : 0]
+            xG = (__m128)__lsx_vand_v((__m128i)xG, (__m128i)xY);                               // xG <- [Y!=0 ? G : 0]
+            xZ = (__m128)__lsx_vandn_v((__m128i)xZ, (__m128i)SIMD_GET_PS(sn));               // xZ <- [sign(!Z)]
+            xY = (__m128)__lsx_vandn_v((__m128i)xY, (__m128i)SIMD_GET_PS(sn));               // xY <- [sign(!Y)]
+
+            xG = (__m128)__lsx_vxor_v((__m128i)xG, (__m128i)xZ);                               // xG <- [Y!=0 ? (Z==0 ? G : -G) : 0]
+            xR = (__m128)__lsx_vxor_v((__m128i)xR, (__m128i)xY);                               // xR <- [X!=0 ? (Y==0 ? R : -R) : 0]
+
+            // G is the accumulator
+            xG = __lsx_vfadd_s(xG, xR);                               // xG <- [Rx + Gx]
+            xB = (__m128)__lsx_vxor_v((__m128i)xB, (__m128i)xY);                               // xB <- [Z!=0 ? (Y==0 ? B : -B) : 0]
+
+            xC = __lsx_vfmul_s(xC, SIMD_GET_PS(m6_m6_m6_m6));         // xC <- [C*6     ]
+            xG = __lsx_vfsub_s(xG, xB);                               // xG <- [Rx+Gx+Bx]
+
+            xH = (__m128)__lsx_vand_v((__m128i)xX, (__m128i)SIMD_GET_PS(m4o6_m4o6_m4o6_m4o6)); // xH <- [V==R ?0 :-4/6]
+            xG = __lsx_vfdiv_s(xG, xC);                               // xG <- [(Rx+Gx+Bx)/6C]
+
+            // Correct achromatic cases (H/S may be infinite due to zero division)
+            xH = (__m128)__lsx_vxor_v((__m128i)xH, (__m128i)xZ);                               // xH <- [V==R ? 0 : V==G ? -4/6 : 4/6]
+            xC = (__m128)__lsx_vfcmp_cle_s(SIMD_GET_PS(eps), xC);
+            xH = __lsx_vfadd_s(xH, SIMD_GET_PS(p1));                  // xH <- [V==R ? 1 : V==G ?  2/6 :10/6]
+
+            xG = __lsx_vfadd_s(xG, xH);
+
+            // Normalize H to fraction
+            xH = (__m128)__lsx_vfcmp_cle_s(SIMD_GET_PS(p1), xG);
+
+            xH = (__m128)__lsx_vand_v((__m128i)xH, (__m128i)SIMD_GET_PS(p1));
+            xS = (__m128)__lsx_vand_v((__m128i)xS, (__m128i)xC);
+            xG = (__m128)__lsx_vand_v((__m128i)xG, (__m128i)xC);
+            xG = __lsx_vfsub_s(xG, xH);
+
+            // Modify Hue Values and re-normalize H to fraction
+            xG = __lsx_vfadd_s(xG, pHueShift);
+
+            xH = (__m128)__lsx_vfcmp_cle_s(SIMD_GET_PS(p1), xG);
+            xH = (__m128)__lsx_vand_v((__m128i)xH, (__m128i)SIMD_GET_PS(p1));
+
+            xG = __lsx_vfsub_s(xG, xH);
+
+            // Modify Saturation Values
+            xS = __lsx_vfmul_s(xS, pSaturationFactor);
+            xS = __lsx_vfmin_s(xS, pOnes);
+            xS = __lsx_vfmax_s(xS, pZeros);
+
+            _MM_TRANSPOSE4_PS (h0, xG, xS, xV);
+
+            __m128 h1, h2, h3;
+
+            h1 = xG;
+            h2 = xS;
+            h3 = xV;
+
+            // Prepare HUE for RGB components (per pixel).
+            x0 = SIMD_SHUFFLE_PS(h0, _MM_SHUFFLE(1, 1, 1, 3));     // x0 <- [H           |H           |H           |V          ]
+            x1 = SIMD_SHUFFLE_PS(h1, _MM_SHUFFLE(1, 1, 1, 3));     // x1 <- [H           |H           |H           |V          ]
+            x2 = SIMD_SHUFFLE_PS(h2, _MM_SHUFFLE(1, 1, 1, 3));     // x2 <- [H           |H           |H           |V          ]
+            x3 = SIMD_SHUFFLE_PS(h3, _MM_SHUFFLE(1, 1, 1, 3));     // x3 <- [H           |H           |H           |V          ]
+
+            // Calculate intervals from HUE.
+            x0 = __lsx_vfsub_s(x0, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x0 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+            x1 = __lsx_vfsub_s(x1, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x1 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+            x2 = __lsx_vfsub_s(x2, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x2 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+            x3 = __lsx_vfsub_s(x3, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x3 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+
+            x0 = (__m128)__lsx_vand_v((__m128i)x0, (__m128i)SIMD_GET_PS(abs));                 // x0 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+            x1 = (__m128)__lsx_vand_v((__m128i)x1, (__m128i)SIMD_GET_PS(abs));                 // x1 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+            x2 = (__m128)__lsx_vand_v((__m128i)x2, (__m128i)SIMD_GET_PS(abs));                 // x2 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+            x3 = (__m128)__lsx_vand_v((__m128i)x3, (__m128i)SIMD_GET_PS(abs));                 // x3 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+
+            x0 = __lsx_vfmul_s(x0, SIMD_GET_PS(m6_m6_p6_p0));         // x0 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+            x1 = __lsx_vfmul_s(x1, SIMD_GET_PS(m6_m6_p6_p0));         // x1 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+            x2 = __lsx_vfmul_s(x2, SIMD_GET_PS(m6_m6_p6_p0));         // x2 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+            x3 = __lsx_vfmul_s(x3, SIMD_GET_PS(m6_m6_p6_p0));         // x3 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+
+            x0 = __lsx_vfadd_s(x0, SIMD_GET_PS(p1_p1_m2_p0));         // x0 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+            x1 = __lsx_vfadd_s(x1, SIMD_GET_PS(p1_p1_m2_p0));         // x1 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+            x2 = __lsx_vfadd_s(x2, SIMD_GET_PS(p1_p1_m2_p0));         // x2 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+            x3 = __lsx_vfadd_s(x3, SIMD_GET_PS(p1_p1_m2_p0));         // x3 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+
+            // Bound intervals.
+            x0 = __lsx_vfmax_s(x0, SIMD_GET_PS(m1_m1_m1_p1));
+            x1 = __lsx_vfmax_s(x1, SIMD_GET_PS(m1_m1_m1_p1));
+            x2 = __lsx_vfmax_s(x2, SIMD_GET_PS(m1_m1_m1_p1));
+            x3 = __lsx_vfmax_s(x3, SIMD_GET_PS(m1_m1_m1_p1));
+
+            x0 = __lsx_vfmin_s(x0, SIMD_GET_PS(p0));                  // x0 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+            x1 = __lsx_vfmin_s(x1, SIMD_GET_PS(p0));                  // x1 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+            x2 = __lsx_vfmin_s(x2, SIMD_GET_PS(p0));                  // x2 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+            x3 = __lsx_vfmin_s(x3, SIMD_GET_PS(p0));                  // x3 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+
+            // Prepare S/V vectors.
+            a0 = SIMD_SHUFFLE_PS(h0, _MM_SHUFFLE(2, 2, 2, 2));     // a0 <- [S           |S           |S           |S          ]
+            a1 = SIMD_SHUFFLE_PS(h1, _MM_SHUFFLE(2, 2, 2, 2));     // a1 <- [S           |S           |S           |S          ]
+            h0 = SIMD_SHUFFLE_PS(h0, _MM_SHUFFLE(3, 3, 3, 0));     // h0 <- [V           |V           |V           |A          ]
+            h1 = SIMD_SHUFFLE_PS(h1, _MM_SHUFFLE(3, 3, 3, 0));     // h1 <- [V           |V           |V           |A          ]
+
+            // Multiply with 'S*V' and add 'V'.
+            x0 = __lsx_vfmul_s(x0, a0);                               // x0 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            x1 = __lsx_vfmul_s(x1, a1);                               // x1 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            a0 = SIMD_SHUFFLE_PS(h2, _MM_SHUFFLE(2, 2, 2, 2));     // a0 <- [S           |S           |S           |S          ]
+            a1 = SIMD_SHUFFLE_PS(h3, _MM_SHUFFLE(2, 2, 2, 2));     // a1 <- [S           |S           |S           |S          ]
+
+            x0 = __lsx_vfmul_s(x0, h0);                               // x0 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            x1 = __lsx_vfmul_s(x1, h1);                               // x1 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            h2 = SIMD_SHUFFLE_PS(h2, _MM_SHUFFLE(3, 3, 3, 0));     // h2 <- [V           |V           |V           |A          ]
+            h3 = SIMD_SHUFFLE_PS(h3, _MM_SHUFFLE(3, 3, 3, 0));     // h3 <- [V           |V           |V           |A          ]
+
+            x2 = __lsx_vfmul_s(x2, a0);                               // x2 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            x3 = __lsx_vfmul_s(x3, a1);                               // x3 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            x0 = __lsx_vfadd_s(x0, h0);                               // x0 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |A          ]
+
+            x2 = __lsx_vfmul_s(x2, h2);                               // x2 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            x3 = __lsx_vfmul_s(x3, h3);                               // x3 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            x1 = __lsx_vfadd_s(x1, h1);                               // x1 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |A          ]
+
+            x2 = __lsx_vfadd_s(x2, h2);                               // x2 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |A          ]
+            x3 = __lsx_vfadd_s(x3, h3);                               // x3 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |A          ]
+
+            // Store
+            _MM_TRANSPOSE4_PS (x0, x1, x2, x3);
+
+            x1 = __lsx_vfmul_s(x1, pFactor);
+            x2 = __lsx_vfmul_s(x2, pFactor);
+            x3 = __lsx_vfmul_s(x3, pFactor);
+
+            x1 = __lsx_vfmul_s(x1, pMul);
+            x2 = __lsx_vfmul_s(x2, pMul);
+            x3 = __lsx_vfmul_s(x3, pMul);
+
+            x1 = __lsx_vfadd_s(x1, pAdd);
+            x2 = __lsx_vfadd_s(x2, pAdd);
+            x3 = __lsx_vfadd_s(x3, pAdd);
+
+            __lsx_vst(x1, dstPtrTempR, 0);
+            __lsx_vst(x2, dstPtrTempG, 0);
+            __lsx_vst(x3, dstPtrTempB, 0);
+
+            srcPtrTempR += 4;
+            srcPtrTempG += 4;
+            srcPtrTempB += 4;
+            dstPtrTempR += 4;
+            dstPtrTempG += 4;
+            dstPtrTempB += 4;
+        }
+        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+        {
+            // RGB to HSV
+
+            Rpp32f hue, sat, val;
+            Rpp32f rf, gf, bf, cmax, cmin, delta;
+            rf = *srcPtrTempR / 255;
+            gf = *srcPtrTempG / 255;
+            bf = *srcPtrTempB / 255;
+            cmax = RPPMAX3(rf, gf, bf);
+            cmin = RPPMIN3(rf, gf, bf);
+            delta = cmax - cmin;
+
+            if (delta == 0)
+            {
+                hue = 0;
+            }
+            else if (cmax == rf)
+            {
+                hue = round(60 * fmod(((gf - bf) / delta),6));
+            }
+            else if (cmax == gf)
+            {
+                hue = round(60 * (((bf - rf) / delta) + 2));
+            }
+            else if (cmax == bf)
+            {
+                hue = round(60 * (((rf - gf) / delta) + 4));
+            }
+
+            while (hue >= 360)
+            {
+                hue = hue - 360;
+            }
+            while (hue < 0)
+            {
+                hue = 360 + hue;
+            }
+
+            if (cmax == 0)
+            {
+                sat = 0;
+            }
+            else
+            {
+                sat = delta / cmax;
+            }
+
+            val = cmax;
+
+            // Modify Hue and Saturation
+
+            hue = hue + hueShiftAngle;
+            while (hue >= 360)
+            {
+                hue = hue - 360;
+            }
+            while (hue < 0)
+            {
+                hue = 360 + hue;
+            }
+
+            sat *= saturationFactor;
+            sat = (sat < (Rpp32f) 1) ? sat : ((Rpp32f) 1);
+            sat = (sat > (Rpp32f) 0) ? sat : ((Rpp32f) 0);
+
+            // HSV to RGB
+
+            Rpp32f c, x, m;
+            c = val * sat;
+            x = c * (1 - abs(int(fmod((hue / 60), 2)) - 1));
+            m = val - c;
+
+            if ((0 <= hue) && (hue < 60))
+            {
+                rf = c;
+                gf = x;
+                bf = 0;
+            }
+            else if ((60 <= hue) && (hue < 120))
+            {
+                rf = x;
+                gf = c;
+                bf = 0;
+            }
+            else if ((120 <= hue) && (hue < 180))
+            {
+                rf = 0;
+                gf = c;
+                bf = x;
+            }
+            else if ((180 <= hue) && (hue < 240))
+            {
+                rf = 0;
+                gf = x;
+                bf = c;
+            }
+            else if ((240 <= hue) && (hue < 300))
+            {
+                rf = x;
+                gf = 0;
+                bf = c;
+            }
+            else if ((300 <= hue) && (hue < 360))
+            {
+                rf = c;
+                gf = 0;
+                bf = x;
+            }
+
+            *dstPtrTempR = round((rf + m) * 255);
+            *dstPtrTempG = round((gf + m) * 255);
+            *dstPtrTempB = round((bf + m) * 255);
+
+            srcPtrTempR++;
+            srcPtrTempG++;
+            srcPtrTempB++;
+            dstPtrTempR++;
+            dstPtrTempG++;
+            dstPtrTempB++;
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        Rpp32f *srcPtrTempPx0, *srcPtrTempPx1, *srcPtrTempPx2, *srcPtrTempPx3;
+        Rpp32f *dstPtrTempPx0, *dstPtrTempPx1, *dstPtrTempPx2, *dstPtrTempPx3;
+
+        srcPtrTempPx0 = srcPtr;
+        srcPtrTempPx1 = srcPtr + 3;
+        srcPtrTempPx2 = srcPtr + 6;
+        srcPtrTempPx3 = srcPtr + 9;
+        dstPtrTempPx0 = dstPtr;
+        dstPtrTempPx1 = dstPtr + 3;
+        dstPtrTempPx2 = dstPtr + 6;
+        dstPtrTempPx3 = dstPtr + 9;
+
+        Rpp64u bufferLength = totalImageDim;
+        Rpp64u alignedLength = (bufferLength / 12) * 12;
+
+        __m128 pZeros = lsx_set1_f32(0.0);
+        __m128 pOnes = lsx_set1_f32(1.0);
+        __m128 pFactor = lsx_set1_f32(255.0);
+        __m128 pHueShift = lsx_set1_f32(hueShift);
+        __m128 pSaturationFactor = lsx_set1_f32(saturationFactor);
+        __m128 xR, xG, xB, xA;
+        __m128 xH, xS, xV, xC;
+        __m128 xX, xY, xZ;
+        __m128 h0, h1, h2, h3;
+        __m128 x0, x1, x2, x3;
+        __m128 a0, a1;
+        h0 = lsx_set1_f32(1.0);
+
+        __m128 pMul = lsx_set1_f32(alpha);
+        __m128 pAdd = lsx_set1_f32(beta);
+
+        Rpp64u vectorLoopCount = 0;
+        for (; vectorLoopCount < alignedLength; vectorLoopCount+=12)
+        {
+            xR = (__m128)__lsx_vld(srcPtrTempPx0, 0);
+            xG = (__m128)__lsx_vld(srcPtrTempPx1, 0);
+            xB = (__m128)__lsx_vld(srcPtrTempPx2, 0);
+            xA = (__m128)__lsx_vld(srcPtrTempPx3, 0);
+
+            _MM_TRANSPOSE4_PS (xR, xG, xB, xA);
+
+            xR = __lsx_vfdiv_s(xR, pFactor);
+            xG = __lsx_vfdiv_s(xG, pFactor);
+            xB = __lsx_vfdiv_s(xB, pFactor);
+
+            // Calculate Saturation, Value, Chroma
+            xS = __lsx_vfmax_s(xG, xB);                               // xS <- [max(G, B)]
+            xC = __lsx_vfmin_s(xG, xB);                               // xC <- [min(G, B)]
+
+            xS = __lsx_vfmax_s(xS, xR);                               // xS <- [max(G, B, R)]
+            xC = __lsx_vfmin_s(xC, xR);                               // xC <- [min(G, B, R)]
+
+            xV = xS;                                               // xV <- [V    ]
+            xS = __lsx_vfsub_s(xS, xC);                               // xS <- [V - m]
+            xS = __lsx_vfdiv_s(xS, xV);                               // xS <- [S    ]
+
+            xC = __lsx_vfsub_s(xC, xV);                               // xC <- [V + m]
+
+            // Calculate Hue
+            xZ = (__m128)__lsx_vfcmp_ceq_s(xV, xG);                             // xZ <- [V==G]
+            xX = (__m128)__lsx_vfcmp_cune_s(xV, xR);                            // xX <- [V!=R]
+
+            xY = (__m128)__lsx_vand_v((__m128i)xZ, (__m128i)xX);                               // xY <- [V!=R && V==G]
+            xZ = (__m128)__lsx_vandn_v((__m128i)xZ, (__m128i)xX);                            // xZ <- [V!=R && V!=G]
+
+            xY = (__m128)__lsx_vxor_v((__m128i)xY, (__m128i)SIMD_GET_PS(full));                // xY <- [V==R || V!=G]
+            xZ = (__m128)__lsx_vxor_v((__m128i)xZ, (__m128i)SIMD_GET_PS(full));                // xZ <- [V==R || V==G]
+
+            xR = (__m128)__lsx_vand_v((__m128i)xR, (__m128i)xX);                               // xR <- [X!=0 ? R : 0]
+            xB = (__m128)__lsx_vand_v((__m128i)xB, (__m128i)xZ);                               // xB <- [Z!=0 ? B : 0]
+            xG = (__m128)__lsx_vand_v((__m128i)xG, (__m128i)xY);                               // xG <- [Y!=0 ? G : 0]
+
+            xZ = (__m128)__lsx_vandn_v((__m128i)xZ, (__m128i)SIMD_GET_PS(sn));               // xZ <- [sign(!Z)]
+            xY = (__m128)__lsx_vandn_v((__m128i)xY, (__m128i)SIMD_GET_PS(sn));               // xY <- [sign(!Y)]
+
+            xG = (__m128)__lsx_vxor_v((__m128i)xG, (__m128i)xZ);                               // xG <- [Y!=0 ? (Z==0 ? G : -G) : 0]
+            xR = (__m128)__lsx_vxor_v((__m128i)xR, (__m128i)xY);                               // xR <- [X!=0 ? (Y==0 ? R : -R) : 0]
+
+            // G is the accumulator
+            xG = __lsx_vfadd_s(xG, xR);                               // xG <- [Rx + Gx]
+            xB = (__m128)__lsx_vxor_v((__m128i)xB, (__m128i)xY);                               // xB <- [Z!=0 ? (Y==0 ? B : -B) : 0]
+
+            xC = __lsx_vfmul_s(xC, SIMD_GET_PS(m6_m6_m6_m6));         // xC <- [C*6     ]
+            xG = __lsx_vfsub_s(xG, xB);                               // xG <- [Rx+Gx+Bx]
+
+            xH = (__m128)__lsx_vand_v((__m128i)xX, (__m128i)SIMD_GET_PS(m4o6_m4o6_m4o6_m4o6)); // xH <- [V==R ?0 :-4/6]
+            xG = __lsx_vfdiv_s(xG, xC);                               // xG <- [(Rx+Gx+Bx)/6C]
+
+            // Correct achromatic cases (H/S may be infinite due to zero division)
+            xH = (__m128)__lsx_vxor_v((__m128i)xH, (__m128i)xZ);                               // xH <- [V==R ? 0 : V==G ? -4/6 : 4/6]
+            xC = (__m128)__lsx_vfcmp_cle_s(SIMD_GET_PS(eps), xC);
+            xH = __lsx_vfadd_s(xH, SIMD_GET_PS(p1));                  // xH <- [V==R ? 1 : V==G ?  2/6 :10/6]
+
+            xG = __lsx_vfadd_s(xG, xH);
+
+            // Normalize H to fraction
+            xH = (__m128)__lsx_vfcmp_cle_s(SIMD_GET_PS(p1), xG);
+
+            xH = (__m128)__lsx_vand_v((__m128i)xH, (__m128i)SIMD_GET_PS(p1));
+            xS = (__m128)__lsx_vand_v((__m128i)xS, (__m128i)xC);
+            xG = (__m128)__lsx_vand_v((__m128i)xG, (__m128i)xC);
+            xG = __lsx_vfsub_s(xG, xH);
+
+            // Modify Hue Values and re-normalize H to fraction
+            xG = __lsx_vfadd_s(xG, pHueShift);
+
+            xH = (__m128)__lsx_vfcmp_cle_s(SIMD_GET_PS(p1), xG);
+            xH = (__m128)__lsx_vand_v((__m128i)xH, (__m128i)SIMD_GET_PS(p1));
+
+            xG = __lsx_vfsub_s(xG, xH);
+
+            // Modify Saturation Values
+            xS = __lsx_vfmul_s(xS, pSaturationFactor);
+            xS = __lsx_vfmin_s(xS, pOnes);
+            xS = __lsx_vfmax_s(xS, pZeros);
+
+            _MM_TRANSPOSE4_PS (h0, xG, xS, xV);
+
+            __m128 h1, h2, h3;
+
+            h1 = xG;
+            h2 = xS;
+            h3 = xV;
+
+            // Prepare HUE for RGB components (per pixel).
+            x0 = SIMD_SHUFFLE_PS(h0, _MM_SHUFFLE(1, 1, 1, 3));     // x0 <- [H           |H           |H           |V          ]
+            x1 = SIMD_SHUFFLE_PS(h1, _MM_SHUFFLE(1, 1, 1, 3));     // x1 <- [H           |H           |H           |V          ]
+            x2 = SIMD_SHUFFLE_PS(h2, _MM_SHUFFLE(1, 1, 1, 3));     // x2 <- [H           |H           |H           |V          ]
+            x3 = SIMD_SHUFFLE_PS(h3, _MM_SHUFFLE(1, 1, 1, 3));     // x3 <- [H           |H           |H           |V          ]
+
+            // Calculate intervals from HUE.
+            x0 = __lsx_vfsub_s(x0, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x0 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+            x1 = __lsx_vfsub_s(x1, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x1 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+            x2 = __lsx_vfsub_s(x2, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x2 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+            x3 = __lsx_vfsub_s(x3, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x3 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+
+            x0 = (__m128)__lsx_vand_v((__m128i)x0, (__m128i)SIMD_GET_PS(abs));                 // x0 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+            x1 = (__m128)__lsx_vand_v((__m128i)x1, (__m128i)SIMD_GET_PS(abs));                 // x1 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+            x2 = (__m128)__lsx_vand_v((__m128i)x2, (__m128i)SIMD_GET_PS(abs));                 // x2 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+            x3 = (__m128)__lsx_vand_v((__m128i)x3, (__m128i)SIMD_GET_PS(abs));                 // x3 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+
+            x0 = __lsx_vfmul_s(x0, SIMD_GET_PS(m6_m6_p6_p0));         // x0 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+            x1 = __lsx_vfmul_s(x1, SIMD_GET_PS(m6_m6_p6_p0));         // x1 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+            x2 = __lsx_vfmul_s(x2, SIMD_GET_PS(m6_m6_p6_p0));         // x2 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+            x3 = __lsx_vfmul_s(x3, SIMD_GET_PS(m6_m6_p6_p0));         // x3 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+
+            x0 = __lsx_vfadd_s(x0, SIMD_GET_PS(p1_p1_m2_p0));         // x0 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+            x1 = __lsx_vfadd_s(x1, SIMD_GET_PS(p1_p1_m2_p0));         // x1 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+            x2 = __lsx_vfadd_s(x2, SIMD_GET_PS(p1_p1_m2_p0));         // x2 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+            x3 = __lsx_vfadd_s(x3, SIMD_GET_PS(p1_p1_m2_p0));         // x3 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+
+            // Bound intervals.
+            x0 = __lsx_vfmax_s(x0, SIMD_GET_PS(m1_m1_m1_p1));
+            x1 = __lsx_vfmax_s(x1, SIMD_GET_PS(m1_m1_m1_p1));
+            x2 = __lsx_vfmax_s(x2, SIMD_GET_PS(m1_m1_m1_p1));
+            x3 = __lsx_vfmax_s(x3, SIMD_GET_PS(m1_m1_m1_p1));
+
+            x0 = __lsx_vfmin_s(x0, SIMD_GET_PS(p0));                  // x0 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+            x1 = __lsx_vfmin_s(x1, SIMD_GET_PS(p0));                  // x1 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+            x2 = __lsx_vfmin_s(x2, SIMD_GET_PS(p0));                  // x2 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+            x3 = __lsx_vfmin_s(x3, SIMD_GET_PS(p0));                  // x3 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+
+            // Prepare S/V vectors.
+            a0 = SIMD_SHUFFLE_PS(h0, _MM_SHUFFLE(2, 2, 2, 2));     // a0 <- [S           |S           |S           |S          ]
+            a1 = SIMD_SHUFFLE_PS(h1, _MM_SHUFFLE(2, 2, 2, 2));     // a1 <- [S           |S           |S           |S          ]
+            h0 = SIMD_SHUFFLE_PS(h0, _MM_SHUFFLE(3, 3, 3, 0));     // h0 <- [V           |V           |V           |A          ]
+            h1 = SIMD_SHUFFLE_PS(h1, _MM_SHUFFLE(3, 3, 3, 0));     // h1 <- [V           |V           |V           |A          ]
+
+            // Multiply with 'S*V' and add 'V'.
+            x0 = __lsx_vfmul_s(x0, a0);                               // x0 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            x1 = __lsx_vfmul_s(x1, a1);                               // x1 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            a0 = SIMD_SHUFFLE_PS(h2, _MM_SHUFFLE(2, 2, 2, 2));     // a0 <- [S           |S           |S           |S          ]
+            a1 = SIMD_SHUFFLE_PS(h3, _MM_SHUFFLE(2, 2, 2, 2));     // a1 <- [S           |S           |S           |S          ]
+
+            x0 = __lsx_vfmul_s(x0, h0);                               // x0 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            x1 = __lsx_vfmul_s(x1, h1);                               // x1 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            h2 = SIMD_SHUFFLE_PS(h2, _MM_SHUFFLE(3, 3, 3, 0));     // h2 <- [V           |V           |V           |A          ]
+            h3 = SIMD_SHUFFLE_PS(h3, _MM_SHUFFLE(3, 3, 3, 0));     // h3 <- [V           |V           |V           |A          ]
+
+            x2 = __lsx_vfmul_s(x2, a0);                               // x2 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            x3 = __lsx_vfmul_s(x3, a1);                               // x3 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            x0 = __lsx_vfadd_s(x0, h0);                               // x0 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |A          ]
+
+            x2 = __lsx_vfmul_s(x2, h2);                               // x2 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            x3 = __lsx_vfmul_s(x3, h3);                               // x3 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            x1 = __lsx_vfadd_s(x1, h1);                               // x1 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |A          ]
+
+            x2 = __lsx_vfadd_s(x2, h2);                               // x2 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |A          ]
+            x3 = __lsx_vfadd_s(x3, h3);                               // x3 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |A          ]
+
+            // Store
+            x0 = (__m128)__lsx_vpermi_w((__m128i)x0, (__m128i)x0, _MM_SHUFFLE(0,3,2,1));
+            x1 = (__m128)__lsx_vpermi_w((__m128i)x1, (__m128i)x1, _MM_SHUFFLE(0,3,2,1));
+            x2 = (__m128)__lsx_vpermi_w((__m128i)x2, (__m128i)x2, _MM_SHUFFLE(0,3,2,1));
+            x3 = (__m128)__lsx_vpermi_w((__m128i)x3, (__m128i)x3, _MM_SHUFFLE(0,3,2,1));
+
+            x0 = __lsx_vfmul_s(x0, pFactor);
+            x1 = __lsx_vfmul_s(x1, pFactor);
+            x2 = __lsx_vfmul_s(x2, pFactor);
+            x3 = __lsx_vfmul_s(x3, pFactor);
+
+            x0 = __lsx_vfmul_s(x0, pMul);
+            x1 = __lsx_vfmul_s(x1, pMul);
+            x2 = __lsx_vfmul_s(x2, pMul);
+            x3 = __lsx_vfmul_s(x3, pMul);
+
+            x0 = __lsx_vfadd_s(x0, pAdd);
+            x1 = __lsx_vfadd_s(x1, pAdd);
+            x2 = __lsx_vfadd_s(x2, pAdd);
+            x3 = __lsx_vfadd_s(x3, pAdd);
+
+            __lsx_vst(x0, dstPtrTempPx0, 0);
+            __lsx_vst(x1, dstPtrTempPx1, 0);
+            __lsx_vst(x2, dstPtrTempPx2, 0);
+            __lsx_vst(x3, dstPtrTempPx3, 0);
+
+            srcPtrTempPx0 += 12;
+            srcPtrTempPx1 += 12;
+            srcPtrTempPx2 += 12;
+            srcPtrTempPx3 += 12;
+            dstPtrTempPx0 += 12;
+            dstPtrTempPx1 += 12;
+            dstPtrTempPx2 += 12;
+            dstPtrTempPx3 += 12;
+        }
+        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+        {
+            Rpp32f *srcPtrTempR, *srcPtrTempG, *srcPtrTempB;
+            Rpp32f *dstPtrTempR, *dstPtrTempG, *dstPtrTempB;
+
+            srcPtrTempR = srcPtrTempPx0;
+            srcPtrTempG = srcPtrTempPx0 + 1;
+            srcPtrTempB = srcPtrTempPx0 + 2;
+            dstPtrTempR = dstPtrTempPx0;
+            dstPtrTempG = dstPtrTempPx0 + 1;
+            dstPtrTempB = dstPtrTempPx0 + 2;
+
+            // RGB to HSV
+
+            Rpp32f hue, sat, val;
+            Rpp32f rf, gf, bf, cmax, cmin, delta;
+            rf = *srcPtrTempR / 255;
+            gf = *srcPtrTempG / 255;
+            bf = *srcPtrTempB / 255;
+            cmax = RPPMAX3(rf, gf, bf);
+            cmin = RPPMIN3(rf, gf, bf);
+            delta = cmax - cmin;
+
+            if (delta == 0)
+            {
+                hue = 0;
+            }
+            else if (cmax == rf)
+            {
+                hue = round(60 * fmod(((gf - bf) / delta),6));
+            }
+            else if (cmax == gf)
+            {
+                hue = round(60 * (((bf - rf) / delta) + 2));
+            }
+            else if (cmax == bf)
+            {
+                hue = round(60 * (((rf - gf) / delta) + 4));
+            }
+
+            while (hue >= 360)
+            {
+                hue = hue - 360;
+            }
+            while (hue < 0)
+            {
+                hue = 360 + hue;
+            }
+
+            if (cmax == 0)
+            {
+                sat = 0;
+            }
+            else
+            {
+                sat = delta / cmax;
+            }
+
+            val = cmax;
+
+            // Modify Hue and Saturation
+
+            hue = hue + hueShiftAngle;
+            while (hue >= 360)
+            {
+                hue = hue - 360;
+            }
+            while (hue < 0)
+            {
+                hue = 360 + hue;
+            }
+
+            sat *= saturationFactor;
+            sat = (sat < (Rpp32f) 1) ? sat : ((Rpp32f) 1);
+            sat = (sat > (Rpp32f) 0) ? sat : ((Rpp32f) 0);
+
+            // HSV to RGB
+
+            Rpp32f c, x, m;
+            c = val * sat;
+            x = c * (1 - abs(int(fmod((hue / 60), 2)) - 1));
+            m = val - c;
+
+            if ((0 <= hue) && (hue < 60))
+            {
+                rf = c;
+                gf = x;
+                bf = 0;
+            }
+            else if ((60 <= hue) && (hue < 120))
+            {
+                rf = x;
+                gf = c;
+                bf = 0;
+            }
+            else if ((120 <= hue) && (hue < 180))
+            {
+                rf = 0;
+                gf = c;
+                bf = x;
+            }
+            else if ((180 <= hue) && (hue < 240))
+            {
+                rf = 0;
+                gf = x;
+                bf = c;
+            }
+            else if ((240 <= hue) && (hue < 300))
+            {
+                rf = x;
+                gf = 0;
+                bf = c;
+            }
+            else if ((300 <= hue) && (hue < 360))
+            {
+                rf = c;
+                gf = 0;
+                bf = x;
+            }
+
+            *dstPtrTempR = round((rf + m) * 255);
+            *dstPtrTempG = round((gf + m) * 255);
+            *dstPtrTempB = round((bf + m) * 255);
+
+            srcPtrTempPx0 += 3;
+            dstPtrTempPx0 += 3;
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus color_twist_f16_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                         Rpp32f *batch_alpha, Rpp32f *batch_beta,
+                         Rpp32f *batch_hueShift, Rpp32f *batch_saturationFactor,
+                         RppiROI *roiPoints, Rpp32u outputFormatToggle, Rpp32u nbatchSize,
+                         RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp64u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32f hueShift = batch_hueShift[batchCount];
+            Rpp32f saturationFactor = batch_saturationFactor[batchCount];
+            Rpp32f alpha = batch_alpha[batchCount];
+            Rpp32f beta = batch_beta[batchCount];
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            color_twist_f16_host(srcPtrImage, batch_srcSizeMax[batchCount], dstPtrImage, alpha, beta, hueShift, saturationFactor, chnFormat, channel);
+
+            if (outputFormatToggle == 1)
+            {
+                T *dstPtrImageUnpadded = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+                T *dstPtrImageUnpaddedCopy = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width * sizeof(T));
+
+                compute_planar_to_packed_host(dstPtrImageUnpaddedCopy, batch_srcSize[batchCount], dstPtrImageUnpadded, channel);
+
+                memset(dstPtrImage, (T) 0, imageDimMax * channel * sizeof(T));
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImage, RPPI_CHN_PACKED, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+    else if(chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp64u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32f hueShift = batch_hueShift[batchCount];
+            Rpp32f saturationFactor = batch_saturationFactor[batchCount];
+            Rpp32f alpha = batch_alpha[batchCount];
+            Rpp32f beta = batch_beta[batchCount];
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            color_twist_f16_host(srcPtrImage, batch_srcSizeMax[batchCount], dstPtrImage, alpha, beta, hueShift, saturationFactor, chnFormat, channel);
+
+            if (outputFormatToggle == 1)
+            {
+                T *dstPtrImageUnpadded = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+                T *dstPtrImageUnpaddedCopy = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width * sizeof(T));
+
+                compute_packed_to_planar_host(dstPtrImageUnpaddedCopy, batch_srcSize[batchCount], dstPtrImageUnpadded, channel);
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImage, RPPI_CHN_PLANAR, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+RppStatus color_twist_f16_host(Rpp16f* srcPtr, RppiSize srcSize, Rpp16f* dstPtr,
+                    Rpp32f alpha, Rpp32f beta,
+                    Rpp32f hueShift, Rpp32f saturationFactor,
+                    RppiChnFormat chnFormat, Rpp32u channel)
+{
+    hueShift = (int)hueShift % 360;
+    Rpp32f hueShiftAngle = hueShift;
+    hueShift *= 0.002778f;
+
+    if (hueShift < 0)
+    {
+        hueShift += 1;
+        hueShiftAngle += 360;
+    }
+
+    Rpp64u totalImageDim = channel * srcSize.height * srcSize.width;
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        Rpp16f *srcPtrTempR, *srcPtrTempG, *srcPtrTempB;
+        Rpp16f *dstPtrTempR, *dstPtrTempG, *dstPtrTempB;
+
+        Rpp32f srcPtrTempRps[4], srcPtrTempGps[4], srcPtrTempBps[4];
+        Rpp32f dstPtrTempRps[4], dstPtrTempGps[4], dstPtrTempBps[4];
+
+        Rpp64u imageDim = srcSize.height * srcSize.width;
+        srcPtrTempR = srcPtr;
+        srcPtrTempG = srcPtr + (imageDim);
+        srcPtrTempB = srcPtr + (2 * imageDim);
+        dstPtrTempR = dstPtr;
+        dstPtrTempG = dstPtr + (imageDim);
+        dstPtrTempB = dstPtr + (2 * imageDim);
+
+        Rpp64u bufferLength = srcSize.height * srcSize.width;
+        Rpp64u alignedLength = bufferLength & ~3;
+
+        __m128 pZeros = lsx_set1_f32(0.0);
+        __m128 pOnes = lsx_set1_f32(1.0);
+        __m128 pFactor = lsx_set1_f32(255.0);
+        __m128 pHueShift = lsx_set1_f32(hueShift);
+        __m128 pSaturationFactor = lsx_set1_f32(saturationFactor);
+        __m128 xR, xG, xB;
+        __m128 xH, xS, xV, xC;
+        __m128 xX, xY, xZ;
+        __m128 h0, h1, h2, h3;
+        __m128 x0, x1, x2, x3;
+        __m128 a0, a1;
+        h0 = lsx_set1_f32(1.0);
+
+        __m128 pMul = lsx_set1_f32(alpha);
+        __m128 pAdd = lsx_set1_f32(beta);
+
+        Rpp64u vectorLoopCount = 0;
+        for (; vectorLoopCount < alignedLength; vectorLoopCount+=4)
+        {
+            for(int cnt = 0; cnt < 4; cnt++)
+            {
+                *(srcPtrTempRps + cnt) = (Rpp32f) (*(srcPtrTempR + cnt));
+                *(srcPtrTempGps + cnt) = (Rpp32f) (*(srcPtrTempG + cnt));
+                *(srcPtrTempBps + cnt) = (Rpp32f) (*(srcPtrTempB + cnt));
+            }
+
+            xR = (__m128)__lsx_vld(srcPtrTempRps, 0);
+            xG = (__m128)__lsx_vld(srcPtrTempGps, 0);
+            xB = (__m128)__lsx_vld(srcPtrTempBps, 0);
+
+            xR = __lsx_vfdiv_s(xR, pFactor);
+            xG = __lsx_vfdiv_s(xG, pFactor);
+            xB = __lsx_vfdiv_s(xB, pFactor);
+
+            // Calculate Saturation, Value, Chroma
+            xS = __lsx_vfmax_s(xG, xB);                               // xS <- [max(G, B)]
+            xC = __lsx_vfmin_s(xG, xB);                               // xC <- [min(G, B)]
+
+            xS = __lsx_vfmax_s(xS, xR);                               // xS <- [max(G, B, R)]
+            xC = __lsx_vfmin_s(xC, xR);                               // xC <- [min(G, B, R)]
+
+            xV = xS;                                               // xV <- [V    ]
+            xS = __lsx_vfsub_s(xS, xC);                               // xS <- [V - m]
+            xS = __lsx_vfdiv_s(xS, xV);                               // xS <- [S    ]
+
+            xC = __lsx_vfsub_s(xC, xV);                               // xC <- [V + m]
+
+            // Calculate Hue
+            xZ = (__m128)__lsx_vfcmp_ceq_s(xV, xG);                             // xZ <- [V==G]
+            xX = (__m128)__lsx_vfcmp_cune_s(xV, xR);                            // xX <- [V!=R]
+
+            xY = (__m128)__lsx_vand_v((__m128i)xZ, (__m128i)xX);                               // xY <- [V!=R && V==G]
+            xZ = (__m128)__lsx_vandn_v((__m128i)xZ, (__m128i)xX);                            // xZ <- [V!=R && V!=G]
+
+            xY = (__m128)__lsx_vxor_v((__m128i)xY, (__m128i)SIMD_GET_PS(full));                // xY <- [V==R || V!=G]
+            xZ = (__m128)__lsx_vxor_v((__m128i)xZ, (__m128i)SIMD_GET_PS(full));                // xZ <- [V==R || V==G]
+
+            xR = (__m128)__lsx_vand_v((__m128i)xR, (__m128i)xX);                               // xR <- [X!=0 ? R : 0]
+            xB = (__m128)__lsx_vand_v((__m128i)xB, (__m128i)xZ);                               // xB <- [Z!=0 ? B : 0]
+            xG = (__m128)__lsx_vand_v((__m128i)xG, (__m128i)xY);                               // xG <- [Y!=0 ? G : 0]
+            xZ = (__m128)__lsx_vandn_v((__m128i)xZ, (__m128i)SIMD_GET_PS(sn));               // xZ <- [sign(!Z)]
+            xY = (__m128)__lsx_vandn_v((__m128i)xY, (__m128i)SIMD_GET_PS(sn));               // xY <- [sign(!Y)]
+
+            xG = (__m128)__lsx_vxor_v((__m128i)xG, (__m128i)xZ);                               // xG <- [Y!=0 ? (Z==0 ? G : -G) : 0]
+            xR = (__m128)__lsx_vxor_v((__m128i)xR, (__m128i)xY);                               // xR <- [X!=0 ? (Y==0 ? R : -R) : 0]
+
+            // G is the accumulator
+            xG = __lsx_vfadd_s(xG, xR);                               // xG <- [Rx + Gx]
+            xB = (__m128)__lsx_vxor_v((__m128i)xB, (__m128i)xY);                               // xB <- [Z!=0 ? (Y==0 ? B : -B) : 0]
+
+            xC = __lsx_vfmul_s(xC, SIMD_GET_PS(m6_m6_m6_m6));         // xC <- [C*6     ]
+            xG = __lsx_vfsub_s(xG, xB);                               // xG <- [Rx+Gx+Bx]
+
+            xH = (__m128)__lsx_vand_v((__m128i)xX, (__m128i)SIMD_GET_PS(m4o6_m4o6_m4o6_m4o6)); // xH <- [V==R ?0 :-4/6]
+            xG = __lsx_vfdiv_s(xG, xC);                               // xG <- [(Rx+Gx+Bx)/6C]
+
+            // Correct achromatic cases (H/S may be infinite due to zero division)
+            xH = (__m128)__lsx_vxor_v((__m128i)xH, (__m128i)xZ);                               // xH <- [V==R ? 0 : V==G ? -4/6 : 4/6]
+            xC = (__m128)__lsx_vfcmp_cle_s(SIMD_GET_PS(eps), xC);
+            xH = __lsx_vfadd_s(xH, SIMD_GET_PS(p1));                  // xH <- [V==R ? 1 : V==G ?  2/6 :10/6]
+
+            xG = __lsx_vfadd_s(xG, xH);
+
+            // Normalize H to fraction
+            xH = (__m128)__lsx_vfcmp_cle_s(SIMD_GET_PS(p1), xG);
+
+            xH = (__m128)__lsx_vand_v((__m128i)xH, (__m128i)SIMD_GET_PS(p1));
+            xS = (__m128)__lsx_vand_v((__m128i)xS, (__m128i)xC);
+            xG = (__m128)__lsx_vand_v((__m128i)xG, (__m128i)xC);
+            xG = __lsx_vfsub_s(xG, xH);
+
+            // Modify Hue Values and re-normalize H to fraction
+            xG = __lsx_vfadd_s(xG, pHueShift);
+
+            xH = (__m128)__lsx_vfcmp_cle_s(SIMD_GET_PS(p1), xG);
+            xH = (__m128)__lsx_vand_v((__m128i)xH, (__m128i)SIMD_GET_PS(p1));
+
+            xG = __lsx_vfsub_s(xG, xH);
+
+            // Modify Saturation Values
+            xS = __lsx_vfmul_s(xS, pSaturationFactor);
+            xS = __lsx_vfmin_s(xS, pOnes);
+            xS = __lsx_vfmax_s(xS, pZeros);
+
+            _MM_TRANSPOSE4_PS (h0, xG, xS, xV);
+
+            __m128 h1, h2, h3;
+
+            h1 = xG;
+            h2 = xS;
+            h3 = xV;
+
+            // Prepare HUE for RGB components (per pixel).
+            x0 = SIMD_SHUFFLE_PS(h0, _MM_SHUFFLE(1, 1, 1, 3));     // x0 <- [H           |H           |H           |V          ]
+            x1 = SIMD_SHUFFLE_PS(h1, _MM_SHUFFLE(1, 1, 1, 3));     // x1 <- [H           |H           |H           |V          ]
+            x2 = SIMD_SHUFFLE_PS(h2, _MM_SHUFFLE(1, 1, 1, 3));     // x2 <- [H           |H           |H           |V          ]
+            x3 = SIMD_SHUFFLE_PS(h3, _MM_SHUFFLE(1, 1, 1, 3));     // x3 <- [H           |H           |H           |V          ]
+
+            // Calculate intervals from HUE.
+            x0 = __lsx_vfsub_s(x0, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x0 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+            x1 = __lsx_vfsub_s(x1, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x1 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+            x2 = __lsx_vfsub_s(x2, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x2 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+            x3 = __lsx_vfsub_s(x3, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x3 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+
+            x0 = (__m128)__lsx_vand_v((__m128i)x0, (__m128i)SIMD_GET_PS(abs));                 // x0 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+            x1 = (__m128)__lsx_vand_v((__m128i)x1, (__m128i)SIMD_GET_PS(abs));                 // x1 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+            x2 = (__m128)__lsx_vand_v((__m128i)x2, (__m128i)SIMD_GET_PS(abs));                 // x2 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+            x3 = (__m128)__lsx_vand_v((__m128i)x3, (__m128i)SIMD_GET_PS(abs));                 // x3 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+
+            x0 = __lsx_vfmul_s(x0, SIMD_GET_PS(m6_m6_p6_p0));         // x0 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+            x1 = __lsx_vfmul_s(x1, SIMD_GET_PS(m6_m6_p6_p0));         // x1 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+            x2 = __lsx_vfmul_s(x2, SIMD_GET_PS(m6_m6_p6_p0));         // x2 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+            x3 = __lsx_vfmul_s(x3, SIMD_GET_PS(m6_m6_p6_p0));         // x3 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+
+            x0 = __lsx_vfadd_s(x0, SIMD_GET_PS(p1_p1_m2_p0));         // x0 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+            x1 = __lsx_vfadd_s(x1, SIMD_GET_PS(p1_p1_m2_p0));         // x1 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+            x2 = __lsx_vfadd_s(x2, SIMD_GET_PS(p1_p1_m2_p0));         // x2 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+            x3 = __lsx_vfadd_s(x3, SIMD_GET_PS(p1_p1_m2_p0));         // x3 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+
+            // Bound intervals.
+            x0 = __lsx_vfmax_s(x0, SIMD_GET_PS(m1_m1_m1_p1));
+            x1 = __lsx_vfmax_s(x1, SIMD_GET_PS(m1_m1_m1_p1));
+            x2 = __lsx_vfmax_s(x2, SIMD_GET_PS(m1_m1_m1_p1));
+            x3 = __lsx_vfmax_s(x3, SIMD_GET_PS(m1_m1_m1_p1));
+
+            x0 = __lsx_vfmin_s(x0, SIMD_GET_PS(p0));                  // x0 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+            x1 = __lsx_vfmin_s(x1, SIMD_GET_PS(p0));                  // x1 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+            x2 = __lsx_vfmin_s(x2, SIMD_GET_PS(p0));                  // x2 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+            x3 = __lsx_vfmin_s(x3, SIMD_GET_PS(p0));                  // x3 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+
+            // Prepare S/V vectors.
+            a0 = SIMD_SHUFFLE_PS(h0, _MM_SHUFFLE(2, 2, 2, 2));     // a0 <- [S           |S           |S           |S          ]
+            a1 = SIMD_SHUFFLE_PS(h1, _MM_SHUFFLE(2, 2, 2, 2));     // a1 <- [S           |S           |S           |S          ]
+            h0 = SIMD_SHUFFLE_PS(h0, _MM_SHUFFLE(3, 3, 3, 0));     // h0 <- [V           |V           |V           |A          ]
+            h1 = SIMD_SHUFFLE_PS(h1, _MM_SHUFFLE(3, 3, 3, 0));     // h1 <- [V           |V           |V           |A          ]
+
+            // Multiply with 'S*V' and add 'V'.
+            x0 = __lsx_vfmul_s(x0, a0);                               // x0 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            x1 = __lsx_vfmul_s(x1, a1);                               // x1 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            a0 = SIMD_SHUFFLE_PS(h2, _MM_SHUFFLE(2, 2, 2, 2));     // a0 <- [S           |S           |S           |S          ]
+            a1 = SIMD_SHUFFLE_PS(h3, _MM_SHUFFLE(2, 2, 2, 2));     // a1 <- [S           |S           |S           |S          ]
+
+            x0 = __lsx_vfmul_s(x0, h0);                               // x0 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            x1 = __lsx_vfmul_s(x1, h1);                               // x1 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            h2 = SIMD_SHUFFLE_PS(h2, _MM_SHUFFLE(3, 3, 3, 0));     // h2 <- [V           |V           |V           |A          ]
+            h3 = SIMD_SHUFFLE_PS(h3, _MM_SHUFFLE(3, 3, 3, 0));     // h3 <- [V           |V           |V           |A          ]
+
+            x2 = __lsx_vfmul_s(x2, a0);                               // x2 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            x3 = __lsx_vfmul_s(x3, a1);                               // x3 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            x0 = __lsx_vfadd_s(x0, h0);                               // x0 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |A          ]
+
+            x2 = __lsx_vfmul_s(x2, h2);                               // x2 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            x3 = __lsx_vfmul_s(x3, h3);                               // x3 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            x1 = __lsx_vfadd_s(x1, h1);                               // x1 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |A          ]
+
+            x2 = __lsx_vfadd_s(x2, h2);                               // x2 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |A          ]
+            x3 = __lsx_vfadd_s(x3, h3);                               // x3 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |A          ]
+
+            // Store
+            _MM_TRANSPOSE4_PS (x0, x1, x2, x3);
+
+            x1 = __lsx_vfmul_s(x1, pFactor);
+            x2 = __lsx_vfmul_s(x2, pFactor);
+            x3 = __lsx_vfmul_s(x3, pFactor);
+
+            x1 = __lsx_vfmul_s(x1, pMul);
+            x2 = __lsx_vfmul_s(x2, pMul);
+            x3 = __lsx_vfmul_s(x3, pMul);
+
+            x1 = __lsx_vfadd_s(x1, pAdd);
+            x2 = __lsx_vfadd_s(x2, pAdd);
+            x3 = __lsx_vfadd_s(x3, pAdd);
+
+            __lsx_vst(x1, dstPtrTempRps,0);
+            __lsx_vst(x2, dstPtrTempGps,0);
+            __lsx_vst(x3, dstPtrTempBps,0);
+
+            for(int cnt = 0; cnt < 4; cnt++)
+            {
+                *(dstPtrTempR + cnt) = (Rpp16f) (*(dstPtrTempRps + cnt));
+                *(dstPtrTempG + cnt) = (Rpp16f) (*(dstPtrTempGps + cnt));
+                *(dstPtrTempB + cnt) = (Rpp16f) (*(dstPtrTempBps + cnt));
+            }
+
+            srcPtrTempR += 4;
+            srcPtrTempG += 4;
+            srcPtrTempB += 4;
+            dstPtrTempR += 4;
+            dstPtrTempG += 4;
+            dstPtrTempB += 4;
+        }
+        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+        {
+            // RGB to HSV
+
+            Rpp32f hue, sat, val;
+            Rpp32f rf, gf, bf, cmax, cmin, delta;
+            rf = *srcPtrTempR / 255;
+            gf = *srcPtrTempG / 255;
+            bf = *srcPtrTempB / 255;
+            cmax = RPPMAX3(rf, gf, bf);
+            cmin = RPPMIN3(rf, gf, bf);
+            delta = cmax - cmin;
+
+            if (delta == 0)
+            {
+                hue = 0;
+            }
+            else if (cmax == rf)
+            {
+                hue = round(60 * fmod(((gf - bf) / delta),6));
+            }
+            else if (cmax == gf)
+            {
+                hue = round(60 * (((bf - rf) / delta) + 2));
+            }
+            else if (cmax == bf)
+            {
+                hue = round(60 * (((rf - gf) / delta) + 4));
+            }
+
+            while (hue >= 360)
+            {
+                hue = hue - 360;
+            }
+            while (hue < 0)
+            {
+                hue = 360 + hue;
+            }
+
+            if (cmax == 0)
+            {
+                sat = 0;
+            }
+            else
+            {
+                sat = delta / cmax;
+            }
+
+            val = cmax;
+
+            // Modify Hue and Saturation
+
+            hue = hue + hueShiftAngle;
+            while (hue >= 360)
+            {
+                hue = hue - 360;
+            }
+            while (hue < 0)
+            {
+                hue = 360 + hue;
+            }
+
+            sat *= saturationFactor;
+            sat = (sat < (Rpp32f) 1) ? sat : ((Rpp32f) 1);
+            sat = (sat > (Rpp32f) 0) ? sat : ((Rpp32f) 0);
+
+            // HSV to RGB
+
+            Rpp32f c, x, m;
+            c = val * sat;
+            x = c * (1 - abs(int(fmod((hue / 60), 2)) - 1));
+            m = val - c;
+
+            if ((0 <= hue) && (hue < 60))
+            {
+                rf = c;
+                gf = x;
+                bf = 0;
+            }
+            else if ((60 <= hue) && (hue < 120))
+            {
+                rf = x;
+                gf = c;
+                bf = 0;
+            }
+            else if ((120 <= hue) && (hue < 180))
+            {
+                rf = 0;
+                gf = c;
+                bf = x;
+            }
+            else if ((180 <= hue) && (hue < 240))
+            {
+                rf = 0;
+                gf = x;
+                bf = c;
+            }
+            else if ((240 <= hue) && (hue < 300))
+            {
+                rf = x;
+                gf = 0;
+                bf = c;
+            }
+            else if ((300 <= hue) && (hue < 360))
+            {
+                rf = c;
+                gf = 0;
+                bf = x;
+            }
+
+            *dstPtrTempR = round((rf + m) * 255);
+            *dstPtrTempG = round((gf + m) * 255);
+            *dstPtrTempB = round((bf + m) * 255);
+
+            srcPtrTempR++;
+            srcPtrTempG++;
+            srcPtrTempB++;
+            dstPtrTempR++;
+            dstPtrTempG++;
+            dstPtrTempB++;
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        Rpp16f *srcPtrTempPx0, *srcPtrTempPx1, *srcPtrTempPx2, *srcPtrTempPx3;
+        Rpp16f *dstPtrTempPx0, *dstPtrTempPx1, *dstPtrTempPx2, *dstPtrTempPx3;
+
+        Rpp32f srcPtrTempPx0ps[4], srcPtrTempPx1ps[4], srcPtrTempPx2ps[4], srcPtrTempPx3ps[4];
+        Rpp32f dstPtrTempPx0ps[4], dstPtrTempPx1ps[4], dstPtrTempPx2ps[4], dstPtrTempPx3ps[4];
+
+        srcPtrTempPx0 = srcPtr;
+        srcPtrTempPx1 = srcPtr + 3;
+        srcPtrTempPx2 = srcPtr + 6;
+        srcPtrTempPx3 = srcPtr + 9;
+        dstPtrTempPx0 = dstPtr;
+        dstPtrTempPx1 = dstPtr + 3;
+        dstPtrTempPx2 = dstPtr + 6;
+        dstPtrTempPx3 = dstPtr + 9;
+
+        Rpp64u bufferLength = totalImageDim;
+        Rpp64u alignedLength = (bufferLength / 12) * 12;
+
+        __m128 pZeros = lsx_set1_f32(0.0);
+        __m128 pOnes = lsx_set1_f32(1.0);
+        __m128 pFactor = lsx_set1_f32(255.0);
+        __m128 pHueShift = lsx_set1_f32(hueShift);
+        __m128 pSaturationFactor = lsx_set1_f32(saturationFactor);
+        __m128 xR, xG, xB, xA;
+        __m128 xH, xS, xV, xC;
+        __m128 xX, xY, xZ;
+        __m128 h0, h1, h2, h3;
+        __m128 x0, x1, x2, x3;
+        __m128 a0, a1;
+        h0 = lsx_set1_f32(1.0);
+
+        __m128 pMul = lsx_set1_f32(alpha);
+        __m128 pAdd = lsx_set1_f32(beta);
+
+        Rpp64u vectorLoopCount = 0;
+        for (; vectorLoopCount < alignedLength; vectorLoopCount+=12)
+        {
+            for(int cnt = 0; cnt < 4; cnt++)
+            {
+                *(srcPtrTempPx0ps + cnt) = (Rpp32f) (*(srcPtrTempPx0 + cnt));
+                *(srcPtrTempPx1ps + cnt) = (Rpp32f) (*(srcPtrTempPx1 + cnt));
+                *(srcPtrTempPx2ps + cnt) = (Rpp32f) (*(srcPtrTempPx2 + cnt));
+                *(srcPtrTempPx3ps + cnt) = (Rpp32f) (*(srcPtrTempPx3 + cnt));
+            }
+
+            xR = (__m128)__lsx_vld(srcPtrTempPx0ps, 0);
+            xG = (__m128)__lsx_vld(srcPtrTempPx1ps, 0);
+            xB = (__m128)__lsx_vld(srcPtrTempPx2ps, 0);
+            xA = (__m128)__lsx_vld(srcPtrTempPx3ps, 0);
+
+            _MM_TRANSPOSE4_PS (xR, xG, xB, xA);
+
+            xR = __lsx_vfdiv_s(xR, pFactor);
+            xG = __lsx_vfdiv_s(xG, pFactor);
+            xB = __lsx_vfdiv_s(xB, pFactor);
+
+            // Calculate Saturation, Value, Chroma
+            xS = __lsx_vfmax_s(xG, xB);                               // xS <- [max(G, B)]
+            xC = __lsx_vfmin_s(xG, xB);                               // xC <- [min(G, B)]
+
+            xS = __lsx_vfmax_s(xS, xR);                               // xS <- [max(G, B, R)]
+            xC = __lsx_vfmin_s(xC, xR);                               // xC <- [min(G, B, R)]
+
+            xV = xS;                                               // xV <- [V    ]
+            xS = __lsx_vfsub_s(xS, xC);                               // xS <- [V - m]
+            xS = __lsx_vfdiv_s(xS, xV);                               // xS <- [S    ]
+
+            xC = __lsx_vfsub_s(xC, xV);                               // xC <- [V + m]
+
+            // Calculate Hue
+            xZ = (__m128)__lsx_vfcmp_ceq_s(xV, xG);                             // xZ <- [V==G]
+            xX = (__m128)__lsx_vfcmp_cune_s(xV, xR);                            // xX <- [V!=R]
+
+            xY = (__m128)__lsx_vand_v((__m128i)xZ, (__m128i)xX);                               // xY <- [V!=R && V==G]
+            xZ = (__m128)__lsx_vandn_v((__m128i)xZ, (__m128i)xX);                            // xZ <- [V!=R && V!=G]
+
+            xY = (__m128)__lsx_vxor_v((__m128i)xY, (__m128i)SIMD_GET_PS(full));                // xY <- [V==R || V!=G]
+            xZ = (__m128)__lsx_vxor_v((__m128i)xZ, (__m128i)SIMD_GET_PS(full));                // xZ <- [V==R || V==G]
+
+            xR = (__m128)__lsx_vand_v((__m128i)xR, (__m128i)xX);                               // xR <- [X!=0 ? R : 0]
+            xB = (__m128)__lsx_vand_v((__m128i)xB, (__m128i)xZ);                               // xB <- [Z!=0 ? B : 0]
+            xG = (__m128)__lsx_vand_v((__m128i)xG, (__m128i)xY);                               // xG <- [Y!=0 ? G : 0]
+
+            xZ = (__m128)__lsx_vandn_v((__m128i)xZ, (__m128i)SIMD_GET_PS(sn));               // xZ <- [sign(!Z)]
+            xY = (__m128)__lsx_vandn_v((__m128i)xY, (__m128i)SIMD_GET_PS(sn));               // xY <- [sign(!Y)]
+
+            xG = (__m128)__lsx_vxor_v((__m128i)xG, (__m128i)xZ);                               // xG <- [Y!=0 ? (Z==0 ? G : -G) : 0]
+            xR = (__m128)__lsx_vxor_v((__m128i)xR, (__m128i)xY);                               // xR <- [X!=0 ? (Y==0 ? R : -R) : 0]
+
+            // G is the accumulator
+            xG = __lsx_vfadd_s(xG, xR);                               // xG <- [Rx + Gx]
+            xB = (__m128)__lsx_vxor_v((__m128i)xB, (__m128i)xY);                               // xB <- [Z!=0 ? (Y==0 ? B : -B) : 0]
+
+            xC = __lsx_vfmul_s(xC, SIMD_GET_PS(m6_m6_m6_m6));         // xC <- [C*6     ]
+            xG = __lsx_vfsub_s(xG, xB);                               // xG <- [Rx+Gx+Bx]
+
+            xH = (__m128)__lsx_vand_v((__m128i)xX, (__m128i)SIMD_GET_PS(m4o6_m4o6_m4o6_m4o6)); // xH <- [V==R ?0 :-4/6]
+            xG = __lsx_vfdiv_s(xG, xC);                               // xG <- [(Rx+Gx+Bx)/6C]
+
+            // Correct achromatic cases (H/S may be infinite due to zero division)
+            xH = (__m128)__lsx_vxor_v((__m128i)xH, (__m128i)xZ);                               // xH <- [V==R ? 0 : V==G ? -4/6 : 4/6]
+            xC = (__m128)__lsx_vfcmp_cle_s(SIMD_GET_PS(eps), xC);
+            xH = __lsx_vfadd_s(xH, SIMD_GET_PS(p1));                  // xH <- [V==R ? 1 : V==G ?  2/6 :10/6]
+
+            xG = __lsx_vfadd_s(xG, xH);
+
+            // Normalize H to fraction
+            xH = (__m128)__lsx_vfcmp_cle_s(SIMD_GET_PS(p1), xG);
+
+            xH = (__m128)__lsx_vand_v((__m128i)xH, (__m128i)SIMD_GET_PS(p1));
+            xS = (__m128)__lsx_vand_v((__m128i)xS, (__m128i)xC);
+            xG = (__m128)__lsx_vand_v((__m128i)xG, (__m128i)xC);
+            xG = __lsx_vfsub_s(xG, xH);
+
+            // Modify Hue Values and re-normalize H to fraction
+            xG = __lsx_vfadd_s(xG, pHueShift);
+
+            xH = (__m128)__lsx_vfcmp_cle_s(SIMD_GET_PS(p1), xG);
+            xH = (__m128)__lsx_vand_v((__m128i)xH, (__m128i)SIMD_GET_PS(p1));
+
+            xG = __lsx_vfsub_s(xG, xH);
+
+            // Modify Saturation Values
+            xS = __lsx_vfmul_s(xS, pSaturationFactor);
+            xS = __lsx_vfmin_s(xS, pOnes);
+            xS = __lsx_vfmax_s(xS, pZeros);
+
+            _MM_TRANSPOSE4_PS (h0, xG, xS, xV);
+
+            __m128 h1, h2, h3;
+
+            h1 = xG;
+            h2 = xS;
+            h3 = xV;
+
+            // Prepare HUE for RGB components (per pixel).
+            x0 = SIMD_SHUFFLE_PS(h0, _MM_SHUFFLE(1, 1, 1, 3));     // x0 <- [H           |H           |H           |V          ]
+            x1 = SIMD_SHUFFLE_PS(h1, _MM_SHUFFLE(1, 1, 1, 3));     // x1 <- [H           |H           |H           |V          ]
+            x2 = SIMD_SHUFFLE_PS(h2, _MM_SHUFFLE(1, 1, 1, 3));     // x2 <- [H           |H           |H           |V          ]
+            x3 = SIMD_SHUFFLE_PS(h3, _MM_SHUFFLE(1, 1, 1, 3));     // x3 <- [H           |H           |H           |V          ]
+
+            // Calculate intervals from HUE.
+            x0 = __lsx_vfsub_s(x0, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x0 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+            x1 = __lsx_vfsub_s(x1, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x1 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+            x2 = __lsx_vfsub_s(x2, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x2 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+            x3 = __lsx_vfsub_s(x3, SIMD_GET_PS(p4o6_p2o6_p3o6_p0));   // x3 <- [H-4/6       |H-2/6       |H-3/6       |V          ]
+
+            x0 = (__m128)__lsx_vand_v((__m128i)x0,(__m128i)SIMD_GET_PS(abs));                 // x0 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+            x1 = (__m128)__lsx_vand_v((__m128i)x1,(__m128i)SIMD_GET_PS(abs));                 // x1 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+            x2 = (__m128)__lsx_vand_v((__m128i)x2,(__m128i)SIMD_GET_PS(abs));                 // x2 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+            x3 = (__m128)__lsx_vand_v((__m128i)x3,(__m128i)SIMD_GET_PS(abs));                 // x3 <- [Abs(H-4/6)  |Abs(H-2/6)  |Abs(H-3/6)  |0          ]
+
+            x0 = __lsx_vfmul_s(x0, SIMD_GET_PS(m6_m6_p6_p0));         // x0 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+            x1 = __lsx_vfmul_s(x1, SIMD_GET_PS(m6_m6_p6_p0));         // x1 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+            x2 = __lsx_vfmul_s(x2, SIMD_GET_PS(m6_m6_p6_p0));         // x2 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+            x3 = __lsx_vfmul_s(x3, SIMD_GET_PS(m6_m6_p6_p0));         // x3 <- [-Abs(H*6-4) |-Abs(H*6-2) |Abs(H*6-3)  |0          ]
+
+            x0 = __lsx_vfadd_s(x0, SIMD_GET_PS(p1_p1_m2_p0));         // x0 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+            x1 = __lsx_vfadd_s(x1, SIMD_GET_PS(p1_p1_m2_p0));         // x1 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+            x2 = __lsx_vfadd_s(x2, SIMD_GET_PS(p1_p1_m2_p0));         // x2 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+            x3 = __lsx_vfadd_s(x3, SIMD_GET_PS(p1_p1_m2_p0));         // x3 <- [1-Abs(H*6-4)|1-Abs(H*6-2)|Abs(H*6-3)-2|0          ]
+
+            // Bound intervals.
+            x0 = __lsx_vfmax_s(x0, SIMD_GET_PS(m1_m1_m1_p1));
+            x1 = __lsx_vfmax_s(x1, SIMD_GET_PS(m1_m1_m1_p1));
+            x2 = __lsx_vfmax_s(x2, SIMD_GET_PS(m1_m1_m1_p1));
+            x3 = __lsx_vfmax_s(x3, SIMD_GET_PS(m1_m1_m1_p1));
+
+            x0 = __lsx_vfmin_s(x0, SIMD_GET_PS(p0));                  // x0 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+            x1 = __lsx_vfmin_s(x1, SIMD_GET_PS(p0));                  // x1 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+            x2 = __lsx_vfmin_s(x2, SIMD_GET_PS(p0));                  // x2 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+            x3 = __lsx_vfmin_s(x3, SIMD_GET_PS(p0));                  // x3 <- [(R-1)       |(G-1)       |(B-1)       |0          ]
+
+            // Prepare S/V vectors.
+            a0 = SIMD_SHUFFLE_PS(h0, _MM_SHUFFLE(2, 2, 2, 2));     // a0 <- [S           |S           |S           |S          ]
+            a1 = SIMD_SHUFFLE_PS(h1, _MM_SHUFFLE(2, 2, 2, 2));     // a1 <- [S           |S           |S           |S          ]
+            h0 = SIMD_SHUFFLE_PS(h0, _MM_SHUFFLE(3, 3, 3, 0));     // h0 <- [V           |V           |V           |A          ]
+            h1 = SIMD_SHUFFLE_PS(h1, _MM_SHUFFLE(3, 3, 3, 0));     // h1 <- [V           |V           |V           |A          ]
+
+            // Multiply with 'S*V' and add 'V'.
+            x0 = __lsx_vfmul_s(x0, a0);                               // x0 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            x1 = __lsx_vfmul_s(x1, a1);                               // x1 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            a0 = SIMD_SHUFFLE_PS(h2, _MM_SHUFFLE(2, 2, 2, 2));     // a0 <- [S           |S           |S           |S          ]
+            a1 = SIMD_SHUFFLE_PS(h3, _MM_SHUFFLE(2, 2, 2, 2));     // a1 <- [S           |S           |S           |S          ]
+
+            x0 = __lsx_vfmul_s(x0, h0);                               // x0 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            x1 = __lsx_vfmul_s(x1, h1);                               // x1 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            h2 = SIMD_SHUFFLE_PS(h2, _MM_SHUFFLE(3, 3, 3, 0));     // h2 <- [V           |V           |V           |A          ]
+            h3 = SIMD_SHUFFLE_PS(h3, _MM_SHUFFLE(3, 3, 3, 0));     // h3 <- [V           |V           |V           |A          ]
+
+            x2 = __lsx_vfmul_s(x2, a0);                               // x2 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            x3 = __lsx_vfmul_s(x3, a1);                               // x3 <- [(R-1)*S     |(G-1)*S     |(B-1)*S     |0          ]
+            x0 = __lsx_vfadd_s(x0, h0);                               // x0 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |A          ]
+
+            x2 = __lsx_vfmul_s(x2, h2);                               // x2 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            x3 = __lsx_vfmul_s(x3, h3);                               // x3 <- [(R-1)*S*V   |(G-1)*S*V   |(B-1)*S*V   |0          ]
+            x1 = __lsx_vfadd_s(x1, h1);                               // x1 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |A          ]
+
+            x2 = __lsx_vfadd_s(x2, h2);                               // x2 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |A          ]
+            x3 = __lsx_vfadd_s(x3, h3);                               // x3 <- [(R-1)*S*V+V |(G-1)*S*V+V |(B-1)*S*V+V |A          ]
+
+            // Store
+            x0 = (__m128)__lsx_vpermi_w((__m128i)x0, (__m128i)x0, _MM_SHUFFLE(0,3,2,1));
+            x1 = (__m128)__lsx_vpermi_w((__m128i)x1, (__m128i)x1, _MM_SHUFFLE(0,3,2,1));
+            x2 = (__m128)__lsx_vpermi_w((__m128i)x2, (__m128i)x2, _MM_SHUFFLE(0,3,2,1));
+            x3 = (__m128)__lsx_vpermi_w((__m128i)x3, (__m128i)x3, _MM_SHUFFLE(0,3,2,1));
+
+            x0 = __lsx_vfmul_s(x0, pFactor);
+            x1 = __lsx_vfmul_s(x1, pFactor);
+            x2 = __lsx_vfmul_s(x2, pFactor);
+            x3 = __lsx_vfmul_s(x3, pFactor);
+
+            x0 = __lsx_vfmul_s(x0, pMul);
+            x1 = __lsx_vfmul_s(x1, pMul);
+            x2 = __lsx_vfmul_s(x2, pMul);
+            x3 = __lsx_vfmul_s(x3, pMul);
+
+            x0 = __lsx_vfadd_s(x0, pAdd);
+            x1 = __lsx_vfadd_s(x1, pAdd);
+            x2 = __lsx_vfadd_s(x2, pAdd);
+            x3 = __lsx_vfadd_s(x3, pAdd);
+
+            __lsx_vst(x0, dstPtrTempPx0ps, 0);
+            __lsx_vst(x1, dstPtrTempPx1ps, 0);
+            __lsx_vst(x2, dstPtrTempPx2ps, 0);
+            __lsx_vst(x3, dstPtrTempPx3ps, 0);
+
+            for(int cnt = 0; cnt < 3; cnt++)
+            {
+                *(dstPtrTempPx0 + cnt) = (Rpp16f) (*(dstPtrTempPx0ps + cnt));
+                *(dstPtrTempPx1 + cnt) = (Rpp16f) (*(dstPtrTempPx1ps + cnt));
+                *(dstPtrTempPx2 + cnt) = (Rpp16f) (*(dstPtrTempPx2ps + cnt));
+                *(dstPtrTempPx3 + cnt) = (Rpp16f) (*(dstPtrTempPx3ps + cnt));
+            }
+
+            srcPtrTempPx0 += 12;
+            srcPtrTempPx1 += 12;
+            srcPtrTempPx2 += 12;
+            srcPtrTempPx3 += 12;
+            dstPtrTempPx0 += 12;
+            dstPtrTempPx1 += 12;
+            dstPtrTempPx2 += 12;
+            dstPtrTempPx3 += 12;
+        }
+        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+        {
+            Rpp16f *srcPtrTempR, *srcPtrTempG, *srcPtrTempB;
+            Rpp16f *dstPtrTempR, *dstPtrTempG, *dstPtrTempB;
+
+            srcPtrTempR = srcPtrTempPx0;
+            srcPtrTempG = srcPtrTempPx0 + 1;
+            srcPtrTempB = srcPtrTempPx0 + 2;
+            dstPtrTempR = dstPtrTempPx0;
+            dstPtrTempG = dstPtrTempPx0 + 1;
+            dstPtrTempB = dstPtrTempPx0 + 2;
+
+            // RGB to HSV
+
+            Rpp32f hue, sat, val;
+            Rpp32f rf, gf, bf, cmax, cmin, delta;
+            rf = *srcPtrTempR / 255;
+            gf = *srcPtrTempG / 255;
+            bf = *srcPtrTempB / 255;
+            cmax = RPPMAX3(rf, gf, bf);
+            cmin = RPPMIN3(rf, gf, bf);
+            delta = cmax - cmin;
+
+            if (delta == 0)
+            {
+                hue = 0;
+            }
+            else if (cmax == rf)
+            {
+                hue = round(60 * fmod(((gf - bf) / delta),6));
+            }
+            else if (cmax == gf)
+            {
+                hue = round(60 * (((bf - rf) / delta) + 2));
+            }
+            else if (cmax == bf)
+            {
+                hue = round(60 * (((rf - gf) / delta) + 4));
+            }
+
+            while (hue >= 360)
+            {
+                hue = hue - 360;
+            }
+            while (hue < 0)
+            {
+                hue = 360 + hue;
+            }
+
+            if (cmax == 0)
+            {
+                sat = 0;
+            }
+            else
+            {
+                sat = delta / cmax;
+            }
+
+            val = cmax;
+
+            // Modify Hue and Saturation
+
+            hue = hue + hueShiftAngle;
+            while (hue >= 360)
+            {
+                hue = hue - 360;
+            }
+            while (hue < 0)
+            {
+                hue = 360 + hue;
+            }
+
+            sat *= saturationFactor;
+            sat = (sat < (Rpp32f) 1) ? sat : ((Rpp32f) 1);
+            sat = (sat > (Rpp32f) 0) ? sat : ((Rpp32f) 0);
+
+            // HSV to RGB
+
+            Rpp32f c, x, m;
+            c = val * sat;
+            x = c * (1 - abs(int(fmod((hue / 60), 2)) - 1));
+            m = val - c;
+
+            if ((0 <= hue) && (hue < 60))
+            {
+                rf = c;
+                gf = x;
+                bf = 0;
+            }
+            else if ((60 <= hue) && (hue < 120))
+            {
+                rf = x;
+                gf = c;
+                bf = 0;
+            }
+            else if ((120 <= hue) && (hue < 180))
+            {
+                rf = 0;
+                gf = c;
+                bf = x;
+            }
+            else if ((180 <= hue) && (hue < 240))
+            {
+                rf = 0;
+                gf = x;
+                bf = c;
+            }
+            else if ((240 <= hue) && (hue < 300))
+            {
+                rf = x;
+                gf = 0;
+                bf = c;
+            }
+            else if ((300 <= hue) && (hue < 360))
+            {
+                rf = c;
+                gf = 0;
+                bf = x;
+            }
+
+            *dstPtrTempR = round((rf + m) * 255);
+            *dstPtrTempG = round((gf + m) * 255);
+            *dstPtrTempB = round((bf + m) * 255);
+
+            srcPtrTempPx0 += 3;
+            dstPtrTempPx0 += 3;
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus color_twist_i8_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                         Rpp32f *batch_alpha, Rpp32f *batch_beta,
+                         Rpp32f *batch_hueShift, Rpp32f *batch_saturationFactor,
+                         RppiROI *roiPoints, Rpp32u outputFormatToggle, Rpp32u nbatchSize,
+                         RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp64u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32f hueShift = batch_hueShift[batchCount];
+            Rpp32f saturationFactor = batch_saturationFactor[batchCount];
+            Rpp32f alpha = batch_alpha[batchCount];
+            Rpp32f beta = batch_beta[batchCount];
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            Rpp8u *srcPtrImage8u = (Rpp8u*) calloc(channel * imageDimMax, sizeof(Rpp8u));
+            Rpp8u *dstPtrImage8u = (Rpp8u*) calloc(channel * imageDimMax, sizeof(Rpp8u));
+
+            T *srcPtrImageTemp;
+            srcPtrImageTemp = srcPtrImage;
+
+            Rpp8u *srcPtrImage8uTemp;
+            srcPtrImage8uTemp = srcPtrImage8u;
+
+            for (int i = 0; i < channel * imageDimMax; i++)
+            {
+                *srcPtrImage8uTemp = (Rpp8u) RPPPIXELCHECK(((Rpp32s) *srcPtrImageTemp) + 128);
+                srcPtrImageTemp++;
+                srcPtrImage8uTemp++;
+            }
+
+            color_twist_host(srcPtrImage8u, batch_srcSizeMax[batchCount], dstPtrImage8u, alpha, beta, hueShift, saturationFactor, chnFormat, channel);
+
+            if (outputFormatToggle == 1)
+            {
+                Rpp8u *dstPtrImageUnpadded = (Rpp8u*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(Rpp8u));
+                Rpp8u *dstPtrImageUnpaddedCopy = (Rpp8u*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(Rpp8u));
+
+                compute_unpadded_from_padded_host(dstPtrImage8u, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width * sizeof(Rpp8u));
+
+                compute_planar_to_packed_host(dstPtrImageUnpaddedCopy, batch_srcSize[batchCount], dstPtrImageUnpadded, channel);
+
+                memset(dstPtrImage8u, (Rpp8u) 0, imageDimMax * channel * sizeof(Rpp8u));
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImage8u, RPPI_CHN_PACKED, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+
+            T *dstPtrImageTemp;
+            dstPtrImageTemp = dstPtrImage;
+
+            Rpp8u *dstPtrImage8uTemp;
+            dstPtrImage8uTemp = dstPtrImage8u;
+
+            for (int i = 0; i < channel * imageDimMax; i++)
+            {
+                *dstPtrImageTemp = (Rpp8s) (((Rpp32s) *dstPtrImage8uTemp) - 128);
+                dstPtrImageTemp++;
+                dstPtrImage8uTemp++;
+            }
+
+            free(srcPtrImage8u);
+            free(dstPtrImage8u);
+        }
+    }
+    else if(chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp64u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32f hueShift = batch_hueShift[batchCount];
+            Rpp32f saturationFactor = batch_saturationFactor[batchCount];
+            Rpp32f alpha = batch_alpha[batchCount];
+            Rpp32f beta = batch_beta[batchCount];
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            Rpp8u *srcPtrImage8u = (Rpp8u*) calloc(channel * imageDimMax, sizeof(Rpp8u));
+            Rpp8u *dstPtrImage8u = (Rpp8u*) calloc(channel * imageDimMax, sizeof(Rpp8u));
+
+            T *srcPtrImageTemp;
+            srcPtrImageTemp = srcPtrImage;
+
+            Rpp8u *srcPtrImage8uTemp;
+            srcPtrImage8uTemp = srcPtrImage8u;
+
+            for (int i = 0; i < channel * imageDimMax; i++)
+            {
+                *srcPtrImage8uTemp = (Rpp8u) RPPPIXELCHECK(((Rpp32s) *srcPtrImageTemp) + 128);
+                srcPtrImageTemp++;
+                srcPtrImage8uTemp++;
+            }
+
+            color_twist_host(srcPtrImage8u, batch_srcSizeMax[batchCount], dstPtrImage8u, alpha, beta, hueShift, saturationFactor, chnFormat, channel);
+
+            if (outputFormatToggle == 1)
+            {
+                Rpp8u *dstPtrImageUnpadded = (Rpp8u*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(Rpp8u));
+                Rpp8u *dstPtrImageUnpaddedCopy = (Rpp8u*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(Rpp8u));
+
+                compute_unpadded_from_padded_host(dstPtrImage8u, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width * sizeof(Rpp8u));
+
+                compute_packed_to_planar_host(dstPtrImageUnpaddedCopy, batch_srcSize[batchCount], dstPtrImageUnpadded, channel);
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImage8u, RPPI_CHN_PLANAR, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+
+            T *dstPtrImageTemp;
+            dstPtrImageTemp = dstPtrImage;
+
+            Rpp8u *dstPtrImage8uTemp;
+            dstPtrImage8uTemp = dstPtrImage8u;
+
+            for (int i = 0; i < channel * imageDimMax; i++)
+            {
+                *dstPtrImageTemp = (Rpp8s) (((Rpp32s) *dstPtrImage8uTemp) - 128);
+                dstPtrImageTemp++;
+                dstPtrImage8uTemp++;
+            }
+
+            free(srcPtrImage8u);
+            free(dstPtrImage8u);
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+/**************** crop_mirror ***************/
+
+template <typename T, typename U>
+RppStatus crop_mirror_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, U* dstPtr, RppiSize *batch_dstSize, RppiSize *batch_dstSizeMax,
+                                     Rpp32u *batch_crop_pos_x, Rpp32u *batch_crop_pos_y,
+                                     Rpp32u *batch_mirrorFlag, Rpp32u outputFormatToggle,
+                                     Rpp32u nbatchSize,
+                                     RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u srcImageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+            Rpp32u dstImageDimMax = batch_dstSizeMax[batchCount].height * batch_dstSizeMax[batchCount].width;
+
+            Rpp32u x1 = batch_crop_pos_x[batchCount];
+            Rpp32u y1 = batch_crop_pos_y[batchCount];
+            Rpp32u x2 = x1 + batch_dstSize[batchCount].width - 1;
+            Rpp32u y2 = y1 + batch_dstSize[batchCount].height - 1;
+
+            Rpp32u mirrorFlag = batch_mirrorFlag[batchCount];
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+            compute_image_location_host(batch_dstSizeMax, batchCount, &dstLoc, channel);
+            srcPtrImage = srcPtr + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+
+            if (mirrorFlag == 0)
+            {
+                for(int c = 0; c < channel; c++)
+                {
+                    T *dstPtrChannel, *srcPtrChannelROI;
+                    dstPtrChannel = dstPtrImage + (c * dstImageDimMax);
+                    srcPtrChannelROI = srcPtrImage + (c * srcImageDimMax) + (y1 * batch_srcSizeMax[batchCount].width) + x1;
+
+
+                    for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                    {
+                        memcpy(dstPtrChannel, srcPtrChannelROI, batch_dstSize[batchCount].width * sizeof(T));
+                        dstPtrChannel += batch_dstSizeMax[batchCount].width;
+                        srcPtrChannelROI += batch_srcSizeMax[batchCount].width;
+                    }
+                }
+            }
+            else if (mirrorFlag == 1)
+            {
+                Rpp32u srcROIIncrement = batch_srcSizeMax[batchCount].width + batch_dstSize[batchCount].width;
+                for(int c = 0; c < channel; c++)
+                {
+                    T *dstPtrChannel, *srcPtrChannelROI;
+                    dstPtrChannel = dstPtrImage + (c * dstImageDimMax);
+                    srcPtrChannelROI = srcPtrImage + (c * srcImageDimMax) + (y1 * batch_srcSizeMax[batchCount].width) + x2;
+
+
+                    for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                    {
+                        T *dstPtrTemp;
+                        dstPtrTemp = dstPtrChannel + (i * batch_dstSizeMax[batchCount].width);
+
+                        Rpp32u bufferLength = batch_dstSize[batchCount].width;
+                        Rpp32u alignedLength = (bufferLength / 16) * 16;
+
+                        __m128i px0;
+                        __m128i vMask = lsx_setr_i8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
+
+                        int vectorLoopCount = 0;
+                        for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+                        {
+                            srcPtrChannelROI -= 15;
+                            px0 =  __lsx_vld((__m128i *)srcPtrChannelROI, 0);
+                            px0 = lsx_shuffle_i8(px0, vMask);
+                            __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+                            srcPtrChannelROI -= 1;
+                            dstPtrTemp += 16;
+                        }
+                        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                        {
+                            *dstPtrTemp++ = (T) *srcPtrChannelROI--;
+                        }
+
+                        srcPtrChannelROI += srcROIIncrement;
+                    }
+                }
+            }
+            if (outputFormatToggle == 1)
+            {
+                T *dstPtrImageUnpadded = (T*) calloc(channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width, sizeof(T));
+                T *dstPtrImageUnpaddedCopy = (T*) calloc(channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width, sizeof(T));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_dstSize[batchCount], batch_dstSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width * sizeof(T));
+
+                compute_planar_to_packed_host(dstPtrImageUnpaddedCopy, batch_dstSize[batchCount], dstPtrImageUnpadded, channel);
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_dstSize[batchCount], batch_dstSizeMax[batchCount], dstPtrImage, RPPI_CHN_PACKED, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u srcImageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+            Rpp32u dstImageDimMax = batch_dstSizeMax[batchCount].height * batch_dstSizeMax[batchCount].width;
+
+            Rpp32u x1 = batch_crop_pos_x[batchCount];
+            Rpp32u y1 = batch_crop_pos_y[batchCount];
+            Rpp32u x2 = x1 + batch_dstSize[batchCount].width - 1;
+            Rpp32u y2 = y1 + batch_dstSize[batchCount].height - 1;
+
+            Rpp32u mirrorFlag = batch_mirrorFlag[batchCount];
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+            compute_image_location_host(batch_dstSizeMax, batchCount, &dstLoc, channel);
+            srcPtrImage = srcPtr + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+
+            Rpp32u srcElementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+            Rpp32u dstElementsInRowMax = channel * batch_dstSizeMax[batchCount].width;
+            Rpp32u elementsInRowROI = channel * batch_dstSize[batchCount].width;
+
+            if (mirrorFlag == 0)
+            {
+                T *srcPtrImageROI, *dstPtrImageTemp;
+                srcPtrImageROI = srcPtrImage + (y1 * srcElementsInRowMax) + (x1 * channel);
+                dstPtrImageTemp = dstPtrImage;
+
+
+                for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                {
+                    memcpy(dstPtrImageTemp, srcPtrImageROI, elementsInRowROI * sizeof(T));
+                    dstPtrImageTemp += dstElementsInRowMax;
+                    srcPtrImageROI += srcElementsInRowMax;
+                }
+            }
+            else if (mirrorFlag == 1)
+            {
+                T  *srcPtrROI;
+                srcPtrROI = srcPtrImage + (y1 * srcElementsInRowMax) + ((x2 - 1) * channel);
+
+                Rpp32u srcROIIncrement = srcElementsInRowMax + elementsInRowROI;
+
+
+                for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                {
+                    T *dstPtrTemp;
+                    dstPtrTemp = dstPtrImage + (i * dstElementsInRowMax);
+
+                    Rpp32u bufferLength = elementsInRowROI;
+                    Rpp32u alignedLength = ((bufferLength / 15) * 15) - 1;
+
+                    __m128i px0;
+                    __m128i vMask = lsx_setr_i8(13, 14, 15, 10, 11, 12, 7, 8, 9, 4, 5, 6, 1, 2, 3, 0);
+
+                    int vectorLoopCount = 0;
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount+=15)
+                    {
+                        srcPtrROI -= 13;
+                        px0 =  __lsx_vld((__m128i *)srcPtrROI, 0);
+                        px0 = lsx_shuffle_i8(px0, vMask);
+                        __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+                        srcPtrROI -= 2;
+                        dstPtrTemp += 15;
+                    }
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount+=channel)
+                    {
+                        memcpy(dstPtrTemp, srcPtrROI, channel * sizeof(T));
+                        dstPtrTemp += channel;
+                        srcPtrROI -= channel;
+                    }
+
+                    srcPtrROI += srcROIIncrement;
+                }
+            }
+            if (outputFormatToggle == 1)
+            {
+                T *dstPtrImageUnpadded = (T*) calloc(channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width, sizeof(T));
+                T *dstPtrImageUnpaddedCopy = (T*) calloc(channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width, sizeof(T));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_dstSize[batchCount], batch_dstSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width * sizeof(T));
+
+                compute_packed_to_planar_host(dstPtrImageUnpaddedCopy, batch_dstSize[batchCount], dstPtrImageUnpadded, channel);
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_dstSize[batchCount], batch_dstSizeMax[batchCount], dstPtrImage, RPPI_CHN_PLANAR, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T, typename U>
+RppStatus crop_mirror_host(T* srcPtr, RppiSize srcSize, U* dstPtr, RppiSize dstSize,
+                                     Rpp32u crop_pos_x, Rpp32u crop_pos_y,
+                                     Rpp32u mirrorFlag, Rpp32u outputFormatToggle,
+                                     RppiChnFormat chnFormat, Rpp32u channel)
+{
+    U *dstPtrTemp;
+    dstPtrTemp = dstPtr;
+
+    Rpp32u srcImageDim = srcSize.height * srcSize.width;
+
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        if (mirrorFlag == 0)
+        {
+            for(int c = 0; c < channel; c++)
+            {
+                T *srcPtrChannelROI;
+                srcPtrChannelROI = srcPtr + (c * srcImageDim) + (crop_pos_y * srcSize.width) + crop_pos_x;
+
+                for(int i = 0; i < dstSize.height; i++)
+                {
+                    memcpy(dstPtrTemp, srcPtrChannelROI, dstSize.width * sizeof(T));
+                    dstPtrTemp += dstSize.width;
+                    srcPtrChannelROI += srcSize.width;
+                }
+            }
+        }
+        else if (mirrorFlag == 1)
+        {
+            Rpp32u srcROIIncrement = srcSize.width + dstSize.width;
+            for(int c = 0; c < channel; c++)
+            {
+                T *srcPtrChannelROI;
+                srcPtrChannelROI = srcPtr + (c * srcImageDim) + (crop_pos_y * srcSize.width) + crop_pos_x + dstSize.width - 1;
+
+                for(int i = 0; i < dstSize.height; i++)
+                {
+                    Rpp32u bufferLength = dstSize.width;
+                    Rpp32u alignedLength = (bufferLength / 16) * 16;
+
+                    __m128i px0;
+                    __m128i vMask = lsx_setr_i8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
+
+                    int vectorLoopCount = 0;
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+                    {
+                        srcPtrChannelROI -= 15;
+                        px0 =  __lsx_vld((__m128i *)srcPtrChannelROI, 0);
+                        px0 = lsx_shuffle_i8(px0, vMask);
+
+                        __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+                        srcPtrChannelROI -= 1;
+                        dstPtrTemp += 16;
+                    }
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                    {
+                        *dstPtrTemp++ = *srcPtrChannelROI--;
+                    }
+
+                    srcPtrChannelROI += srcROIIncrement;
+                }
+            }
+        }
+    }
+    else if(chnFormat == RPPI_CHN_PACKED)
+    {
+        Rpp32u srcElementsInRow = channel * srcSize.width;
+        Rpp32u dstElementsInRow = channel * dstSize.width;
+
+        if (mirrorFlag == 0)
+        {
+            T *srcPtrROI;
+            srcPtrROI = srcPtr + (crop_pos_y * srcElementsInRow) + (crop_pos_x * channel);
+
+            for(int i = 0; i < dstSize.height; i++)
+            {
+                memcpy(dstPtrTemp, srcPtrROI, dstElementsInRow * sizeof(T));
+                dstPtrTemp += dstElementsInRow;
+                srcPtrROI += srcElementsInRow;
+            }
+        }
+        else if (mirrorFlag == 1)
+        {
+            T  *srcPtrROI;
+            srcPtrROI = srcPtr + (crop_pos_y * srcElementsInRow) + ((crop_pos_x + dstSize.width - 1) * channel);
+
+            Rpp32u srcROIIncrement = srcElementsInRow + dstElementsInRow;
+
+            for(int i = 0; i < dstSize.height; i++)
+            {
+                Rpp32u bufferLength = dstElementsInRow;
+                Rpp32u alignedLength = ((bufferLength / 15) * 15) - 1;
+
+                __m128i px0;
+                __m128i vMask = lsx_setr_i8(13, 14, 15, 10, 11, 12, 7, 8, 9, 4, 5, 6, 1, 2, 3, 0);
+
+                int vectorLoopCount = 0;
+                for (; vectorLoopCount < alignedLength; vectorLoopCount+=15)
+                {
+                    srcPtrROI -= 13;
+                    px0 =  __lsx_vld((__m128i *)srcPtrROI, 0);
+                    px0 = lsx_shuffle_i8(px0, vMask);
+
+                    __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+                    srcPtrROI -= 2;
+                    dstPtrTemp += 15;
+                }
+                for (; vectorLoopCount < bufferLength; vectorLoopCount+=channel)
+                {
+                    memcpy(dstPtrTemp, srcPtrROI, channel * sizeof(T));
+                    dstPtrTemp += channel;
+                    srcPtrROI -= channel;
+                }
+
+                srcPtrROI += srcROIIncrement;
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+/**************** crop_mirror_normalize ***************/
+
+template <typename T, typename U>
+RppStatus crop_mirror_normalize_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, U* dstPtr, RppiSize *batch_dstSize, RppiSize *batch_dstSizeMax,
+                                     Rpp32u *batch_crop_pos_x, Rpp32u *batch_crop_pos_y,
+                                     Rpp32f *batch_mean, Rpp32f *batch_stdDev,
+                                     Rpp32u *batch_mirrorFlag, Rpp32u outputFormatToggle,
+                                     Rpp32u nbatchSize,
+                                     RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u srcImageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+            Rpp32u dstImageDimMax = batch_dstSizeMax[batchCount].height * batch_dstSizeMax[batchCount].width;
+
+            Rpp32u x1 = batch_crop_pos_x[batchCount];
+            Rpp32u y1 = batch_crop_pos_y[batchCount];
+            Rpp32u x2 = x1 + batch_dstSize[batchCount].width - 1;
+            Rpp32u y2 = y1 + batch_dstSize[batchCount].height - 1;
+
+            Rpp32u mirrorFlag = batch_mirrorFlag[batchCount];
+            Rpp32f mean = batch_mean[batchCount];
+            Rpp32f stdDev = batch_stdDev[batchCount];
+            Rpp32f invStdDev = 1.0 / stdDev;
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+            compute_image_location_host(batch_dstSizeMax, batchCount, &dstLoc, channel);
+            srcPtrImage = srcPtr + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+
+            if (mirrorFlag == 0)
+            {
+                Rpp32u srcROIIncrement = batch_srcSizeMax[batchCount].width - batch_dstSize[batchCount].width;
+                for(int c = 0; c < channel; c++)
+                {
+                    T *dstPtrChannel, *srcPtrChannelROI;
+                    dstPtrChannel = dstPtrImage + (c * dstImageDimMax);
+                    srcPtrChannelROI = srcPtrImage + (c * srcImageDimMax) + (y1 * batch_srcSizeMax[batchCount].width) + x1;
+
+
+                    for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                    {
+                        T *dstPtrTemp;
+                        dstPtrTemp = dstPtrChannel + (i * batch_dstSizeMax[batchCount].width);
+
+                        Rpp32u bufferLength = batch_dstSize[batchCount].width;
+                        Rpp32u alignedLength = (bufferLength / 16) * 16;
+
+                        __m128i const zero = __lsx_vldi(0);
+                        __m128i px0, px1, px2, px3;
+                        __m128 p0, p1, p2, p3;
+                        __m128 vMean = lsx_set1_f32(mean);
+                        __m128 vInvStdDev = lsx_set1_f32(invStdDev);
+
+                        int vectorLoopCount = 0;
+                        for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+                        {
+                            px0 =  __lsx_vld((__m128i *)srcPtrChannelROI, 0);
+
+                            px1 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+                            px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                            p0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+                            p1 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px0));    // pixels 4-7
+                            p2 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 8-11
+                            p3 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px1));    // pixels 12-15
+
+                            p0 = __lsx_vfsub_s(p0, vMean);
+                            p1 = __lsx_vfsub_s(p1, vMean);
+                            p2 = __lsx_vfsub_s(p2, vMean);
+                            p3 = __lsx_vfsub_s(p3, vMean);
+                            px0 = __lsx_vftint_w_s(__lsx_vfmul_s(p0, vInvStdDev));
+                            px1 = __lsx_vftint_w_s(__lsx_vfmul_s(p1, vInvStdDev));
+                            px2 = __lsx_vftint_w_s(__lsx_vfmul_s(p2, vInvStdDev));
+                            px3 = __lsx_vftint_w_s(__lsx_vfmul_s(p3, vInvStdDev));
+
+                            px0 = lsx_packus_i32(px0, px1);    // pixels 0-7
+                            px1 = lsx_packus_i32(px2, px3);    // pixels 8-15
+                            px0 = lsx_packus_i16(px0, px1);    // pixels 0-15
+
+                            __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+                            srcPtrChannelROI += 16;
+                            dstPtrTemp += 16;
+                        }
+                        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                        {
+                            *dstPtrTemp = (T) RPPPIXELCHECK(((Rpp32f)(*srcPtrChannelROI) - mean) * invStdDev);
+                            dstPtrTemp++;
+                            srcPtrChannelROI++;
+                        }
+
+                        srcPtrChannelROI += srcROIIncrement;
+
+                    }
+                }
+            }
+            else if (mirrorFlag == 1)
+            {
+                Rpp32u srcROIIncrement = batch_srcSizeMax[batchCount].width + batch_dstSize[batchCount].width;
+                for(int c = 0; c < channel; c++)
+                {
+                    T *dstPtrChannel, *srcPtrChannelROI;
+                    dstPtrChannel = dstPtrImage + (c * dstImageDimMax);
+                    srcPtrChannelROI = srcPtrImage + (c * srcImageDimMax) + (y1 * batch_srcSizeMax[batchCount].width) + x2;
+
+
+                    for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                    {
+                        T *dstPtrTemp;
+                        dstPtrTemp = dstPtrChannel + (i * batch_dstSizeMax[batchCount].width);
+
+                        Rpp32u bufferLength = batch_dstSize[batchCount].width;
+                        Rpp32u alignedLength = (bufferLength / 16) * 16;
+
+                        __m128i const zero = __lsx_vldi(0);
+                        __m128i px0, px1, px2, px3;
+                        __m128 p0, p1, p2, p3;
+                        __m128 vMean = lsx_set1_f32(mean);
+                        __m128 vInvStdDev = lsx_set1_f32(invStdDev);
+                        __m128i vMask = lsx_setr_i8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
+
+                        int vectorLoopCount = 0;
+                        for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+                        {
+                            srcPtrChannelROI -= 15;
+                            px0 =  __lsx_vld((__m128i *)srcPtrChannelROI, 0);
+                            px0 = lsx_shuffle_i8(px0, vMask);
+
+                            px1 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+                            px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                            p0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+                            p1 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px0));    // pixels 4-7
+                            p2 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 8-11
+                            p3 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px1));    // pixels 12-15
+
+                            p0 = __lsx_vfsub_s(p0, vMean);
+                            p1 = __lsx_vfsub_s(p1, vMean);
+                            p2 = __lsx_vfsub_s(p2, vMean);
+                            p3 = __lsx_vfsub_s(p3, vMean);
+                            px0 = __lsx_vftint_w_s(__lsx_vfmul_s(p0, vInvStdDev));
+                            px1 = __lsx_vftint_w_s(__lsx_vfmul_s(p1, vInvStdDev));
+                            px2 = __lsx_vftint_w_s(__lsx_vfmul_s(p2, vInvStdDev));
+                            px3 = __lsx_vftint_w_s(__lsx_vfmul_s(p3, vInvStdDev));
+
+                            px0 = lsx_packus_i32(px0, px1);    // pixels 0-7
+                            px1 = lsx_packus_i32(px2, px3);    // pixels 8-15
+                            px0 = lsx_packus_i16(px0, px1);    // pixels 0-15
+
+                            __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+                            srcPtrChannelROI -= 1;
+                            dstPtrTemp += 16;
+                        }
+                        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                        {
+                            *dstPtrTemp = (T) RPPPIXELCHECK(((Rpp32f)(*srcPtrChannelROI) - mean) * invStdDev);
+                            dstPtrTemp++;
+                            srcPtrChannelROI--;
+                        }
+
+                        srcPtrChannelROI += srcROIIncrement;
+                    }
+                }
+            }
+            if (outputFormatToggle == 1)
+            {
+                T *dstPtrImageUnpadded = (T*) calloc(channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width, sizeof(T));
+                T *dstPtrImageUnpaddedCopy = (T*) calloc(channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width, sizeof(T));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_dstSize[batchCount], batch_dstSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width * sizeof(T));
+
+                compute_planar_to_packed_host(dstPtrImageUnpaddedCopy, batch_dstSize[batchCount], dstPtrImageUnpadded, channel);
+
+                memset(dstPtrImage, (T) 0, dstImageDimMax * channel * sizeof(T));
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_dstSize[batchCount], batch_dstSizeMax[batchCount], dstPtrImage, RPPI_CHN_PACKED, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u srcImageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+            Rpp32u dstImageDimMax = batch_dstSizeMax[batchCount].height * batch_dstSizeMax[batchCount].width;
+
+            Rpp32u x1 = batch_crop_pos_x[batchCount];
+            Rpp32u y1 = batch_crop_pos_y[batchCount];
+            Rpp32u x2 = x1 + batch_dstSize[batchCount].width - 1;
+            Rpp32u y2 = y1 + batch_dstSize[batchCount].height - 1;
+
+            Rpp32u mirrorFlag = batch_mirrorFlag[batchCount];
+            Rpp32f mean = batch_mean[batchCount];
+            Rpp32f stdDev = batch_stdDev[batchCount];
+            Rpp32f invStdDev = 1.0 / stdDev;
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+            compute_image_location_host(batch_dstSizeMax, batchCount, &dstLoc, channel);
+            srcPtrImage = srcPtr + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+
+            Rpp32u srcElementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+            Rpp32u dstElementsInRowMax = channel * batch_dstSizeMax[batchCount].width;
+            Rpp32u elementsInRowROI = channel * batch_dstSize[batchCount].width;
+
+            if (mirrorFlag == 0)
+            {
+                T  *srcPtrROI;
+                srcPtrROI = srcPtrImage + (y1 * srcElementsInRowMax) + (x1 * channel);
+
+                Rpp32u srcROIIncrement = srcElementsInRowMax - elementsInRowROI;
+
+
+                for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                {
+                    T *dstPtrTemp;
+                    dstPtrTemp = dstPtrImage + (i * dstElementsInRowMax);
+
+                    Rpp32u bufferLength = elementsInRowROI;
+                    Rpp32u alignedLength = (bufferLength / 16) * 16;
+
+                    __m128i const zero = __lsx_vldi(0);
+                    __m128i px0, px1, px2, px3;
+                    __m128 p0, p1, p2, p3;
+                    __m128 vMean = lsx_set1_f32(mean);
+                    __m128 vInvStdDev = lsx_set1_f32(invStdDev);
+
+                    int vectorLoopCount = 0;
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+                    {
+                        px0 =  __lsx_vld((__m128i *)srcPtrROI, 0);
+
+                        px1 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+                        px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                        p0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+                        p1 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px0));    // pixels 4-7
+                        p2 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 8-11
+                        p3 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px1));    // pixels 12-15
+
+                        p0 = __lsx_vfsub_s(p0, vMean);
+                        p1 = __lsx_vfsub_s(p1, vMean);
+                        p2 = __lsx_vfsub_s(p2, vMean);
+                        p3 = __lsx_vfsub_s(p3, vMean);
+                        px0 = __lsx_vftint_w_s(__lsx_vfmul_s(p0, vInvStdDev));
+                        px1 = __lsx_vftint_w_s(__lsx_vfmul_s(p1, vInvStdDev));
+                        px2 = __lsx_vftint_w_s(__lsx_vfmul_s(p2, vInvStdDev));
+                        px3 = __lsx_vftint_w_s(__lsx_vfmul_s(p3, vInvStdDev));
+
+                        px0 = lsx_packus_i32(px0, px1);    // pixels 0-7
+                        px1 = lsx_packus_i32(px2, px3);    // pixels 8-15
+                        px0 = lsx_packus_i16(px0, px1);    // pixels 0-15
+
+                        __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+                        srcPtrROI += 16;
+                        dstPtrTemp += 16;
+                    }
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                    {
+                        *dstPtrTemp = (T) RPPPIXELCHECK(((Rpp32f)(*srcPtrROI) - mean) * invStdDev);
+                        dstPtrTemp++;
+                        srcPtrROI++;
+                    }
+
+                    srcPtrROI += srcROIIncrement;
+                }
+            }
+            else if (mirrorFlag == 1)
+            {
+                T  *srcPtrROI;
+                srcPtrROI = srcPtrImage + (y1 * srcElementsInRowMax) + ((x2 - 1) * channel);
+
+                Rpp32u srcROIIncrement = srcElementsInRowMax + elementsInRowROI;
+
+
+                for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                {
+                    T *dstPtrTemp;
+                    dstPtrTemp = dstPtrImage + (i * dstElementsInRowMax);
+
+                    Rpp32u bufferLength = elementsInRowROI;
+                    Rpp32u alignedLength = ((bufferLength / 15) * 15) - 1;
+
+                    __m128i const zero = __lsx_vldi(0);
+                    __m128i px0, px1, px2, px3;
+                    __m128 p0, p1, p2, p3;
+                    __m128 vMean = lsx_set1_f32(mean);
+                    __m128 vInvStdDev = lsx_set1_f32(invStdDev);
+                    __m128i vMask = lsx_setr_i8(13, 14, 15, 10, 11, 12, 7, 8, 9, 4, 5, 6, 1, 2, 3, 0);
+
+                    int vectorLoopCount = 0;
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount+=15)
+                    {
+                        srcPtrROI -= 13;
+                        px0 =  __lsx_vld((__m128i *)srcPtrROI, 0);
+                        px0 = lsx_shuffle_i8(px0, vMask);
+
+                        px1 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+                        px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                        p0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+                        p1 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px0));    // pixels 4-7
+                        p2 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 8-11
+                        p3 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px1));    // pixels 12-15
+
+                        p0 = __lsx_vfsub_s(p0, vMean);
+                        p1 = __lsx_vfsub_s(p1, vMean);
+                        p2 = __lsx_vfsub_s(p2, vMean);
+                        p3 = __lsx_vfsub_s(p3, vMean);
+                        px0 = __lsx_vftint_w_s(__lsx_vfmul_s(p0, vInvStdDev));
+                        px1 = __lsx_vftint_w_s(__lsx_vfmul_s(p1, vInvStdDev));
+                        px2 = __lsx_vftint_w_s(__lsx_vfmul_s(p2, vInvStdDev));
+                        px3 = __lsx_vftint_w_s(__lsx_vfmul_s(p3, vInvStdDev));
+
+                        px0 = lsx_packus_i32(px0, px1);    // pixels 0-7
+                        px1 = lsx_packus_i32(px2, px3);    // pixels 8-15
+                        px0 = lsx_packus_i16(px0, px1);    // pixels 0-15
+
+                        __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+                        srcPtrROI -= 2;
+                        dstPtrTemp += 15;
+                    }
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount+=channel)
+                    {
+                        for(int c = 0; c < channel; c++)
+                        {
+                            *dstPtrTemp = (T) RPPPIXELCHECK(((Rpp32f)(*srcPtrROI) - mean) * invStdDev);
+                            dstPtrTemp++;
+                            srcPtrROI++;
+                        }
+                        srcPtrROI -= (2 * channel);
+                    }
+
+                    srcPtrROI += srcROIIncrement;
+                }
+            }
+            if (outputFormatToggle == 1)
+            {
+                T *dstPtrImageUnpadded = (T*) calloc(channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width, sizeof(T));
+                T *dstPtrImageUnpaddedCopy = (T*) calloc(channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width, sizeof(T));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_dstSize[batchCount], batch_dstSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width * sizeof(T));
+
+                compute_packed_to_planar_host(dstPtrImageUnpaddedCopy, batch_dstSize[batchCount], dstPtrImageUnpadded, channel);
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_dstSize[batchCount], batch_dstSizeMax[batchCount], dstPtrImage, RPPI_CHN_PLANAR, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T, typename U>
+RppStatus crop_mirror_normalize_host(T* srcPtr, RppiSize srcSize, U* dstPtr, RppiSize dstSize,
+                                     Rpp32u crop_pos_x, Rpp32u crop_pos_y,
+                                     Rpp32f mean, Rpp32f stdDev,
+                                     Rpp32u mirrorFlag, Rpp32u outputFormatToggle,
+                                     RppiChnFormat chnFormat, Rpp32u channel)
+{
+    U *dstPtrTemp;
+    dstPtrTemp = dstPtr;
+
+    Rpp32u srcImageDim = srcSize.height * srcSize.width;
+    Rpp32f invStdDev = 1.0 / stdDev;
+
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        if (mirrorFlag == 0)
+        {
+            Rpp32u srcROIIncrement = srcSize.width - dstSize.width;
+            for(int c = 0; c < channel; c++)
+            {
+                T *srcPtrChannelROI;
+                srcPtrChannelROI = srcPtr + (c * srcImageDim) + (crop_pos_y * srcSize.width) + crop_pos_x;
+
+                for(int i = 0; i < dstSize.height; i++)
+                {
+                    Rpp32u bufferLength = dstSize.width;
+                    Rpp32u alignedLength = (bufferLength / 16) * 16;
+
+                    __m128i const zero = __lsx_vldi(0);
+                    __m128i px0, px1, px2, px3;
+                    __m128 p0, p1, p2, p3;
+                    __m128 vMean = lsx_set1_f32(mean);
+                    __m128 vInvStdDev = lsx_set1_f32(invStdDev);
+
+                    int vectorLoopCount = 0;
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+                    {
+                        px0 =  __lsx_vld((__m128i *)srcPtrChannelROI, 0);
+
+                        px1 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+                        px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                        p0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+                        p1 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px0));    // pixels 4-7
+                        p2 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 8-11
+                        p3 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px1));    // pixels 12-15
+
+                        p0 = __lsx_vfsub_s(p0, vMean);
+                        p1 = __lsx_vfsub_s(p1, vMean);
+                        p2 = __lsx_vfsub_s(p2, vMean);
+                        p3 = __lsx_vfsub_s(p3, vMean);
+                        px0 = __lsx_vftint_w_s(__lsx_vfmul_s(p0, vInvStdDev));
+                        px1 = __lsx_vftint_w_s(__lsx_vfmul_s(p1, vInvStdDev));
+                        px2 = __lsx_vftint_w_s(__lsx_vfmul_s(p2, vInvStdDev));
+                        px3 = __lsx_vftint_w_s(__lsx_vfmul_s(p3, vInvStdDev));
+
+                        px0 = lsx_packus_i32(px0, px1);    // pixels 0-7
+                        px1 = lsx_packus_i32(px2, px3);    // pixels 8-15
+                        px0 = lsx_packus_i16(px0, px1);    // pixels 0-15
+
+                        __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+                        srcPtrChannelROI += 16;
+                        dstPtrTemp += 16;
+                    }
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                    {
+                        *dstPtrTemp = (T) RPPPIXELCHECK(((Rpp32f)(*srcPtrChannelROI) - mean) * invStdDev);
+                        dstPtrTemp++;
+                        srcPtrChannelROI++;
+                    }
+
+                    srcPtrChannelROI += srcROIIncrement;
+                }
+            }
+        }
+        else if (mirrorFlag == 1)
+        {
+            Rpp32u srcROIIncrement = srcSize.width + dstSize.width;
+            for(int c = 0; c < channel; c++)
+            {
+                T *srcPtrChannelROI;
+                srcPtrChannelROI = srcPtr + (c * srcImageDim) + (crop_pos_y * srcSize.width) + crop_pos_x + dstSize.width - 1;
+
+                for(int i = 0; i < dstSize.height; i++)
+                {
+                    Rpp32u bufferLength = dstSize.width;
+                    Rpp32u alignedLength = (bufferLength / 16) * 16;
+
+                    __m128i const zero = __lsx_vldi(0);
+                    __m128i px0, px1, px2, px3;
+                    __m128 p0, p1, p2, p3;
+                    __m128 vMean = lsx_set1_f32(mean);
+                    __m128 vInvStdDev = lsx_set1_f32(invStdDev);
+                    __m128i vMask = lsx_setr_i8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
+
+                    int vectorLoopCount = 0;
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+                    {
+                        srcPtrChannelROI -= 15;
+                        px0 =  __lsx_vld((__m128i *)srcPtrChannelROI, 0);
+                        px0 = lsx_shuffle_i8(px0, vMask);
+
+                        px1 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+                        px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                        p0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+                        p1 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px0));    // pixels 4-7
+                        p2 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 8-11
+                        p3 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px1));    // pixels 12-15
+
+                        p0 = __lsx_vfsub_s(p0, vMean);
+                        p1 = __lsx_vfsub_s(p1, vMean);
+                        p2 = __lsx_vfsub_s(p2, vMean);
+                        p3 = __lsx_vfsub_s(p3, vMean);
+                        px0 = __lsx_vftint_w_s(__lsx_vfmul_s(p0, vInvStdDev));
+                        px1 = __lsx_vftint_w_s(__lsx_vfmul_s(p1, vInvStdDev));
+                        px2 = __lsx_vftint_w_s(__lsx_vfmul_s(p2, vInvStdDev));
+                        px3 = __lsx_vftint_w_s(__lsx_vfmul_s(p3, vInvStdDev));
+
+                        px0 = lsx_packus_i32(px0, px1);    // pixels 0-7
+                        px1 = lsx_packus_i32(px2, px3);    // pixels 8-15
+                        px0 = lsx_packus_i16(px0, px1);    // pixels 0-15
+
+                        __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+                        srcPtrChannelROI -= 1;
+                        dstPtrTemp += 16;
+                    }
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                    {
+                        *dstPtrTemp = (T) RPPPIXELCHECK(((Rpp32f)(*srcPtrChannelROI) - mean) * invStdDev);
+                        dstPtrTemp++;
+                        srcPtrChannelROI--;
+                    }
+
+                    srcPtrChannelROI += srcROIIncrement;
+                }
+            }
+        }
+    }
+    else if(chnFormat == RPPI_CHN_PACKED)
+    {
+        Rpp32u srcElementsInRow = channel * srcSize.width;
+        Rpp32u dstElementsInRow = channel * dstSize.width;
+
+        if (mirrorFlag == 0)
+        {
+            T  *srcPtrROI;
+            srcPtrROI = srcPtr + (crop_pos_y * srcElementsInRow) + (crop_pos_x * channel);
+
+            Rpp32u srcROIIncrement = srcElementsInRow - dstElementsInRow;
+
+            for(int i = 0; i < dstSize.height; i++)
+            {
+                Rpp32u bufferLength = dstElementsInRow;
+                Rpp32u alignedLength = (bufferLength / 16) * 16;
+
+                __m128i const zero = __lsx_vldi(0);
+                __m128i px0, px1, px2, px3;
+                __m128 p0, p1, p2, p3;
+                __m128 vMean = lsx_set1_f32(mean);
+                __m128 vInvStdDev = lsx_set1_f32(invStdDev);
+
+                int vectorLoopCount = 0;
+                for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+                {
+                    px0 =  __lsx_vld((__m128i *)srcPtrROI, 0);
+
+                    px1 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+                    px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                    p0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+                    p1 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px0));    // pixels 4-7
+                    p2 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 8-11
+                    p3 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px1));    // pixels 12-15
+
+                    p0 = __lsx_vfsub_s(p0, vMean);
+                    p1 = __lsx_vfsub_s(p1, vMean);
+                    p2 = __lsx_vfsub_s(p2, vMean);
+                    p3 = __lsx_vfsub_s(p3, vMean);
+                    px0 = __lsx_vftint_w_s(__lsx_vfmul_s(p0, vInvStdDev));
+                    px1 = __lsx_vftint_w_s(__lsx_vfmul_s(p1, vInvStdDev));
+                    px2 = __lsx_vftint_w_s(__lsx_vfmul_s(p2, vInvStdDev));
+                    px3 = __lsx_vftint_w_s(__lsx_vfmul_s(p3, vInvStdDev));
+
+                    px0 = lsx_packus_i32(px0, px1);    // pixels 0-7
+                    px1 = lsx_packus_i32(px2, px3);    // pixels 8-15
+                    px0 = lsx_packus_i16(px0, px1);    // pixels 0-15
+
+                    __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+                    srcPtrROI += 16;
+                    dstPtrTemp += 16;
+                }
+                for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                {
+                    *dstPtrTemp = (T) RPPPIXELCHECK(((Rpp32f)(*srcPtrROI) - mean) * invStdDev);
+                    dstPtrTemp++;
+                    srcPtrROI++;
+                }
+
+                srcPtrROI += srcROIIncrement;
+            }
+        }
+        else if (mirrorFlag == 1)
+        {
+            T  *srcPtrROI;
+            srcPtrROI = srcPtr + (crop_pos_y * srcElementsInRow) + ((crop_pos_x + dstSize.width - 1) * channel);
+
+            Rpp32u srcROIIncrement = srcElementsInRow + dstElementsInRow;
+
+            for(int i = 0; i < dstSize.height; i++)
+            {
+                Rpp32u bufferLength = dstElementsInRow;
+                Rpp32u alignedLength = ((bufferLength / 15) * 15) - 1;
+
+                __m128i const zero = __lsx_vldi(0);
+                __m128i px0, px1, px2, px3;
+                __m128 p0, p1, p2, p3;
+                __m128 vMean = lsx_set1_f32(mean);
+                __m128 vInvStdDev = lsx_set1_f32(invStdDev);
+                __m128i vMask = lsx_setr_i8(13, 14, 15, 10, 11, 12, 7, 8, 9, 4, 5, 6, 1, 2, 3, 0);
+
+                int vectorLoopCount = 0;
+                for (; vectorLoopCount < alignedLength; vectorLoopCount+=15)
+                {
+                    srcPtrROI -= 13;
+                    px0 =  __lsx_vld((__m128i *)srcPtrROI, 0);
+                    px0 = lsx_shuffle_i8(px0, vMask);
+
+                    px1 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+                    px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                    p0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+                    p1 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px0));    // pixels 4-7
+                    p2 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 8-11
+                    p3 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px1));    // pixels 12-15
+
+                    p0 = __lsx_vfsub_s(p0, vMean);
+                    p1 = __lsx_vfsub_s(p1, vMean);
+                    p2 = __lsx_vfsub_s(p2, vMean);
+                    p3 = __lsx_vfsub_s(p3, vMean);
+                    px0 = __lsx_vftint_w_s(__lsx_vfmul_s(p0, vInvStdDev));
+                    px1 = __lsx_vftint_w_s(__lsx_vfmul_s(p1, vInvStdDev));
+                    px2 = __lsx_vftint_w_s(__lsx_vfmul_s(p2, vInvStdDev));
+                    px3 = __lsx_vftint_w_s(__lsx_vfmul_s(p3, vInvStdDev));
+
+                    px0 = lsx_packus_i32(px0, px1);    // pixels 0-7
+                    px1 = lsx_packus_i32(px2, px3);    // pixels 8-15
+                    px0 = lsx_packus_i16(px0, px1);    // pixels 0-15
+
+                    __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+                    srcPtrROI -= 2;
+                    dstPtrTemp += 15;
+                }
+                for (; vectorLoopCount < bufferLength; vectorLoopCount+=channel)
+                {
+                    for(int c = 0; c < channel; c++)
+                    {
+                        *dstPtrTemp = (T) RPPPIXELCHECK(((Rpp32f)(*srcPtrROI) - mean) * invStdDev);
+                        dstPtrTemp++;
+                        srcPtrROI++;
+                    }
+                    srcPtrROI -= (2 * channel);
+                }
+
+                srcPtrROI += srcROIIncrement;
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+RppStatus crop_mirror_normalize_f32_host_batch(Rpp32f* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, Rpp32f* dstPtr, RppiSize *batch_dstSize, RppiSize *batch_dstSizeMax,
+                                     Rpp32u *batch_crop_pos_x, Rpp32u *batch_crop_pos_y,
+                                     Rpp32f *batch_mean, Rpp32f *batch_stdDev,
+                                     Rpp32u *batch_mirrorFlag, Rpp32u outputFormatToggle,
+                                     Rpp32u nbatchSize,
+                                     RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u srcImageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+            Rpp32u dstImageDimMax = batch_dstSizeMax[batchCount].height * batch_dstSizeMax[batchCount].width;
+
+            Rpp32u x1 = batch_crop_pos_x[batchCount];
+            Rpp32u y1 = batch_crop_pos_y[batchCount];
+            Rpp32u x2 = x1 + batch_dstSize[batchCount].width - 1;
+            Rpp32u y2 = y1 + batch_dstSize[batchCount].height - 1;
+
+            Rpp32u mirrorFlag = batch_mirrorFlag[batchCount];
+            Rpp32f mean = batch_mean[batchCount];
+            Rpp32f stdDev = batch_stdDev[batchCount];
+            Rpp32f invStdDev = 1.0 / stdDev;
+
+            Rpp32f *srcPtrImage, *dstPtrImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+            compute_image_location_host(batch_dstSizeMax, batchCount, &dstLoc, channel);
+            srcPtrImage = srcPtr + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+
+            if (mirrorFlag == 0)
+            {
+                Rpp32u srcROIIncrement = batch_srcSizeMax[batchCount].width - batch_dstSize[batchCount].width;
+                for(int c = 0; c < channel; c++)
+                {
+                    Rpp32f *dstPtrChannel, *srcPtrChannelROI;
+                    dstPtrChannel = dstPtrImage + (c * dstImageDimMax);
+                    srcPtrChannelROI = srcPtrImage + (c * srcImageDimMax) + (y1 * batch_srcSizeMax[batchCount].width) + x1;
+
+
+                    for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                    {
+                        Rpp32f *dstPtrTemp;
+                        dstPtrTemp = dstPtrChannel + (i * batch_dstSizeMax[batchCount].width);
+
+                        Rpp32u bufferLength = batch_dstSize[batchCount].width;
+                        Rpp32u alignedLength = (bufferLength / 4) * 4;
+
+                        __m128 p0;
+                        __m128 vMean = lsx_set1_f32(mean);
+                        __m128 vInvStdDev = lsx_set1_f32(invStdDev);
+
+                        int vectorLoopCount = 0;
+                        for (; vectorLoopCount < alignedLength; vectorLoopCount+=4)
+                        {
+                            p0 = (__m128)__lsx_vld(srcPtrChannelROI, 0);
+                            p0 = __lsx_vfsub_s(p0, vMean);
+                            p0 = __lsx_vfmul_s(p0, vInvStdDev);
+                            __lsx_vst(p0, dstPtrTemp, 0);
+
+                            srcPtrChannelROI += 4;
+                            dstPtrTemp += 4;
+                        }
+                        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                        {
+                            *dstPtrTemp = (*srcPtrChannelROI - mean) * invStdDev;
+                            dstPtrTemp++;
+                            srcPtrChannelROI++;
+                        }
+
+                        srcPtrChannelROI += srcROIIncrement;
+
+                    }
+                }
+            }
+            else if (mirrorFlag == 1)
+            {
+                Rpp32u srcROIIncrement = batch_srcSizeMax[batchCount].width + batch_dstSize[batchCount].width;
+                for(int c = 0; c < channel; c++)
+                {
+                    Rpp32f *dstPtrChannel, *srcPtrChannelROI;
+                    dstPtrChannel = dstPtrImage + (c * dstImageDimMax);
+                    srcPtrChannelROI = srcPtrImage + (c * srcImageDimMax) + (y1 * batch_srcSizeMax[batchCount].width) + x2;
+
+                    for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                    {
+                        Rpp32f *dstPtrTemp;
+                        dstPtrTemp = dstPtrChannel + (i * batch_dstSizeMax[batchCount].width);
+
+                        Rpp32u bufferLength = batch_dstSize[batchCount].width;
+                        Rpp32u alignedLength = (bufferLength / 4) * 4;
+
+                        __m128 p0;
+                        __m128 vMean = lsx_set1_f32(mean);
+                        __m128 vInvStdDev = lsx_set1_f32(invStdDev);
+
+                        int vectorLoopCount = 0;
+                        for (; vectorLoopCount < alignedLength; vectorLoopCount+=4)
+                        {
+                            srcPtrChannelROI -= 3;
+                            p0 = (__m128)__lsx_vld(srcPtrChannelROI, 0);
+                            p0 = (__m128)__lsx_vpermi_w((__m128i)p0, (__m128i)p0, _MM_SHUFFLE(0,1,2,3));
+                            p0 = __lsx_vfsub_s(p0, vMean);
+                            p0 = __lsx_vfmul_s(p0, vInvStdDev);
+                            __lsx_vst(p0, dstPtrTemp, 0);
+
+                            srcPtrChannelROI -= 1;
+                            dstPtrTemp += 4;
+                        }
+                        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                        {
+                            *dstPtrTemp = (*srcPtrChannelROI - mean) * invStdDev;
+                            dstPtrTemp++;
+                            srcPtrChannelROI--;
+                        }
+
+                        srcPtrChannelROI += srcROIIncrement;
+                    }
+                }
+            }
+            if (outputFormatToggle == 1)
+            {
+                Rpp32f *dstPtrImageUnpadded = (Rpp32f*) calloc(channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width, sizeof(Rpp32f));
+                Rpp32f *dstPtrImageUnpaddedCopy = (Rpp32f*) calloc(channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width, sizeof(Rpp32f));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_dstSize[batchCount], batch_dstSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width * sizeof(Rpp32f));
+
+                compute_planar_to_packed_host(dstPtrImageUnpaddedCopy, batch_dstSize[batchCount], dstPtrImageUnpadded, channel);
+
+                memset(dstPtrImage, (Rpp32f) 0, dstImageDimMax * channel * sizeof(Rpp32f));
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_dstSize[batchCount], batch_dstSizeMax[batchCount], dstPtrImage, RPPI_CHN_PACKED, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u srcImageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+            Rpp32u dstImageDimMax = batch_dstSizeMax[batchCount].height * batch_dstSizeMax[batchCount].width;
+
+            Rpp32u x1 = batch_crop_pos_x[batchCount];
+            Rpp32u y1 = batch_crop_pos_y[batchCount];
+            Rpp32u x2 = x1 + batch_dstSize[batchCount].width - 1;
+            Rpp32u y2 = y1 + batch_dstSize[batchCount].height - 1;
+
+            Rpp32u mirrorFlag = batch_mirrorFlag[batchCount];
+            Rpp32f mean = batch_mean[batchCount];
+            Rpp32f stdDev = batch_stdDev[batchCount];
+            Rpp32f invStdDev = 1.0 / stdDev;
+
+            Rpp32f *srcPtrImage, *dstPtrImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+            compute_image_location_host(batch_dstSizeMax, batchCount, &dstLoc, channel);
+            srcPtrImage = srcPtr + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+
+            Rpp32u srcElementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+            Rpp32u dstElementsInRowMax = channel * batch_dstSizeMax[batchCount].width;
+            Rpp32u elementsInRowROI = channel * batch_dstSize[batchCount].width;
+
+            if (mirrorFlag == 0)
+            {
+                Rpp32f *srcPtrROI;
+                srcPtrROI = srcPtrImage + (y1 * srcElementsInRowMax) + (x1 * channel);
+
+                Rpp32u srcROIIncrement = srcElementsInRowMax - elementsInRowROI;
+
+
+                for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                {
+                    Rpp32f *dstPtrTemp;
+                    dstPtrTemp = dstPtrImage + (i * dstElementsInRowMax);
+
+                    Rpp32u bufferLength = elementsInRowROI;
+                    Rpp32u alignedLength = (bufferLength / 4) * 4;
+
+                    __m128 p0;
+                    __m128 vMean = lsx_set1_f32(mean);
+                    __m128 vInvStdDev = lsx_set1_f32(invStdDev);
+
+                    int vectorLoopCount = 0;
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount+=4)
+                    {
+                        p0 = (__m128)__lsx_vld(srcPtrROI, 0);
+                        p0 = __lsx_vfsub_s(p0, vMean);
+                        p0 = __lsx_vfmul_s(p0, vInvStdDev);
+                        __lsx_vst(p0, dstPtrTemp, 0);
+
+                        srcPtrROI += 4;
+                        dstPtrTemp += 4;
+                    }
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                    {
+                        *dstPtrTemp = (*srcPtrROI - mean) * invStdDev;
+                        dstPtrTemp++;
+                        srcPtrROI++;
+                    }
+
+                    srcPtrROI += srcROIIncrement;
+                }
+            }
+            else if (mirrorFlag == 1)
+            {
+                Rpp32f *srcPtrROI;
+                srcPtrROI = srcPtrImage + (y1 * srcElementsInRowMax) + ((x2 - 1) * channel);
+
+                Rpp32u srcROIIncrement = srcElementsInRowMax + elementsInRowROI;
+
+
+                for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                {
+                    Rpp32f *dstPtrTemp;
+                    dstPtrTemp = dstPtrImage + (i * dstElementsInRowMax);
+
+                    Rpp32u bufferLength = elementsInRowROI;
+
+                    __m128 p0;
+                    __m128 vMean = lsx_set1_f32(mean);
+                    __m128 vInvStdDev = lsx_set1_f32(invStdDev);
+
+                    int vectorLoopCount = 0;
+                    for (; vectorLoopCount < 3; vectorLoopCount+=3)
+                    {
+                        for(int c = 0; c < channel; c++)
+                        {
+                            *dstPtrTemp = (*srcPtrROI - mean) * invStdDev;
+                            dstPtrTemp++;
+                            srcPtrROI++;
+                        }
+                        srcPtrROI -= (2 * channel);
+                    }
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount+=3)
+                    {
+                        p0 = (__m128)__lsx_vld(srcPtrROI, 0);
+                        p0 = __lsx_vfsub_s(p0, vMean);
+                        p0 = __lsx_vfmul_s(p0, vInvStdDev);
+                        __lsx_vst(p0, dstPtrTemp, 0);
+
+                        srcPtrROI -= 3;
+                        dstPtrTemp += 3;
+                    }
+
+                    srcPtrROI += srcROIIncrement;
+                }
+            }
+            if (outputFormatToggle == 1)
+            {
+                Rpp32f *dstPtrImageUnpadded = (Rpp32f*) calloc(channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width, sizeof(Rpp32f));
+                Rpp32f *dstPtrImageUnpaddedCopy = (Rpp32f*) calloc(channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width, sizeof(Rpp32f));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_dstSize[batchCount], batch_dstSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width * sizeof(Rpp32f));
+
+                compute_packed_to_planar_host(dstPtrImageUnpaddedCopy, batch_dstSize[batchCount], dstPtrImageUnpadded, channel);
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_dstSize[batchCount], batch_dstSizeMax[batchCount], dstPtrImage, RPPI_CHN_PLANAR, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+RppStatus crop_mirror_normalize_f16_host_batch(Rpp16f* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, Rpp16f* dstPtr, RppiSize *batch_dstSize, RppiSize *batch_dstSizeMax,
+                                     Rpp32u *batch_crop_pos_x, Rpp32u *batch_crop_pos_y,
+                                     Rpp32f *batch_mean, Rpp32f *batch_stdDev,
+                                     Rpp32u *batch_mirrorFlag, Rpp32u outputFormatToggle,
+                                     Rpp32u nbatchSize,
+                                     RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u srcImageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+            Rpp32u dstImageDimMax = batch_dstSizeMax[batchCount].height * batch_dstSizeMax[batchCount].width;
+
+            Rpp32u x1 = batch_crop_pos_x[batchCount];
+            Rpp32u y1 = batch_crop_pos_y[batchCount];
+            Rpp32u x2 = x1 + batch_dstSize[batchCount].width - 1;
+            Rpp32u y2 = y1 + batch_dstSize[batchCount].height - 1;
+
+            Rpp32u mirrorFlag = batch_mirrorFlag[batchCount];
+            Rpp32f mean = batch_mean[batchCount];
+            Rpp32f stdDev = batch_stdDev[batchCount];
+            Rpp32f invStdDev = 1.0 / stdDev;
+
+            Rpp16f *srcPtrImage, *dstPtrImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+            compute_image_location_host(batch_dstSizeMax, batchCount, &dstLoc, channel);
+            srcPtrImage = srcPtr + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+
+            Rpp32f srcPtrChannelROIps[4], dstPtrTempps[4];
+
+            if (mirrorFlag == 0)
+            {
+                Rpp32u srcROIIncrement = batch_srcSizeMax[batchCount].width - batch_dstSize[batchCount].width;
+                for(int c = 0; c < channel; c++)
+                {
+                    Rpp16f *dstPtrChannel, *srcPtrChannelROI;
+                    dstPtrChannel = dstPtrImage + (c * dstImageDimMax);
+                    srcPtrChannelROI = srcPtrImage + (c * srcImageDimMax) + (y1 * batch_srcSizeMax[batchCount].width) + x1;
+
+
+                    for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                    {
+                        Rpp16f *dstPtrTemp;
+                        dstPtrTemp = dstPtrChannel + (i * batch_dstSizeMax[batchCount].width);
+
+                        Rpp32u bufferLength = batch_dstSize[batchCount].width;
+                        Rpp32u alignedLength = (bufferLength / 4) * 4;
+
+                        __m128 p0;
+                        __m128 vMean = lsx_set1_f32(mean);
+                        __m128 vInvStdDev = lsx_set1_f32(invStdDev);
+
+                        int vectorLoopCount = 0;
+                        for (; vectorLoopCount < alignedLength; vectorLoopCount+=4)
+                        {
+                            for(int cnt = 0; cnt < 4; cnt++)
+                            {
+                                *(srcPtrChannelROIps + cnt) = (Rpp32f) (*(srcPtrChannelROI + cnt));
+                            }
+
+                            p0 = (__m128)__lsx_vld(srcPtrChannelROIps, 0);
+                            p0 = __lsx_vfsub_s(p0, vMean);
+                            p0 = __lsx_vfmul_s(p0, vInvStdDev);
+                            __lsx_vst(p0, dstPtrTempps, 0);
+
+                            for(int cnt = 0; cnt < 4; cnt++)
+                            {
+                                *(dstPtrTemp + cnt) = (Rpp16f) (*(dstPtrTempps + cnt));
+                            }
+
+                            srcPtrChannelROI += 4;
+                            dstPtrTemp += 4;
+                        }
+                        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                        {
+                            *dstPtrTemp = (*srcPtrChannelROI - mean) * invStdDev;
+                            dstPtrTemp++;
+                            srcPtrChannelROI++;
+                        }
+
+                        srcPtrChannelROI += srcROIIncrement;
+
+                    }
+                }
+            }
+            else if (mirrorFlag == 1)
+            {
+                Rpp32u srcROIIncrement = batch_srcSizeMax[batchCount].width + batch_dstSize[batchCount].width;
+                for(int c = 0; c < channel; c++)
+                {
+                    Rpp16f *dstPtrChannel, *srcPtrChannelROI;
+                    dstPtrChannel = dstPtrImage + (c * dstImageDimMax);
+                    srcPtrChannelROI = srcPtrImage + (c * srcImageDimMax) + (y1 * batch_srcSizeMax[batchCount].width) + x2;
+
+                    for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                    {
+                        Rpp16f *dstPtrTemp;
+                        dstPtrTemp = dstPtrChannel + (i * batch_dstSizeMax[batchCount].width);
+
+                        Rpp32u bufferLength = batch_dstSize[batchCount].width;
+                        Rpp32u alignedLength = (bufferLength / 4) * 4;
+
+                        __m128 p0;
+                        __m128 vMean = lsx_set1_f32(mean);
+                        __m128 vInvStdDev = lsx_set1_f32(invStdDev);
+
+                        int vectorLoopCount = 0;
+                        for (; vectorLoopCount < alignedLength; vectorLoopCount+=4)
+                        {
+                            srcPtrChannelROI -= 3;
+
+                            for(int cnt = 0; cnt < 4; cnt++)
+                            {
+                                *(srcPtrChannelROIps + cnt) = (Rpp32f) (*(srcPtrChannelROI + cnt));
+                            }
+
+                            p0 = (__m128)__lsx_vld(srcPtrChannelROIps, 0);
+                            p0 = (__m128)__lsx_vpermi_w((__m128i)p0, (__m128i)p0, _MM_SHUFFLE(0,1,2,3));
+                            p0 = __lsx_vfsub_s(p0, vMean);
+                            p0 = __lsx_vfmul_s(p0, vInvStdDev);
+                            __lsx_vst(p0, dstPtrTempps, 0);
+
+                            for(int cnt = 0; cnt < 4; cnt++)
+                            {
+                                *(dstPtrTemp + cnt) = (Rpp16f) (*(dstPtrTempps + cnt));
+                            }
+
+                            srcPtrChannelROI -= 1;
+                            dstPtrTemp += 4;
+                        }
+                        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                        {
+                            *dstPtrTemp = (*srcPtrChannelROI - mean) * invStdDev;
+                            dstPtrTemp++;
+                            srcPtrChannelROI--;
+                        }
+
+                        srcPtrChannelROI += srcROIIncrement;
+                    }
+                }
+            }
+            if (outputFormatToggle == 1)
+            {
+                Rpp16f *dstPtrImageUnpadded = (Rpp16f*) calloc(channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width, sizeof(Rpp16f));
+                Rpp16f *dstPtrImageUnpaddedCopy = (Rpp16f*) calloc(channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width, sizeof(Rpp16f));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_dstSize[batchCount], batch_dstSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width * sizeof(Rpp16f));
+
+                compute_planar_to_packed_host(dstPtrImageUnpaddedCopy, batch_dstSize[batchCount], dstPtrImageUnpadded, channel);
+
+                memset(dstPtrImage, (Rpp16f) 0, dstImageDimMax * channel * sizeof(Rpp16f));
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_dstSize[batchCount], batch_dstSizeMax[batchCount], dstPtrImage, RPPI_CHN_PACKED, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u srcImageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+            Rpp32u dstImageDimMax = batch_dstSizeMax[batchCount].height * batch_dstSizeMax[batchCount].width;
+
+            Rpp32u x1 = batch_crop_pos_x[batchCount];
+            Rpp32u y1 = batch_crop_pos_y[batchCount];
+            Rpp32u x2 = x1 + batch_dstSize[batchCount].width - 1;
+            Rpp32u y2 = y1 + batch_dstSize[batchCount].height - 1;
+
+            Rpp32u mirrorFlag = batch_mirrorFlag[batchCount];
+            Rpp32f mean = batch_mean[batchCount];
+            Rpp32f stdDev = batch_stdDev[batchCount];
+            Rpp32f invStdDev = 1.0 / stdDev;
+
+            Rpp16f *srcPtrImage, *dstPtrImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+            compute_image_location_host(batch_dstSizeMax, batchCount, &dstLoc, channel);
+            srcPtrImage = srcPtr + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+
+            Rpp32u srcElementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+            Rpp32u dstElementsInRowMax = channel * batch_dstSizeMax[batchCount].width;
+            Rpp32u elementsInRowROI = channel * batch_dstSize[batchCount].width;
+
+            Rpp32f srcPtrROIps[4], dstPtrTempps[4];
+
+            if (mirrorFlag == 0)
+            {
+                Rpp16f *srcPtrROI;
+                srcPtrROI = srcPtrImage + (y1 * srcElementsInRowMax) + (x1 * channel);
+
+                Rpp32u srcROIIncrement = srcElementsInRowMax - elementsInRowROI;
+
+
+                for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                {
+                    Rpp16f *dstPtrTemp;
+                    dstPtrTemp = dstPtrImage + (i * dstElementsInRowMax);
+
+                    Rpp32u bufferLength = elementsInRowROI;
+                    Rpp32u alignedLength = (bufferLength / 4) * 4;
+
+                    __m128 p0;
+                    __m128 vMean = lsx_set1_f32(mean);
+                    __m128 vInvStdDev = lsx_set1_f32(invStdDev);
+
+                    int vectorLoopCount = 0;
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount+=4)
+                    {
+                        for(int cnt = 0; cnt < 4; cnt++)
+                        {
+                            *(srcPtrROIps + cnt) = (Rpp32f) (*(srcPtrROI + cnt));
+                        }
+
+                        p0 = (__m128)__lsx_vld(srcPtrROIps, 0);
+                        p0 = __lsx_vfsub_s(p0, vMean);
+                        p0 = __lsx_vfmul_s(p0, vInvStdDev);
+                        __lsx_vst(p0, dstPtrTempps, 0);
+
+                        for(int cnt = 0; cnt < 4; cnt++)
+                        {
+                            *(dstPtrTemp + cnt) = (Rpp16f) (*(dstPtrTempps + cnt));
+                        }
+
+                        srcPtrROI += 4;
+                        dstPtrTemp += 4;
+                    }
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                    {
+                        *dstPtrTemp = (*srcPtrROI - mean) * invStdDev;
+                        dstPtrTemp++;
+                        srcPtrROI++;
+                    }
+
+                    srcPtrROI += srcROIIncrement;
+                }
+            }
+            else if (mirrorFlag == 1)
+            {
+                Rpp16f *srcPtrROI;
+                srcPtrROI = srcPtrImage + (y1 * srcElementsInRowMax) + ((x2 - 1) * channel);
+
+                Rpp32u srcROIIncrement = srcElementsInRowMax + elementsInRowROI;
+
+
+                for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                {
+                    Rpp16f *dstPtrTemp;
+                    dstPtrTemp = dstPtrImage + (i * dstElementsInRowMax);
+
+                    Rpp32u bufferLength = elementsInRowROI;
+
+                    __m128 p0;
+                    __m128 vMean = lsx_set1_f32(mean);
+                    __m128 vInvStdDev = lsx_set1_f32(invStdDev);
+
+                    int vectorLoopCount = 0;
+                    for (; vectorLoopCount < 3; vectorLoopCount+=3)
+                    {
+                        for(int c = 0; c < channel; c++)
+                        {
+                            *dstPtrTemp = (*srcPtrROI - mean) * invStdDev;
+                            dstPtrTemp++;
+                            srcPtrROI++;
+                        }
+                        srcPtrROI -= (2 * channel);
+                    }
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount+=3)
+                    {
+                        for(int cnt = 0; cnt < 4; cnt++)
+                        {
+                            *(srcPtrROIps + cnt) = (Rpp32f) (*(srcPtrROI + cnt));
+                        }
+
+                        p0 = (__m128)__lsx_vld(srcPtrROIps, 0);
+                        p0 = __lsx_vfsub_s(p0, vMean);
+                        p0 = __lsx_vfmul_s(p0, vInvStdDev);
+                        __lsx_vst(p0, dstPtrTempps, 0);
+
+                        for(int cnt = 0; cnt < 4; cnt++)
+                        {
+                            *(dstPtrTemp + cnt) = (Rpp16f) (*(dstPtrTempps + cnt));
+                        }
+
+                        srcPtrROI -= 3;
+                        dstPtrTemp += 3;
+                    }
+
+                    srcPtrROI += srcROIIncrement;
+                }
+            }
+            if (outputFormatToggle == 1)
+            {
+                Rpp16f *dstPtrImageUnpadded = (Rpp16f*) calloc(channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width, sizeof(Rpp16f));
+                Rpp16f *dstPtrImageUnpaddedCopy = (Rpp16f*) calloc(channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width, sizeof(Rpp16f));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_dstSize[batchCount], batch_dstSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width * sizeof(Rpp16f));
+
+                compute_packed_to_planar_host(dstPtrImageUnpaddedCopy, batch_dstSize[batchCount], dstPtrImageUnpadded, channel);
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_dstSize[batchCount], batch_dstSizeMax[batchCount], dstPtrImage, RPPI_CHN_PLANAR, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T, typename U>
+RppStatus crop_mirror_normalize_u8_f_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, U* dstPtr, RppiSize *batch_dstSize, RppiSize *batch_dstSizeMax,
+                                     Rpp32u *batch_crop_pos_x, Rpp32u *batch_crop_pos_y,
+                                     Rpp32f *batch_mean, Rpp32f *batch_stdDev,
+                                     Rpp32u *batch_mirrorFlag, Rpp32u outputFormatToggle,
+                                     Rpp32u nbatchSize,
+                                     RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u srcImageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+            Rpp32u dstImageDimMax = batch_dstSizeMax[batchCount].height * batch_dstSizeMax[batchCount].width;
+
+            Rpp32u x1 = batch_crop_pos_x[batchCount];
+            Rpp32u y1 = batch_crop_pos_y[batchCount];
+            Rpp32u x2 = x1 + batch_dstSize[batchCount].width - 1;
+            Rpp32u y2 = y1 + batch_dstSize[batchCount].height - 1;
+
+            Rpp32u mirrorFlag = batch_mirrorFlag[batchCount];
+            Rpp32f mean = batch_mean[batchCount];
+            Rpp32f stdDev = 255.0 * batch_stdDev[batchCount];
+            Rpp32f invStdDev = 1.0 / stdDev;
+
+            T *srcPtrImage;
+            U *dstPtrImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+            compute_image_location_host(batch_dstSizeMax, batchCount, &dstLoc, channel);
+            srcPtrImage = srcPtr + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+
+            Rpp32f dst[16];
+            Rpp32f *dst0, *dst1, *dst2, *dst3;
+            dst0 = dst;
+            dst1 = dst + 4;
+            dst2 = dst + 8;
+            dst3 = dst + 12;
+
+            if (mirrorFlag == 0)
+            {
+                Rpp32u srcROIIncrement = batch_srcSizeMax[batchCount].width - batch_dstSize[batchCount].width;
+                for(int c = 0; c < channel; c++)
+                {
+                    T *srcPtrChannelROI;
+                    U *dstPtrChannel;
+                    dstPtrChannel = dstPtrImage + (c * dstImageDimMax);
+                    srcPtrChannelROI = srcPtrImage + (c * srcImageDimMax) + (y1 * batch_srcSizeMax[batchCount].width) + x1;
+
+                    for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                    {
+                        U *dstPtrTemp;
+                        dstPtrTemp = dstPtrChannel + (i * batch_dstSizeMax[batchCount].width);
+
+                        Rpp32u bufferLength = batch_dstSize[batchCount].width;
+                        Rpp32u alignedLength = (bufferLength / 16) * 16;
+
+                        __m128i const zero = __lsx_vldi(0);
+                        __m128i px0, px1, px2, px3;
+                        __m128 p0, p1, p2, p3;
+                        __m128 vMean = lsx_set1_f32(mean);
+                        __m128 vInvStdDev = lsx_set1_f32(invStdDev);
+
+                        int vectorLoopCount = 0;
+                        for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+                        {
+                            px0 =  __lsx_vld((__m128i *)srcPtrChannelROI, 0);
+
+                            px1 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+                            px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                            p0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+                            p1 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px0));    // pixels 4-7
+                            p2 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 8-11
+                            p3 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px1));    // pixels 12-15
+
+                            p0 = __lsx_vfsub_s(p0, vMean);
+                            p1 = __lsx_vfsub_s(p1, vMean);
+                            p2 = __lsx_vfsub_s(p2, vMean);
+                            p3 = __lsx_vfsub_s(p3, vMean);
+
+                            p0 = __lsx_vfmul_s(p0, vInvStdDev);
+                            p1 = __lsx_vfmul_s(p1, vInvStdDev);
+                            p2 = __lsx_vfmul_s(p2, vInvStdDev);
+                            p3 = __lsx_vfmul_s(p3, vInvStdDev);
+
+                            __lsx_vst(p0, dst0, 0);
+                            __lsx_vst(p1, dst1, 0);
+                            __lsx_vst(p2, dst2, 0);
+                            __lsx_vst(p3, dst3, 0);
+
+                            for (int j = 0; j < 16; j++)
+                            {
+                                *dstPtrTemp = (U) (*(dst + j));
+                                dstPtrTemp++;
+                            }
+
+                            srcPtrChannelROI += 16;
+                        }
+                        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                        {
+                            *dstPtrTemp = (U) RPPPIXELCHECK(((Rpp32f)(*srcPtrChannelROI) - mean) * invStdDev);
+                            dstPtrTemp++;
+                            srcPtrChannelROI++;
+                        }
+
+                        srcPtrChannelROI += srcROIIncrement;
+
+                    }
+                }
+            }
+            else if (mirrorFlag == 1)
+            {
+                Rpp32u srcROIIncrement = batch_srcSizeMax[batchCount].width + batch_dstSize[batchCount].width;
+                for(int c = 0; c < channel; c++)
+                {
+                    T *srcPtrChannelROI;
+                    U *dstPtrChannel;
+                    dstPtrChannel = dstPtrImage + (c * dstImageDimMax);
+                    srcPtrChannelROI = srcPtrImage + (c * srcImageDimMax) + (y1 * batch_srcSizeMax[batchCount].width) + x2;
+
+                    for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                    {
+                        U *dstPtrTemp;
+                        dstPtrTemp = dstPtrChannel + (i * batch_dstSizeMax[batchCount].width);
+
+                        Rpp32u bufferLength = batch_dstSize[batchCount].width;
+                        Rpp32u alignedLength = (bufferLength / 16) * 16;
+
+                        __m128i const zero = __lsx_vldi(0);
+                        __m128i px0, px1, px2, px3;
+                        __m128 p0, p1, p2, p3;
+                        __m128 vMean = lsx_set1_f32(mean);
+                        __m128 vInvStdDev = lsx_set1_f32(invStdDev);
+                        __m128i vMask = lsx_setr_i8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
+
+                        int vectorLoopCount = 0;
+                        for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+                        {
+                            srcPtrChannelROI -= 15;
+                            px0 =  __lsx_vld((__m128i *)srcPtrChannelROI, 0);
+                            px0 = lsx_shuffle_i8(px0, vMask);
+
+                            px1 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+                            px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                            p0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+                            p1 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px0));    // pixels 4-7
+                            p2 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 8-11
+                            p3 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px1));    // pixels 12-15
+
+                            p0 = __lsx_vfsub_s(p0, vMean);
+                            p1 = __lsx_vfsub_s(p1, vMean);
+                            p2 = __lsx_vfsub_s(p2, vMean);
+                            p3 = __lsx_vfsub_s(p3, vMean);
+
+                            p0 = __lsx_vfmul_s(p0, vInvStdDev);
+                            p1 = __lsx_vfmul_s(p1, vInvStdDev);
+                            p2 = __lsx_vfmul_s(p2, vInvStdDev);
+                            p3 = __lsx_vfmul_s(p3, vInvStdDev);
+
+                            __lsx_vst(p0, dst0, 0);
+                            __lsx_vst(p1, dst1, 0);
+                            __lsx_vst(p2, dst2, 0);
+                            __lsx_vst(p3, dst3, 0);
+
+                            for (int j = 0; j < 16; j++)
+                            {
+                                *dstPtrTemp = (U) (*(dst + j));
+                                dstPtrTemp++;
+                            }
+
+                            srcPtrChannelROI -= 1;
+                        }
+                        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                        {
+                            *dstPtrTemp = (U) RPPPIXELCHECK(((Rpp32f)(*srcPtrChannelROI) - mean) * invStdDev);
+                            dstPtrTemp++;
+                            srcPtrChannelROI--;
+                        }
+
+                        srcPtrChannelROI += srcROIIncrement;
+                    }
+                }
+            }
+            if (outputFormatToggle == 1)
+            {
+                U *dstPtrImageUnpadded = (U*) calloc(channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width, sizeof(U));
+                U *dstPtrImageUnpaddedCopy = (U*) calloc(channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width, sizeof(U));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_dstSize[batchCount], batch_dstSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width * sizeof(U));
+
+                compute_planar_to_packed_host(dstPtrImageUnpaddedCopy, batch_dstSize[batchCount], dstPtrImageUnpadded, channel);
+
+                memset(dstPtrImage, (U) 0, dstImageDimMax * channel * sizeof(U));
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_dstSize[batchCount], batch_dstSizeMax[batchCount], dstPtrImage, RPPI_CHN_PACKED, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u srcImageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+            Rpp32u dstImageDimMax = batch_dstSizeMax[batchCount].height * batch_dstSizeMax[batchCount].width;
+
+            Rpp32u x1 = batch_crop_pos_x[batchCount];
+            Rpp32u y1 = batch_crop_pos_y[batchCount];
+            Rpp32u x2 = x1 + batch_dstSize[batchCount].width - 1;
+            Rpp32u y2 = y1 + batch_dstSize[batchCount].height - 1;
+
+            Rpp32u mirrorFlag = batch_mirrorFlag[batchCount];
+            Rpp32f mean = batch_mean[batchCount];
+            Rpp32f stdDev = 255.0 * batch_stdDev[batchCount];
+            Rpp32f invStdDev = 1.0 / stdDev;
+
+            T *srcPtrImage;
+            U *dstPtrImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+            compute_image_location_host(batch_dstSizeMax, batchCount, &dstLoc, channel);
+            srcPtrImage = srcPtr + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+
+            Rpp32u srcElementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+            Rpp32u dstElementsInRowMax = channel * batch_dstSizeMax[batchCount].width;
+            Rpp32u elementsInRowROI = channel * batch_dstSize[batchCount].width;
+
+            Rpp32f dst[16];
+            Rpp32f *dst0, *dst1, *dst2, *dst3;
+            dst0 = dst;
+            dst1 = dst + 4;
+            dst2 = dst + 8;
+            dst3 = dst + 12;
+
+            if (mirrorFlag == 0)
+            {
+                T *srcPtrROI;
+                srcPtrROI = srcPtrImage + (y1 * srcElementsInRowMax) + (x1 * channel);
+
+                Rpp32u srcROIIncrement = srcElementsInRowMax - elementsInRowROI;
+
+                for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                {
+                    U *dstPtrTemp;
+                    dstPtrTemp = dstPtrImage + (i * dstElementsInRowMax);
+
+                    Rpp32u bufferLength = elementsInRowROI;
+                    Rpp32u alignedLength = (bufferLength / 16) * 16;
+
+                    __m128i const zero = __lsx_vldi(0);
+                    __m128i px0, px1, px2, px3;
+                    __m128 p0, p1, p2, p3;
+                    __m128 vMean = lsx_set1_f32(mean);
+                    __m128 vInvStdDev = lsx_set1_f32(invStdDev);
+
+                    int vectorLoopCount = 0;
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+                    {
+                        px0 =  __lsx_vld((__m128i *)srcPtrROI, 0);
+
+                        px1 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+                        px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                        p0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+                        p1 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px0));    // pixels 4-7
+                        p2 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 8-11
+                        p3 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px1));    // pixels 12-15
+
+                        p0 = __lsx_vfsub_s(p0, vMean);
+                        p1 = __lsx_vfsub_s(p1, vMean);
+                        p2 = __lsx_vfsub_s(p2, vMean);
+                        p3 = __lsx_vfsub_s(p3, vMean);
+
+                        p0 = __lsx_vfmul_s(p0, vInvStdDev);
+                        p1 = __lsx_vfmul_s(p1, vInvStdDev);
+                        p2 = __lsx_vfmul_s(p2, vInvStdDev);
+                        p3 = __lsx_vfmul_s(p3, vInvStdDev);
+
+                        __lsx_vst(p0, dst0, 0);
+                        __lsx_vst(p1, dst1, 0);
+                        __lsx_vst(p2, dst2, 0);
+                        __lsx_vst(p3, dst3, 0);
+
+                        for (int j = 0; j < 16; j++)
+                        {
+                            *dstPtrTemp = (U) (*(dst + j));
+                            dstPtrTemp++;
+                        }
+
+                        srcPtrROI += 16;
+                    }
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                    {
+                        *dstPtrTemp = (U) RPPPIXELCHECK(((Rpp32f)(*srcPtrROI) - mean) * invStdDev);
+                        dstPtrTemp++;
+                        srcPtrROI++;
+                    }
+
+                    srcPtrROI += srcROIIncrement;
+                }
+            }
+            else if (mirrorFlag == 1)
+            {
+                T *srcPtrROI;
+                srcPtrROI = srcPtrImage + (y1 * srcElementsInRowMax) + ((x2 - 1) * channel);
+
+                Rpp32u srcROIIncrement = srcElementsInRowMax + elementsInRowROI;
+
+                for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                {
+                    U *dstPtrTemp;
+                    dstPtrTemp = dstPtrImage + (i * dstElementsInRowMax);
+
+                    Rpp32u bufferLength = elementsInRowROI;
+                    Rpp32u alignedLength = ((bufferLength / 15) * 15) - 1;
+
+                    __m128i const zero = __lsx_vldi(0);
+                    __m128i px0, px1, px2, px3;
+                    __m128 p0, p1, p2, p3;
+                    __m128 vMean = lsx_set1_f32(mean);
+                    __m128 vInvStdDev = lsx_set1_f32(invStdDev);
+                    __m128i vMask = lsx_setr_i8(13, 14, 15, 10, 11, 12, 7, 8, 9, 4, 5, 6, 1, 2, 3, 0);
+
+                    int vectorLoopCount = 0;
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount+=15)
+                    {
+                        srcPtrROI -= 13;
+                        px0 =  __lsx_vld((__m128i *)srcPtrROI, 0);
+                        px0 = lsx_shuffle_i8(px0, vMask);
+
+                        px1 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+                        px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                        p0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+                        p1 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px0));    // pixels 4-7
+                        p2 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 8-11
+                        p3 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px1));    // pixels 12-15
+
+                        p0 = __lsx_vfsub_s(p0, vMean);
+                        p1 = __lsx_vfsub_s(p1, vMean);
+                        p2 = __lsx_vfsub_s(p2, vMean);
+                        p3 = __lsx_vfsub_s(p3, vMean);
+
+                        p0 = __lsx_vfmul_s(p0, vInvStdDev);
+                        p1 = __lsx_vfmul_s(p1, vInvStdDev);
+                        p2 = __lsx_vfmul_s(p2, vInvStdDev);
+                        p3 = __lsx_vfmul_s(p3, vInvStdDev);
+
+                        __lsx_vst(p0, dst0, 0);
+                        __lsx_vst(p1, dst1, 0);
+                        __lsx_vst(p2, dst2, 0);
+                        __lsx_vst(p3, dst3, 0);
+
+                        for (int j = 0; j < 16; j++)
+                        {
+                            *dstPtrTemp = (U) (*(dst + j));
+                            dstPtrTemp++;
+                        }
+
+                        srcPtrROI -= 2;
+                        dstPtrTemp -= 1;
+                    }
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount+=channel)
+                    {
+                        for(int c = 0; c < channel; c++)
+                        {
+                            *dstPtrTemp = (U) RPPPIXELCHECK(((Rpp32f)(*srcPtrROI) - mean) * invStdDev);
+                            dstPtrTemp++;
+                            srcPtrROI++;
+                        }
+                        srcPtrROI -= (2 * channel);
+                    }
+
+                    srcPtrROI += srcROIIncrement;
+                }
+            }
+            if (outputFormatToggle == 1)
+            {
+                U *dstPtrImageUnpadded = (U*) calloc(channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width, sizeof(U));
+                U *dstPtrImageUnpaddedCopy = (U*) calloc(channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width, sizeof(U));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_dstSize[batchCount], batch_dstSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width * sizeof(U));
+
+                compute_packed_to_planar_host(dstPtrImageUnpaddedCopy, batch_dstSize[batchCount], dstPtrImageUnpadded, channel);
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_dstSize[batchCount], batch_dstSizeMax[batchCount], dstPtrImage, RPPI_CHN_PLANAR, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+RppStatus crop_mirror_normalize_u8_i8_host_batch(Rpp8u* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, Rpp8s* dstPtr, RppiSize *batch_dstSize, RppiSize *batch_dstSizeMax,
+                                     Rpp32u *batch_crop_pos_x, Rpp32u *batch_crop_pos_y,
+                                     Rpp32f *batch_mean, Rpp32f *batch_stdDev,
+                                     Rpp32u *batch_mirrorFlag, Rpp32u outputFormatToggle,
+                                     Rpp32u nbatchSize,
+                                     RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u srcBufferSize = nbatchSize * batch_srcSizeMax[0].height * batch_srcSizeMax[0].width * channel;
+    Rpp32u dstBufferSize = nbatchSize * batch_dstSizeMax[0].height * batch_dstSizeMax[0].width * channel;
+
+    Rpp32f *srcPtrf32 = (Rpp32f *)calloc(srcBufferSize, sizeof(Rpp32f));
+    Rpp32f *dstPtrf32 = (Rpp32f *)calloc(dstBufferSize, sizeof(Rpp32f));
+
+    Rpp8u *srcPtrTemp;
+    Rpp32f *srcPtrf32Temp;
+    srcPtrTemp = srcPtr;
+    srcPtrf32Temp = srcPtrf32;
+
+    for (int i = 0; i < srcBufferSize; i++)
+    {
+        *srcPtrf32Temp = (Rpp32f) (*srcPtrTemp) / 255.0;
+        srcPtrTemp++;
+        srcPtrf32Temp++;
+    }
+
+    crop_mirror_normalize_f32_host_batch(srcPtrf32, batch_srcSize, batch_srcSizeMax, dstPtrf32, batch_dstSize, batch_dstSizeMax,
+                                    batch_crop_pos_x, batch_crop_pos_y, batch_mean, batch_stdDev, batch_mirrorFlag, outputFormatToggle,
+                                    nbatchSize, chnFormat, channel, handle);
+
+    Rpp8s *dstPtrTemp;
+    Rpp32f *dstPtrf32Temp;
+    dstPtrTemp = dstPtr;
+    dstPtrf32Temp = dstPtrf32;
+
+    for (int i = 0; i < srcBufferSize; i++)
+    {
+        *dstPtrTemp = (Rpp8s) RPPPIXELCHECKI8((*dstPtrf32Temp * 255.0) - 128);
+        dstPtrTemp++;
+        dstPtrf32Temp++;
+    }
+
+    free(srcPtrf32);
+    free(dstPtrf32);
+
+    return RPP_SUCCESS;
+}
+
+/**************** crop ***************/
+
+template <typename T, typename U>
+RppStatus crop_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, U* dstPtr, RppiSize *batch_dstSize, RppiSize *batch_dstSizeMax,
+                                     Rpp32u *batch_crop_pos_x, Rpp32u *batch_crop_pos_y,
+                                     Rpp32u outputFormatToggle, Rpp32u nbatchSize,
+                                     RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u srcImageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+            Rpp32u dstImageDimMax = batch_dstSizeMax[batchCount].height * batch_dstSizeMax[batchCount].width;
+
+            Rpp32u x1 = batch_crop_pos_x[batchCount];
+            Rpp32u y1 = batch_crop_pos_y[batchCount];
+            Rpp32u x2 = x1 + batch_dstSize[batchCount].width - 1;
+            Rpp32u y2 = y1 + batch_dstSize[batchCount].height - 1;
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+            compute_image_location_host(batch_dstSizeMax, batchCount, &dstLoc, channel);
+            srcPtrImage = srcPtr + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+
+            if (outputFormatToggle == 1)
+            {
+                T *srcPtrImageRoiR, *srcPtrImageRoiG, *srcPtrImageRoiB;
+                T *dstPtrImageTemp;
+
+                srcPtrImageRoiR = srcPtrImage + (y1 * batch_srcSizeMax[batchCount].width) + x1;
+                srcPtrImageRoiG = srcPtrImageRoiR + srcImageDimMax;
+                srcPtrImageRoiB = srcPtrImageRoiG + srcImageDimMax;
+
+                dstPtrImageTemp = dstPtrImage;
+
+                Rpp32u incrementSrc = batch_srcSizeMax[batchCount].width - batch_dstSize[batchCount].width;
+                Rpp32u incrementDst = (batch_dstSizeMax[batchCount].width - batch_dstSize[batchCount].width) * channel;
+
+                for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                {
+                    for(int j = 0; j < batch_dstSize[batchCount].width; j++)
+                    {
+                        *dstPtrImageTemp = *srcPtrImageRoiR;
+                        srcPtrImageRoiR++;
+                        dstPtrImageTemp++;
+
+                        *dstPtrImageTemp = *srcPtrImageRoiG;
+                        srcPtrImageRoiG++;
+                        dstPtrImageTemp++;
+
+                        *dstPtrImageTemp = *srcPtrImageRoiB;
+                        srcPtrImageRoiB++;
+                        dstPtrImageTemp++;
+                    }
+                    dstPtrImageTemp += incrementDst;
+                    srcPtrImageRoiR += incrementSrc;
+                    srcPtrImageRoiG += incrementSrc;
+                    srcPtrImageRoiB += incrementSrc;
+                }
+            }
+            else
+            {
+                for(int c = 0; c < channel; c++)
+                {
+                    T *dstPtrChannel, *srcPtrChannelROI;
+                    dstPtrChannel = dstPtrImage + (c * dstImageDimMax);
+                    srcPtrChannelROI = srcPtrImage + (c * srcImageDimMax) + (y1 * batch_srcSizeMax[batchCount].width) + x1;
+
+                    for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                    {
+                        memcpy(dstPtrChannel, srcPtrChannelROI, batch_dstSize[batchCount].width * sizeof(T));
+                        dstPtrChannel += batch_dstSizeMax[batchCount].width;
+                        srcPtrChannelROI += batch_srcSizeMax[batchCount].width;
+                    }
+                }
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u srcImageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+            Rpp32u dstImageDimMax = batch_dstSizeMax[batchCount].height * batch_dstSizeMax[batchCount].width;
+
+            Rpp32u x1 = batch_crop_pos_x[batchCount];
+            Rpp32u y1 = batch_crop_pos_y[batchCount];
+            Rpp32u x2 = x1 + batch_dstSize[batchCount].width - 1;
+            Rpp32u y2 = y1 + batch_dstSize[batchCount].height - 1;
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+            compute_image_location_host(batch_dstSizeMax, batchCount, &dstLoc, channel);
+            srcPtrImage = srcPtr + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+
+            Rpp32u srcElementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+            Rpp32u dstElementsInRowMax = channel * batch_dstSizeMax[batchCount].width;
+            Rpp32u elementsInRowROI = channel * batch_dstSize[batchCount].width;
+
+            T *srcPtrImageROI, *dstPtrImageTemp;
+            srcPtrImageROI = srcPtrImage + (y1 * srcElementsInRowMax) + (x1 * channel);
+            dstPtrImageTemp = dstPtrImage;
+
+            if (outputFormatToggle == 1)
+            {
+                T *srcPtrImageROITemp;
+
+                T *dstPtrImageTempG, *dstPtrImageTempB;
+                dstPtrImageTempG = dstPtrImageTemp + (batch_dstSizeMax[batchCount].height * batch_dstSizeMax[batchCount].width);
+                dstPtrImageTempB = dstPtrImageTempG + (batch_dstSizeMax[batchCount].height * batch_dstSizeMax[batchCount].width);
+
+                Rpp32u increment = batch_dstSizeMax[batchCount].width - batch_dstSize[batchCount].width;
+
+                for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                {
+                    srcPtrImageROITemp = srcPtrImageROI;
+                    for(int j = 0; j < batch_dstSize[batchCount].width; j++)
+                    {
+                        *dstPtrImageTemp = *srcPtrImageROITemp;
+                        srcPtrImageROITemp++;
+                        dstPtrImageTemp++;
+
+                        *dstPtrImageTempG = *srcPtrImageROITemp;
+                        srcPtrImageROITemp++;
+                        dstPtrImageTempG++;
+
+                        *dstPtrImageTempB = *srcPtrImageROITemp;
+                        srcPtrImageROITemp++;
+                        dstPtrImageTempB++;
+                    }
+                    dstPtrImageTemp += increment;
+                    dstPtrImageTempG += increment;
+                    dstPtrImageTempB += increment;
+                    srcPtrImageROI += srcElementsInRowMax;
+                }
+            }
+            else
+            {
+                for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                {
+                    memcpy(dstPtrImageTemp, srcPtrImageROI, elementsInRowROI * sizeof(T));
+                    dstPtrImageTemp += dstElementsInRowMax;
+                    srcPtrImageROI += srcElementsInRowMax;
+                }
+            }
+
+        }
+    }
+    return RPP_SUCCESS;
+}
+
+template <typename T, typename U>
+RppStatus crop_host_u_f_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, U* dstPtr, RppiSize *batch_dstSize, RppiSize *batch_dstSizeMax,
+                                     Rpp32u *batch_crop_pos_x, Rpp32u *batch_crop_pos_y,
+                                     Rpp32u outputFormatToggle, Rpp32u nbatchSize,
+                                     RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u srcImageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+            Rpp32u dstImageDimMax = batch_dstSizeMax[batchCount].height * batch_dstSizeMax[batchCount].width;
+
+            Rpp32u x1 = batch_crop_pos_x[batchCount];
+            Rpp32u y1 = batch_crop_pos_y[batchCount];
+            Rpp32u x2 = x1 + batch_dstSize[batchCount].width - 1;
+            Rpp32u y2 = y1 + batch_dstSize[batchCount].height - 1;
+
+            T *srcPtrImage;
+            U *dstPtrImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+            compute_image_location_host(batch_dstSizeMax, batchCount, &dstLoc, channel);
+            srcPtrImage = srcPtr + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+
+            U multiplier = (U) (1.0 / 255.0);
+
+            if (outputFormatToggle == 1)
+            {
+                T *srcPtrImageRoiR, *srcPtrImageRoiG, *srcPtrImageRoiB;
+                U *dstPtrImageTemp;
+
+                srcPtrImageRoiR = srcPtrImage + (y1 * batch_srcSizeMax[batchCount].width) + x1;
+                srcPtrImageRoiG = srcPtrImageRoiR + srcImageDimMax;
+                srcPtrImageRoiB = srcPtrImageRoiG + srcImageDimMax;
+
+                dstPtrImageTemp = dstPtrImage;
+
+                Rpp32u incrementSrc = batch_srcSizeMax[batchCount].width - batch_dstSize[batchCount].width;
+                Rpp32u incrementDst = (batch_dstSizeMax[batchCount].width - batch_dstSize[batchCount].width) * channel;
+
+                for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                {
+                    for(int j = 0; j < batch_dstSize[batchCount].width; j++)
+                    {
+                        *dstPtrImageTemp = (U) *srcPtrImageRoiR * multiplier;
+                        srcPtrImageRoiR++;
+                        dstPtrImageTemp++;
+
+                        *dstPtrImageTemp = (U) *srcPtrImageRoiG * multiplier;
+                        srcPtrImageRoiG++;
+                        dstPtrImageTemp++;
+
+                        *dstPtrImageTemp = (U) *srcPtrImageRoiB * multiplier;
+                        srcPtrImageRoiB++;
+                        dstPtrImageTemp++;
+                    }
+                    dstPtrImageTemp += incrementDst;
+                    srcPtrImageRoiR += incrementSrc;
+                    srcPtrImageRoiG += incrementSrc;
+                    srcPtrImageRoiB += incrementSrc;
+                }
+            }
+            else
+            {
+                for(int c = 0; c < channel; c++)
+                {
+                    T *srcPtrChannelROI, *srcPtrChannelROITemp;
+                    U *dstPtrChannel, *dstPtrChannelTemp;
+                    dstPtrChannel = dstPtrImage + (c * dstImageDimMax);
+                    srcPtrChannelROI = srcPtrImage + (c * srcImageDimMax) + (y1 * batch_srcSizeMax[batchCount].width) + x1;
+
+
+                    for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                    {
+                        dstPtrChannelTemp = dstPtrChannel;
+                        srcPtrChannelROITemp = srcPtrChannelROI;
+
+                        for (int j = 0; j < batch_dstSize[batchCount].width; j++)
+                        {
+                            *dstPtrChannelTemp = (U) *srcPtrChannelROITemp * multiplier;
+                            dstPtrChannelTemp++;
+                            srcPtrChannelROITemp++;
+                        }
+
+                        dstPtrChannel += batch_dstSizeMax[batchCount].width;
+                        srcPtrChannelROI += batch_srcSizeMax[batchCount].width;
+                    }
+                }
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u srcImageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+            Rpp32u dstImageDimMax = batch_dstSizeMax[batchCount].height * batch_dstSizeMax[batchCount].width;
+
+            Rpp32u x1 = batch_crop_pos_x[batchCount];
+            Rpp32u y1 = batch_crop_pos_y[batchCount];
+            Rpp32u x2 = x1 + batch_dstSize[batchCount].width - 1;
+            Rpp32u y2 = y1 + batch_dstSize[batchCount].height - 1;
+
+            T *srcPtrImage;
+            U *dstPtrImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+            compute_image_location_host(batch_dstSizeMax, batchCount, &dstLoc, channel);
+            srcPtrImage = srcPtr + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+
+            U multiplier = (U) (1.0 / 255.0);
+
+            Rpp32u srcElementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+            Rpp32u dstElementsInRowMax = channel * batch_dstSizeMax[batchCount].width;
+            Rpp32u elementsInRowROI = channel * batch_dstSize[batchCount].width;
+
+            T *srcPtrImageROI, *srcPtrImageROITemp;
+            U *dstPtrImageRow, *dstPtrImageRowTemp;
+            srcPtrImageROI = srcPtrImage + (y1 * srcElementsInRowMax) + (x1 * channel);
+            dstPtrImageRow = dstPtrImage;
+
+            if (outputFormatToggle == 1)
+            {
+                T *srcPtrImageROITemp;
+
+                U *dstPtrImageTempG, *dstPtrImageTempB;
+                dstPtrImageTempG = dstPtrImageRow + (batch_dstSizeMax[batchCount].height * batch_dstSizeMax[batchCount].width);
+                dstPtrImageTempB = dstPtrImageTempG + (batch_dstSizeMax[batchCount].height * batch_dstSizeMax[batchCount].width);
+
+                Rpp32u increment = batch_dstSizeMax[batchCount].width - batch_dstSize[batchCount].width;
+
+                for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                {
+                    srcPtrImageROITemp = srcPtrImageROI;
+                    for(int j = 0; j < batch_dstSize[batchCount].width; j++)
+                    {
+                        *dstPtrImageRow = (U) *srcPtrImageROITemp * multiplier;
+                        srcPtrImageROITemp++;
+                        dstPtrImageRow++;
+
+                        *dstPtrImageTempG = (U) *srcPtrImageROITemp * multiplier;
+                        srcPtrImageROITemp++;
+                        dstPtrImageTempG++;
+
+                        *dstPtrImageTempB = (U) *srcPtrImageROITemp * multiplier;
+                        srcPtrImageROITemp++;
+                        dstPtrImageTempB++;
+                    }
+                    dstPtrImageRow += increment;
+                    dstPtrImageTempG += increment;
+                    dstPtrImageTempB += increment;
+                    srcPtrImageROI += srcElementsInRowMax;
+                }
+            }
+            else
+            {
+                for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                {
+                    srcPtrImageROITemp = srcPtrImageROI;
+                    dstPtrImageRowTemp = dstPtrImageRow;
+
+                    for (int j = 0; j < elementsInRowROI; j++)
+                    {
+                        *dstPtrImageRowTemp = (U) *srcPtrImageROITemp * multiplier;
+                        dstPtrImageRowTemp++;
+                        srcPtrImageROITemp++;
+                    }
+
+                    dstPtrImageRow += dstElementsInRowMax;
+                    srcPtrImageROI += srcElementsInRowMax;
+                }
+            }
+        }
+    }
+    return RPP_SUCCESS;
+}
+
+template <typename T, typename U>
+RppStatus crop_host_u_i_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, U* dstPtr, RppiSize *batch_dstSize, RppiSize *batch_dstSizeMax,
+                                     Rpp32u *batch_crop_pos_x, Rpp32u *batch_crop_pos_y,
+                                     Rpp32u outputFormatToggle, Rpp32u nbatchSize,
+                                     RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u srcImageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+            Rpp32u dstImageDimMax = batch_dstSizeMax[batchCount].height * batch_dstSizeMax[batchCount].width;
+
+            Rpp32u x1 = batch_crop_pos_x[batchCount];
+            Rpp32u y1 = batch_crop_pos_y[batchCount];
+            Rpp32u x2 = x1 + batch_dstSize[batchCount].width - 1;
+            Rpp32u y2 = y1 + batch_dstSize[batchCount].height - 1;
+
+            T *srcPtrImage;
+            U *dstPtrImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+            compute_image_location_host(batch_dstSizeMax, batchCount, &dstLoc, channel);
+            srcPtrImage = srcPtr + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+
+            if (outputFormatToggle == 1)
+            {
+                T *srcPtrImageRoiR, *srcPtrImageRoiG, *srcPtrImageRoiB;
+                U *dstPtrImageTemp;
+
+                srcPtrImageRoiR = srcPtrImage + (y1 * batch_srcSizeMax[batchCount].width) + x1;
+                srcPtrImageRoiG = srcPtrImageRoiR + srcImageDimMax;
+                srcPtrImageRoiB = srcPtrImageRoiG + srcImageDimMax;
+
+                dstPtrImageTemp = dstPtrImage;
+
+                Rpp32u incrementSrc = batch_srcSizeMax[batchCount].width - batch_dstSize[batchCount].width;
+                Rpp32u incrementDst = (batch_dstSizeMax[batchCount].width - batch_dstSize[batchCount].width) * channel;
+
+                for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                {
+                    for(int j = 0; j < batch_dstSize[batchCount].width; j++)
+                    {
+                        *dstPtrImageTemp = (U) (((Rpp32s) *srcPtrImageRoiR) - 128);
+                        srcPtrImageRoiR++;
+                        dstPtrImageTemp++;
+
+                        *dstPtrImageTemp = (U) (((Rpp32s) *srcPtrImageRoiG) - 128);
+                        srcPtrImageRoiG++;
+                        dstPtrImageTemp++;
+
+                        *dstPtrImageTemp = (U) (((Rpp32s) *srcPtrImageRoiB) - 128);
+                        srcPtrImageRoiB++;
+                        dstPtrImageTemp++;
+                    }
+                    dstPtrImageTemp += incrementDst;
+                    srcPtrImageRoiR += incrementSrc;
+                    srcPtrImageRoiG += incrementSrc;
+                    srcPtrImageRoiB += incrementSrc;
+                }
+            }
+            else
+            {
+                for(int c = 0; c < channel; c++)
+                {
+                    T *srcPtrChannelROI, *srcPtrChannelROITemp;
+                    U *dstPtrChannel, *dstPtrChannelTemp;
+                    dstPtrChannel = dstPtrImage + (c * dstImageDimMax);
+                    srcPtrChannelROI = srcPtrImage + (c * srcImageDimMax) + (y1 * batch_srcSizeMax[batchCount].width) + x1;
+
+
+                    for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                    {
+                        dstPtrChannelTemp = dstPtrChannel;
+                        srcPtrChannelROITemp = srcPtrChannelROI;
+
+                        for (int j = 0; j < batch_dstSize[batchCount].width; j++)
+                        {
+                            *dstPtrChannelTemp = (U) (((Rpp32s) *srcPtrChannelROITemp) - 128);
+                            dstPtrChannelTemp++;
+                            srcPtrChannelROITemp++;
+                        }
+
+                        dstPtrChannel += batch_dstSizeMax[batchCount].width;
+                        srcPtrChannelROI += batch_srcSizeMax[batchCount].width;
+                    }
+                }
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u srcImageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+            Rpp32u dstImageDimMax = batch_dstSizeMax[batchCount].height * batch_dstSizeMax[batchCount].width;
+
+            Rpp32u x1 = batch_crop_pos_x[batchCount];
+            Rpp32u y1 = batch_crop_pos_y[batchCount];
+            Rpp32u x2 = x1 + batch_dstSize[batchCount].width - 1;
+            Rpp32u y2 = y1 + batch_dstSize[batchCount].height - 1;
+
+            T *srcPtrImage;
+            U *dstPtrImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+            compute_image_location_host(batch_dstSizeMax, batchCount, &dstLoc, channel);
+            srcPtrImage = srcPtr + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+
+            Rpp32u srcElementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+            Rpp32u dstElementsInRowMax = channel * batch_dstSizeMax[batchCount].width;
+            Rpp32u elementsInRowROI = channel * batch_dstSize[batchCount].width;
+
+            T *srcPtrImageROI, *srcPtrImageROITemp;
+            U *dstPtrImageRow, *dstPtrImageRowTemp;
+            srcPtrImageROI = srcPtrImage + (y1 * srcElementsInRowMax) + (x1 * channel);
+            dstPtrImageRow = dstPtrImage;
+
+            if (outputFormatToggle == 1)
+            {
+                T *srcPtrImageROITemp;
+
+                U *dstPtrImageTempG, *dstPtrImageTempB;
+                dstPtrImageTempG = dstPtrImageRow + (batch_dstSizeMax[batchCount].height * batch_dstSizeMax[batchCount].width);
+                dstPtrImageTempB = dstPtrImageTempG + (batch_dstSizeMax[batchCount].height * batch_dstSizeMax[batchCount].width);
+
+                Rpp32u increment = batch_dstSizeMax[batchCount].width - batch_dstSize[batchCount].width;
+
+                for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                {
+                    srcPtrImageROITemp = srcPtrImageROI;
+                    for(int j = 0; j < batch_dstSize[batchCount].width; j++)
+                    {
+                        *dstPtrImageRow = (U) (((Rpp32s) *srcPtrImageROITemp) - 128);
+                        srcPtrImageROITemp++;
+                        dstPtrImageRow++;
+
+                        *dstPtrImageTempG = (U) (((Rpp32s) *srcPtrImageROITemp) - 128);
+                        srcPtrImageROITemp++;
+                        dstPtrImageTempG++;
+
+                        *dstPtrImageTempB = (U) (((Rpp32s) *srcPtrImageROITemp) - 128);
+                        srcPtrImageROITemp++;
+                        dstPtrImageTempB++;
+                    }
+                    dstPtrImageRow += increment;
+                    dstPtrImageTempG += increment;
+                    dstPtrImageTempB += increment;
+                    srcPtrImageROI += srcElementsInRowMax;
+                }
+            }
+            else
+            {
+                for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                {
+                    srcPtrImageROITemp = srcPtrImageROI;
+                    dstPtrImageRowTemp = dstPtrImageRow;
+
+                    for (int j = 0; j < elementsInRowROI; j++)
+                    {
+                        *dstPtrImageRowTemp = (U) (((Rpp32s) *srcPtrImageROITemp) - 128);
+                        dstPtrImageRowTemp++;
+                        srcPtrImageROITemp++;
+                    }
+
+                    dstPtrImageRow += dstElementsInRowMax;
+                    srcPtrImageROI += srcElementsInRowMax;
+                }
+            }
+        }
+    }
+    return RPP_SUCCESS;
+}
+
+// /**************** resize_crop_mirror ***************/
+
+template <typename T>
+RppStatus resize_crop_mirror_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr, RppiSize *batch_dstSize, RppiSize *batch_dstSizeMax,
+                                        Rpp32u *batch_x1, Rpp32u *batch_x2, Rpp32u *batch_y1, Rpp32u *batch_y2, Rpp32u *batch_mirrorFlag,
+                                        Rpp32u outputFormatToggle, Rpp32u nbatchSize,
+                                        RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u x1 = batch_x1[batchCount];
+            Rpp32u x2 = batch_x2[batchCount];
+            Rpp32u y1 = batch_y1[batchCount];
+            Rpp32u y2 = batch_y2[batchCount];
+            Rpp32u mirrorFlag = batch_mirrorFlag[batchCount];
+
+            Rpp64u srcImageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+            Rpp64u dstImageDimMax = batch_dstSizeMax[batchCount].height * batch_dstSizeMax[batchCount].width;
+
+            Rpp32f hRatio = (((Rpp32f) (batch_dstSizeMax[batchCount].height - 1)) / ((Rpp32f) (batch_srcSizeMax[batchCount].height - 1)));
+            Rpp32f wRatio = (((Rpp32f) (batch_dstSizeMax[batchCount].width - 1)) / ((Rpp32f) (batch_srcSizeMax[batchCount].width - 1)));
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+            compute_image_location_host(batch_dstSizeMax, batchCount, &dstLoc, channel);
+            srcPtrImage = srcPtr + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+
+            RppiSize srcSizeROI, dstSize;
+            srcSizeROI.height = RPPABS(y2 - y1) + 1;
+            srcSizeROI.width = RPPABS(x2 - x1) + 1;
+            dstSize.height = batch_dstSize[batchCount].height;
+            dstSize.width = batch_dstSize[batchCount].width;
+
+            T *srcPtrROI = (T *)calloc(srcSizeROI.height * srcSizeROI.width * channel, sizeof(T));
+            T *dstPtrROI = (T *)calloc(dstSize.height * dstSize.width * channel, sizeof(T));
+            T *srcPtrImageTemp, *srcPtrROITemp;
+
+            srcPtrROITemp = srcPtrROI;
+            for (int c = 0; c < channel; c++)
+            {
+                srcPtrImageTemp = srcPtrImage + (c * srcImageDimMax) + ((Rpp32u) y1 * batch_srcSizeMax[batchCount].width) + (Rpp32u) x1;
+                for (int i = 0; i < srcSizeROI.height; i++)
+                {
+                    memcpy(srcPtrROITemp, srcPtrImageTemp, srcSizeROI.width * sizeof(T));
+                    srcPtrImageTemp += batch_srcSizeMax[batchCount].width;
+                    srcPtrROITemp += srcSizeROI.width;
+                }
+            }
+
+            if (mirrorFlag == 0)
+            {
+                resize_kernel_host(srcPtrROI, srcSizeROI, dstPtrROI, dstSize, chnFormat, channel);
+            }
+            else if (mirrorFlag == 1)
+            {
+                T *srcPtrROIMirrorred = (T *)calloc(srcSizeROI.height * srcSizeROI.width * channel, sizeof(T));
+                T *srcPtrROITemp2, *srcPtrROIMirrorredTemp;
+                srcPtrROIMirrorredTemp = srcPtrROIMirrorred;
+                Rpp32u bufferLength = srcSizeROI.width;
+                Rpp32u alignedLength = (bufferLength / 16) * 16;
+
+                for (int c = 0; c < channel; c++)
+                {
+                    srcPtrROITemp = srcPtrROI + (c * srcSizeROI.height * srcSizeROI.width) + srcSizeROI.width - 1;
+                    for (int i = 0; i < srcSizeROI.height; i++)
+                    {
+                        srcPtrROITemp2 = srcPtrROITemp;
+
+                        __m128i px0;
+                        __m128i vMask = lsx_setr_i8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
+
+                        int vectorLoopCount = 0;
+                        for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+                        {
+                            srcPtrROITemp2 -= 15;
+                            px0 = __lsx_vld((__m128i *)srcPtrROITemp2, 0);
+                            px0 = lsx_shuffle_i8(px0, vMask);
+                            __lsx_vst(px0, (__m128i *)srcPtrROIMirrorredTemp, 0);
+                            srcPtrROITemp2 -= 1;
+                            srcPtrROIMirrorredTemp += 16;
+                        }
+                        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                        {
+                            *srcPtrROIMirrorredTemp++ = (T) *srcPtrROITemp2--;
+                        }
+                        srcPtrROITemp = srcPtrROITemp + srcSizeROI.width;
+                    }
+                }
+
+                resize_kernel_host(srcPtrROIMirrorred, srcSizeROI, dstPtrROI, dstSize, chnFormat, channel);
+
+                free(srcPtrROIMirrorred);
+            }
+
+            if (outputFormatToggle == 1)
+            {
+                T *dstPtrROICopy = (T *)calloc(dstSize.height * dstSize.width * channel, sizeof(T));
+                compute_planar_to_packed_host(dstPtrROI, dstSize, dstPtrROICopy, channel);
+                compute_padded_from_unpadded_host(dstPtrROICopy, dstSize, batch_dstSizeMax[batchCount], dstPtrImage, RPPI_CHN_PACKED, channel);
+                free(dstPtrROICopy);
+            }
+            else
+            {
+                compute_padded_from_unpadded_host(dstPtrROI, dstSize, batch_dstSizeMax[batchCount], dstPtrImage, chnFormat, channel);
+            }
+
+            free(srcPtrROI);
+            free(dstPtrROI);
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u x1 = batch_x1[batchCount];
+            Rpp32u x2 = batch_x2[batchCount];
+            Rpp32u y1 = batch_y1[batchCount];
+            Rpp32u y2 = batch_y2[batchCount];
+            Rpp32u mirrorFlag = batch_mirrorFlag[batchCount];
+
+            Rpp32f hRatio = (((Rpp32f) (batch_dstSizeMax[batchCount].height - 1)) / ((Rpp32f) (batch_srcSizeMax[batchCount].height - 1)));
+            Rpp32f wRatio = (((Rpp32f) (batch_dstSizeMax[batchCount].width - 1)) / ((Rpp32f) (batch_srcSizeMax[batchCount].width - 1)));
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+            compute_image_location_host(batch_dstSizeMax, batchCount, &dstLoc, channel);
+            srcPtrImage = srcPtr + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+
+            RppiSize srcSizeROI, dstSize;
+            srcSizeROI.height = RPPABS(y2 - y1) + 1;
+            srcSizeROI.width = RPPABS(x2 - x1) + 1;
+            dstSize.height = batch_dstSize[batchCount].height;
+            dstSize.width = batch_dstSize[batchCount].width;
+
+            Rpp32u elementsInRow = channel * batch_srcSize[batchCount].width;
+            Rpp32u elementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+            Rpp32u elementsInRowROI = channel * srcSizeROI.width;
+
+            T *srcPtrROI = (T *)calloc(srcSizeROI.height * srcSizeROI.width * channel, sizeof(T));
+            T *dstPtrROI = (T *)calloc(dstSize.height * dstSize.width * channel, sizeof(T));
+            T *srcPtrImageTemp, *srcPtrROITemp;
+            srcPtrROITemp = srcPtrROI;
+
+            srcPtrImageTemp = srcPtrImage + ((Rpp32u) y1 * elementsInRowMax) + (channel * (Rpp32u) x1);
+            for (int i = 0; i < srcSizeROI.height; i++)
+            {
+                memcpy(srcPtrROITemp, srcPtrImageTemp, elementsInRowROI * sizeof(T));
+                srcPtrImageTemp += elementsInRowMax;
+                srcPtrROITemp += elementsInRowROI;
+            }
+
+            if (mirrorFlag == 0)
+            {
+                resize_kernel_host(srcPtrROI, srcSizeROI, dstPtrROI, dstSize, chnFormat, channel);
+            }
+            else if (mirrorFlag == 1)
+            {
+                T *srcPtrROIMirrorred = (T *)calloc(srcSizeROI.height * srcSizeROI.width * channel, sizeof(T));
+                T *srcPtrROIMirrorredTemp;
+                srcPtrROIMirrorredTemp = srcPtrROIMirrorred;
+                Rpp32u bufferLength = channel * srcSizeROI.width;
+                Rpp32u alignedLength = (bufferLength / 15) * 15;
+
+                srcPtrROITemp = srcPtrROI + (channel * (srcSizeROI.width - 1));
+
+                for (int i = 0; i < srcSizeROI.height; i++)
+                {
+                    __m128i px0;
+                    __m128i vMask = lsx_setr_i8(13, 14, 15, 10, 11, 12, 7, 8, 9, 4, 5, 6, 1, 2, 3, 0);
+
+                    int vectorLoopCount = 0;
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount+=15)
+                    {
+                        srcPtrROITemp -= 13;
+                        px0 = __lsx_vld((__m128i *)srcPtrROITemp, 0);
+                        px0 = lsx_shuffle_i8(px0, vMask);
+                        __lsx_vst(px0, (__m128i *)srcPtrROIMirrorredTemp, 0);
+                        srcPtrROITemp -= 2;
+                        srcPtrROIMirrorredTemp += 15;
+                    }
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount+=channel)
+                    {
+                        memcpy(srcPtrROIMirrorredTemp, srcPtrROITemp, channel * sizeof(T));
+                        srcPtrROIMirrorredTemp += channel;
+                        srcPtrROITemp -= channel;
+                    }
+
+                    srcPtrROITemp = srcPtrROITemp + (channel * (2 * srcSizeROI.width));
+                }
+
+                resize_kernel_host(srcPtrROIMirrorred, srcSizeROI, dstPtrROI, dstSize, chnFormat, channel);
+
+                free(srcPtrROIMirrorred);
+            }
+
+            if (outputFormatToggle == 1)
+            {
+                T *dstPtrROICopy = (T *)calloc(dstSize.height * dstSize.width * channel, sizeof(T));
+                compute_packed_to_planar_host(dstPtrROI, dstSize, dstPtrROICopy, channel);
+                compute_padded_from_unpadded_host(dstPtrROICopy, dstSize, batch_dstSizeMax[batchCount], dstPtrImage, RPPI_CHN_PLANAR, channel);
+                free(dstPtrROICopy);
+            }
+            else
+            {
+                compute_padded_from_unpadded_host(dstPtrROI, dstSize, batch_dstSizeMax[batchCount], dstPtrImage, chnFormat, channel);
+            }
+
+            free(srcPtrROI);
+            free(dstPtrROI);
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+RppStatus resize_crop_mirror_f32_host_batch(Rpp32f* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, Rpp32f* dstPtr, RppiSize *batch_dstSize, RppiSize *batch_dstSizeMax,
+                                        Rpp32u *batch_x1, Rpp32u *batch_x2, Rpp32u *batch_y1, Rpp32u *batch_y2, Rpp32u *batch_mirrorFlag,
+                                        Rpp32u outputFormatToggle, Rpp32u nbatchSize,
+                                        RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u x1 = batch_x1[batchCount];
+            Rpp32u x2 = batch_x2[batchCount];
+            Rpp32u y1 = batch_y1[batchCount];
+            Rpp32u y2 = batch_y2[batchCount];
+            Rpp32u mirrorFlag = batch_mirrorFlag[batchCount];
+
+            Rpp64u srcImageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+            Rpp64u dstImageDimMax = batch_dstSizeMax[batchCount].height * batch_dstSizeMax[batchCount].width;
+
+            Rpp32f hRatio = (((Rpp32f) (batch_dstSizeMax[batchCount].height - 1)) / ((Rpp32f) (batch_srcSizeMax[batchCount].height - 1)));
+            Rpp32f wRatio = (((Rpp32f) (batch_dstSizeMax[batchCount].width - 1)) / ((Rpp32f) (batch_srcSizeMax[batchCount].width - 1)));
+
+            Rpp32f *srcPtrImage, *dstPtrImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+            compute_image_location_host(batch_dstSizeMax, batchCount, &dstLoc, channel);
+            srcPtrImage = srcPtr + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+
+            RppiSize srcSizeROI, dstSize;
+            srcSizeROI.height = RPPABS(y2 - y1) + 1;
+            srcSizeROI.width = RPPABS(x2 - x1) + 1;
+            dstSize.height = batch_dstSize[batchCount].height;
+            dstSize.width = batch_dstSize[batchCount].width;
+
+            Rpp32f *srcPtrROI = (Rpp32f *)calloc(srcSizeROI.height * srcSizeROI.width * channel, sizeof(Rpp32f));
+            Rpp32f *dstPtrROI = (Rpp32f *)calloc(dstSize.height * dstSize.width * channel, sizeof(Rpp32f));
+            Rpp32f *srcPtrImageTemp, *srcPtrROITemp;
+
+            srcPtrROITemp = srcPtrROI;
+            for (int c = 0; c < channel; c++)
+            {
+                srcPtrImageTemp = srcPtrImage + (c * srcImageDimMax) + ((Rpp32u) y1 * batch_srcSizeMax[batchCount].width) + (Rpp32u) x1;
+                for (int i = 0; i < srcSizeROI.height; i++)
+                {
+                    memcpy(srcPtrROITemp, srcPtrImageTemp, srcSizeROI.width * sizeof(Rpp32f));
+                    srcPtrImageTemp += batch_srcSizeMax[batchCount].width;
+                    srcPtrROITemp += srcSizeROI.width;
+                }
+            }
+
+            if (mirrorFlag == 0)
+            {
+                resize_kernel_host(srcPtrROI, srcSizeROI, dstPtrROI, dstSize, chnFormat, channel);
+            }
+            else if (mirrorFlag == 1)
+            {
+                Rpp32f *srcPtrROIMirrorred = (Rpp32f *)calloc(srcSizeROI.height * srcSizeROI.width * channel, sizeof(Rpp32f));
+                Rpp32f *srcPtrROITemp2, *srcPtrROIMirrorredTemp;
+                srcPtrROIMirrorredTemp = srcPtrROIMirrorred;
+                Rpp32u bufferLength = srcSizeROI.width;
+                Rpp32u alignedLength = (bufferLength / 4) * 4;
+
+                for (int c = 0; c < channel; c++)
+                {
+                    srcPtrROITemp = srcPtrROI + (c * srcSizeROI.height * srcSizeROI.width) + srcSizeROI.width - 1;
+                    for (int i = 0; i < srcSizeROI.height; i++)
+                    {
+                        srcPtrROITemp2 = srcPtrROITemp;
+
+                        __m128 p0;
+
+                        int vectorLoopCount = 0;
+                        for (; vectorLoopCount < alignedLength; vectorLoopCount+=4)
+                        {
+                            srcPtrROITemp2 -= 3;
+                            p0 = (__m128)__lsx_vld(srcPtrROITemp2, 0);
+                            p0 = (__m128)__lsx_vpermi_w((__m128i)p0, (__m128i)p0, _MM_SHUFFLE(0,1,2,3));
+                            __lsx_vst(p0, srcPtrROIMirrorredTemp, 0);
+
+                            srcPtrROITemp2 -= 1;
+                            srcPtrROIMirrorredTemp += 4;
+                        }
+                        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                        {
+                            *srcPtrROIMirrorredTemp++ = *srcPtrROITemp2--;
+                        }
+                        srcPtrROITemp = srcPtrROITemp + srcSizeROI.width;
+                    }
+                }
+
+                resize_kernel_host(srcPtrROIMirrorred, srcSizeROI, dstPtrROI, dstSize, chnFormat, channel);
+
+                free(srcPtrROIMirrorred);
+            }
+
+            if (outputFormatToggle == 1)
+            {
+                Rpp32f *dstPtrROICopy = (Rpp32f *)calloc(dstSize.height * dstSize.width * channel, sizeof(Rpp32f));
+                compute_planar_to_packed_host(dstPtrROI, dstSize, dstPtrROICopy, channel);
+                compute_padded_from_unpadded_host(dstPtrROICopy, dstSize, batch_dstSizeMax[batchCount], dstPtrImage, RPPI_CHN_PACKED, channel);
+                free(dstPtrROICopy);
+            }
+            else
+            {
+                compute_padded_from_unpadded_host(dstPtrROI, dstSize, batch_dstSizeMax[batchCount], dstPtrImage, chnFormat, channel);
+            }
+
+            free(srcPtrROI);
+            free(dstPtrROI);
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u x1 = batch_x1[batchCount];
+            Rpp32u x2 = batch_x2[batchCount];
+            Rpp32u y1 = batch_y1[batchCount];
+            Rpp32u y2 = batch_y2[batchCount];
+            Rpp32u mirrorFlag = batch_mirrorFlag[batchCount];
+
+            Rpp32f hRatio = (((Rpp32f) (batch_dstSizeMax[batchCount].height - 1)) / ((Rpp32f) (batch_srcSizeMax[batchCount].height - 1)));
+            Rpp32f wRatio = (((Rpp32f) (batch_dstSizeMax[batchCount].width - 1)) / ((Rpp32f) (batch_srcSizeMax[batchCount].width - 1)));
+
+            Rpp32f *srcPtrImage, *dstPtrImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+            compute_image_location_host(batch_dstSizeMax, batchCount, &dstLoc, channel);
+            srcPtrImage = srcPtr + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+
+            RppiSize srcSizeROI, dstSize;
+            srcSizeROI.height = RPPABS(y2 - y1) + 1;
+            srcSizeROI.width = RPPABS(x2 - x1) + 1;
+            dstSize.height = batch_dstSize[batchCount].height;
+            dstSize.width = batch_dstSize[batchCount].width;
+
+            Rpp32u elementsInRow = channel * batch_srcSize[batchCount].width;
+            Rpp32u elementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+            Rpp32u elementsInRowROI = channel * srcSizeROI.width;
+
+            Rpp32f *srcPtrROI = (Rpp32f *)calloc(srcSizeROI.height * srcSizeROI.width * channel, sizeof(Rpp32f));
+            Rpp32f *dstPtrROI = (Rpp32f *)calloc(dstSize.height * dstSize.width * channel, sizeof(Rpp32f));
+            Rpp32f *srcPtrImageTemp, *srcPtrROITemp;
+            srcPtrROITemp = srcPtrROI;
+
+            srcPtrImageTemp = srcPtrImage + ((Rpp32u) y1 * elementsInRowMax) + (channel * (Rpp32u) x1);
+            for (int i = 0; i < srcSizeROI.height; i++)
+            {
+                memcpy(srcPtrROITemp, srcPtrImageTemp, elementsInRowROI * sizeof(Rpp32f));
+                srcPtrImageTemp += elementsInRowMax;
+                srcPtrROITemp += elementsInRowROI;
+            }
+
+            if (mirrorFlag == 0)
+            {
+                resize_kernel_host(srcPtrROI, srcSizeROI, dstPtrROI, dstSize, chnFormat, channel);
+            }
+            else if (mirrorFlag == 1)
+            {
+                Rpp32f *srcPtrROIMirrorred = (Rpp32f *)calloc(srcSizeROI.height * srcSizeROI.width * channel, sizeof(Rpp32f));
+                Rpp32f *srcPtrROIMirrorredTemp;
+                srcPtrROIMirrorredTemp = srcPtrROIMirrorred;
+                Rpp32u bufferLength = channel * srcSizeROI.width;
+
+                srcPtrROITemp = srcPtrROI + (channel * (srcSizeROI.width - 1));
+
+                for (int i = 0; i < srcSizeROI.height; i++)
+                {
+                    __m128 p0;
+
+                    int vectorLoopCount = 0;
+                    for (; vectorLoopCount < 3; vectorLoopCount+=3)
+                    {
+                        for(int c = 0; c < channel; c++)
+                        {
+                            *srcPtrROIMirrorredTemp = *srcPtrROITemp;
+                            srcPtrROIMirrorredTemp++;
+                            srcPtrROITemp++;
+                        }
+                        srcPtrROITemp -= (2 * channel);
+                    }
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount+=3)
+                    {
+                        p0 = (__m128)__lsx_vld(srcPtrROITemp, 0);
+                        __lsx_vst(p0, srcPtrROIMirrorredTemp, 0);
+
+                        srcPtrROITemp -= 3;
+                        srcPtrROIMirrorredTemp += 3;
+                    }
+
+                    srcPtrROITemp = srcPtrROITemp + (channel * (2 * srcSizeROI.width));
+                }
+
+                resize_kernel_host(srcPtrROIMirrorred, srcSizeROI, dstPtrROI, dstSize, chnFormat, channel);
+
+                free(srcPtrROIMirrorred);
+            }
+
+            if (outputFormatToggle == 1)
+            {
+                Rpp32f *dstPtrROICopy = (Rpp32f *)calloc(dstSize.height * dstSize.width * channel, sizeof(Rpp32f));
+                compute_packed_to_planar_host(dstPtrROI, dstSize, dstPtrROICopy, channel);
+                compute_padded_from_unpadded_host(dstPtrROICopy, dstSize, batch_dstSizeMax[batchCount], dstPtrImage, RPPI_CHN_PLANAR, channel);
+                free(dstPtrROICopy);
+            }
+            else
+            {
+                compute_padded_from_unpadded_host(dstPtrROI, dstSize, batch_dstSizeMax[batchCount], dstPtrImage, chnFormat, channel);
+            }
+
+            free(srcPtrROI);
+            free(dstPtrROI);
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+RppStatus resize_crop_mirror_f16_host_batch(Rpp16f* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, Rpp16f* dstPtr, RppiSize *batch_dstSize, RppiSize *batch_dstSizeMax,
+                                        Rpp32u *batch_x1, Rpp32u *batch_x2, Rpp32u *batch_y1, Rpp32u *batch_y2, Rpp32u *batch_mirrorFlag,
+                                        Rpp32u outputFormatToggle, Rpp32u nbatchSize,
+                                        RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u x1 = batch_x1[batchCount];
+            Rpp32u x2 = batch_x2[batchCount];
+            Rpp32u y1 = batch_y1[batchCount];
+            Rpp32u y2 = batch_y2[batchCount];
+            Rpp32u mirrorFlag = batch_mirrorFlag[batchCount];
+
+            Rpp64u srcImageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+            Rpp64u dstImageDimMax = batch_dstSizeMax[batchCount].height * batch_dstSizeMax[batchCount].width;
+
+            Rpp32f hRatio = (((Rpp32f) (batch_dstSizeMax[batchCount].height - 1)) / ((Rpp32f) (batch_srcSizeMax[batchCount].height - 1)));
+            Rpp32f wRatio = (((Rpp32f) (batch_dstSizeMax[batchCount].width - 1)) / ((Rpp32f) (batch_srcSizeMax[batchCount].width - 1)));
+
+            Rpp16f *srcPtrImage, *dstPtrImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+            compute_image_location_host(batch_dstSizeMax, batchCount, &dstLoc, channel);
+            srcPtrImage = srcPtr + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+
+            RppiSize srcSizeROI, dstSize;
+            srcSizeROI.height = RPPABS(y2 - y1) + 1;
+            srcSizeROI.width = RPPABS(x2 - x1) + 1;
+            dstSize.height = batch_dstSize[batchCount].height;
+            dstSize.width = batch_dstSize[batchCount].width;
+
+            Rpp16f *srcPtrROI = (Rpp16f *)calloc(srcSizeROI.height * srcSizeROI.width * channel, sizeof(Rpp16f));
+            Rpp16f *dstPtrROI = (Rpp16f *)calloc(dstSize.height * dstSize.width * channel, sizeof(Rpp16f));
+            Rpp16f *srcPtrImageTemp, *srcPtrROITemp;
+
+            srcPtrROITemp = srcPtrROI;
+            for (int c = 0; c < channel; c++)
+            {
+                srcPtrImageTemp = srcPtrImage + (c * srcImageDimMax) + ((Rpp32u) y1 * batch_srcSizeMax[batchCount].width) + (Rpp32u) x1;
+                for (int i = 0; i < srcSizeROI.height; i++)
+                {
+                    memcpy(srcPtrROITemp, srcPtrImageTemp, srcSizeROI.width * sizeof(Rpp16f));
+                    srcPtrImageTemp += batch_srcSizeMax[batchCount].width;
+                    srcPtrROITemp += srcSizeROI.width;
+                }
+            }
+
+            if (mirrorFlag == 0)
+            {
+                resize_kernel_host(srcPtrROI, srcSizeROI, dstPtrROI, dstSize, chnFormat, channel);
+            }
+            else if (mirrorFlag == 1)
+            {
+                Rpp16f *srcPtrROIMirrorred = (Rpp16f *)calloc(srcSizeROI.height * srcSizeROI.width * channel, sizeof(Rpp16f));
+                Rpp16f *srcPtrROITemp2, *srcPtrROIMirrorredTemp;
+                srcPtrROIMirrorredTemp = srcPtrROIMirrorred;
+                Rpp32u bufferLength = srcSizeROI.width;
+                Rpp32u alignedLength = (bufferLength / 4) * 4;
+
+                for (int c = 0; c < channel; c++)
+                {
+                    srcPtrROITemp = srcPtrROI + (c * srcSizeROI.height * srcSizeROI.width) + srcSizeROI.width - 1;
+                    for (int i = 0; i < srcSizeROI.height; i++)
+                    {
+                        srcPtrROITemp2 = srcPtrROITemp;
+
+                        __m128 p0;
+
+                        int vectorLoopCount = 0;
+                        for (; vectorLoopCount < alignedLength; vectorLoopCount+=4)
+                        {
+                            srcPtrROITemp2 -= 3;
+
+                            *srcPtrROIMirrorredTemp = *(srcPtrROITemp2 + 3);
+                            *(srcPtrROIMirrorredTemp + 1) = *(srcPtrROITemp2 + 2);
+                            *(srcPtrROIMirrorredTemp + 2) = *(srcPtrROITemp2 + 1);
+                            *(srcPtrROIMirrorredTemp + 3) = *srcPtrROITemp2;
+
+                            srcPtrROITemp2 -= 1;
+                            srcPtrROIMirrorredTemp += 4;
+                        }
+                        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                        {
+                            *srcPtrROIMirrorredTemp++ = *srcPtrROITemp2--;
+                        }
+                        srcPtrROITemp = srcPtrROITemp + srcSizeROI.width;
+                    }
+                }
+
+                resize_kernel_host(srcPtrROIMirrorred, srcSizeROI, dstPtrROI, dstSize, chnFormat, channel);
+
+                free(srcPtrROIMirrorred);
+            }
+
+            if (outputFormatToggle == 1)
+            {
+                Rpp16f *dstPtrROICopy = (Rpp16f *)calloc(dstSize.height * dstSize.width * channel, sizeof(Rpp16f));
+                compute_planar_to_packed_host(dstPtrROI, dstSize, dstPtrROICopy, channel);
+                compute_padded_from_unpadded_host(dstPtrROICopy, dstSize, batch_dstSizeMax[batchCount], dstPtrImage, RPPI_CHN_PACKED, channel);
+                free(dstPtrROICopy);
+            }
+            else
+            {
+                compute_padded_from_unpadded_host(dstPtrROI, dstSize, batch_dstSizeMax[batchCount], dstPtrImage, chnFormat, channel);
+            }
+
+            free(srcPtrROI);
+            free(dstPtrROI);
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u x1 = batch_x1[batchCount];
+            Rpp32u x2 = batch_x2[batchCount];
+            Rpp32u y1 = batch_y1[batchCount];
+            Rpp32u y2 = batch_y2[batchCount];
+            Rpp32u mirrorFlag = batch_mirrorFlag[batchCount];
+
+            Rpp32f hRatio = (((Rpp32f) (batch_dstSizeMax[batchCount].height - 1)) / ((Rpp32f) (batch_srcSizeMax[batchCount].height - 1)));
+            Rpp32f wRatio = (((Rpp32f) (batch_dstSizeMax[batchCount].width - 1)) / ((Rpp32f) (batch_srcSizeMax[batchCount].width - 1)));
+
+            Rpp16f *srcPtrImage, *dstPtrImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+            compute_image_location_host(batch_dstSizeMax, batchCount, &dstLoc, channel);
+            srcPtrImage = srcPtr + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+
+            RppiSize srcSizeROI, dstSize;
+            srcSizeROI.height = RPPABS(y2 - y1) + 1;
+            srcSizeROI.width = RPPABS(x2 - x1) + 1;
+            dstSize.height = batch_dstSize[batchCount].height;
+            dstSize.width = batch_dstSize[batchCount].width;
+
+            Rpp32u elementsInRow = channel * batch_srcSize[batchCount].width;
+            Rpp32u elementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+            Rpp32u elementsInRowROI = channel * srcSizeROI.width;
+
+            Rpp16f *srcPtrROI = (Rpp16f *)calloc(srcSizeROI.height * srcSizeROI.width * channel, sizeof(Rpp16f));
+            Rpp16f *dstPtrROI = (Rpp16f *)calloc(dstSize.height * dstSize.width * channel, sizeof(Rpp16f));
+            Rpp16f *srcPtrImageTemp, *srcPtrROITemp;
+            srcPtrROITemp = srcPtrROI;
+
+            srcPtrImageTemp = srcPtrImage + ((Rpp32u) y1 * elementsInRowMax) + (channel * (Rpp32u) x1);
+            for (int i = 0; i < srcSizeROI.height; i++)
+            {
+                memcpy(srcPtrROITemp, srcPtrImageTemp, elementsInRowROI * sizeof(Rpp16f));
+                srcPtrImageTemp += elementsInRowMax;
+                srcPtrROITemp += elementsInRowROI;
+            }
+
+            if (mirrorFlag == 0)
+            {
+                resize_kernel_host(srcPtrROI, srcSizeROI, dstPtrROI, dstSize, chnFormat, channel);
+            }
+            else if (mirrorFlag == 1)
+            {
+                Rpp16f *srcPtrROIMirrorred = (Rpp16f *)calloc(srcSizeROI.height * srcSizeROI.width * channel, sizeof(Rpp16f));
+                Rpp16f *srcPtrROIMirrorredTemp;
+                srcPtrROIMirrorredTemp = srcPtrROIMirrorred;
+                Rpp32u bufferLength = channel * srcSizeROI.width;
+
+                srcPtrROITemp = srcPtrROI + (channel * (srcSizeROI.width - 1));
+
+                for (int i = 0; i < srcSizeROI.height; i++)
+                {
+                    __m128 p0;
+
+                    int vectorLoopCount = 0;
+                    for (; vectorLoopCount < 3; vectorLoopCount+=3)
+                    {
+                        for(int c = 0; c < channel; c++)
+                        {
+                            *srcPtrROIMirrorredTemp = *srcPtrROITemp;
+                            srcPtrROIMirrorredTemp++;
+                            srcPtrROITemp++;
+                        }
+                        srcPtrROITemp -= (2 * channel);
+                    }
+
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount+=3)
+                    {
+                        memcpy(srcPtrROIMirrorredTemp, srcPtrROITemp, 3 * sizeof(Rpp16f));
+
+                        srcPtrROITemp -= 3;
+                        srcPtrROIMirrorredTemp += 3;
+                    }
+
+                    srcPtrROITemp = srcPtrROITemp + (channel * (2 * srcSizeROI.width));
+                }
+
+                resize_kernel_host(srcPtrROIMirrorred, srcSizeROI, dstPtrROI, dstSize, chnFormat, channel);
+
+                free(srcPtrROIMirrorred);
+            }
+
+            if (outputFormatToggle == 1)
+            {
+                Rpp16f *dstPtrROICopy = (Rpp16f *)calloc(dstSize.height * dstSize.width * channel, sizeof(Rpp16f));
+                compute_packed_to_planar_host(dstPtrROI, dstSize, dstPtrROICopy, channel);
+                compute_padded_from_unpadded_host(dstPtrROICopy, dstSize, batch_dstSizeMax[batchCount], dstPtrImage, RPPI_CHN_PLANAR, channel);
+                free(dstPtrROICopy);
+            }
+            else
+            {
+                compute_padded_from_unpadded_host(dstPtrROI, dstSize, batch_dstSizeMax[batchCount], dstPtrImage, chnFormat, channel);
+            }
+
+            free(srcPtrROI);
+            free(dstPtrROI);
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus resize_mirror_normalize_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr, RppiSize *batch_dstSize, RppiSize *batch_dstSizeMax,
+                                        Rpp32f *batch_mean, Rpp32f *batch_stdDev, Rpp32u *batch_mirrorFlag,
+                                        Rpp32u outputFormatToggle, Rpp32u nbatchSize,
+                                        RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        T *dstPtrCopy = (T*) calloc(channel * batch_dstSizeMax[0].height * batch_dstSizeMax[0].width * nbatchSize, sizeof(T));
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u srcImageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+            Rpp32u dstImageDimMax = batch_dstSizeMax[batchCount].height * batch_dstSizeMax[batchCount].width;
+
+            Rpp32u mirrorFlag = batch_mirrorFlag[batchCount];
+            Rpp32f mean = batch_mean[batchCount];
+            Rpp32f stdDev = batch_stdDev[batchCount];
+            Rpp32f invStdDev = 1.0 / stdDev;
+
+            T *srcPtrImage, *dstPtrImage, *dstPtrCopyImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+            compute_image_location_host(batch_dstSizeMax, batchCount, &dstLoc, channel);
+            srcPtrImage = srcPtr + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+            dstPtrCopyImage = dstPtrCopy + dstLoc;
+
+            T *srcPtrImageUnpadded = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+            T *dstPtrImageUnpadded = (T*) calloc(channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width, sizeof(T));
+            compute_unpadded_from_padded_host(srcPtrImage, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], srcPtrImageUnpadded, chnFormat, channel);
+            resize_kernel_host(srcPtrImageUnpadded, batch_srcSize[batchCount], dstPtrImageUnpadded, batch_dstSize[batchCount], chnFormat, channel);
+            compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_dstSize[batchCount], batch_dstSizeMax[batchCount], dstPtrImage, chnFormat, channel);
+            free(srcPtrImageUnpadded);
+            free(dstPtrImageUnpadded);
+
+            if (mirrorFlag == 0)
+            {
+                if ((mean != 0) || (stdDev != 1))
+                {
+                    memcpy(dstPtrCopyImage, dstPtrImage, dstImageDimMax * channel);
+                    Rpp32u dstElementsInRowMax = batch_dstSizeMax[batchCount].width;
+                    Rpp32u dstElementsInRow = batch_dstSize[batchCount].width;
+
+                    Rpp32u dstROIIncrement = dstElementsInRowMax - dstElementsInRow;
+                    for(int c = 0; c < channel; c++)
+                    {
+                        T *dstPtrChannel, *dstPtrCopyChannel;
+                        dstPtrChannel = dstPtrImage + (c * dstImageDimMax);
+                        dstPtrCopyChannel = dstPtrCopyImage + (c * dstImageDimMax);
+
+
+                        for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                        {
+                            T *dstPtrTemp;
+                            dstPtrTemp = dstPtrChannel + (i * dstElementsInRowMax);
+
+                            Rpp32u bufferLength = dstElementsInRow;
+                            Rpp32u alignedLength = (bufferLength / 16) * 16;
+
+                            __m128i const zero = __lsx_vldi(0);
+                            __m128i px0, px1, px2, px3;
+                            __m128 p0, p1, p2, p3;
+                            __m128 vMean = lsx_set1_f32(mean);
+                            __m128 vInvStdDev = lsx_set1_f32(invStdDev);
+
+                            int vectorLoopCount = 0;
+                            for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+                            {
+                                px0 =  __lsx_vld((__m128i *)dstPtrCopyChannel, 0);
+
+                                px1 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+                                px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                                p0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+                                p1 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px0));    // pixels 4-7
+                                p2 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 8-11
+                                p3 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px1));    // pixels 12-15
+
+                                p0 = __lsx_vfsub_s(p0, vMean);
+                                p1 = __lsx_vfsub_s(p1, vMean);
+                                p2 = __lsx_vfsub_s(p2, vMean);
+                                p3 = __lsx_vfsub_s(p3, vMean);
+                                px0 = __lsx_vftint_w_s(__lsx_vfmul_s(p0, vInvStdDev));
+                                px1 = __lsx_vftint_w_s(__lsx_vfmul_s(p1, vInvStdDev));
+                                px2 = __lsx_vftint_w_s(__lsx_vfmul_s(p2, vInvStdDev));
+                                px3 = __lsx_vftint_w_s(__lsx_vfmul_s(p3, vInvStdDev));
+
+                                px0 = lsx_packus_i32(px0, px1);    // pixels 0-7
+                                px1 = lsx_packus_i32(px2, px3);    // pixels 8-15
+                                px0 = lsx_packus_i16(px0, px1);    // pixels 0-15
+
+                                __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+                                dstPtrCopyChannel += 16;
+                                dstPtrTemp += 16;
+                            }
+                            for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                            {
+                                *dstPtrTemp = (T) RPPPIXELCHECK(((Rpp32f)(*dstPtrCopyChannel) - mean) * invStdDev);
+                                dstPtrTemp++;
+                                dstPtrCopyChannel++;
+                            }
+
+                            dstPtrCopyChannel += dstROIIncrement;
+
+                        }
+                    }
+                }
+            }
+            else if (mirrorFlag == 1)
+            {
+                memcpy(dstPtrCopyImage, dstPtrImage, dstImageDimMax * channel);
+                Rpp32u dstElementsInRowMax = batch_dstSizeMax[batchCount].width;
+                Rpp32u dstElementsInRow = batch_dstSize[batchCount].width;
+
+                if ((mean != 0) || (stdDev != 1))
+                {
+                    Rpp32u dstROIIncrement = dstElementsInRowMax + dstElementsInRow;
+                    for(int c = 0; c < channel; c++)
+                    {
+                        T *dstPtrChannel, *dstPtrCopyChannel;
+                        dstPtrChannel = dstPtrImage + (c * dstImageDimMax);
+                        dstPtrCopyChannel = dstPtrCopyImage + (c * dstImageDimMax) + dstElementsInRow - 1;
+
+                        for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                        {
+                            T *dstPtrTemp;
+                            dstPtrTemp = dstPtrChannel + (i * dstElementsInRowMax);
+
+                            Rpp32u bufferLength = dstElementsInRow;
+                            Rpp32u alignedLength = (bufferLength / 16) * 16;
+
+                            __m128i const zero = __lsx_vldi(0);
+                            __m128i px0, px1, px2, px3;
+                            __m128 p0, p1, p2, p3;
+                            __m128 vMean = lsx_set1_f32(mean);
+                            __m128 vInvStdDev = lsx_set1_f32(invStdDev);
+                            __m128i vMask = lsx_setr_i8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
+
+                            int vectorLoopCount = 0;
+                            for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+                            {
+                                dstPtrCopyChannel -= 15;
+                                px0 =  __lsx_vld((__m128i *)dstPtrCopyChannel, 0);
+                                px0 = lsx_shuffle_i8(px0, vMask);
+
+                                px1 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+                                px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                                p0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+                                p1 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px0));    // pixels 4-7
+                                p2 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 8-11
+                                p3 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px1));    // pixels 12-15
+
+                                p0 = __lsx_vfsub_s(p0, vMean);
+                                p1 = __lsx_vfsub_s(p1, vMean);
+                                p2 = __lsx_vfsub_s(p2, vMean);
+                                p3 = __lsx_vfsub_s(p3, vMean);
+                                px0 = __lsx_vftint_w_s(__lsx_vfmul_s(p0, vInvStdDev));
+                                px1 = __lsx_vftint_w_s(__lsx_vfmul_s(p1, vInvStdDev));
+                                px2 = __lsx_vftint_w_s(__lsx_vfmul_s(p2, vInvStdDev));
+                                px3 = __lsx_vftint_w_s(__lsx_vfmul_s(p3, vInvStdDev));
+
+                                px0 = lsx_packus_i32(px0, px1);    // pixels 0-7
+                                px1 = lsx_packus_i32(px2, px3);    // pixels 8-15
+                                px0 = lsx_packus_i16(px0, px1);    // pixels 0-15
+
+                                __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+                                dstPtrCopyChannel -= 1;
+                                dstPtrTemp += 16;
+                            }
+                            for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                            {
+                                *dstPtrTemp = (T) RPPPIXELCHECK(((Rpp32f)(*dstPtrCopyChannel) - mean) * invStdDev);
+                                dstPtrTemp++;
+                                dstPtrCopyChannel--;
+                            }
+
+                            dstPtrCopyChannel += dstROIIncrement;
+                        }
+                    }
+                }
+                else if ((mean == 0) && (stdDev == 1))
+                {
+                    Rpp32u dstROIIncrement = dstElementsInRowMax + dstElementsInRow;
+                    for(int c = 0; c < channel; c++)
+                    {
+                        T *dstPtrChannel, *dstPtrCopyChannel;
+                        dstPtrChannel = dstPtrImage + (c * dstImageDimMax);
+                        dstPtrCopyChannel = dstPtrCopyImage + (c * dstImageDimMax) + dstElementsInRow - 1;
+
+                        for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                        {
+                            T *dstPtrTemp;
+                            dstPtrTemp = dstPtrChannel + (i * dstElementsInRowMax);
+
+                            Rpp32u bufferLength = dstElementsInRow;
+                            Rpp32u alignedLength = (bufferLength / 16) * 16;
+
+                            __m128i vMask = lsx_setr_i8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
+                            __m128i px0;
+
+                            int vectorLoopCount = 0;
+                            for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+                            {
+                                dstPtrCopyChannel -= 15;
+                                px0 =  __lsx_vld((__m128i *)dstPtrCopyChannel, 0);
+                                px0 = lsx_shuffle_i8(px0, vMask);
+
+                                __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+                                dstPtrCopyChannel -= 1;
+                                dstPtrTemp += 16;
+                            }
+                            for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                            {
+                                *dstPtrTemp = (T) RPPPIXELCHECK(*dstPtrCopyChannel);
+                                dstPtrTemp++;
+                                dstPtrCopyChannel--;
+                            }
+
+                            dstPtrCopyChannel += dstROIIncrement;
+                        }
+                    }
+                }
+            }
+            if (outputFormatToggle == 1)
+            {
+                T *dstPtrImageUnpadded = (T*) calloc(channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width, sizeof(T));
+                T *dstPtrImageUnpaddedCopy = (T*) calloc(channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width, sizeof(T));
+                compute_unpadded_from_padded_host(dstPtrImage, batch_dstSize[batchCount], batch_dstSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+                compute_planar_to_packed_host(dstPtrImageUnpadded, batch_dstSize[batchCount], dstPtrImageUnpaddedCopy, channel);
+                memset(dstPtrImage, (T) 0, dstImageDimMax * channel * sizeof(T));
+                compute_padded_from_unpadded_host(dstPtrImageUnpaddedCopy, batch_dstSize[batchCount], batch_dstSizeMax[batchCount], dstPtrImage, RPPI_CHN_PACKED, channel);
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+
+        free(dstPtrCopy);
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        T *dstPtrCopy = (T*) calloc(channel * batch_dstSizeMax[0].height * batch_dstSizeMax[0].width * nbatchSize, sizeof(T));
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u srcImageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+            Rpp32u dstImageDimMax = batch_dstSizeMax[batchCount].height * batch_dstSizeMax[batchCount].width;
+
+            Rpp32u mirrorFlag = batch_mirrorFlag[batchCount];
+            Rpp32f mean = batch_mean[batchCount];
+            Rpp32f stdDev = batch_stdDev[batchCount];
+            Rpp32f invStdDev = 1.0 / stdDev;
+
+            T *srcPtrImage, *dstPtrImage, *dstPtrCopyImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+            compute_image_location_host(batch_dstSizeMax, batchCount, &dstLoc, channel);
+            srcPtrImage = srcPtr + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+            dstPtrCopyImage = dstPtrCopy + dstLoc;
+
+            T *srcPtrImageUnpadded = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+            T *dstPtrImageUnpadded = (T*) calloc(channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width, sizeof(T));
+            compute_unpadded_from_padded_host(srcPtrImage, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], srcPtrImageUnpadded, chnFormat, channel);
+            resize_kernel_host(srcPtrImageUnpadded, batch_srcSize[batchCount], dstPtrImageUnpadded, batch_dstSize[batchCount], chnFormat, channel);
+            compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_dstSize[batchCount], batch_dstSizeMax[batchCount], dstPtrImage, chnFormat, channel);
+            free(srcPtrImageUnpadded);
+            free(dstPtrImageUnpadded);
+
+            if (mirrorFlag == 0)
+            {
+                if ((mean != 0) || (stdDev != 1))
+                {
+                    memcpy(dstPtrCopyImage, dstPtrImage, dstImageDimMax * channel);
+                    Rpp32u dstElementsInRowMax = channel * batch_dstSizeMax[batchCount].width;
+                    Rpp32u dstElementsInRow = channel * batch_dstSize[batchCount].width;
+
+                    T  *dstPtrCopyImageTemp;
+                    dstPtrCopyImageTemp = dstPtrCopyImage;
+
+                    Rpp32u dstROIIncrement = dstElementsInRowMax - dstElementsInRow;
+
+                    for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                    {
+                        T *dstPtrTemp;
+                        dstPtrTemp = dstPtrImage + (i * dstElementsInRowMax);
+
+                        Rpp32u bufferLength = dstElementsInRow;
+                        Rpp32u alignedLength = (bufferLength / 16) * 16;
+
+                        __m128i const zero = __lsx_vldi(0);
+                        __m128i px0, px1, px2, px3;
+                        __m128 p0, p1, p2, p3;
+                        __m128 vMean = lsx_set1_f32(mean);
+                        __m128 vInvStdDev = lsx_set1_f32(invStdDev);
+
+                        int vectorLoopCount = 0;
+                        for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+                        {
+                            px0 =  __lsx_vld((__m128i *)dstPtrCopyImageTemp, 0);
+
+                            px1 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+                            px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                            p0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+                            p1 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px0));    // pixels 4-7
+                            p2 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 8-11
+                            p3 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px1));    // pixels 12-15
+
+                            p0 = __lsx_vfsub_s(p0, vMean);
+                            p1 = __lsx_vfsub_s(p1, vMean);
+                            p2 = __lsx_vfsub_s(p2, vMean);
+                            p3 = __lsx_vfsub_s(p3, vMean);
+                            px0 = __lsx_vftint_w_s(__lsx_vfmul_s(p0, vInvStdDev));
+                            px1 = __lsx_vftint_w_s(__lsx_vfmul_s(p1, vInvStdDev));
+                            px2 = __lsx_vftint_w_s(__lsx_vfmul_s(p2, vInvStdDev));
+                            px3 = __lsx_vftint_w_s(__lsx_vfmul_s(p3, vInvStdDev));
+
+                            px0 = lsx_packus_i32(px0, px1);    // pixels 0-7
+                            px1 = lsx_packus_i32(px2, px3);    // pixels 8-15
+                            px0 = lsx_packus_i16(px0, px1);    // pixels 0-15
+
+                            __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+                            dstPtrCopyImageTemp += 16;
+                            dstPtrTemp += 16;
+                        }
+                        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                        {
+                            *dstPtrTemp = (T) RPPPIXELCHECK(((Rpp32f)(*dstPtrCopyImageTemp) - mean) * invStdDev);
+                            dstPtrTemp++;
+                            dstPtrCopyImageTemp++;
+                        }
+
+                        dstPtrCopyImageTemp += dstROIIncrement;
+                    }
+                }
+            }
+            else if (mirrorFlag == 1)
+            {
+                memcpy(dstPtrCopyImage, dstPtrImage, dstImageDimMax * channel);
+                Rpp32u dstElementsInRowMax = channel * batch_dstSizeMax[batchCount].width;
+                Rpp32u dstElementsInRow = channel * batch_dstSize[batchCount].width;
+
+                if ((mean != 0) || (stdDev != 1))
+                {
+                    T  *dstPtrCopyImageTemp;
+                    dstPtrCopyImageTemp = dstPtrCopyImage + dstElementsInRow - channel;
+                    Rpp32u dstROIIncrement = dstElementsInRowMax + dstElementsInRow;
+
+                    for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                    {
+                        T *dstPtrTemp;
+                        dstPtrTemp = dstPtrImage + (i * dstElementsInRowMax);
+
+                        Rpp32u bufferLength = dstElementsInRow;
+                        Rpp32u alignedLength = ((bufferLength / 15) * 15) - 1;
+
+                        __m128i const zero = __lsx_vldi(0);
+                        __m128i px0, px1, px2, px3;
+                        __m128 p0, p1, p2, p3;
+                        __m128 vMean = lsx_set1_f32(mean);
+                        __m128 vInvStdDev = lsx_set1_f32(invStdDev);
+                        __m128i vMask = lsx_setr_i8(13, 14, 15, 10, 11, 12, 7, 8, 9, 4, 5, 6, 1, 2, 3, 0);
+
+                        int vectorLoopCount = 0;
+                        for (; vectorLoopCount < alignedLength; vectorLoopCount+=15)
+                        {
+                            dstPtrCopyImageTemp -= 13;
+                            px0 =  __lsx_vld((__m128i *)dstPtrCopyImageTemp, 0);
+                            px0 = lsx_shuffle_i8(px0, vMask);
+
+                            px1 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+                            px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                            p0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+                            p1 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px0));    // pixels 4-7
+                            p2 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 8-11
+                            p3 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px1));    // pixels 12-15
+
+                            p0 = __lsx_vfsub_s(p0, vMean);
+                            p1 = __lsx_vfsub_s(p1, vMean);
+                            p2 = __lsx_vfsub_s(p2, vMean);
+                            p3 = __lsx_vfsub_s(p3, vMean);
+                            px0 = __lsx_vftint_w_s(__lsx_vfmul_s(p0, vInvStdDev));
+                            px1 = __lsx_vftint_w_s(__lsx_vfmul_s(p1, vInvStdDev));
+                            px2 = __lsx_vftint_w_s(__lsx_vfmul_s(p2, vInvStdDev));
+                            px3 = __lsx_vftint_w_s(__lsx_vfmul_s(p3, vInvStdDev));
+
+                            px0 = lsx_packus_i32(px0, px1);    // pixels 0-7
+                            px1 = lsx_packus_i32(px2, px3);    // pixels 8-15
+                            px0 = lsx_packus_i16(px0, px1);    // pixels 0-15
+
+                            __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+                            dstPtrCopyImageTemp -= 2;
+                            dstPtrTemp += 15;
+                        }
+                        for (; vectorLoopCount < bufferLength; vectorLoopCount+=channel)
+                        {
+                            for(int c = 0; c < channel; c++)
+                            {
+                                *dstPtrTemp = (T) RPPPIXELCHECK(((Rpp32f)(*dstPtrCopyImageTemp) - mean) * invStdDev);
+                                dstPtrTemp++;
+                                dstPtrCopyImageTemp++;
+                            }
+                            dstPtrCopyImageTemp -= (2 * channel);
+                        }
+
+                        dstPtrCopyImageTemp += dstROIIncrement;
+                    }
+                }
+                else if ((mean == 0) && (stdDev == 1))
+                {
+                    T  *dstPtrCopyImageTemp;
+                    dstPtrCopyImageTemp = dstPtrCopyImage + dstElementsInRow - channel;
+                    Rpp32u dstROIIncrement = dstElementsInRowMax + dstElementsInRow;
+
+                    for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                    {
+                        T *dstPtrTemp;
+                        dstPtrTemp = dstPtrImage + (i * dstElementsInRowMax);
+
+                        Rpp32u bufferLength = dstElementsInRow;
+                        Rpp32u alignedLength = ((bufferLength / 15) * 15) - 1;
+
+                        __m128i vMask = lsx_setr_i8(13, 14, 15, 10, 11, 12, 7, 8, 9, 4, 5, 6, 1, 2, 3, 0);
+                        __m128i px0;
+
+                        int vectorLoopCount = 0;
+                        for (; vectorLoopCount < alignedLength; vectorLoopCount+=15)
+                        {
+                            dstPtrCopyImageTemp -= 13;
+                            px0 =  __lsx_vld((__m128i *)dstPtrCopyImageTemp, 0);
+                            px0 = lsx_shuffle_i8(px0, vMask);
+                            __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+                            dstPtrCopyImageTemp -= 2;
+                            dstPtrTemp += 15;
+                        }
+                        for (; vectorLoopCount < bufferLength; vectorLoopCount+=channel)
+                        {
+                            for(int c = 0; c < channel; c++)
+                            {
+                                *dstPtrTemp = (T) RPPPIXELCHECK(*dstPtrCopyImageTemp);
+                                dstPtrTemp++;
+                                dstPtrCopyImageTemp++;
+                            }
+                            dstPtrCopyImageTemp -= (2 * channel);
+                        }
+
+                        dstPtrCopyImageTemp += dstROIIncrement;
+                    }
+                }
+            }
+            if (outputFormatToggle == 1)
+            {
+                T *dstPtrImageUnpadded = (T*) calloc(channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width, sizeof(T));
+                T *dstPtrImageUnpaddedCopy = (T*) calloc(channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width, sizeof(T));
+                compute_unpadded_from_padded_host(dstPtrImage, batch_dstSize[batchCount], batch_dstSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+                compute_packed_to_planar_host(dstPtrImageUnpadded, batch_dstSize[batchCount], dstPtrImageUnpaddedCopy, channel);
+                compute_padded_from_unpadded_host(dstPtrImageUnpaddedCopy, batch_dstSize[batchCount], batch_dstSizeMax[batchCount], dstPtrImage, RPPI_CHN_PLANAR, channel);
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+
+        free(dstPtrCopy);
+    }
+
+    return RPP_SUCCESS;
+}
+
+#endif
diff --git a/src/modules/cpu/loongarch_geometry_transforms.hpp b/src/modules/cpu/loongarch_geometry_transforms.hpp
new file mode 100644
index 00000000..055a1d07
--- /dev/null
+++ b/src/modules/cpu/loongarch_geometry_transforms.hpp
@@ -0,0 +1,4303 @@
+/*
+MIT License
+
+Copyright (c) 2019 - 2024 Advanced Micro Devices, Inc.
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
+*/
+
+#ifndef HOST_GEOMETRIC_TRASFORMS_HPP
+#define HOST_GEOMETRIC_TRASFORMS_HPP
+
+#include "rpp_loongarch_common.hpp"
+
+/**************** flip ***************/
+
+template <typename T>
+RppStatus flip_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                          Rpp32u *batch_flipAxis, RppiROI *roiPoints,
+                          Rpp32u nbatchSize,
+                          RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32u x1 = roiPoints[batchCount].x;
+            Rpp32u y1 = roiPoints[batchCount].y;
+            Rpp32u x2 = x1 + roiPoints[batchCount].roiWidth;
+            Rpp32u y2 = y1 + roiPoints[batchCount].roiHeight;
+            if (x2 == 0)
+            {
+                x2 = batch_srcSize[batchCount].width;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == 0)
+            {
+                y2 = batch_srcSize[batchCount].height;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp32u remainingElementsAfterROI = (batch_srcSize[batchCount].width - (roiPoints[batchCount].x + roiPoints[batchCount].roiWidth));
+
+            Rpp32u flipAxis = batch_flipAxis[batchCount];
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            if (flipAxis == RPPI_HORIZONTAL_AXIS)
+            {
+                for(int c = 0; c < channel; c++)
+                {
+                    T *srcPtrChannel, *dstPtrChannel, *srcPtrChannelROI;
+                    srcPtrChannel = srcPtrImage + (c * imageDimMax);
+                    dstPtrChannel = dstPtrImage + (c * imageDimMax);
+                    srcPtrChannelROI = srcPtrImage + (c * imageDimMax) + (y2 * batch_srcSizeMax[batchCount].width) + x1;
+
+
+                    for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+                    {
+                        T *srcPtrTemp, *dstPtrTemp;
+                        srcPtrTemp = srcPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+                        dstPtrTemp = dstPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+
+                        if (!((y1 <= i) && (i <= y2)))
+                        {
+                            memcpy(dstPtrTemp, srcPtrTemp, batch_srcSize[batchCount].width * sizeof(T));
+
+                            srcPtrTemp += batch_srcSizeMax[batchCount].width;
+                            dstPtrTemp += batch_srcSizeMax[batchCount].width;
+                        }
+                        else
+                        {
+                            memcpy(dstPtrTemp, srcPtrTemp, x1 * sizeof(T));
+                            srcPtrTemp += x1;
+                            dstPtrTemp += x1;
+
+                            memcpy(dstPtrTemp, srcPtrChannelROI, roiPoints[batchCount].roiWidth * sizeof(T));
+                            srcPtrTemp += roiPoints[batchCount].roiWidth;
+                            dstPtrTemp += roiPoints[batchCount].roiWidth;
+                            srcPtrChannelROI -= batch_srcSizeMax[batchCount].width;
+
+                            memcpy(dstPtrTemp, srcPtrTemp, remainingElementsAfterROI * sizeof(T));
+                            srcPtrTemp += remainingElementsAfterROI;
+                            dstPtrTemp += remainingElementsAfterROI;
+                        }
+                    }
+                }
+            }
+            else if (flipAxis == RPPI_VERTICAL_AXIS)
+            {
+                Rpp32u bufferLength = roiPoints[batchCount].roiWidth;
+                Rpp32u alignedLength = (bufferLength / 16) * 16;
+
+                Rpp32u srcROIIncrement = batch_srcSizeMax[batchCount].width + roiPoints[batchCount].roiWidth;
+                for(int c = 0; c < channel; c++)
+                {
+                    T *srcPtrChannel, *dstPtrChannel, *srcPtrChannelROI;
+                    srcPtrChannel = srcPtrImage + (c * imageDimMax);
+                    dstPtrChannel = dstPtrImage + (c * imageDimMax);
+                    srcPtrChannelROI = srcPtrImage + (c * imageDimMax) + (y1 * batch_srcSizeMax[batchCount].width) + x2;
+
+
+                    for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+                    {
+                        T *srcPtrTemp, *dstPtrTemp;
+                        srcPtrTemp = srcPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+                        dstPtrTemp = dstPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+
+                        if (!((y1 <= i) && (i <= y2)))
+                        {
+                            memcpy(dstPtrTemp, srcPtrTemp, batch_srcSize[batchCount].width * sizeof(T));
+
+                            srcPtrTemp += batch_srcSizeMax[batchCount].width;
+                            dstPtrTemp += batch_srcSizeMax[batchCount].width;
+                        }
+                        else
+                        {
+                            memcpy(dstPtrTemp, srcPtrTemp, x1 * sizeof(T));
+                            srcPtrTemp += x1;
+                            dstPtrTemp += x1;
+
+                            __m128i px0;
+                            __m128i vMask = lsx_setr_i8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
+
+                            int vectorLoopCount = 0;
+                            for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+                            {
+                                srcPtrChannelROI -= 15;
+                                px0 = __lsx_vld((__m128i *)srcPtrChannelROI, 0);
+                                px0 = lsx_shuffle_i8(px0, vMask);
+                                __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+                                srcPtrChannelROI -= 1;
+                                dstPtrTemp += 16;
+                            }
+                            for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                            {
+                                *dstPtrTemp++ = (T) *srcPtrChannelROI--;
+                            }
+
+                            srcPtrTemp += roiPoints[batchCount].roiWidth;
+                            srcPtrChannelROI += srcROIIncrement;
+
+                            memcpy(dstPtrTemp, srcPtrTemp, remainingElementsAfterROI * sizeof(T));
+                            srcPtrTemp += remainingElementsAfterROI;
+                            dstPtrTemp += remainingElementsAfterROI;
+                        }
+                    }
+                }
+            }
+            else if (flipAxis == RPPI_BOTH_AXIS)
+            {
+                Rpp32u bufferLength = roiPoints[batchCount].roiWidth;
+                Rpp32u alignedLength = (bufferLength / 16) * 16;
+
+                Rpp32u srcROIIncrement = batch_srcSizeMax[batchCount].width - roiPoints[batchCount].roiWidth;
+                for(int c = 0; c < channel; c++)
+                {
+                    T *srcPtrChannel, *dstPtrChannel, *srcPtrChannelROI;
+                    srcPtrChannel = srcPtrImage + (c * imageDimMax);
+                    dstPtrChannel = dstPtrImage + (c * imageDimMax);
+                    srcPtrChannelROI = srcPtrImage + (c * imageDimMax) + (y2 * batch_srcSizeMax[batchCount].width) + x2;
+
+
+                    for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+                    {
+                        T *srcPtrTemp, *dstPtrTemp;
+                        srcPtrTemp = srcPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+                        dstPtrTemp = dstPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+
+                        if (!((y1 <= i) && (i <= y2)))
+                        {
+                            memcpy(dstPtrTemp, srcPtrTemp, batch_srcSize[batchCount].width * sizeof(T));
+
+                            srcPtrTemp += batch_srcSizeMax[batchCount].width;
+                            dstPtrTemp += batch_srcSizeMax[batchCount].width;
+                        }
+                        else
+                        {
+                            memcpy(dstPtrTemp, srcPtrTemp, x1 * sizeof(T));
+                            srcPtrTemp += x1;
+                            dstPtrTemp += x1;
+
+                            __m128i px0;
+                            __m128i vMask = lsx_setr_i8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
+
+                            int vectorLoopCount = 0;
+                            for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+                            {
+                                srcPtrChannelROI -= 15;
+                                px0 = __lsx_vld((__m128i *)srcPtrChannelROI, 0);
+                                px0 = lsx_shuffle_i8(px0, vMask);
+                                __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+                                srcPtrChannelROI -= 1;
+                                dstPtrTemp += 16;
+                            }
+                            for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                            {
+                                *dstPtrTemp++ = (T) *srcPtrChannelROI--;
+                            }
+
+                            srcPtrTemp += roiPoints[batchCount].roiWidth;
+                            srcPtrChannelROI -= srcROIIncrement;
+
+                            memcpy(dstPtrTemp, srcPtrTemp, remainingElementsAfterROI * sizeof(T));
+                            srcPtrTemp += remainingElementsAfterROI;
+                            dstPtrTemp += remainingElementsAfterROI;
+                        }
+                    }
+                }
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32u x1 = roiPoints[batchCount].x;
+            Rpp32u y1 = roiPoints[batchCount].y;
+            Rpp32u x2 = x1 + roiPoints[batchCount].roiWidth;
+            Rpp32u y2 = y1 + roiPoints[batchCount].roiHeight;
+            if (x2 == 0)
+            {
+                x2 = batch_srcSize[batchCount].width;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == 0)
+            {
+                y2 = batch_srcSize[batchCount].height;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp32u elementsBeforeROI = channel * x1;
+            Rpp32u remainingElementsAfterROI = channel * (batch_srcSize[batchCount].width - (roiPoints[batchCount].x + roiPoints[batchCount].roiWidth));
+
+            Rpp32u flipAxis = batch_flipAxis[batchCount];
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            Rpp32u elementsInRow = channel * batch_srcSize[batchCount].width;
+            Rpp32u elementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+            Rpp32u elementsInRowROI = channel * roiPoints[batchCount].roiWidth;
+
+            if (flipAxis == RPPI_HORIZONTAL_AXIS)
+            {
+                T  *srcPtrROI;
+                srcPtrROI = srcPtrImage + (y2 * elementsInRowMax) + (x1 * channel);
+
+
+                for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+                {
+                    T *srcPtrTemp, *dstPtrTemp;
+                    srcPtrTemp = srcPtrImage + (i * elementsInRowMax);
+                    dstPtrTemp = dstPtrImage + (i * elementsInRowMax);
+
+                    if (!((y1 <= i) && (i <= y2)))
+                    {
+                        memcpy(dstPtrTemp, srcPtrTemp, elementsInRow * sizeof(T));
+
+                        dstPtrTemp += elementsInRowMax;
+                        srcPtrTemp += elementsInRowMax;
+                    }
+                    else
+                    {
+                        memcpy(dstPtrTemp, srcPtrTemp, elementsBeforeROI * sizeof(T));
+                        srcPtrTemp += elementsBeforeROI;
+                        dstPtrTemp += elementsBeforeROI;
+
+                        memcpy(dstPtrTemp, srcPtrROI, elementsInRowROI * sizeof(T));
+                        srcPtrTemp += elementsInRowROI;
+                        dstPtrTemp += elementsInRowROI;
+                        srcPtrROI -= elementsInRowMax;
+
+                        memcpy(dstPtrTemp, srcPtrTemp, remainingElementsAfterROI * sizeof(T));
+                        srcPtrTemp += remainingElementsAfterROI;
+                        dstPtrTemp += remainingElementsAfterROI;
+                    }
+                }
+            }
+            else if (flipAxis == RPPI_VERTICAL_AXIS)
+            {
+                T  *srcPtrROI;
+                srcPtrROI = srcPtrImage + (y1 * elementsInRowMax) + ((x2 - 1) * channel);
+
+                Rpp32u bufferLength = elementsInRowROI;
+                Rpp32u alignedLength = (bufferLength / 15) * 15;
+
+                Rpp32u srcROIIncrement = elementsInRowMax + elementsInRowROI;
+
+
+                for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+                {
+                    T *srcPtrTemp, *dstPtrTemp;
+                    srcPtrTemp = srcPtrImage + (i * elementsInRowMax);
+                    dstPtrTemp = dstPtrImage + (i * elementsInRowMax);
+
+                    if (!((y1 <= i) && (i <= y2)))
+                    {
+                        memcpy(dstPtrTemp, srcPtrTemp, elementsInRow * sizeof(T));
+
+                        dstPtrTemp += elementsInRowMax;
+                        srcPtrTemp += elementsInRowMax;
+                    }
+                    else
+                    {
+                        memcpy(dstPtrTemp, srcPtrTemp, elementsBeforeROI * sizeof(T));
+                        srcPtrTemp += elementsBeforeROI;
+                        dstPtrTemp += elementsBeforeROI;
+
+                        __m128i px0;
+                        __m128i vMask = lsx_setr_i8(13, 14, 15, 10, 11, 12, 7, 8, 9, 4, 5, 6, 1, 2, 3, 0);
+
+                        int vectorLoopCount = 0;
+                        for (; vectorLoopCount < alignedLength; vectorLoopCount+=15)
+                        {
+                            srcPtrROI -= 13;
+                            px0 = __lsx_vld((__m128i *)srcPtrROI, 0);
+                            px0 = lsx_shuffle_i8(px0, vMask);
+                            __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+                            srcPtrROI -= 2;
+                            dstPtrTemp += 15;
+                        }
+                        for (; vectorLoopCount < bufferLength; vectorLoopCount+=channel)
+                        {
+                            memcpy(dstPtrTemp, srcPtrROI, channel * sizeof(T));
+                            dstPtrTemp += channel;
+                            srcPtrROI -= channel;
+                        }
+
+                        srcPtrTemp += elementsInRowROI;
+                        srcPtrROI += srcROIIncrement;
+
+                        memcpy(dstPtrTemp, srcPtrTemp, remainingElementsAfterROI * sizeof(T));
+                        srcPtrTemp += remainingElementsAfterROI;
+                        dstPtrTemp += remainingElementsAfterROI;
+                    }
+                }
+            }
+            else if (flipAxis == RPPI_BOTH_AXIS)
+            {
+                T  *srcPtrROI;
+                srcPtrROI = srcPtrImage + (y2 * elementsInRowMax) + ((x2 - 1) * channel);
+
+                Rpp32u bufferLength = elementsInRowROI;
+                Rpp32u alignedLength = (bufferLength / 15) * 15;
+
+                Rpp32u srcROIIncrement = elementsInRowMax - elementsInRowROI;
+
+
+                for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+                {
+                    T *srcPtrTemp, *dstPtrTemp;
+                    srcPtrTemp = srcPtrImage + (i * elementsInRowMax);
+                    dstPtrTemp = dstPtrImage + (i * elementsInRowMax);
+
+                    if (!((y1 <= i) && (i <= y2)))
+                    {
+                        memcpy(dstPtrTemp, srcPtrTemp, elementsInRow * sizeof(T));
+
+                        dstPtrTemp += elementsInRowMax;
+                        srcPtrTemp += elementsInRowMax;
+                    }
+                    else
+                    {
+                        memcpy(dstPtrTemp, srcPtrTemp, elementsBeforeROI * sizeof(T));
+                        srcPtrTemp += elementsBeforeROI;
+                        dstPtrTemp += elementsBeforeROI;
+
+                        __m128i px0;
+                        __m128i vMask = lsx_setr_i8(13, 14, 15, 10, 11, 12, 7, 8, 9, 4, 5, 6, 1, 2, 3, 0);
+
+                        int vectorLoopCount = 0;
+                        for (; vectorLoopCount < alignedLength; vectorLoopCount+=15)
+                        {
+                            srcPtrROI -= 13;
+                            px0 = __lsx_vld((__m128i *)srcPtrROI, 0);
+                            px0 = lsx_shuffle_i8(px0, vMask);
+                            __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+                            srcPtrROI -= 2;
+                            dstPtrTemp += 15;
+                        }
+                        for (; vectorLoopCount < bufferLength; vectorLoopCount+=channel)
+                        {
+                            memcpy(dstPtrTemp, srcPtrROI, channel * sizeof(T));
+                            dstPtrTemp += channel;
+                            srcPtrROI -= channel;
+                        }
+
+                        srcPtrTemp += elementsInRowROI;
+                        srcPtrROI -= srcROIIncrement;
+
+                        memcpy(dstPtrTemp, srcPtrTemp, remainingElementsAfterROI * sizeof(T));
+                        srcPtrTemp += remainingElementsAfterROI;
+                        dstPtrTemp += remainingElementsAfterROI;
+                    }
+                }
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus flip_host(T* srcPtr, RppiSize srcSize, T* dstPtr,
+                    Rpp32u flipAxis,
+                    RppiChnFormat chnFormat, Rpp32u channel)
+{
+    T *srcPtrTemp, *dstPtrTemp;
+    srcPtrTemp = srcPtr;
+    dstPtrTemp = dstPtr;
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        if (flipAxis == RPPI_HORIZONTAL_AXIS)
+        {
+            for (int c = 0; c < channel; c++)
+            {
+                srcPtrTemp = srcPtr + ((c + 1) * srcSize.height * srcSize.width) - srcSize.width;
+                for (int i = 0; i < srcSize.height; i++)
+                {
+                    memcpy(dstPtrTemp, srcPtrTemp, srcSize.width * sizeof(T));
+                    dstPtrTemp += srcSize.width;
+                    srcPtrTemp -= srcSize.width;
+                }
+            }
+        }
+        else if (flipAxis == RPPI_VERTICAL_AXIS)
+        {
+            T *srcPtrTemp2;
+            srcPtrTemp2 = srcPtr;
+
+            Rpp32u bufferLength = srcSize.width;
+            Rpp32u alignedLength = (bufferLength / 16) * 16;
+
+            for (int c = 0; c < channel; c++)
+            {
+                srcPtrTemp = srcPtr + (c * srcSize.height * srcSize.width) + srcSize.width - 1;
+                for (int i = 0; i < srcSize.height; i++)
+                {
+                    srcPtrTemp2 = srcPtrTemp;
+
+                    __m128i px0;
+                    __m128i vMask = lsx_setr_i8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
+
+                    int vectorLoopCount = 0;
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+                    {
+                        srcPtrTemp2 -= 15;
+                        px0 = __lsx_vld((__m128i *)srcPtrTemp2, 0);
+                        px0 = lsx_shuffle_i8(px0, vMask);
+                        __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+                        srcPtrTemp2 -= 1;
+                        dstPtrTemp += 16;
+                    }
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                    {
+                        *dstPtrTemp++ = (T) *srcPtrTemp2--;
+                    }
+                    srcPtrTemp = srcPtrTemp + srcSize.width;
+                }
+            }
+        }
+        else if (flipAxis == RPPI_BOTH_AXIS)
+        {
+            T *srcPtrTemp2;
+            srcPtrTemp2 = srcPtr;
+
+            Rpp32u bufferLength = srcSize.width;
+            Rpp32u alignedLength = (bufferLength / 16) * 16;
+
+            for (int c = 0; c < channel; c++)
+            {
+                srcPtrTemp = srcPtr + ((c+1) * srcSize.height * srcSize.width) - 1;
+                for (int i = 0; i < srcSize.height; i++)
+                {
+                    srcPtrTemp2 = srcPtrTemp;
+
+                    __m128i px0;
+                    __m128i vMask = lsx_setr_i8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);
+
+                    int vectorLoopCount = 0;
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+                    {
+                        srcPtrTemp2 -= 15;
+                        px0 = __lsx_vld((__m128i *)srcPtrTemp2, 0);
+                        px0 = lsx_shuffle_i8(px0, vMask);
+                        __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+                        srcPtrTemp2 -= 1;
+                        dstPtrTemp += 16;
+                    }
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                    {
+                        *dstPtrTemp++ = (T) *srcPtrTemp2--;
+                    }
+                    srcPtrTemp = srcPtrTemp - srcSize.width;
+                }
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        Rpp32u elementsInRow = channel * srcSize.width;
+        if (flipAxis == RPPI_HORIZONTAL_AXIS)
+        {
+            srcPtrTemp = srcPtr + (channel * ((srcSize.height * srcSize.width) - srcSize.width));
+            for (int i = 0; i < srcSize.height; i++)
+            {
+                memcpy(dstPtrTemp, srcPtrTemp, elementsInRow * sizeof(T));
+                dstPtrTemp += elementsInRow;
+                srcPtrTemp -= elementsInRow;
+            }
+        }
+        else if (flipAxis == RPPI_VERTICAL_AXIS)
+        {
+            Rpp32u bufferLength = channel * srcSize.width;
+            Rpp32u alignedLength = (bufferLength / 15) * 15;
+
+            srcPtrTemp = srcPtr + (channel * (srcSize.width - 1));
+            for (int i = 0; i < srcSize.height; i++)
+            {
+                __m128i px0;
+                __m128i vMask = lsx_setr_i8(13, 14, 15, 10, 11, 12, 7, 8, 9, 4, 5, 6, 1, 2, 3, 0);
+
+                int vectorLoopCount = 0;
+                for (; vectorLoopCount < alignedLength; vectorLoopCount+=15)
+                {
+                    srcPtrTemp -= 13;
+                    px0 = __lsx_vld((__m128i *)srcPtrTemp, 0);
+                    px0 = lsx_shuffle_i8(px0, vMask);
+                    __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+                    srcPtrTemp -= 2;
+                    dstPtrTemp += 15;
+                }
+                for (; vectorLoopCount < bufferLength; vectorLoopCount+=channel)
+                {
+                    memcpy(dstPtrTemp, srcPtrTemp, channel * sizeof(T));
+                    dstPtrTemp += channel;
+                    srcPtrTemp -= channel;
+                }
+
+                srcPtrTemp = srcPtrTemp + (channel * (2 * srcSize.width));
+            }
+        }
+        else if (flipAxis == RPPI_BOTH_AXIS)
+        {
+            Rpp32u bufferLength = channel * srcSize.width;
+            Rpp32u alignedLength = (bufferLength / 15) * 15;
+
+            srcPtrTemp = srcPtr + (channel * ((srcSize.height * srcSize.width) - 1));
+            for (int i = 0; i < srcSize.height; i++)
+            {
+                __m128i px0;
+                __m128i vMask = lsx_setr_i8(13, 14, 15, 10, 11, 12, 7, 8, 9, 4, 5, 6, 1, 2, 3, 0);
+
+                int vectorLoopCount = 0;
+                for (; vectorLoopCount < alignedLength; vectorLoopCount+=15)
+                {
+                    srcPtrTemp -= 13;
+                    px0 = __lsx_vld((__m128i *)srcPtrTemp, 0);
+                    px0 = lsx_shuffle_i8(px0, vMask);
+                    __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+                    srcPtrTemp -= 2;
+                    dstPtrTemp += 15;
+                }
+                for (; vectorLoopCount < bufferLength; vectorLoopCount+=channel)
+                {
+                    memcpy(dstPtrTemp, srcPtrTemp, channel * sizeof(T));
+                    dstPtrTemp += channel;
+                    srcPtrTemp -= channel;
+                }
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+/**************** fisheye ***************/
+
+template <typename T>
+RppStatus fisheye_base_host(T* srcPtrTemp, RppiSize srcSize, T* dstPtrTemp,
+                            Rpp64u j, Rpp32u elementsPerChannel, Rpp32u elements,
+                            Rpp32f newI, Rpp32f newIsquared,
+                            RppiChnFormat chnFormat, Rpp32u channel)
+{
+    Rpp32f newJ, newIsrc, newJsrc, newJsquared, euclideanDistance, newEuclideanDistance, theta;
+    int iSrc, jSrc, srcPosition;
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        newJ = (((Rpp32f) (j * 2.0)) / ((Rpp32f)(srcSize.width))) - 1.0;
+        newJsquared = newJ * newJ;
+        euclideanDistance = sqrt(newIsquared + newJsquared);
+        if (euclideanDistance >= 0 && euclideanDistance <= 1)
+        {
+            newEuclideanDistance = sqrt(1.0 - (euclideanDistance * euclideanDistance));
+            newEuclideanDistance = (euclideanDistance + (1.0 - newEuclideanDistance)) / 2.0;
+
+            if (newEuclideanDistance <= 1.0)
+            {
+                theta = atan2(newI, newJ);
+
+                newIsrc = newEuclideanDistance * sin(theta);
+                newJsrc = newEuclideanDistance * cos(theta);
+
+                iSrc = (int) (((newIsrc + 1.0) * ((Rpp32f) srcSize.height)) / 2.0);
+                jSrc = (int) (((newJsrc + 1.0) * ((Rpp32f) srcSize.width)) / 2.0);
+
+                srcPosition = (int)((iSrc * srcSize.width) + jSrc);
+
+                if ((srcPosition >= 0) && (srcPosition < elementsPerChannel))
+                {
+                    *dstPtrTemp++ = *(srcPtrTemp + srcPosition);
+                }
+                else
+                {
+                    *dstPtrTemp++ = (T) 0;
+                }
+            }
+            else
+            {
+                *dstPtrTemp++ = (T) 0;
+            }
+        }
+        else
+        {
+            *dstPtrTemp++ = (T) 0;
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        newJ = (((Rpp32f) (j * 2.0)) / ((Rpp32f)(srcSize.width))) - 1.0;
+        newJsquared = newJ * newJ;
+        euclideanDistance = sqrt(newIsquared + newJsquared);
+        if (euclideanDistance >= 0 && euclideanDistance <= 1)
+        {
+            newEuclideanDistance = sqrt(1.0 - (euclideanDistance * euclideanDistance));
+            newEuclideanDistance = (euclideanDistance + (1.0 - newEuclideanDistance)) / 2.0;
+
+            if (newEuclideanDistance <= 1.0)
+            {
+                theta = atan2(newI, newJ);
+
+                newIsrc = newEuclideanDistance * sin(theta);
+                newJsrc = newEuclideanDistance * cos(theta);
+
+                iSrc = (int) (((newIsrc + 1.0) * ((Rpp32f) srcSize.height)) / 2.0);
+                jSrc = (int) (((newJsrc + 1.0) * ((Rpp32f) srcSize.width)) / 2.0);
+
+                srcPosition = (int)(channel * ((iSrc * srcSize.width) + jSrc));
+
+                if ((srcPosition >= 0) && (srcPosition < elements))
+                {
+                    for (int c = 0; c < channel; c++)
+                    {
+                        *dstPtrTemp++ = *(srcPtrTemp + srcPosition + c);
+                    }
+                }
+                else
+                {
+                    memset(dstPtrTemp, 0, 3 * sizeof(T));
+                    dstPtrTemp += 3;
+                }
+            }
+            else
+            {
+                memset(dstPtrTemp, 0, 3 * sizeof(T));
+                dstPtrTemp += 3;
+            }
+        }
+        else
+        {
+            memset(dstPtrTemp, 0, 3 * sizeof(T));
+            dstPtrTemp += 3;
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus fisheye_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr, RppiROI *roiPoints,
+                             Rpp32u nbatchSize,
+                             RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32u x1 = roiPoints[batchCount].x;
+            Rpp32u y1 = roiPoints[batchCount].y;
+            Rpp32u x2 = x1 + roiPoints[batchCount].roiWidth - 1;
+            Rpp32u y2 = y1 + roiPoints[batchCount].roiHeight - 1;
+            if (x2 == -1)
+            {
+                x2 = batch_srcSize[batchCount].width - 1;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == -1)
+            {
+                y2 = batch_srcSize[batchCount].height - 1;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp32u remainingElementsAfterROI = (batch_srcSize[batchCount].width - (roiPoints[batchCount].x + roiPoints[batchCount].roiWidth));
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            Rpp32f newI, newIsquared;
+            Rpp32u elementsPerChannelMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+            Rpp32u elementsMax = channel * elementsPerChannelMax;
+
+            Rpp32f halfHeight = batch_srcSize[batchCount].height / 2;
+            Rpp32f halfWidth = batch_srcSize[batchCount].width / 2;
+
+            Rpp32u bufferLength = roiPoints[batchCount].roiWidth;
+            Rpp32u alignedLength = (bufferLength / 4) * 4;
+
+            Rpp32s srcPositionArrayInt[4] = {0};
+            Rpp32f eD[4] = {0};
+            Rpp32f nED[4] = {0};
+
+            __m128i px0, px1;
+            __m128 p0, p1, p2;
+            __m128 q0, pCmp1, pCmp2, pMask;
+            __m128 pZero = lsx_set1_f32(0.0);
+            __m128 pOne = lsx_set1_f32(1.0);
+            __m128 pMul = lsx_set1_f32(2.0 / (Rpp32f) batch_srcSize[batchCount].width);
+            __m128 pMul2 = lsx_set1_f32(0.5);
+            __m128 pMul3 = lsx_set1_f32(halfHeight);
+            __m128 pMul4 = lsx_set1_f32(halfWidth);
+            __m128 pWidthMax = lsx_set1_f32((Rpp32f) batch_srcSizeMax[batchCount].width);
+            __m128i pxSrcPosition = __lsx_vreplgr2vr_w(0);
+
+            for(int c = 0; c < channel; c++)
+            {
+                T *srcPtrChannel, *dstPtrChannel;
+                srcPtrChannel = srcPtrImage + (c * imageDimMax);
+                dstPtrChannel = dstPtrImage + (c * imageDimMax);
+
+
+                for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+                {
+                    T *srcPtrTemp, *srcPtrTemp2, *dstPtrTemp;
+                    srcPtrTemp = srcPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+                    dstPtrTemp = dstPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+                    srcPtrTemp2 = srcPtrChannel;
+
+                    if (!((y1 <= i) && (i <= y2)))
+                    {
+                        memcpy(dstPtrTemp, srcPtrTemp, batch_srcSize[batchCount].width * sizeof(T));
+
+                        dstPtrTemp += batch_srcSizeMax[batchCount].width;
+                        srcPtrTemp += batch_srcSizeMax[batchCount].width;
+                    }
+                    else
+                    {
+                        memcpy(dstPtrTemp, srcPtrTemp, x1 * sizeof(T));
+                        srcPtrTemp += x1;
+                        dstPtrTemp += x1;
+
+                        Rpp32f newI = (((Rpp32f) (i * 2.0)) / ((Rpp32f)(batch_srcSize[batchCount].height))) - 1.0;
+                        Rpp32f newIsquared = newI * newI;
+
+                        __m128 pNewI = lsx_set1_f32(newI);
+                        __m128 pNewIsquared = lsx_set1_f32(newIsquared);
+
+                        Rpp64u vectorLoopCount = x1;
+                        for (; vectorLoopCount < alignedLength + x1; vectorLoopCount+=4)
+                        {
+                            pMask = lsx_set1_f32(1.0);
+
+                            p0 = lsx_setr_f32(vectorLoopCount, vectorLoopCount + 1, vectorLoopCount + 2, vectorLoopCount + 3);
+                            p0 = __lsx_vfmul_s(p0, pMul);
+                            p0 = __lsx_vfsub_s(p0, pOne);
+
+                            p1 = __lsx_vfmul_s(p0, p0);
+                            p1 = __lsx_vfadd_s(pNewIsquared, p1);
+                            p1 = __lsx_vfsqrt_s(p1);
+
+                            pCmp1 = (__m128)__lsx_vfcmp_cle_s(p1, pZero);
+                            pCmp2 = (__m128)__lsx_vfcmp_cle_s(p1, pOne);
+                            pMask = (__m128)__lsx_vand_v((__m128i)pMask, (__m128i)__lsx_vand_v((__m128i)pCmp1, (__m128i)pCmp2));
+
+                            __lsx_vst(pMask, eD, 0);
+
+                            if(eD[0] != 0 && eD[1] != 0 && eD[2] != 0 && eD[3] != 0)
+                            {
+                                p2 = __lsx_vfmul_s(p1, p1);
+                                p2 = __lsx_vfsub_s(pOne, p2);
+                                p2 = __lsx_vfsqrt_s(p2);
+                                p2 = __lsx_vfsub_s(pOne, p2);
+                                p2 = __lsx_vfadd_s(p1, p2);
+                                p2 = __lsx_vfmul_s(p2, pMul2);
+
+                                __lsx_vst(p2, nED, 0);
+
+                                if (nED[0] <= 1.0 && nED[1] <= 1.0 && nED[2] <= 1.0 && nED[3] <= 1.0)
+                                {
+                                    q0 = atan2_ps(pNewI, p0);
+
+                                    sincos_ps(q0, &p0, &p1);
+
+                                    p0 = __lsx_vfmul_s(p2, p0);
+                                    p1 = __lsx_vfmul_s(p2, p1);
+
+                                    p0 = __lsx_vfadd_s(p0, pOne);
+                                    p0 = __lsx_vfmul_s(p0, pMul3);
+                                    p1 = __lsx_vfadd_s(p1, pOne);
+                                    p1 = __lsx_vfmul_s(p1, pMul4);
+
+                                    p0 = __lsx_vfmul_s(__lsx_vfrintrm_s(p0), pWidthMax);
+
+                                    px0 = __lsx_vftint_w_s(p0);
+                                    px1 = __lsx_vftint_w_s(__lsx_vfrintrm_s(p1));
+
+                                    pxSrcPosition = __lsx_vadd_w(px0, px1);
+
+                                    __lsx_vst(pxSrcPosition, (__m128i *)srcPositionArrayInt, 0);
+
+                                    for (int pos = 0; pos < 4; pos++)
+                                    {
+                                        if ((srcPositionArrayInt[pos] >= 0) && (srcPositionArrayInt[pos] < elementsPerChannelMax))
+                                        {
+                                            *dstPtrTemp = *(srcPtrTemp2 + srcPositionArrayInt[pos]);
+                                        }
+                                        else
+                                        {
+                                            *dstPtrTemp = (T) 0;
+                                        }
+                                        dstPtrTemp++;
+                                    }
+                                }
+                                else
+                                {
+                                    for (int id = 0; id < 4; id++)
+                                    {
+                                        if (nED[id] <= 1.0)
+                                        {
+                                            fisheye_base_host(srcPtrTemp2, batch_srcSize[batchCount], dstPtrTemp, vectorLoopCount + id, elementsPerChannelMax, elementsMax, newI, newIsquared, chnFormat, channel);
+                                            dstPtrTemp++;
+                                        }
+                                        else
+                                        {
+                                            *dstPtrTemp++ = (T) 0;
+                                        }
+                                    }
+                                }
+                            }
+                            else
+                            {
+                                for (int id = 0; id < 4; id++)
+                                {
+                                    if (eD[id] != 0.0)
+                                    {
+                                        fisheye_base_host(srcPtrTemp2, batch_srcSize[batchCount], dstPtrTemp, vectorLoopCount + id, elementsPerChannelMax, elementsMax, newI, newIsquared, chnFormat, channel);
+                                        dstPtrTemp++;
+                                    }
+                                    else
+                                    {
+                                        *dstPtrTemp++ = (T) 0;
+                                    }
+                                }
+                            }
+                        }
+                        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                        {
+                            fisheye_base_host(srcPtrTemp2, batch_srcSize[batchCount], dstPtrTemp, vectorLoopCount, elementsPerChannelMax, elementsMax, newI, newIsquared, chnFormat, channel);
+                            dstPtrTemp++;
+                        }
+
+                        srcPtrTemp += bufferLength;
+
+                        memcpy(dstPtrTemp, srcPtrTemp, remainingElementsAfterROI * sizeof(T));
+                        srcPtrTemp += remainingElementsAfterROI;
+                        dstPtrTemp += remainingElementsAfterROI;
+                    }
+                }
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32u x1 = roiPoints[batchCount].x;
+            Rpp32u y1 = roiPoints[batchCount].y;
+            Rpp32u x2 = x1 + roiPoints[batchCount].roiWidth - 1;
+            Rpp32u y2 = y1 + roiPoints[batchCount].roiHeight - 1;
+            if (x2 == -1)
+            {
+                x2 = batch_srcSize[batchCount].width - 1;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == -1)
+            {
+                y2 = batch_srcSize[batchCount].height - 1;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp32u elementsBeforeROI = channel * x1;
+            Rpp32u remainingElementsAfterROI = channel * (batch_srcSize[batchCount].width - (roiPoints[batchCount].x + roiPoints[batchCount].roiWidth));
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            Rpp32u elementsInRow = channel * batch_srcSize[batchCount].width;
+            Rpp32u elementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+
+            Rpp32f newI, newIsquared;
+            Rpp32u elementsPerChannelMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+            Rpp32u elementsMax = channel * elementsPerChannelMax;
+
+            Rpp32f halfHeight = batch_srcSize[batchCount].height / 2;
+            Rpp32f halfWidth = batch_srcSize[batchCount].width / 2;
+
+            Rpp32u bufferLength = roiPoints[batchCount].roiWidth;
+            Rpp32u alignedLength = (bufferLength / 4) * 4;
+
+            Rpp32u elementsInBuffer = channel * bufferLength;
+
+            Rpp32s srcPositionArrayInt[4] = {0};
+            Rpp32f eD[4] = {0};
+            Rpp32f nED[4] = {0};
+
+            __m128i px0, px1;
+            __m128 p0, p1, p2;
+            __m128 q0, pCmp1, pCmp2, pMask;
+            __m128 pZero = lsx_set1_f32(0.0);
+            __m128 pOne = lsx_set1_f32(1.0);
+            __m128 pThree = lsx_set1_f32(3.0);
+            __m128 pMul = lsx_set1_f32(2.0 / (Rpp32f) batch_srcSize[batchCount].width);
+            __m128 pMul2 = lsx_set1_f32(0.5);
+            __m128 pMul3 = lsx_set1_f32(halfHeight);
+            __m128 pMul4 = lsx_set1_f32(halfWidth);
+            __m128 pWidthMax = lsx_set1_f32((Rpp32f) elementsInRowMax);
+            __m128i pxSrcPosition = __lsx_vreplgr2vr_w(0);
+
+
+            for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+            {
+                Rpp32f newI = (((Rpp32f) (i * 2.0)) / ((Rpp32f)(batch_srcSize[batchCount].height))) - 1.0;
+                Rpp32f newIsquared = newI * newI;
+
+                T *srcPtrTemp, *srcPtrTemp2, *dstPtrTemp;
+                srcPtrTemp = srcPtrImage + (i * elementsInRowMax);
+                dstPtrTemp = dstPtrImage + (i * elementsInRowMax);
+                srcPtrTemp2 = srcPtrImage;
+
+                if (!((y1 <= i) && (i <= y2)))
+                {
+                    memcpy(dstPtrTemp, srcPtrTemp, elementsInRow * sizeof(T));
+
+                    dstPtrTemp += elementsInRowMax;
+                    srcPtrTemp += elementsInRowMax;
+                }
+                else
+                {
+                    memcpy(dstPtrTemp, srcPtrTemp, elementsBeforeROI * sizeof(T));
+                    srcPtrTemp += elementsBeforeROI;
+                    dstPtrTemp += elementsBeforeROI;
+
+                    Rpp32f newI = (((Rpp32f) (i * 2.0)) / ((Rpp32f)(batch_srcSize[batchCount].height))) - 1.0;
+                    Rpp32f newIsquared = newI * newI;
+
+                    __m128 pNewI = lsx_set1_f32(newI);
+                    __m128 pNewIsquared = lsx_set1_f32(newIsquared);
+
+                    Rpp64u vectorLoopCount = x1;
+                    for (; vectorLoopCount < alignedLength + x1; vectorLoopCount+=4)
+                    {
+                        pMask = lsx_set1_f32(1.0);
+
+                        p0 = lsx_setr_f32(vectorLoopCount, vectorLoopCount + 1, vectorLoopCount + 2, vectorLoopCount + 3);
+                        p0 = __lsx_vfmul_s(p0, pMul);
+                        p0 = __lsx_vfsub_s(p0, pOne);
+
+                        p1 = __lsx_vfmul_s(p0, p0);
+                        p1 = __lsx_vfadd_s(pNewIsquared, p1);
+                        p1 = __lsx_vfsqrt_s(p1);
+
+                        pCmp1 = (__m128)__lsx_vfcmp_cle_s(p1, pZero);
+                        pCmp2 = (__m128)__lsx_vfcmp_cle_s(p1, pOne);
+                        pMask = (__m128)__lsx_vand_v((__m128i)pMask, (__m128i)__lsx_vand_v((__m128i)pCmp1, (__m128i)pCmp2));
+
+                        __lsx_vst(pMask, eD, 0);
+
+                        if (eD[0] != 0 && eD[1] != 0 && eD[2] != 0 && eD[3] != 0)
+                        {
+                            p2 = __lsx_vfmul_s(p1, p1);
+                            p2 = __lsx_vfsub_s(pOne, p2);
+                            p2 = __lsx_vfsqrt_s(p2);
+                            p2 = __lsx_vfsub_s(pOne, p2);
+                            p2 = __lsx_vfadd_s(p1, p2);
+                            p2 = __lsx_vfmul_s(p2, pMul2);
+
+                            __lsx_vst(p2, nED, 0);
+
+                            if (nED[0] <= 1.0 && nED[1] <= 1.0 && nED[2] <= 1.0 && nED[3] <= 1.0)
+                            {
+                                q0 = atan2_ps(pNewI, p0);
+
+                                sincos_ps(q0, &p0, &p1);
+
+                                p0 = __lsx_vfmul_s(p2, p0);
+                                p1 = __lsx_vfmul_s(p2, p1);
+
+                                p0 = __lsx_vfadd_s(p0, pOne);
+                                p0 = __lsx_vfmul_s(p0, pMul3);
+                                p1 = __lsx_vfadd_s(p1, pOne);
+                                p1 = __lsx_vfmul_s(p1, pMul4);
+
+                                p0 = __lsx_vfmul_s(__lsx_vfrintrm_s(p0), pWidthMax);
+
+                                px0 = __lsx_vftint_w_s(p0);
+                                px1 = __lsx_vftint_w_s(__lsx_vfmul_s(__lsx_vfrintrm_s(p1), pThree));
+
+                                pxSrcPosition = __lsx_vadd_w(px0, px1);
+
+                                __lsx_vst(pxSrcPosition, (__m128i *)srcPositionArrayInt, 0);
+
+                                for (int pos = 0; pos < 4; pos++)
+                                {
+                                    if ((srcPositionArrayInt[pos] >= 0) && (srcPositionArrayInt[pos] < elementsMax))
+                                    {
+                                        for (int c = 0; c < channel; c++)
+                                        {
+                                            *dstPtrTemp++ = *(srcPtrTemp2 + srcPositionArrayInt[pos] + c);
+                                        }
+                                    }
+                                    else
+                                    {
+                                        memset(dstPtrTemp, 0, 3 * sizeof(T));
+                                        dstPtrTemp += 3;
+                                    }
+                                }
+                            }
+                            else
+                            {
+                                for (int id = 0; id < 4; id++)
+                                {
+                                    if (nED[id] <= 1.0)
+                                    {
+                                        fisheye_base_host(srcPtrTemp2, batch_srcSize[batchCount], dstPtrTemp, vectorLoopCount + id, elementsPerChannelMax, elementsMax, newI, newIsquared, chnFormat, channel);
+                                        dstPtrTemp += 3;
+                                    }
+                                    else
+                                    {
+                                        memset(dstPtrTemp, 0, 3 * sizeof(T));
+                                        dstPtrTemp += 3;
+                                    }
+                                }
+                            }
+                        }
+                        else
+                        {
+                            for (int id = 0; id < 4; id++)
+                            {
+                                if (eD[id] != 0.0)
+                                {
+                                    fisheye_base_host(srcPtrTemp2, batch_srcSize[batchCount], dstPtrTemp, vectorLoopCount + id, elementsPerChannelMax, elementsMax, newI, newIsquared, chnFormat, channel);
+                                    dstPtrTemp += 3;
+                                }
+                                else
+                                {
+                                    memset(dstPtrTemp, 0, 3 * sizeof(T));
+                                    dstPtrTemp += 3;
+                                }
+                            }
+                        }
+                    }
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                    {
+                        fisheye_base_host(srcPtrTemp2, batch_srcSize[batchCount], dstPtrTemp, vectorLoopCount, elementsPerChannelMax, elementsMax, newI, newIsquared, chnFormat, channel);
+                        dstPtrTemp += 3;
+                    }
+
+                    srcPtrTemp += elementsInBuffer;
+
+                    memcpy(dstPtrTemp, srcPtrTemp, remainingElementsAfterROI * sizeof(T));
+                    srcPtrTemp += remainingElementsAfterROI;
+                    dstPtrTemp += remainingElementsAfterROI;
+                }
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus fisheye_host(T* srcPtr, RppiSize srcSize, T* dstPtr,
+                    RppiChnFormat chnFormat, Rpp32u channel)
+{
+    T *srcPtrTemp, *dstPtrTemp;
+    srcPtrTemp = srcPtr;
+    dstPtrTemp = dstPtr;
+
+    Rpp32f newI, newIsquared;
+    Rpp32u elementsPerChannel = srcSize.height * srcSize.width;
+    Rpp32u elements = channel * elementsPerChannel;
+
+    Rpp32f halfHeight = srcSize.height / 2;
+    Rpp32f halfWidth = srcSize.width / 2;
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        Rpp32u bufferLength = srcSize.width;
+        Rpp32u alignedLength = (bufferLength / 4) * 4;
+
+        Rpp32s srcPositionArrayInt[4] = {0};
+        Rpp32f eD[4] = {0};
+        Rpp32f nED[4] = {0};
+
+        __m128i px0, px1;
+        __m128 p0, p1, p2;
+        __m128 q0, pCmp1, pCmp2, pMask;
+        __m128 pZero = lsx_set1_f32(0.0);
+        __m128 pOne = lsx_set1_f32(1.0);
+        __m128 pMul = lsx_set1_f32(2.0 / (Rpp32f) srcSize.width);
+        __m128 pMul2 = lsx_set1_f32(0.5);
+        __m128 pMul3 = lsx_set1_f32(halfHeight);
+        __m128 pMul4 = lsx_set1_f32(halfWidth);
+        __m128 pWidth = lsx_set1_f32((Rpp32f) srcSize.width);
+        __m128i pxSrcPosition = __lsx_vreplgr2vr_w(0);
+
+        for(int c = 0; c < channel; c++)
+        {
+            srcPtrTemp = srcPtr + (c * srcSize.height * srcSize.width);
+            for(int i = 0; i < srcSize.height; i++)
+            {
+                newI = (((Rpp32f) (i * 2.0)) / ((Rpp32f)(srcSize.height))) - 1.0;
+                newIsquared = newI * newI;
+
+                __m128 pNewI = lsx_set1_f32(newI);
+                __m128 pNewIsquared = lsx_set1_f32(newIsquared);
+
+                Rpp64u vectorLoopCount = 0;
+                for (; vectorLoopCount < alignedLength; vectorLoopCount+=4)
+                {
+                    pMask = lsx_set1_f32(1.0);
+
+                    p0 = lsx_setr_f32(vectorLoopCount, vectorLoopCount + 1, vectorLoopCount + 2, vectorLoopCount + 3);
+                    p0 = __lsx_vfmul_s(p0, pMul);
+                    p0 = __lsx_vfsub_s(p0, pOne);
+
+                    p1 = __lsx_vfmul_s(p0, p0);
+                    p1 = __lsx_vfadd_s(pNewIsquared, p1);
+                    p1 = __lsx_vfsqrt_s(p1);
+
+                    pCmp1 = (__m128)__lsx_vfcmp_cle_s(p1, pZero);
+                    pCmp2 = (__m128)__lsx_vfcmp_cle_s(p1, pOne);
+                    pMask = (__m128)__lsx_vand_v((__m128i)pMask, (__m128i)__lsx_vand_v((__m128i)pCmp1, (__m128i)pCmp2));
+
+                    __lsx_vst(pMask, eD, 0);
+
+                    if(eD[0] != 0 && eD[1] != 0 && eD[2] != 0 && eD[3] != 0)
+                    {
+                        p2 = __lsx_vfmul_s(p1, p1);
+                        p2 = __lsx_vfsub_s(pOne, p2);
+                        p2 = __lsx_vfsqrt_s(p2);
+                        p2 = __lsx_vfsub_s(pOne, p2);
+                        p2 = __lsx_vfadd_s(p1, p2);
+                        p2 = __lsx_vfmul_s(p2, pMul2);
+
+                        __lsx_vst(p2, nED, 0);
+
+                        if (nED[0] <= 1.0 && nED[1] <= 1.0 && nED[2] <= 1.0 && nED[3] <= 1.0)
+                        {
+                            q0 = atan2_ps(pNewI, p0);
+
+                            sincos_ps(q0, &p0, &p1);
+
+                            p0 = __lsx_vfmul_s(p2, p0);
+                            p1 = __lsx_vfmul_s(p2, p1);
+
+                            p0 = __lsx_vfadd_s(p0, pOne);
+                            p0 = __lsx_vfmul_s(p0, pMul3);
+                            p1 = __lsx_vfadd_s(p1, pOne);
+                            p1 = __lsx_vfmul_s(p1, pMul4);
+
+                            p0 = __lsx_vfmul_s(__lsx_vfrintrm_s(p0), pWidth);
+
+                            px0 = __lsx_vftint_w_s(p0);
+                            px1 = __lsx_vftint_w_s(__lsx_vfrintrm_s(p1));
+
+                            pxSrcPosition = __lsx_vadd_w(px0, px1);
+
+                            __lsx_vst(pxSrcPosition, (__m128i *)srcPositionArrayInt, 0);
+
+                            for (int pos = 0; pos < 4; pos++)
+                            {
+                                if ((srcPositionArrayInt[pos] >= 0) && (srcPositionArrayInt[pos] < elementsPerChannel))
+                                {
+                                    *dstPtrTemp = *(srcPtrTemp + srcPositionArrayInt[pos]);
+                                }
+                                else
+                                {
+                                    *dstPtrTemp = (T) 0;
+                                }
+                                dstPtrTemp++;
+                            }
+                        }
+                        else
+                        {
+                            for (int id = 0; id < 4; id++)
+                            {
+                                if (nED[id] <= 1.0)
+                                {
+                                    fisheye_base_host(srcPtrTemp, srcSize, dstPtrTemp, vectorLoopCount + id, elementsPerChannel, elements, newI, newIsquared, chnFormat, channel);
+                                    dstPtrTemp++;
+                                }
+                                else
+                                {
+                                    *dstPtrTemp++ = (T) 0;
+                                }
+                            }
+                        }
+                    }
+                    else
+                    {
+                        for (int id = 0; id < 4; id++)
+                        {
+                            if (eD[id] != 0.0)
+                            {
+                                fisheye_base_host(srcPtrTemp, srcSize, dstPtrTemp, vectorLoopCount + id, elementsPerChannel, elements, newI, newIsquared, chnFormat, channel);
+                                dstPtrTemp++;
+                            }
+                            else
+                            {
+                                *dstPtrTemp++ = (T) 0;
+                            }
+                        }
+                    }
+                }
+                for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                {
+                    fisheye_base_host(srcPtrTemp, srcSize, dstPtrTemp, vectorLoopCount, elementsPerChannel, elements, newI, newIsquared, chnFormat, channel);
+                    dstPtrTemp++;
+                }
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        Rpp32f elementsInRow = channel * srcSize.width;
+
+        Rpp32u bufferLength = srcSize.width;
+        Rpp32u alignedLength = (bufferLength / 4) * 4;
+
+        Rpp32s srcPositionArrayInt[4] = {0};
+        Rpp32f eD[4] = {0};
+        Rpp32f nED[4] = {0};
+
+        __m128i px0, px1;
+        __m128 p0, p1, p2;
+        __m128 q0, pCmp1, pCmp2, pMask;
+        __m128 pZero = lsx_set1_f32(0.0);
+        __m128 pOne = lsx_set1_f32(1.0);
+        __m128 pThree = lsx_set1_f32(3.0);
+        __m128 pMul = lsx_set1_f32(2.0 / (Rpp32f) srcSize.width);
+        __m128 pMul2 = lsx_set1_f32(0.5);
+        __m128 pMul3 = lsx_set1_f32(halfHeight);
+        __m128 pMul4 = lsx_set1_f32(halfWidth);
+        __m128 pWidth = lsx_set1_f32(elementsInRow);
+        __m128i pxSrcPosition = __lsx_vreplgr2vr_w(0);
+
+        for (int i = 0; i < srcSize.height; i++)
+        {
+            newI = (((Rpp32f) (i * 2.0)) / ((Rpp32f)(srcSize.height))) - 1.0;
+            newIsquared = newI * newI;
+
+            __m128 pNewI = lsx_set1_f32(newI);
+            __m128 pNewIsquared = lsx_set1_f32(newIsquared);
+
+            Rpp64u vectorLoopCount = 0;
+            for (; vectorLoopCount < alignedLength; vectorLoopCount+=4)
+            {
+                pMask = lsx_set1_f32(1.0);
+
+                p0 = lsx_setr_f32(vectorLoopCount, vectorLoopCount + 1, vectorLoopCount + 2, vectorLoopCount + 3);
+                p0 = __lsx_vfmul_s(p0, pMul);
+                p0 = __lsx_vfsub_s(p0, pOne);
+
+                p1 = __lsx_vfmul_s(p0, p0);
+                p1 = __lsx_vfadd_s(pNewIsquared, p1);
+                p1 = __lsx_vfsqrt_s(p1);
+
+                pCmp1 = (__m128)__lsx_vfcmp_cle_s(p1, pZero);
+                pCmp2 = (__m128)__lsx_vfcmp_cle_s(p1, pOne);
+                pMask = (__m128)__lsx_vand_v((__m128i)pMask, (__m128i)__lsx_vand_v((__m128i)pCmp1, (__m128i)pCmp2));
+
+                __lsx_vst(pMask, eD, 0);
+
+                if (eD[0] != 0 && eD[1] != 0 && eD[2] != 0 && eD[3] != 0)
+                {
+                    p2 = __lsx_vfmul_s(p1, p1);
+                    p2 = __lsx_vfsub_s(pOne, p2);
+                    p2 = __lsx_vfsqrt_s(p2);
+                    p2 = __lsx_vfsub_s(pOne, p2);
+                    p2 = __lsx_vfadd_s(p1, p2);
+                    p2 = __lsx_vfmul_s(p2, pMul2);
+
+                    __lsx_vst(p2, nED, 0);
+
+                    if (nED[0] <= 1.0 && nED[1] <= 1.0 && nED[2] <= 1.0 && nED[3] <= 1.0)
+                    {
+                        q0 = atan2_ps(pNewI, p0);
+
+                        sincos_ps(q0, &p0, &p1);
+
+                        p0 = __lsx_vfmul_s(p2, p0);
+                        p1 = __lsx_vfmul_s(p2, p1);
+
+                        p0 = __lsx_vfadd_s(p0, pOne);
+                        p0 = __lsx_vfmul_s(p0, pMul3);
+                        p1 = __lsx_vfadd_s(p1, pOne);
+                        p1 = __lsx_vfmul_s(p1, pMul4);
+
+                        p0 = __lsx_vfmul_s(__lsx_vfrintrm_s(p0), pWidth);
+
+                        px0 = __lsx_vftint_w_s(p0);
+                        px1 = __lsx_vftint_w_s(__lsx_vfmul_s(__lsx_vfrintrm_s(p1), pThree));
+
+                        pxSrcPosition = __lsx_vadd_w(px0, px1);
+
+                        __lsx_vst(pxSrcPosition, (__m128i *)srcPositionArrayInt, 0);
+
+                        for (int pos = 0; pos < 4; pos++)
+                        {
+                            if ((srcPositionArrayInt[pos] >= 0) && (srcPositionArrayInt[pos] < elements))
+                            {
+                                for (int c = 0; c < channel; c++)
+                                {
+                                    *dstPtrTemp++ = *(srcPtrTemp + srcPositionArrayInt[pos] + c);
+                                }
+                            }
+                            else
+                            {
+                                memset(dstPtrTemp, 0, 3 * sizeof(T));
+                                dstPtrTemp += 3;
+                            }
+                        }
+                    }
+                    else
+                    {
+                        for (int id = 0; id < 4; id++)
+                        {
+                            if (nED[id] <= 1.0)
+                            {
+                                fisheye_base_host(srcPtrTemp, srcSize, dstPtrTemp, vectorLoopCount + id, elementsPerChannel, elements, newI, newIsquared, chnFormat, channel);
+                                dstPtrTemp += 3;
+                            }
+                            else
+                            {
+                                memset(dstPtrTemp, 0, 3 * sizeof(T));
+                                dstPtrTemp += 3;
+                            }
+                        }
+                    }
+                }
+                else
+                {
+                    for (int id = 0; id < 4; id++)
+                    {
+                        if (eD[id] != 0.0)
+                        {
+                            fisheye_base_host(srcPtrTemp, srcSize, dstPtrTemp, vectorLoopCount + id, elementsPerChannel, elements, newI, newIsquared, chnFormat, channel);
+                            dstPtrTemp += 3;
+                        }
+                        else
+                        {
+                            memset(dstPtrTemp, 0, 3 * sizeof(T));
+                            dstPtrTemp += 3;
+                        }
+                    }
+                }
+            }
+            for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+            {
+                fisheye_base_host(srcPtrTemp, srcSize, dstPtrTemp, vectorLoopCount, elementsPerChannel, elements, newI, newIsquared, chnFormat, channel);
+                dstPtrTemp += 3;
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+/**************** lens_correction ***************/
+
+template <typename T>
+RppStatus lens_correction_base_host(T* srcPtrTemp, RppiSize srcSize, T* dstPtrTemp,
+                            Rpp64u j, Rpp32u heightLimit, Rpp32u widthLimit, Rpp32f halfHeight, Rpp32f halfWidth,
+                            Rpp32f newIsquared, Rpp32f newIZoom, Rpp32f invCorrectionRadius, Rpp32f zoom, Rpp32u elementsInRow,
+                            RppiChnFormat chnFormat, Rpp32u channel)
+{
+    Rpp32f newJ, euclideanDistance, correctedDistance, theta;
+    Rpp32f srcLocationRow, srcLocationColumn;
+    Rpp32u srcLocationRowFloor, srcLocationColumnFloor;
+    T *srcPtrTopRow, *srcPtrBottomRow;
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        newJ = j - halfWidth;
+        euclideanDistance = sqrt(newIsquared + newJ * newJ);
+        correctedDistance = euclideanDistance * invCorrectionRadius;
+        theta = atan(correctedDistance) / correctedDistance;
+
+        srcLocationRow = halfHeight + theta * newIZoom;
+        srcLocationColumn = halfWidth + theta * newJ * zoom;
+
+        if ((srcLocationRow >= 0) && (srcLocationColumn >= 0) &&
+            (srcLocationRow < srcSize.height) && (srcLocationColumn < srcSize.width))
+        {
+            srcLocationRowFloor = (Rpp32u) RPPFLOOR(srcLocationRow);
+            srcLocationColumnFloor = (Rpp32u) RPPFLOOR(srcLocationColumn);
+
+            Rpp32f weightedHeight = srcLocationRow - srcLocationRowFloor;
+            if (srcLocationRowFloor > heightLimit)
+            {
+                srcLocationRowFloor = heightLimit;
+            }
+
+            srcPtrTopRow = srcPtrTemp + srcLocationRowFloor * srcSize.width;
+            srcPtrBottomRow  = srcPtrTopRow + srcSize.width;
+
+            Rpp32f weightedWidth = srcLocationColumn - srcLocationColumnFloor;
+            if (srcLocationColumnFloor > widthLimit)
+            {
+                srcLocationColumnFloor = widthLimit;
+            }
+
+            *dstPtrTemp = (T) (((*(srcPtrTopRow + srcLocationColumnFloor)) * (1 - weightedHeight) * (1 - weightedWidth))
+                    + ((*(srcPtrTopRow + srcLocationColumnFloor + 1)) * (1 - weightedHeight) * (weightedWidth))
+                    + ((*(srcPtrBottomRow + srcLocationColumnFloor)) * (weightedHeight) * (1 - weightedWidth))
+                    + ((*(srcPtrBottomRow + srcLocationColumnFloor + 1)) * (weightedHeight) * (weightedWidth)));
+        }
+        else
+        {
+            *dstPtrTemp = (T) 0;
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        newJ = j - halfWidth;
+        euclideanDistance = sqrt(newIsquared + newJ * newJ);
+        correctedDistance = euclideanDistance * invCorrectionRadius;
+        theta = atan(correctedDistance) / correctedDistance;
+
+        srcLocationRow = halfHeight + theta * newIZoom;
+        srcLocationColumn = halfWidth + theta * newJ * zoom;
+
+        if ((srcLocationRow >= 0) && (srcLocationColumn >= 0) &&
+            (srcLocationRow < srcSize.height) && (srcLocationColumn < srcSize.width))
+        {
+            srcLocationRowFloor = (Rpp32u) RPPFLOOR(srcLocationRow);
+            srcLocationColumnFloor = (Rpp32u) RPPFLOOR(srcLocationColumn);
+
+            Rpp32f weightedHeight = srcLocationRow - srcLocationRowFloor;
+            if (srcLocationRowFloor > heightLimit)
+            {
+                srcLocationRowFloor = heightLimit;
+            }
+
+            srcPtrTopRow = srcPtrTemp + srcLocationRowFloor * elementsInRow;
+            srcPtrBottomRow  = srcPtrTopRow + elementsInRow;
+
+            Rpp32f weightedWidth = srcLocationColumn - srcLocationColumnFloor;
+            if (srcLocationColumnFloor > widthLimit)
+            {
+                srcLocationColumnFloor = widthLimit;
+            }
+
+            srcLocationColumnFloor = srcLocationColumnFloor * channel;
+
+            for (int c = 0; c < channel; c++)
+            {
+                *dstPtrTemp++ = (T) (((*(srcPtrTopRow + c + srcLocationColumnFloor)) * (1 - weightedHeight) * (1 - weightedWidth))
+                                  + ((*(srcPtrTopRow + c + srcLocationColumnFloor + channel)) * (1 - weightedHeight) * (weightedWidth))
+                                  + ((*(srcPtrBottomRow + c + srcLocationColumnFloor)) * (weightedHeight) * (1 - weightedWidth))
+                                  + ((*(srcPtrBottomRow + c + srcLocationColumnFloor + channel)) * (weightedHeight) * (weightedWidth)));
+            }
+        }
+        else
+        {
+            memset(dstPtrTemp, 0, 3 * sizeof(T));
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus lens_correction_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                                     Rpp32f *batch_strength, Rpp32f *batch_zoom, RppiROI *roiPoints,
+                                     Rpp32u nbatchSize,
+                                     RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32u x1 = roiPoints[batchCount].x;
+            Rpp32u y1 = roiPoints[batchCount].y;
+            Rpp32u x2 = x1 + roiPoints[batchCount].roiWidth - 1;
+            Rpp32u y2 = y1 + roiPoints[batchCount].roiHeight - 1;
+            if (x2 == -1)
+            {
+                x2 = batch_srcSize[batchCount].width - 1;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == -1)
+            {
+                y2 = batch_srcSize[batchCount].height - 1;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp32u remainingElementsAfterROI = (batch_srcSize[batchCount].width - (roiPoints[batchCount].x + roiPoints[batchCount].roiWidth));
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            Rpp32f strength = batch_strength[batchCount];
+            Rpp32f zoom = batch_zoom[batchCount];
+
+            Rpp32f newI, newIsquared, newIZoom;
+
+            T *srcPtrTopRow, *srcPtrBottomRow;
+
+            Rpp32u elementsInRowMax = batch_srcSizeMax[batchCount].width;
+
+            Rpp32f halfHeight = batch_srcSize[batchCount].height / 2;
+            Rpp32f halfWidth = batch_srcSize[batchCount].width / 2;
+            Rpp32u heightLimit = batch_srcSize[batchCount].height - 2;
+            Rpp32u widthLimit = batch_srcSize[batchCount].width - 2;
+
+            if (strength == 0) strength = 0.000001;
+
+            Rpp32f invCorrectionRadius = 1.0 / (sqrt(batch_srcSize[batchCount].height * batch_srcSize[batchCount].height + batch_srcSize[batchCount].width * batch_srcSize[batchCount].width) / strength);
+
+            Rpp32u bufferLength = roiPoints[batchCount].roiWidth;
+            Rpp32u alignedLength = (bufferLength / 4) * 4;
+
+            Rpp32f mask[4] = {0};
+            Rpp32u srcLocRF[4] = {0};
+            Rpp32u srcLocCF[4] = {0};
+            Rpp32f param1[4] = {0};
+            Rpp32f param2[4] = {0};
+            Rpp32f param3[4] = {0};
+            Rpp32f param4[4] = {0};
+
+            __m128i px0, px1, px3, px4;
+            __m128 p0, p1, p2, p3, p4, p5, p6, p7, pMask;
+            __m128 q0, pCmp1, pCmp2;
+            __m128 pZero = lsx_set1_f32(0.0);
+            __m128 pOne = lsx_set1_f32(1.0);
+            __m128 pAdd1 = lsx_set1_f32(halfHeight);
+            __m128 pAdd2 = lsx_set1_f32(halfWidth);
+            __m128 pHeight = lsx_set1_f32((Rpp32f) batch_srcSize[batchCount].height);
+            __m128 pWidth = lsx_set1_f32((Rpp32f) batch_srcSize[batchCount].width);
+            __m128 pMul = lsx_set1_f32(invCorrectionRadius);
+
+            for(int c = 0; c < channel; c++)
+            {
+                T *srcPtrChannel, *dstPtrChannel;
+                srcPtrChannel = srcPtrImage + (c * imageDimMax);
+                dstPtrChannel = dstPtrImage + (c * imageDimMax);
+
+
+                for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+                {
+                    T *srcPtrTemp, *srcPtrTemp2, *dstPtrTemp;
+                    srcPtrTemp = srcPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+                    dstPtrTemp = dstPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+                    srcPtrTemp2 = srcPtrChannel;
+
+                    if (!((y1 <= i) && (i <= y2)))
+                    {
+                        memcpy(dstPtrTemp, srcPtrTemp, batch_srcSize[batchCount].width * sizeof(T));
+
+                        dstPtrTemp += batch_srcSizeMax[batchCount].width;
+                        srcPtrTemp += batch_srcSizeMax[batchCount].width;
+                    }
+                    else
+                    {
+                        memcpy(dstPtrTemp, srcPtrTemp, x1 * sizeof(T));
+                        srcPtrTemp += x1;
+                        dstPtrTemp += x1;
+
+                        newI = i - halfHeight;
+                        newIsquared = newI * newI;
+                        newIZoom = newI * zoom;
+
+                        __m128 pNewI = lsx_set1_f32(newI);
+                        __m128 pNewIsquared = lsx_set1_f32(newIsquared);
+                        __m128 pZoom = lsx_set1_f32(zoom);
+                        __m128 pNewIZoom = lsx_set1_f32(newIZoom);
+
+                        Rpp64u vectorLoopCount = x1;
+                        for (; vectorLoopCount < alignedLength + x1; vectorLoopCount+=4)
+                        {
+                            pMask = lsx_set1_f32(1.0);
+
+                            p0 = lsx_setr_f32(vectorLoopCount, vectorLoopCount + 1, vectorLoopCount + 2, vectorLoopCount + 3);
+                            p0 = __lsx_vfsub_s(p0, pAdd2);
+
+                            __m128 pNewJZoom = __lsx_vfmul_s(p0, pZoom);
+
+                            p1 = __lsx_vfadd_s(pNewIsquared, __lsx_vfmul_s(p0, p0));
+                            p1 = __lsx_vfsqrt_s(p1);
+                            p1 = __lsx_vfmul_s(p1, pMul);
+
+                            q0 = __lsx_vfdiv_s(atan_ps(p1), p1);
+
+                            p1 = __lsx_vfmul_s(q0, pNewIZoom);
+                            p1 = __lsx_vfadd_s(pAdd1, p1);
+                            p2 = __lsx_vfmul_s(q0, pNewJZoom);
+                            p2 = __lsx_vfadd_s(pAdd2, p2);
+
+                            pCmp1 = (__m128)__lsx_vand_v((__m128i)(__m128)__lsx_vfcmp_cle_s(p1, pZero), (__m128i)(__m128)__lsx_vfcmp_clt_s(p1, pHeight));
+                            pCmp2 = (__m128)__lsx_vand_v((__m128i)(__m128)__lsx_vfcmp_cle_s(p2, pZero), (__m128i)(__m128)__lsx_vfcmp_clt_s(p2, pWidth));
+                            pMask = (__m128)__lsx_vand_v((__m128i)pMask, (__m128i)__lsx_vand_v((__m128i)pCmp1, (__m128i)pCmp2));
+
+                            __lsx_vst(pMask, mask, 0);
+
+                            if(mask[0] != 0 && mask[1] != 0 && mask[2] != 0 && mask[3] != 0)
+                            {
+                                p3 = __lsx_vfrintrm_s(p1);
+                                p4 = __lsx_vfrintrm_s(p2);
+
+                                p1 = __lsx_vfsub_s(p1, p3);
+                                p2 = __lsx_vfsub_s(p2, p4);
+                                p0 = __lsx_vfmul_s(p1, p2);
+
+                                p5 = __lsx_vfadd_s(__lsx_vfsub_s(__lsx_vfsub_s(pOne, p2), p1), p0);
+                                p6 = __lsx_vfsub_s(p2, p0);
+                                p7 = __lsx_vfsub_s(p1, p0);
+
+                                px3 = __lsx_vftint_w_s(p3);
+                                px4 = __lsx_vftint_w_s(p4);
+
+                                __lsx_vst(px3, (__m128i *)srcLocRF, 0);
+                                __lsx_vst(px4, (__m128i *)srcLocCF, 0);
+
+                                __lsx_vst(p5, param1, 0);
+                                __lsx_vst(p6, param2, 0);
+                                __lsx_vst(p7, param3, 0);
+                                __lsx_vst(p0, param4, 0);
+
+                                for (int pos = 0; pos < 4; pos++)
+                                {
+                                    if (srcLocRF[pos] > heightLimit)
+                                    {
+                                        srcLocRF[pos] = heightLimit;
+                                    }
+                                    if (srcLocCF[pos] > widthLimit)
+                                    {
+                                        srcLocCF[pos] = widthLimit;
+                                    }
+
+                                    srcPtrTopRow = srcPtrTemp2 + srcLocRF[pos] * elementsInRowMax;
+                                    srcPtrBottomRow  = srcPtrTopRow + elementsInRowMax;
+
+                                    *dstPtrTemp++ = (T) (((*(srcPtrTopRow + srcLocCF[pos])) * param1[pos])
+                                                    + ((*(srcPtrTopRow + srcLocCF[pos] + 1)) * param2[pos])
+                                                    + ((*(srcPtrBottomRow + srcLocCF[pos])) * param3[pos])
+                                                    + ((*(srcPtrBottomRow + srcLocCF[pos] + 1)) * param4[pos]));
+                                }
+                            }
+                            else
+                            {
+                                for (int id = 0; id < 4; id++)
+                                {
+                                    if (mask[id] != 0)
+                                    {
+                                        lens_correction_base_host(srcPtrTemp2, batch_srcSize[batchCount], dstPtrTemp, vectorLoopCount + id, heightLimit, widthLimit, halfHeight, halfWidth, newIsquared, newIZoom, invCorrectionRadius, zoom, elementsInRowMax, chnFormat, channel);
+                                        dstPtrTemp++;
+                                    }
+                                    else
+                                    {
+                                        *dstPtrTemp++ = (T) 0;
+                                    }
+                                }
+                            }
+                        }
+                        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                        {
+                            lens_correction_base_host(srcPtrTemp2, batch_srcSize[batchCount], dstPtrTemp, vectorLoopCount, heightLimit, widthLimit, halfHeight, halfWidth, newIsquared, newIZoom, invCorrectionRadius, zoom, elementsInRowMax, chnFormat, channel);
+                            dstPtrTemp++;
+                        }
+
+                        srcPtrTemp += bufferLength;
+
+                        memcpy(dstPtrTemp, srcPtrTemp, remainingElementsAfterROI * sizeof(T));
+                        srcPtrTemp += remainingElementsAfterROI;
+                        dstPtrTemp += remainingElementsAfterROI;
+                    }
+                }
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32u x1 = roiPoints[batchCount].x;
+            Rpp32u y1 = roiPoints[batchCount].y;
+            Rpp32u x2 = x1 + roiPoints[batchCount].roiWidth - 1;
+            Rpp32u y2 = y1 + roiPoints[batchCount].roiHeight - 1;
+            if (x2 == -1)
+            {
+                x2 = batch_srcSize[batchCount].width - 1;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == -1)
+            {
+                y2 = batch_srcSize[batchCount].height - 1;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp32u elementsBeforeROI = channel * x1;
+            Rpp32u remainingElementsAfterROI = channel * (batch_srcSize[batchCount].width - (roiPoints[batchCount].x + roiPoints[batchCount].roiWidth));
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            Rpp32f strength = batch_strength[batchCount];
+            Rpp32f zoom = batch_zoom[batchCount];
+
+            Rpp32f newI, newIsquared, newIZoom;
+
+            T *srcPtrTopRow, *srcPtrBottomRow;
+
+            Rpp32u elementsInRow = channel * batch_srcSize[batchCount].width;
+            Rpp32u elementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+
+            Rpp32f halfHeight = batch_srcSize[batchCount].height / 2;
+            Rpp32f halfWidth = batch_srcSize[batchCount].width / 2;
+            Rpp32u heightLimit = batch_srcSize[batchCount].height - 2;
+            Rpp32u widthLimit = batch_srcSize[batchCount].width - 2;
+
+            if (strength == 0) strength = 0.000001;
+
+            Rpp32f invCorrectionRadius = 1.0 / (sqrt(batch_srcSize[batchCount].height * batch_srcSize[batchCount].height + batch_srcSize[batchCount].width * batch_srcSize[batchCount].width) / strength);
+
+            Rpp32u bufferLength = roiPoints[batchCount].roiWidth;
+            Rpp32u alignedLength = (bufferLength / 4) * 4;
+
+            Rpp32u elementsInBuffer = channel * bufferLength;
+
+            Rpp32f mask[4] = {0};
+            Rpp32u srcLocRF[4] = {0};
+            Rpp32u srcLocCF[4] = {0};
+            Rpp32f param1[4] = {0};
+            Rpp32f param2[4] = {0};
+            Rpp32f param3[4] = {0};
+            Rpp32f param4[4] = {0};
+
+            __m128i px0, px1, px3, px4;
+            __m128 p0, p1, p2, p3, p4, p5, p6, p7, pMask;
+            __m128 q0, pCmp1, pCmp2;
+            __m128 pZero = lsx_set1_f32(0.0);
+            __m128 pOne = lsx_set1_f32(1.0);
+            __m128 pAdd1 = lsx_set1_f32(halfHeight);
+            __m128 pAdd2 = lsx_set1_f32(halfWidth);
+            __m128 pHeight = lsx_set1_f32((Rpp32f) batch_srcSize[batchCount].height);
+            __m128 pWidth = lsx_set1_f32((Rpp32f) batch_srcSize[batchCount].width);
+            __m128 pMul = lsx_set1_f32(invCorrectionRadius);
+
+
+            for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+            {
+                T *srcPtrTemp, *srcPtrTemp2, *dstPtrTemp;
+                srcPtrTemp = srcPtrImage + (i * elementsInRowMax);
+                dstPtrTemp = dstPtrImage + (i * elementsInRowMax);
+                srcPtrTemp2 = srcPtrImage;
+
+                if (!((y1 <= i) && (i <= y2)))
+                {
+                    memcpy(dstPtrTemp, srcPtrTemp, elementsInRow * sizeof(T));
+
+                    dstPtrTemp += elementsInRowMax;
+                    srcPtrTemp += elementsInRowMax;
+                }
+                else
+                {
+                    memcpy(dstPtrTemp, srcPtrTemp, elementsBeforeROI * sizeof(T));
+                    srcPtrTemp += elementsBeforeROI;
+                    dstPtrTemp += elementsBeforeROI;
+
+                    newI = i - halfHeight;
+                    newIsquared = newI * newI;
+                    newIZoom = newI * zoom;
+
+                    __m128 pNewI = lsx_set1_f32(newI);
+                    __m128 pNewIsquared = lsx_set1_f32(newIsquared);
+                    __m128 pZoom = lsx_set1_f32(zoom);
+                    __m128 pNewIZoom = lsx_set1_f32(newIZoom);
+
+                    Rpp64u vectorLoopCount = x1;
+                    for (; vectorLoopCount < alignedLength + x1; vectorLoopCount+=4)
+                    {
+                        pMask = lsx_set1_f32(1.0);
+
+                        p0 = lsx_setr_f32(vectorLoopCount, vectorLoopCount + 1, vectorLoopCount + 2, vectorLoopCount + 3);
+                        p0 = __lsx_vfsub_s(p0, pAdd2);
+
+                        __m128 pNewJZoom = __lsx_vfmul_s(p0, pZoom);
+
+                        p1 = __lsx_vfadd_s(pNewIsquared, __lsx_vfmul_s(p0, p0));
+                        p1 = __lsx_vfsqrt_s(p1);
+                        p1 = __lsx_vfmul_s(p1, pMul);
+
+                        q0 = __lsx_vfdiv_s(atan_ps(p1), p1);
+
+                        p1 = __lsx_vfmul_s(q0, pNewIZoom);
+                        p1 = __lsx_vfadd_s(pAdd1, p1);
+                        p2 = __lsx_vfmul_s(q0, pNewJZoom);
+                        p2 = __lsx_vfadd_s(pAdd2, p2);
+
+                        pCmp1 = (__m128)__lsx_vand_v((__m128i)(__m128)__lsx_vfcmp_cle_s(p1, pZero), (__m128i)(__m128)__lsx_vfcmp_clt_s(p1, pHeight));
+                        pCmp2 = (__m128)__lsx_vand_v((__m128i)(__m128)__lsx_vfcmp_cle_s(p2, pZero), (__m128i)(__m128)__lsx_vfcmp_clt_s(p2, pWidth));
+                        pMask = (__m128)__lsx_vand_v((__m128i)pMask, (__m128i)__lsx_vand_v((__m128i)pCmp1, (__m128i)pCmp2));
+
+                        __lsx_vst(pMask, mask, 0);
+
+                        if(mask[0] != 0 && mask[1] != 0 && mask[2] != 0 && mask[3] != 0)
+                        {
+                            p3 = __lsx_vfrintrm_s(p1);
+                            p4 = __lsx_vfrintrm_s(p2);
+
+                            p1 = __lsx_vfsub_s(p1, p3);
+                            p2 = __lsx_vfsub_s(p2, p4);
+                            p0 = __lsx_vfmul_s(p1, p2);
+
+                            p5 = __lsx_vfadd_s(__lsx_vfsub_s(__lsx_vfsub_s(pOne, p2), p1), p0);
+                            p6 = __lsx_vfsub_s(p2, p0);
+                            p7 = __lsx_vfsub_s(p1, p0);
+
+                            px3 = __lsx_vftint_w_s(p3);
+                            px4 = __lsx_vftint_w_s(p4);
+
+                            __lsx_vst(px3, (__m128i *)srcLocRF, 0);
+                            __lsx_vst(px4, (__m128i *)srcLocCF, 0);
+
+                            __lsx_vst(p5, param1, 0);
+                            __lsx_vst(p6, param2, 0);
+                            __lsx_vst(p7, param3, 0);
+                            __lsx_vst(p0, param4, 0);
+
+                            for (int pos = 0; pos < 4; pos++)
+                            {
+                                if (srcLocRF[pos] > heightLimit)
+                                {
+                                    srcLocRF[pos] = heightLimit;
+                                }
+                                if (srcLocCF[pos] > widthLimit)
+                                {
+                                    srcLocCF[pos] = widthLimit;
+                                }
+
+                                srcLocCF[pos] *= channel;
+
+                                srcPtrTopRow = srcPtrTemp2 + srcLocRF[pos] * elementsInRowMax;
+                                srcPtrBottomRow  = srcPtrTopRow + elementsInRowMax;
+
+                                for (int c = 0; c < channel; c++)
+                                {
+                                    *dstPtrTemp++ = (T) (((*(srcPtrTopRow + c + srcLocCF[pos])) * param1[pos])
+                                                        + ((*(srcPtrTopRow + c + srcLocCF[pos] + channel)) * param2[pos])
+                                                        + ((*(srcPtrBottomRow + c + srcLocCF[pos])) * param3[pos])
+                                                        + ((*(srcPtrBottomRow + c + srcLocCF[pos] + channel)) * param4[pos]));
+                                }
+                            }
+                        }
+                        else
+                        {
+                            for (int id = 0; id < 4; id++)
+                            {
+                                if (mask[id] != 0)
+                                {
+                                    lens_correction_base_host(srcPtrTemp2, batch_srcSize[batchCount], dstPtrTemp, vectorLoopCount + id, heightLimit, widthLimit, halfHeight, halfWidth, newIsquared, newIZoom, invCorrectionRadius, zoom, elementsInRowMax, chnFormat, channel);
+                                    dstPtrTemp += 3;
+                                }
+                                else
+                                {
+                                    memset(dstPtrTemp, 0, 3 * sizeof(T));
+                                    dstPtrTemp += 3;
+                                }
+                            }
+                        }
+                    }
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                    {
+                        lens_correction_base_host(srcPtrTemp2, batch_srcSize[batchCount], dstPtrTemp, vectorLoopCount, heightLimit, widthLimit, halfHeight, halfWidth, newIsquared, newIZoom, invCorrectionRadius, zoom, elementsInRowMax, chnFormat, channel);
+                        dstPtrTemp += 3;
+                    }
+
+                    srcPtrTemp += elementsInBuffer;
+
+                    memcpy(dstPtrTemp, srcPtrTemp, remainingElementsAfterROI * sizeof(T));
+                    srcPtrTemp += remainingElementsAfterROI;
+                    dstPtrTemp += remainingElementsAfterROI;
+                }
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus lens_correction_host(T* srcPtr, RppiSize srcSize, T* dstPtr,
+                               Rpp32f strength, Rpp32f zoom,
+                               RppiChnFormat chnFormat, Rpp32u channel)
+{
+    if (strength < 0 || zoom < 1)
+    {
+        return RPP_ERROR;
+    }
+
+    Rpp32u heightLimit, widthLimit;
+    Rpp32f halfHeight, halfWidth, newI, newIsquared, invCorrectionRadius, newIZoom;
+    T *srcPtrTemp, *dstPtrTemp, *srcPtrTopRow, *srcPtrBottomRow;
+    srcPtrTemp = srcPtr;
+    dstPtrTemp = dstPtr;
+
+    Rpp32u elementsInRow = srcSize.width * channel;
+
+    halfHeight = ((Rpp32f) srcSize.height) / 2.0;
+    halfWidth = ((Rpp32f) srcSize.width) / 2.0;
+    heightLimit = srcSize.height - 2;
+    widthLimit = srcSize.width - 2;
+
+    if (strength == 0) strength = 0.000001;
+
+    invCorrectionRadius = 1.0 / (sqrt(srcSize.height * srcSize.height + srcSize.width * srcSize.width) / strength);
+
+    Rpp32u bufferLength = srcSize.width;
+    Rpp32u alignedLength = (bufferLength / 4) * 4;
+
+    Rpp32f mask[4] = {0};
+    Rpp32u srcLocRF[4] = {0};
+    Rpp32u srcLocCF[4] = {0};
+    Rpp32f param1[4] = {0};
+    Rpp32f param2[4] = {0};
+    Rpp32f param3[4] = {0};
+    Rpp32f param4[4] = {0};
+
+    __m128i px0, px1, px3, px4;
+    __m128 p0, p1, p2, p3, p4, p5, p6, p7, pMask;
+    __m128 q0, pCmp1, pCmp2;
+    __m128 pZero = lsx_set1_f32(0.0);
+    __m128 pOne = lsx_set1_f32(1.0);
+    __m128 pAdd1 = lsx_set1_f32(halfHeight);
+    __m128 pAdd2 = lsx_set1_f32(halfWidth);
+    __m128 pHeight = lsx_set1_f32((Rpp32f) srcSize.height);
+    __m128 pWidth = lsx_set1_f32((Rpp32f) srcSize.width);
+    __m128 pMul = lsx_set1_f32(invCorrectionRadius);
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        for(int c = 0; c < channel; c++)
+        {
+            srcPtrTemp = srcPtr + (c * srcSize.height * srcSize.width);
+            for (int i = 0; i < srcSize.height; i++)
+            {
+                newI = i - halfHeight;
+                newIsquared = newI * newI;
+                newIZoom = newI * zoom;
+
+                __m128 pNewI = lsx_set1_f32(newI);
+                __m128 pNewIsquared = lsx_set1_f32(newIsquared);
+                __m128 pZoom = lsx_set1_f32(zoom);
+                __m128 pNewIZoom = lsx_set1_f32(newIZoom);
+
+                Rpp64u vectorLoopCount = 0;
+                for (; vectorLoopCount < alignedLength; vectorLoopCount+=4)
+                {
+                    pMask = lsx_set1_f32(1.0);
+
+                    p0 = lsx_setr_f32(vectorLoopCount, vectorLoopCount + 1, vectorLoopCount + 2, vectorLoopCount + 3);
+                    p0 = __lsx_vfsub_s(p0, pAdd2);
+
+                    __m128 pNewJZoom = __lsx_vfmul_s(p0, pZoom);
+
+                    p1 = __lsx_vfadd_s(pNewIsquared, __lsx_vfmul_s(p0, p0));
+                    p1 = __lsx_vfsqrt_s(p1);
+                    p1 = __lsx_vfmul_s(p1, pMul);
+
+                    q0 = __lsx_vfdiv_s(atan_ps(p1), p1);
+
+                    p1 = __lsx_vfmul_s(q0, pNewIZoom);
+                    p1 = __lsx_vfadd_s(pAdd1, p1);
+                    p2 = __lsx_vfmul_s(q0, pNewJZoom);
+                    p2 = __lsx_vfadd_s(pAdd2, p2);
+
+                    pCmp1 = (__m128)__lsx_vand_v((__m128i)(__m128)__lsx_vfcmp_cle_s(p1, pZero), (__m128i)(__m128)__lsx_vfcmp_clt_s(p1, pHeight));
+                    pCmp2 = (__m128)__lsx_vand_v((__m128i)(__m128)__lsx_vfcmp_cle_s(p2, pZero), (__m128i)(__m128)__lsx_vfcmp_clt_s(p2, pWidth));
+                    pMask = (__m128)__lsx_vand_v((__m128i)pMask, (__m128i)__lsx_vand_v((__m128i)pCmp1, (__m128i)pCmp2));
+
+                    __lsx_vst(pMask, mask, 0);
+
+                    if(mask[0] != 0 && mask[1] != 0 && mask[2] != 0 && mask[3] != 0)
+                    {
+                        p3 = __lsx_vfrintrm_s(p1);
+                        p4 = __lsx_vfrintrm_s(p2);
+
+                        p1 = __lsx_vfsub_s(p1, p3);
+                        p2 = __lsx_vfsub_s(p2, p4);
+                        p0 = __lsx_vfmul_s(p1, p2);
+
+                        p5 = __lsx_vfadd_s(__lsx_vfsub_s(__lsx_vfsub_s(pOne, p2), p1), p0);
+                        p6 = __lsx_vfsub_s(p2, p0);
+                        p7 = __lsx_vfsub_s(p1, p0);
+
+                        px3 = __lsx_vftint_w_s(p3);
+                        px4 = __lsx_vftint_w_s(p4);
+
+                        __lsx_vst(px3, (__m128i *)srcLocRF, 0);
+                        __lsx_vst(px4, (__m128i *)srcLocCF, 0);
+
+                        __lsx_vst(p5, param1, 0);
+                        __lsx_vst(p6, param2, 0);
+                        __lsx_vst(p7, param3, 0);
+                        __lsx_vst(p0, param4, 0);
+
+                        for (int pos = 0; pos < 4; pos++)
+                        {
+                            if (srcLocRF[pos] > heightLimit)
+                            {
+                                srcLocRF[pos] = heightLimit;
+                            }
+                            if (srcLocCF[pos] > widthLimit)
+                            {
+                                srcLocCF[pos] = widthLimit;
+                            }
+
+                            srcPtrTopRow = srcPtrTemp + srcLocRF[pos] * srcSize.width;
+                            srcPtrBottomRow  = srcPtrTopRow + srcSize.width;
+
+                            *dstPtrTemp++ = (T) (((*(srcPtrTopRow + srcLocCF[pos])) * param1[pos])
+                                              + ((*(srcPtrTopRow + srcLocCF[pos] + 1)) * param2[pos])
+                                              + ((*(srcPtrBottomRow + srcLocCF[pos])) * param3[pos])
+                                              + ((*(srcPtrBottomRow + srcLocCF[pos] + 1)) * param4[pos]));
+                        }
+                    }
+                    else
+                    {
+                        for (int id = 0; id < 4; id++)
+                        {
+                            if (mask[id] != 0)
+                            {
+                                lens_correction_base_host(srcPtrTemp, srcSize, dstPtrTemp, vectorLoopCount + id, heightLimit, widthLimit, halfHeight, halfWidth, newIsquared, newIZoom, invCorrectionRadius, zoom, elementsInRow, chnFormat, channel);
+                                dstPtrTemp++;
+                            }
+                            else
+                            {
+                                *dstPtrTemp++ = (T) 0;
+                            }
+                        }
+                    }
+                }
+                for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                {
+                    lens_correction_base_host(srcPtrTemp, srcSize, dstPtrTemp, vectorLoopCount, heightLimit, widthLimit, halfHeight, halfWidth, newIsquared, newIZoom, invCorrectionRadius, zoom, elementsInRow, chnFormat, channel);
+                    dstPtrTemp++;
+                }
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        for (int i = 0; i < srcSize.height; i++)
+        {
+            newI = i - halfHeight;
+            newIsquared = newI * newI;
+            newIZoom = newI * zoom;
+
+            __m128 pNewI = lsx_set1_f32(newI);
+            __m128 pNewIsquared = lsx_set1_f32(newIsquared);
+            __m128 pZoom = lsx_set1_f32(zoom);
+            __m128 pNewIZoom = lsx_set1_f32(newIZoom);
+
+            Rpp64u vectorLoopCount = 0;
+            for (; vectorLoopCount < alignedLength; vectorLoopCount+=4)
+            {
+                pMask = lsx_set1_f32(1.0);
+
+                p0 = lsx_setr_f32(vectorLoopCount, vectorLoopCount + 1, vectorLoopCount + 2, vectorLoopCount + 3);
+                p0 = __lsx_vfsub_s(p0, pAdd2);
+
+                __m128 pNewJZoom = __lsx_vfmul_s(p0, pZoom);
+
+                p1 = __lsx_vfadd_s(pNewIsquared, __lsx_vfmul_s(p0, p0));
+                p1 = __lsx_vfsqrt_s(p1);
+                p1 = __lsx_vfmul_s(p1, pMul);
+
+                q0 = __lsx_vfdiv_s(atan_ps(p1), p1);
+
+                p1 = __lsx_vfmul_s(q0, pNewIZoom);
+                p1 = __lsx_vfadd_s(pAdd1, p1);
+                p2 = __lsx_vfmul_s(q0, pNewJZoom);
+                p2 = __lsx_vfadd_s(pAdd2, p2);
+
+                pCmp1 = (__m128)__lsx_vand_v((__m128i)(__m128)__lsx_vfcmp_cle_s(p1, pZero), (__m128i)(__m128)__lsx_vfcmp_clt_s(p1, pHeight));
+                pCmp2 = (__m128)__lsx_vand_v((__m128i)(__m128)__lsx_vfcmp_cle_s(p2, pZero), (__m128i)(__m128)__lsx_vfcmp_clt_s(p2, pWidth));
+                pMask = (__m128)__lsx_vand_v((__m128i)pMask, (__m128i)__lsx_vand_v((__m128i)pCmp1, (__m128i)pCmp2));
+
+                __lsx_vst(pMask, mask, 0);
+
+                if(mask[0] != 0 && mask[1] != 0 && mask[2] != 0 && mask[3] != 0)
+                {
+                    p3 = __lsx_vfrintrm_s(p1);
+                    p4 = __lsx_vfrintrm_s(p2);
+
+                    p1 = __lsx_vfsub_s(p1, p3);
+                    p2 = __lsx_vfsub_s(p2, p4);
+                    p0 = __lsx_vfmul_s(p1, p2);
+
+                    p5 = __lsx_vfadd_s(__lsx_vfsub_s(__lsx_vfsub_s(pOne, p2), p1), p0);
+                    p6 = __lsx_vfsub_s(p2, p0);
+                    p7 = __lsx_vfsub_s(p1, p0);
+
+                    px3 = __lsx_vftint_w_s(p3);
+                    px4 = __lsx_vftint_w_s(p4);
+
+                    __lsx_vst(px3, (__m128i *)srcLocRF, 0);
+                    __lsx_vst(px4, (__m128i *)srcLocCF, 0);
+
+                    __lsx_vst(p5, param1, 0);
+                    __lsx_vst(p6, param2, 0);
+                    __lsx_vst(p7, param3, 0);
+                    __lsx_vst(p0, param4, 0);
+
+                    for (int pos = 0; pos < 4; pos++)
+                    {
+                        if (srcLocRF[pos] > heightLimit)
+                        {
+                            srcLocRF[pos] = heightLimit;
+                        }
+                        if (srcLocCF[pos] > widthLimit)
+                        {
+                            srcLocCF[pos] = widthLimit;
+                        }
+
+                        srcLocCF[pos] *= channel;
+
+                        srcPtrTopRow = srcPtrTemp + srcLocRF[pos] * elementsInRow;
+                        srcPtrBottomRow  = srcPtrTopRow + elementsInRow;
+
+                        for (int c = 0; c < channel; c++)
+                        {
+                            *dstPtrTemp++ = (T) (((*(srcPtrTopRow + c + srcLocCF[pos])) * param1[pos])
+                                                + ((*(srcPtrTopRow + c + srcLocCF[pos] + channel)) * param2[pos])
+                                                + ((*(srcPtrBottomRow + c + srcLocCF[pos])) * param3[pos])
+                                                + ((*(srcPtrBottomRow + c + srcLocCF[pos] + channel)) * param4[pos]));
+                        }
+                    }
+                }
+                else
+                {
+                    for (int id = 0; id < 4; id++)
+                    {
+                        if (mask[id] != 0)
+                        {
+                            lens_correction_base_host(srcPtrTemp, srcSize, dstPtrTemp, vectorLoopCount + id, heightLimit, widthLimit, halfHeight, halfWidth, newIsquared, newIZoom, invCorrectionRadius, zoom, elementsInRow, chnFormat, channel);
+                            dstPtrTemp += 3;
+                        }
+                        else
+                        {
+                            memset(dstPtrTemp, 0, 3 * sizeof(T));
+                            dstPtrTemp += 3;
+                        }
+                    }
+                }
+            }
+            for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+            {
+                lens_correction_base_host(srcPtrTemp, srcSize, dstPtrTemp, vectorLoopCount, heightLimit, widthLimit, halfHeight, halfWidth, newIsquared, newIZoom, invCorrectionRadius, zoom, elementsInRow, chnFormat, channel);
+                dstPtrTemp += 3;
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+/**************** scale ***************/
+
+template <typename T>
+RppStatus scale_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr, RppiSize *batch_dstSize, RppiSize *batch_dstSizeMax,
+                             Rpp32f *batch_percentage, RppiROI *roiPoints,
+                             Rpp32u nbatchSize,
+                             RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32f x1 = roiPoints[batchCount].x;
+            Rpp32f y1 = roiPoints[batchCount].y;
+            Rpp32f x2 = x1 + roiPoints[batchCount].roiWidth - 1;
+            Rpp32f y2 = y1 + roiPoints[batchCount].roiHeight - 1;
+            if (x2 == -1)
+            {
+                x2 = batch_srcSize[batchCount].width - 1;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == -1)
+            {
+                y2 = batch_srcSize[batchCount].height - 1;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp32u srcImageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+            Rpp32u dstImageDimMax = batch_dstSizeMax[batchCount].height * batch_dstSizeMax[batchCount].width;
+
+            Rpp32f percentage = batch_percentage[batchCount] / 100;
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+            compute_image_location_host(batch_dstSizeMax, batchCount, &dstLoc, channel);
+            srcPtrImage = srcPtr + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+
+            for(int c = 0; c < channel; c++)
+            {
+                T *srcPtrChannel, *dstPtrChannel;
+                srcPtrChannel = srcPtrImage + (c * srcImageDimMax);
+                dstPtrChannel = dstPtrImage + (c * dstImageDimMax);
+
+
+                for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                {
+                    Rpp32f pixel;
+
+                    T *srcPtrTemp, *dstPtrTemp;
+                    srcPtrTemp = srcPtrChannel;
+                    dstPtrTemp = dstPtrChannel + (i * batch_dstSizeMax[batchCount].width);
+
+                    if (!((y1 <= i) && (i <= y2)))
+                    {
+                        memset(dstPtrTemp, (T) 0, batch_dstSize[batchCount].width * sizeof(T));
+
+                        dstPtrTemp += batch_dstSize[batchCount].width;
+                    }
+                    else
+                    {
+                        Rpp32f srcLocationRow = ((Rpp32f) i) / percentage;
+                        Rpp32s srcLocationRowFloor = (Rpp32s) RPPFLOOR(srcLocationRow);
+                        Rpp32f weightedHeight = srcLocationRow - srcLocationRowFloor;
+
+                        T *srcPtrTopRow, *srcPtrBottomRow;
+                        srcPtrTopRow = srcPtrTemp + srcLocationRowFloor * batch_srcSizeMax[batchCount].width;
+                        srcPtrBottomRow  = srcPtrTopRow + batch_srcSizeMax[batchCount].width;
+
+                        for(int j = 0; j < batch_dstSize[batchCount].width; j++)
+                        {
+                            if (!((x1 <= j) && (j <= x2 )))
+                            {
+                                memset(dstPtrTemp, (T) 0, sizeof(T));
+
+                                dstPtrTemp += 1;
+                            }
+                            else
+                            {
+                                Rpp32f srcLocationColumn = ((Rpp32f) j) / percentage;
+                                Rpp32s srcLocationColumnFloor = (Rpp32s) RPPFLOOR(srcLocationColumn);
+                                Rpp32f weightedWidth = srcLocationColumn - srcLocationColumnFloor;
+
+                                if ((srcLocationRowFloor < y1) || (srcLocationRowFloor > y2) || (srcLocationColumnFloor < x1) || (srcLocationColumnFloor > x2))
+                                {
+                                    *dstPtrTemp = (T) 0;
+                                }
+                                else if (srcLocationRow < 0 || srcLocationColumn < 0 || srcLocationRow > (batch_srcSize[batchCount].height - 2) || srcLocationColumn > (batch_srcSize[batchCount].width - 2))
+                                {
+                                    *dstPtrTemp = (T) 0;
+                                }
+                                else
+                                {
+                                    pixel = ((*(srcPtrTopRow + srcLocationColumnFloor)) * (1 - weightedHeight) * (1 - weightedWidth))
+                                            + ((*(srcPtrTopRow + srcLocationColumnFloor + 1)) * (1 - weightedHeight) * (weightedWidth))
+                                            + ((*(srcPtrBottomRow + srcLocationColumnFloor)) * (weightedHeight) * (1 - weightedWidth))
+                                            + ((*(srcPtrBottomRow + srcLocationColumnFloor + 1)) * (weightedHeight) * (weightedWidth));
+
+                                    *dstPtrTemp = (T) pixel;
+                                }
+                                dstPtrTemp++;
+                            }
+                        }
+                    }
+                }
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32f x1 = roiPoints[batchCount].x;
+            Rpp32f y1 = roiPoints[batchCount].y;
+            Rpp32f x2 = x1 + roiPoints[batchCount].roiWidth - 1;
+            Rpp32f y2 = y1 + roiPoints[batchCount].roiHeight - 1;
+            if (x2 == -1)
+            {
+                x2 = batch_srcSize[batchCount].width - 1;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == -1)
+            {
+                y2 = batch_srcSize[batchCount].height - 1;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp32f percentage = batch_percentage[batchCount] / 100;
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+            compute_image_location_host(batch_dstSizeMax, batchCount, &dstLoc, channel);
+            srcPtrImage = srcPtr + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+
+            Rpp32u srcElementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+            Rpp32u dstElementsInRowMax = channel * batch_dstSizeMax[batchCount].width;
+
+            Rpp32u dstElementsInRow = channel * batch_dstSize[batchCount].width;
+
+
+            for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+            {
+                Rpp32f pixel;
+
+                T *srcPtrTemp, *dstPtrTemp;
+                srcPtrTemp = srcPtrImage;
+                dstPtrTemp = dstPtrImage + (i * dstElementsInRowMax);
+
+                if (!((y1 <= i) && (i <= y2)))
+                {
+                    memset(dstPtrTemp, (T) 0, dstElementsInRow * sizeof(T));
+                }
+                else
+                {
+                    Rpp32f srcLocationRow = ((Rpp32f) i) / percentage;
+                    Rpp32s srcLocationRowFloor = (Rpp32s) RPPFLOOR(srcLocationRow);
+                    Rpp32f weightedHeight = srcLocationRow - srcLocationRowFloor;
+
+                    T *srcPtrTopRow, *srcPtrBottomRow;
+                    srcPtrTopRow = srcPtrTemp + srcLocationRowFloor * srcElementsInRowMax;
+                    srcPtrBottomRow  = srcPtrTopRow + srcElementsInRowMax;
+
+                    for(int j = 0; j < batch_dstSize[batchCount].width; j++)
+                    {
+                        if (!((x1 <= j) && (j <= x2 )))
+                        {
+                            memset(dstPtrTemp, (T) 0, channel * sizeof(T));
+
+                            dstPtrTemp += channel;
+                        }
+                        else
+                        {
+                            Rpp32f srcLocationColumn = ((Rpp32f) j) / percentage;
+                            Rpp32s srcLocationColumnFloor = (Rpp32s) RPPFLOOR(srcLocationColumn);
+                            Rpp32f weightedWidth = srcLocationColumn - srcLocationColumnFloor;
+
+                            if ((srcLocationRowFloor < y1) || (srcLocationRowFloor > y2) || (srcLocationColumnFloor < x1) || (srcLocationColumnFloor > x2))
+                            {
+                                for (int c = 0; c < channel; c++)
+                                {
+                                    *dstPtrTemp = 0;
+
+                                    dstPtrTemp++;
+                                }
+                            }
+                            else if (srcLocationRow < 0 || srcLocationColumn < 0 || srcLocationRow > (batch_srcSize[batchCount].height - 2) || srcLocationColumn > (batch_srcSize[batchCount].width - 2))
+                            {
+                                for (int c = 0; c < channel; c++)
+                                {
+                                    *dstPtrTemp = 0;
+
+                                    dstPtrTemp++;
+                                }
+                            }
+                            else
+                            {
+                                Rpp32s srcLocColFloorChanneled = channel * srcLocationColumnFloor;
+
+                                for (int c = 0; c < channel; c++)
+                                {
+                                    pixel = ((*(srcPtrTopRow + c + srcLocColFloorChanneled)) * (1 - weightedHeight) * (1 - weightedWidth))
+                                            + ((*(srcPtrTopRow + c + srcLocColFloorChanneled + channel)) * (1 - weightedHeight) * (weightedWidth))
+                                            + ((*(srcPtrBottomRow + c + srcLocColFloorChanneled)) * (weightedHeight) * (1 - weightedWidth))
+                                            + ((*(srcPtrBottomRow + c + srcLocColFloorChanneled + channel)) * (weightedHeight) * (weightedWidth));
+
+                                    *dstPtrTemp = (T) pixel;
+
+                                    dstPtrTemp ++;
+                                }
+                            }
+                        }
+                    }
+                }
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus scale_host(T* srcPtr, RppiSize srcSize, T* dstPtr, RppiSize dstSize,
+                           Rpp32f percentage,
+                           RppiChnFormat chnFormat, Rpp32u channel)
+{
+    if (dstSize.height < 0 || dstSize.width < 0)
+    {
+        return RPP_ERROR;
+    }
+    if (percentage < 0)
+    {
+        return RPP_ERROR;
+    }
+
+    percentage /= 100;
+
+    Rpp32f srcLocationRow, srcLocationColumn, pixel;
+    Rpp32s srcLocationRowFloor, srcLocationColumnFloor;
+    T *srcPtrTemp, *dstPtrTemp, *srcPtrTopRow, *srcPtrBottomRow;
+    srcPtrTemp = srcPtr;
+    dstPtrTemp = dstPtr;
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        for (int c = 0; c < channel; c++)
+        {
+            for (int i = 0; i < dstSize.height; i++)
+            {
+                srcLocationRow = ((Rpp32f) i) / percentage;
+                srcLocationRowFloor = (Rpp32s) RPPFLOOR(srcLocationRow);
+                Rpp32f weightedHeight = srcLocationRow - srcLocationRowFloor;
+
+                srcPtrTopRow = srcPtrTemp + srcLocationRowFloor * srcSize.width;
+                srcPtrBottomRow  = srcPtrTopRow + srcSize.width;
+
+                for (int j = 0; j < dstSize.width; j++)
+                {
+                    srcLocationColumn = ((Rpp32f) j) / percentage;
+                    srcLocationColumnFloor = (Rpp32s) RPPFLOOR(srcLocationColumn);
+                    Rpp32f weightedWidth = srcLocationColumn - srcLocationColumnFloor;
+
+                    if (srcLocationRow < 0 || srcLocationColumn < 0 || srcLocationRow > (srcSize.height - 2) || srcLocationColumn > (srcSize.width - 2))
+                    {
+                        *dstPtrTemp = (T) 0;
+                        dstPtrTemp++;
+                    }
+                    else
+                    {
+                        pixel = ((*(srcPtrTopRow + srcLocationColumnFloor)) * (1 - weightedHeight) * (1 - weightedWidth))
+                                + ((*(srcPtrTopRow + srcLocationColumnFloor + 1)) * (1 - weightedHeight) * (weightedWidth))
+                                + ((*(srcPtrBottomRow + srcLocationColumnFloor)) * (weightedHeight) * (1 - weightedWidth))
+                                + ((*(srcPtrBottomRow + srcLocationColumnFloor + 1)) * (weightedHeight) * (weightedWidth));
+
+                        *dstPtrTemp = (T) pixel;
+                        dstPtrTemp ++;
+                    }
+                }
+            }
+            srcPtrTemp += srcSize.height * srcSize.width;
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        Rpp32s elementsInRow = srcSize.width * channel;
+        for (int i = 0; i < dstSize.height; i++)
+        {
+            srcLocationRow = ((Rpp32f) i) / percentage;
+            srcLocationRowFloor = (Rpp32s) RPPFLOOR(srcLocationRow);
+            Rpp32f weightedHeight = srcLocationRow - srcLocationRowFloor;
+
+            srcPtrTopRow = srcPtrTemp + srcLocationRowFloor * elementsInRow;
+            srcPtrBottomRow  = srcPtrTopRow + elementsInRow;
+
+            for (int j = 0; j < dstSize.width; j++)
+            {
+                srcLocationColumn = ((Rpp32f) j) / percentage;
+                srcLocationColumnFloor = (Rpp32s) RPPFLOOR(srcLocationColumn);
+                Rpp32f weightedWidth = srcLocationColumn - srcLocationColumnFloor;
+
+                if (srcLocationRow < 0 || srcLocationColumn < 0 || srcLocationRow > (srcSize.height - 2) || srcLocationColumn > (srcSize.width - 2))
+                {
+                    for (int c = 0; c < channel; c++)
+                    {
+                        *dstPtrTemp = 0;
+                        dstPtrTemp++;
+                    }
+                }
+                else
+                {
+                    Rpp32s srcLocColFloorChanneled = channel * srcLocationColumnFloor;
+
+                    for (int c = 0; c < channel; c++)
+                    {
+                        pixel = ((*(srcPtrTopRow + c + srcLocColFloorChanneled)) * (1 - weightedHeight) * (1 - weightedWidth))
+                                + ((*(srcPtrTopRow + c + srcLocColFloorChanneled + channel)) * (1 - weightedHeight) * (weightedWidth))
+                                + ((*(srcPtrBottomRow + c + srcLocColFloorChanneled)) * (weightedHeight) * (1 - weightedWidth))
+                                + ((*(srcPtrBottomRow + c + srcLocColFloorChanneled + channel)) * (weightedHeight) * (weightedWidth));
+
+                        *dstPtrTemp = (T) pixel;
+                        dstPtrTemp ++;
+                    }
+                }
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+/**************** rotate ***************/
+
+template <typename T>
+RppStatus rotate_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr, RppiSize *batch_dstSize, RppiSize *batch_dstSizeMax,
+                             Rpp32f *batch_angleDeg, RppiROI *roiPoints,
+                             Rpp32u outputFormatToggle, Rpp32u nbatchSize,
+                             RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32f x1 = roiPoints[batchCount].x;
+            Rpp32f y1 = roiPoints[batchCount].y;
+            Rpp32f x2 = x1 + roiPoints[batchCount].roiWidth - 1;
+            Rpp32f y2 = y1 + roiPoints[batchCount].roiHeight - 1;
+            if (x2 == -1)
+            {
+                x2 = batch_srcSize[batchCount].width - 1;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == -1)
+            {
+                y2 = batch_srcSize[batchCount].height - 1;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp32u srcImageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+            Rpp32u dstImageDimMax = batch_dstSizeMax[batchCount].height * batch_dstSizeMax[batchCount].width;
+
+            Rpp32f angleDeg = batch_angleDeg[batchCount];
+            Rpp32f angleRad = -RAD(angleDeg);
+            Rpp32f rotate[4] = {0};
+            rotate[0] = cos(angleRad);
+            rotate[1] = sin(angleRad);
+            rotate[2] = -sin(angleRad);
+            rotate[3] = cos(angleRad);
+            Rpp32f divisor = (rotate[1] * rotate[2]) - (rotate[0] * rotate[3]);
+
+            Rpp32f halfSrcHeight = batch_srcSize[batchCount].height / 2;
+            Rpp32f halfSrcWidth = batch_srcSize[batchCount].width / 2;
+            Rpp32f halfDstHeight = batch_dstSize[batchCount].height / 2;
+            Rpp32f halfDstWidth = batch_dstSize[batchCount].width / 2;
+            Rpp32f halfHeightDiff = halfSrcHeight - halfDstHeight;
+            Rpp32f halfWidthDiff = halfSrcWidth - halfDstWidth;
+
+            Rpp32f srcLocationRowParameter = (rotate[0] * halfSrcHeight) + (rotate[1] * halfSrcWidth) - halfSrcHeight + halfHeightDiff;
+            Rpp32f srcLocationColumnParameter = (rotate[2] * halfSrcHeight) + (rotate[3] * halfSrcWidth) - halfSrcWidth + halfWidthDiff;
+            Rpp32f srcLocationRowParameter2 = (-rotate[3] * (Rpp32s)srcLocationRowParameter) + (rotate[1] * (Rpp32s)srcLocationColumnParameter);
+            Rpp32f srcLocationColumnParameter2 = (rotate[2] * (Rpp32s)srcLocationRowParameter) + (-rotate[0] * (Rpp32s)srcLocationColumnParameter);
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+            compute_image_location_host(batch_dstSizeMax, batchCount, &dstLoc, channel);
+            srcPtrImage = srcPtr + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+
+            for(int c = 0; c < channel; c++)
+            {
+                T *srcPtrChannel, *dstPtrChannel;
+                srcPtrChannel = srcPtrImage + (c * srcImageDimMax);
+                dstPtrChannel = dstPtrImage + (c * dstImageDimMax);
+
+
+                for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                {
+                    Rpp32f pixel;
+
+                    T *srcPtrTemp, *dstPtrTemp;
+                    srcPtrTemp = srcPtrChannel;
+                    dstPtrTemp = dstPtrChannel + (i * batch_dstSizeMax[batchCount].width);
+
+                    if (!((y1 <= i) && (i <= y2)))
+                    {
+                        memset(dstPtrTemp, (T) 0, batch_dstSize[batchCount].width * sizeof(T));
+
+                        dstPtrTemp += batch_dstSize[batchCount].width;
+                    }
+                    else
+                    {
+                        Rpp32f srcLocationRowTerm1 = -rotate[3] * i;
+                        Rpp32f srcLocationColumnTerm1 = rotate[2] * i;
+
+                        for(int j = 0; j < batch_dstSize[batchCount].width; j++)
+                        {
+                            if (!((x1 <= j) && (j <= x2 )))
+                            {
+                                memset(dstPtrTemp, (T) 0, sizeof(T));
+
+                                dstPtrTemp += 1;
+                            }
+                            else
+                            {
+                                Rpp32f srcLocationRow = (srcLocationRowTerm1 + (rotate[1] * j) + srcLocationRowParameter2) / divisor;
+                                Rpp32f srcLocationColumn = (srcLocationColumnTerm1 + (-rotate[0] * j) + srcLocationColumnParameter2) / divisor;
+
+                                Rpp32s srcLocationRowFloor = (Rpp32s) RPPFLOOR(srcLocationRow);
+                                Rpp32s srcLocationColumnFloor = (Rpp32s) RPPFLOOR(srcLocationColumn);
+
+                                if ((srcLocationRowFloor < y1) || (srcLocationRowFloor > y2) || (srcLocationColumnFloor < x1) || (srcLocationColumnFloor > x2))
+                                {
+                                    *dstPtrTemp = (T) 0;
+                                }
+                                else if (srcLocationRow < 0 || srcLocationColumn < 0 || srcLocationRow > (batch_srcSize[batchCount].height - 2) || srcLocationColumn > (batch_srcSize[batchCount].width - 2))
+                                {
+                                    *dstPtrTemp = (T) 0;
+                                }
+                                else
+                                {
+                                    Rpp32f weightedHeight = srcLocationRow - srcLocationRowFloor;
+                                    Rpp32f weightedWidth = srcLocationColumn - srcLocationColumnFloor;
+
+                                    T *srcPtrTopRow, *srcPtrBottomRow;
+                                    srcPtrTopRow = srcPtrTemp + srcLocationRowFloor * batch_srcSizeMax[batchCount].width;
+                                    srcPtrBottomRow  = srcPtrTopRow + batch_srcSizeMax[batchCount].width;
+
+                                    Rpp32s srcLocColFloorChanneled = channel * srcLocationColumnFloor;
+
+                                    pixel = ((*(srcPtrTopRow + srcLocationColumnFloor)) * (1 - weightedHeight) * (1 - weightedWidth))
+                                        + ((*(srcPtrTopRow + srcLocationColumnFloor + 1)) * (1 - weightedHeight) * (weightedWidth))
+                                        + ((*(srcPtrBottomRow + srcLocationColumnFloor)) * (weightedHeight) * (1 - weightedWidth))
+                                        + ((*(srcPtrBottomRow + srcLocationColumnFloor + 1)) * (weightedHeight) * (weightedWidth));
+
+                                    *dstPtrTemp = (T) pixel;
+                                }
+                                dstPtrTemp++;
+                            }
+                        }
+                    }
+                }
+            }
+
+            if (outputFormatToggle == 1)
+            {
+                T *dstPtrImageUnpadded = (T*) calloc(channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width, sizeof(T));
+                T *dstPtrImageUnpaddedCopy = (T*) calloc(channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width, sizeof(T));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_dstSize[batchCount], batch_dstSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width * sizeof(T));
+
+                compute_planar_to_packed_host(dstPtrImageUnpaddedCopy, batch_dstSize[batchCount], dstPtrImageUnpadded, channel);
+
+                memset(dstPtrImage, (T) 0, dstImageDimMax * channel * sizeof(T));
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_dstSize[batchCount], batch_dstSizeMax[batchCount], dstPtrImage, RPPI_CHN_PACKED, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32f x1 = roiPoints[batchCount].x;
+            Rpp32f y1 = roiPoints[batchCount].y;
+            Rpp32f x2 = x1 + roiPoints[batchCount].roiWidth - 1;
+            Rpp32f y2 = y1 + roiPoints[batchCount].roiHeight - 1;
+            if (x2 == -1)
+            {
+                x2 = batch_srcSize[batchCount].width - 1;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == -1)
+            {
+                y2 = batch_srcSize[batchCount].height - 1;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp32f angleDeg = batch_angleDeg[batchCount];
+            Rpp32f angleRad = -RAD(angleDeg);
+            Rpp32f rotate[4] = {0};
+            rotate[0] = cos(angleRad);
+            rotate[1] = sin(angleRad);
+            rotate[2] = -sin(angleRad);
+            rotate[3] = cos(angleRad);
+            Rpp32f divisor = (rotate[1] * rotate[2]) - (rotate[0] * rotate[3]);
+
+            Rpp32f halfSrcHeight = batch_srcSize[batchCount].height / 2;
+            Rpp32f halfSrcWidth = batch_srcSize[batchCount].width / 2;
+            Rpp32f halfDstHeight = batch_dstSize[batchCount].height / 2;
+            Rpp32f halfDstWidth = batch_dstSize[batchCount].width / 2;
+            Rpp32f halfHeightDiff = halfSrcHeight - halfDstHeight;
+            Rpp32f halfWidthDiff = halfSrcWidth - halfDstWidth;
+
+            Rpp32f srcLocationRowParameter = (rotate[0] * halfSrcHeight) + (rotate[1] * halfSrcWidth) - halfSrcHeight + halfHeightDiff;
+            Rpp32f srcLocationColumnParameter = (rotate[2] * halfSrcHeight) + (rotate[3] * halfSrcWidth) - halfSrcWidth + halfWidthDiff;
+            Rpp32f srcLocationRowParameter2 = (-rotate[3] * (Rpp32s)srcLocationRowParameter) + (rotate[1] * (Rpp32s)srcLocationColumnParameter);
+            Rpp32f srcLocationColumnParameter2 = (rotate[2] * (Rpp32s)srcLocationRowParameter) + (-rotate[0] * (Rpp32s)srcLocationColumnParameter);
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+            compute_image_location_host(batch_dstSizeMax, batchCount, &dstLoc, channel);
+            srcPtrImage = srcPtr + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+
+            Rpp32u srcElementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+            Rpp32u dstElementsInRowMax = channel * batch_dstSizeMax[batchCount].width;
+
+            Rpp32u dstElementsInRow = channel * batch_dstSize[batchCount].width;
+
+
+            for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+            {
+                Rpp32f pixel;
+
+                T *srcPtrTemp, *dstPtrTemp;
+                srcPtrTemp = srcPtrImage;
+                dstPtrTemp = dstPtrImage + (i * dstElementsInRowMax);
+
+                if (!((y1 <= i) && (i <= y2)))
+                {
+                    memset(dstPtrTemp, (T) 0, dstElementsInRow * sizeof(T));
+                }
+                else
+                {
+                    Rpp32f srcLocationRowTerm1 = -rotate[3] * i;
+                    Rpp32f srcLocationColumnTerm1 = rotate[2] * i;
+
+                    for(int j = 0; j < batch_dstSize[batchCount].width; j++)
+                    {
+                        if (!((x1 <= j) && (j <= x2 )))
+                        {
+                            memset(dstPtrTemp, (T) 0, channel * sizeof(T));
+
+                            dstPtrTemp += channel;
+                        }
+                        else
+                        {
+                            Rpp32f srcLocationRow = (srcLocationRowTerm1 + (rotate[1] * j) + srcLocationRowParameter2) / divisor;
+                            Rpp32f srcLocationColumn = (srcLocationColumnTerm1 + (-rotate[0] * j) + srcLocationColumnParameter2) / divisor;
+
+                            Rpp32s srcLocationRowFloor = (Rpp32s) RPPFLOOR(srcLocationRow);
+                            Rpp32s srcLocationColumnFloor = (Rpp32s) RPPFLOOR(srcLocationColumn);
+
+                            if ((srcLocationRowFloor < y1) || (srcLocationRowFloor > y2) || (srcLocationColumnFloor < x1) || (srcLocationColumnFloor > x2))
+                            {
+                                for (int c = 0; c < channel; c++)
+                                {
+                                    *dstPtrTemp = 0;
+
+                                    dstPtrTemp++;
+                                }
+                            }
+                            else if (srcLocationRow < 0 || srcLocationColumn < 0 || srcLocationRow > (batch_srcSize[batchCount].height - 2) || srcLocationColumn > (batch_srcSize[batchCount].width - 2))
+                            {
+                                memset(dstPtrTemp, (T) 0, channel * sizeof(T));
+                                dstPtrTemp += channel;
+                            }
+                            else
+                            {
+                                Rpp32f weightedHeight = srcLocationRow - srcLocationRowFloor;
+                                Rpp32f weightedWidth = srcLocationColumn - srcLocationColumnFloor;
+
+                                T *srcPtrTopRow, *srcPtrBottomRow;
+                                srcPtrTopRow = srcPtrTemp + srcLocationRowFloor * srcElementsInRowMax;
+                                srcPtrBottomRow  = srcPtrTopRow + srcElementsInRowMax;
+
+                                Rpp32s srcLocColFloorChanneled = channel * srcLocationColumnFloor;
+
+                                for (int c = 0; c < channel; c++)
+                                {
+                                    pixel = ((*(srcPtrTopRow + c + srcLocColFloorChanneled)) * (1 - weightedHeight) * (1 - weightedWidth))
+                                        + ((*(srcPtrTopRow + c + srcLocColFloorChanneled + channel)) * (1 - weightedHeight) * (weightedWidth))
+                                        + ((*(srcPtrBottomRow + c + srcLocColFloorChanneled)) * (weightedHeight) * (1 - weightedWidth))
+                                        + ((*(srcPtrBottomRow + c + srcLocColFloorChanneled + channel)) * (weightedHeight) * (weightedWidth));
+
+                                    *dstPtrTemp = (T) pixel;
+                                    dstPtrTemp ++;
+                                }
+                            }
+                        }
+                    }
+                }
+            }
+
+            if (outputFormatToggle == 1)
+            {
+                T *dstPtrImageUnpadded = (T*) calloc(channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width, sizeof(T));
+                T *dstPtrImageUnpaddedCopy = (T*) calloc(channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width, sizeof(T));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_dstSize[batchCount], batch_dstSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width * sizeof(T));
+
+                compute_packed_to_planar_host(dstPtrImageUnpaddedCopy, batch_dstSize[batchCount], dstPtrImageUnpadded, channel);
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_dstSize[batchCount], batch_dstSizeMax[batchCount], dstPtrImage, RPPI_CHN_PLANAR, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus rotate_host(T* srcPtr, RppiSize srcSize, T* dstPtr, RppiSize dstSize,
+                           Rpp32f angleDeg,
+                           RppiChnFormat chnFormat, Rpp32u channel)
+{
+    Rpp32f angleRad = -RAD(angleDeg);
+    Rpp32f rotate[4] = {0};
+    rotate[0] = cos(angleRad);
+    rotate[1] = sin(angleRad);
+    rotate[2] = -sin(angleRad);
+    rotate[3] = cos(angleRad);
+
+    T *srcPtrTemp, *dstPtrTemp, *srcPtrTopRow, *srcPtrBottomRow;
+    srcPtrTemp = srcPtr;
+    dstPtrTemp = dstPtr;
+
+    Rpp32f divisor = (rotate[1] * rotate[2]) - (rotate[0] * rotate[3]);
+    Rpp32f srcLocationRow, srcLocationColumn, srcLocationRowTerm1, srcLocationColumnTerm1, pixel;
+    Rpp32s srcLocationRowFloor, srcLocationColumnFloor;
+
+    Rpp32f halfSrcHeight = srcSize.height / 2;
+    Rpp32f halfSrcWidth = srcSize.width / 2;
+    Rpp32f halfDstHeight = dstSize.height / 2;
+    Rpp32f halfDstWidth = dstSize.width / 2;
+    Rpp32f halfHeightDiff = halfSrcHeight - halfDstHeight;
+    Rpp32f halfWidthDiff = halfSrcWidth - halfDstWidth;
+
+    Rpp32f srcLocationRowParameter = (rotate[0] * halfSrcHeight) + (rotate[1] * halfSrcWidth) - halfSrcHeight + halfHeightDiff;
+    Rpp32f srcLocationColumnParameter = (rotate[2] * halfSrcHeight) + (rotate[3] * halfSrcWidth) - halfSrcWidth + halfWidthDiff;
+    Rpp32f srcLocationRowParameter2 = (-rotate[3] * (Rpp32s)srcLocationRowParameter) + (rotate[1] * (Rpp32s)srcLocationColumnParameter);
+    Rpp32f srcLocationColumnParameter2 = (rotate[2] * (Rpp32s)srcLocationRowParameter) + (-rotate[0] * (Rpp32s)srcLocationColumnParameter);
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        for (int c = 0; c < channel; c++)
+        {
+            for (int i = 0; i < dstSize.height; i++)
+            {
+                srcLocationRowTerm1 = -rotate[3] * i;
+                srcLocationColumnTerm1 = rotate[2] * i;
+                for (int j = 0; j < dstSize.width; j++)
+                {
+                    srcLocationRow = (srcLocationRowTerm1 + (rotate[1] * j) + srcLocationRowParameter2) / divisor;
+                    srcLocationColumn = (srcLocationColumnTerm1 + (-rotate[0] * j) + srcLocationColumnParameter2) / divisor;
+
+                    if (srcLocationRow < 0 || srcLocationColumn < 0 || srcLocationRow > (srcSize.height - 2) || srcLocationColumn > (srcSize.width - 2))
+                    {
+                        *dstPtrTemp = 0;
+                        dstPtrTemp++;
+                    }
+                    else
+                    {
+                        srcLocationRowFloor = (Rpp32s) RPPFLOOR(srcLocationRow);
+                        srcLocationColumnFloor = (Rpp32s) RPPFLOOR(srcLocationColumn);
+                        Rpp32f weightedHeight = srcLocationRow - srcLocationRowFloor;
+                        Rpp32f weightedWidth = srcLocationColumn - srcLocationColumnFloor;
+
+                        srcPtrTopRow = srcPtrTemp + srcLocationRowFloor * srcSize.width;
+                        srcPtrBottomRow  = srcPtrTopRow + srcSize.width;
+
+                        Rpp32s srcLocColFloorChanneled = channel * srcLocationColumnFloor;
+
+                        pixel = ((*(srcPtrTopRow + srcLocationColumnFloor)) * (1 - weightedHeight) * (1 - weightedWidth))
+                            + ((*(srcPtrTopRow + srcLocationColumnFloor + 1)) * (1 - weightedHeight) * (weightedWidth))
+                            + ((*(srcPtrBottomRow + srcLocationColumnFloor)) * (weightedHeight) * (1 - weightedWidth))
+                            + ((*(srcPtrBottomRow + srcLocationColumnFloor + 1)) * (weightedHeight) * (weightedWidth));
+
+                        *dstPtrTemp = (T) pixel;
+                        dstPtrTemp ++;
+                    }
+                }
+            }
+            srcPtrTemp += srcSize.height * srcSize.width;
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        Rpp32s elementsInRow = srcSize.width * channel;
+        for (int i = 0; i < dstSize.height; i++)
+        {
+            srcLocationRowTerm1 = -rotate[3] * i;
+            srcLocationColumnTerm1 = rotate[2] * i;
+            for (int j = 0; j < dstSize.width; j++)
+            {
+                srcLocationRow = (srcLocationRowTerm1 + (rotate[1] * j) + srcLocationRowParameter2) / divisor;
+                srcLocationColumn = (srcLocationColumnTerm1 + (-rotate[0] * j) + srcLocationColumnParameter2) / divisor;
+
+                if (srcLocationRow < 0 || srcLocationColumn < 0 || srcLocationRow > (srcSize.height - 2) || srcLocationColumn > (srcSize.width - 2))
+                {
+                    memset(dstPtrTemp, (T) 0, channel * sizeof(T));
+                    dstPtrTemp += channel;
+                }
+                else
+                {
+                    srcLocationRowFloor = (Rpp32s) RPPFLOOR(srcLocationRow);
+                    srcLocationColumnFloor = (Rpp32s) RPPFLOOR(srcLocationColumn);
+                    Rpp32f weightedHeight = srcLocationRow - srcLocationRowFloor;
+                    Rpp32f weightedWidth = srcLocationColumn - srcLocationColumnFloor;
+
+                    srcPtrTopRow = srcPtrTemp + srcLocationRowFloor * elementsInRow;
+                    srcPtrBottomRow  = srcPtrTopRow + elementsInRow;
+
+                    Rpp32s srcLocColFloorChanneled = channel * srcLocationColumnFloor;
+
+                    for (int c = 0; c < channel; c++)
+                    {
+                        pixel = ((*(srcPtrTopRow + c + srcLocColFloorChanneled)) * (1 - weightedHeight) * (1 - weightedWidth))
+                            + ((*(srcPtrTopRow + c + srcLocColFloorChanneled + channel)) * (1 - weightedHeight) * (weightedWidth))
+                            + ((*(srcPtrBottomRow + c + srcLocColFloorChanneled)) * (weightedHeight) * (1 - weightedWidth))
+                            + ((*(srcPtrBottomRow + c + srcLocColFloorChanneled + channel)) * (weightedHeight) * (weightedWidth));
+
+                        *dstPtrTemp = (T) pixel;
+                        dstPtrTemp ++;
+                    }
+                }
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+/**************** resize ***************/
+
+template <typename T, typename U>
+RppStatus resize_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, U* dstPtr, RppiSize *batch_dstSize, RppiSize *batch_dstSizeMax,
+                            RppiROI *roiPoints, Rpp32u outputFormatToggle, Rpp32u nbatchSize,
+                            RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u x1 = roiPoints[batchCount].x;
+            Rpp32u y1 = roiPoints[batchCount].y;
+            Rpp32u x2 = x1 + roiPoints[batchCount].roiWidth - 1;
+            Rpp32u y2 = y1 + roiPoints[batchCount].roiHeight - 1;
+            if (x2 == -1)
+            {
+                x2 = batch_srcSize[batchCount].width - 1;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == -1)
+            {
+                y2 = batch_srcSize[batchCount].height - 1;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp64u srcImageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+            Rpp64u dstImageDimMax = batch_dstSizeMax[batchCount].height * batch_dstSizeMax[batchCount].width;
+
+            Rpp32f hRatio = (((Rpp32f) (batch_dstSizeMax[batchCount].height - 1)) / ((Rpp32f) (batch_srcSizeMax[batchCount].height - 1)));
+            Rpp32f wRatio = (((Rpp32f) (batch_dstSizeMax[batchCount].width - 1)) / ((Rpp32f) (batch_srcSizeMax[batchCount].width - 1)));
+
+            T *srcPtrImage;
+            U *dstPtrImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+            compute_image_location_host(batch_dstSizeMax, batchCount, &dstLoc, channel);
+            srcPtrImage = srcPtr + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+
+            RppiSize srcSizeROI, dstSize;
+            srcSizeROI.height = roiPoints[batchCount].roiHeight;
+            srcSizeROI.width = roiPoints[batchCount].roiWidth;
+            dstSize.height = batch_dstSize[batchCount].height;
+            dstSize.width = batch_dstSize[batchCount].width;
+
+            T *srcPtrROI = (T *)calloc(srcSizeROI.height * srcSizeROI.width * channel, sizeof(T));
+            U *dstPtrROI = (U *)calloc(dstSize.height * dstSize.width * channel, sizeof(U));
+            T *srcPtrImageTemp, *srcPtrROITemp;
+            srcPtrROITemp = srcPtrROI;
+            for (int c = 0; c < channel; c++)
+            {
+                srcPtrImageTemp = srcPtrImage + (c * srcImageDimMax) + ((Rpp32u) y1 * batch_srcSizeMax[batchCount].width) + (Rpp32u) x1;
+                for (int i = 0; i < srcSizeROI.height; i++)
+                {
+                    memcpy(srcPtrROITemp, srcPtrImageTemp, srcSizeROI.width * sizeof(T));
+                    srcPtrImageTemp += batch_srcSizeMax[batchCount].width;
+                    srcPtrROITemp += srcSizeROI.width;
+                }
+            }
+
+            if (outputFormatToggle == 1)
+            {
+                U *dstPtrROICopy = (U *)calloc(dstSize.height * dstSize.width * channel, sizeof(U));
+                resize_kernel_host(srcPtrROI, srcSizeROI, dstPtrROICopy, dstSize, chnFormat, channel);
+                compute_planar_to_packed_host(dstPtrROICopy, dstSize, dstPtrROI, channel);
+                if ((typeid(Rpp8u) == typeid(T)) && (typeid(Rpp8u) != typeid(U)))
+                {
+                    normalize_kernel_host(dstPtrROI, dstSize, channel);
+                }
+                compute_padded_from_unpadded_host(dstPtrROI, dstSize, batch_dstSizeMax[batchCount], dstPtrImage, RPPI_CHN_PACKED, channel);
+                free(dstPtrROICopy);
+            }
+            else
+            {
+                resize_kernel_host(srcPtrROI, srcSizeROI, dstPtrROI, dstSize, chnFormat, channel);
+                if ((typeid(Rpp8u) == typeid(T)) && (typeid(Rpp8u) != typeid(U)))
+                {
+                    normalize_kernel_host(dstPtrROI, dstSize, channel);
+                }
+                compute_padded_from_unpadded_host(dstPtrROI, dstSize, batch_dstSizeMax[batchCount], dstPtrImage, chnFormat, channel);
+            }
+
+            free(srcPtrROI);
+            free(dstPtrROI);
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u x1 = roiPoints[batchCount].x;
+            Rpp32u y1 = roiPoints[batchCount].y;
+            Rpp32u x2 = x1 + roiPoints[batchCount].roiWidth - 1;
+            Rpp32u y2 = y1 + roiPoints[batchCount].roiHeight - 1;
+            if (x2 == -1)
+            {
+                x2 = batch_srcSize[batchCount].width - 1;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == -1)
+            {
+                y2 = batch_srcSize[batchCount].height - 1;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp32f hRatio = (((Rpp32f) (batch_dstSizeMax[batchCount].height - 1)) / ((Rpp32f) (batch_srcSizeMax[batchCount].height - 1)));
+            Rpp32f wRatio = (((Rpp32f) (batch_dstSizeMax[batchCount].width - 1)) / ((Rpp32f) (batch_srcSizeMax[batchCount].width - 1)));
+
+            T *srcPtrImage;
+            U *dstPtrImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+            compute_image_location_host(batch_dstSizeMax, batchCount, &dstLoc, channel);
+            srcPtrImage = srcPtr + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+
+            RppiSize srcSizeROI, dstSize;
+            srcSizeROI.height = roiPoints[batchCount].roiHeight;
+            srcSizeROI.width = roiPoints[batchCount].roiWidth;
+            dstSize.height = batch_dstSize[batchCount].height;
+            dstSize.width = batch_dstSize[batchCount].width;
+
+            Rpp32u elementsInRow = channel * batch_srcSize[batchCount].width;
+            Rpp32u elementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+            Rpp32u elementsInRowROI = channel * srcSizeROI.width;
+
+            T *srcPtrROI = (T *)calloc(srcSizeROI.height * srcSizeROI.width * channel, sizeof(T));
+            U *dstPtrROI = (U *)calloc(dstSize.height * dstSize.width * channel, sizeof(U));
+            T *srcPtrImageTemp, *srcPtrROITemp;
+            srcPtrROITemp = srcPtrROI;
+
+            srcPtrImageTemp = srcPtrImage + ((Rpp32u) y1 * elementsInRowMax) + (channel * (Rpp32u) x1);
+            for (int i = 0; i < srcSizeROI.height; i++)
+            {
+                memcpy(srcPtrROITemp, srcPtrImageTemp, elementsInRowROI * sizeof(T));
+                srcPtrImageTemp += elementsInRowMax;
+                srcPtrROITemp += elementsInRowROI;
+            }
+
+            if (outputFormatToggle == 1)
+            {
+                U *dstPtrROICopy = (U *)calloc(dstSize.height * dstSize.width * channel, sizeof(U));
+                resize_kernel_host(srcPtrROI, srcSizeROI, dstPtrROICopy, dstSize, chnFormat, channel);
+                compute_packed_to_planar_host(dstPtrROICopy, dstSize, dstPtrROI, channel);
+                if ((typeid(Rpp8u) == typeid(T)) && (typeid(Rpp8u) != typeid(U)))
+                {
+                    normalize_kernel_host(dstPtrROI, dstSize, channel);
+                }
+                compute_padded_from_unpadded_host(dstPtrROI, dstSize, batch_dstSizeMax[batchCount], dstPtrImage, RPPI_CHN_PLANAR, channel);
+                free(dstPtrROICopy);
+            }
+            else
+            {
+                resize_kernel_host(srcPtrROI, srcSizeROI, dstPtrROI, dstSize, chnFormat, channel);
+                if ((typeid(Rpp8u) == typeid(T)) && (typeid(Rpp8u) != typeid(U)))
+                {
+                    normalize_kernel_host(dstPtrROI, dstSize, channel);
+                }
+                compute_padded_from_unpadded_host(dstPtrROI, dstSize, batch_dstSizeMax[batchCount], dstPtrImage, chnFormat, channel);
+            }
+
+            free(srcPtrROI);
+            free(dstPtrROI);
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T, typename U>
+RppStatus resize_u8_i8_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, U* dstPtr, RppiSize *batch_dstSize, RppiSize *batch_dstSizeMax,
+                            RppiROI *roiPoints, Rpp32u outputFormatToggle, Rpp32u nbatchSize,
+                            RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u x1 = roiPoints[batchCount].x;
+            Rpp32u y1 = roiPoints[batchCount].y;
+            Rpp32u x2 = x1 + roiPoints[batchCount].roiWidth - 1;
+            Rpp32u y2 = y1 + roiPoints[batchCount].roiHeight - 1;
+            if (x2 == -1)
+            {
+                x2 = batch_srcSize[batchCount].width - 1;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == -1)
+            {
+                y2 = batch_srcSize[batchCount].height - 1;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp64u srcImageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+            Rpp64u dstImageDimMax = batch_dstSizeMax[batchCount].height * batch_dstSizeMax[batchCount].width;
+
+            Rpp32f hRatio = (((Rpp32f) (batch_dstSizeMax[batchCount].height - 1)) / ((Rpp32f) (batch_srcSizeMax[batchCount].height - 1)));
+            Rpp32f wRatio = (((Rpp32f) (batch_dstSizeMax[batchCount].width - 1)) / ((Rpp32f) (batch_srcSizeMax[batchCount].width - 1)));
+
+            T *srcPtrImage;
+            U *dstPtrImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+            compute_image_location_host(batch_dstSizeMax, batchCount, &dstLoc, channel);
+            srcPtrImage = srcPtr + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+
+            RppiSize srcSizeROI, dstSize;
+            srcSizeROI.height = roiPoints[batchCount].roiHeight;
+            srcSizeROI.width = roiPoints[batchCount].roiWidth;
+            dstSize.height = batch_dstSize[batchCount].height;
+            dstSize.width = batch_dstSize[batchCount].width;
+
+            T *srcPtrROI = (T *)calloc(srcSizeROI.height * srcSizeROI.width * channel, sizeof(T));
+            T *dstPtrROIipType = (T *)calloc(dstSize.height * dstSize.width * channel, sizeof(T));
+            U *dstPtrROI = (U *)calloc(dstSize.height * dstSize.width * channel, sizeof(U));
+            T *srcPtrImageTemp, *srcPtrROITemp;
+            srcPtrROITemp = srcPtrROI;
+            for (int c = 0; c < channel; c++)
+            {
+                srcPtrImageTemp = srcPtrImage + (c * srcImageDimMax) + ((Rpp32u) y1 * batch_srcSizeMax[batchCount].width) + (Rpp32u) x1;
+                for (int i = 0; i < srcSizeROI.height; i++)
+                {
+                    memcpy(srcPtrROITemp, srcPtrImageTemp, srcSizeROI.width * sizeof(T));
+                    srcPtrImageTemp += batch_srcSizeMax[batchCount].width;
+                    srcPtrROITemp += srcSizeROI.width;
+                }
+            }
+
+            resize_kernel_host(srcPtrROI, srcSizeROI, dstPtrROIipType, dstSize, chnFormat, channel);
+
+            T *dstPtrROIipTypeTemp;
+            dstPtrROIipTypeTemp = dstPtrROIipType;
+
+            U *dstPtrROITemp;
+            dstPtrROITemp = dstPtrROI;
+
+            for (int i = 0; i < (dstSize.height * dstSize.width * channel); i++)
+            {
+                *dstPtrROITemp = (U) (((Rpp32s) *dstPtrROIipTypeTemp) - 128);
+                dstPtrROITemp++;
+                dstPtrROIipTypeTemp++;
+            }
+
+            if (outputFormatToggle == 1)
+            {
+                U *dstPtrROI2 = (U *)calloc(dstSize.height * dstSize.width * channel, sizeof(U));
+                compute_planar_to_packed_host(dstPtrROI, dstSize, dstPtrROI2, channel);
+                compute_padded_from_unpadded_host(dstPtrROI2, dstSize, batch_dstSizeMax[batchCount], dstPtrImage, RPPI_CHN_PACKED, channel);
+                free(dstPtrROI2);
+            }
+            else
+            {
+                compute_padded_from_unpadded_host(dstPtrROI, dstSize, batch_dstSizeMax[batchCount], dstPtrImage, chnFormat, channel);
+            }
+
+            free(srcPtrROI);
+            free(dstPtrROI);
+            free(dstPtrROIipType);
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u x1 = roiPoints[batchCount].x;
+            Rpp32u y1 = roiPoints[batchCount].y;
+            Rpp32u x2 = x1 + roiPoints[batchCount].roiWidth - 1;
+            Rpp32u y2 = y1 + roiPoints[batchCount].roiHeight - 1;
+            if (x2 == -1)
+            {
+                x2 = batch_srcSize[batchCount].width - 1;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == -1)
+            {
+                y2 = batch_srcSize[batchCount].height - 1;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp32f hRatio = (((Rpp32f) (batch_dstSizeMax[batchCount].height - 1)) / ((Rpp32f) (batch_srcSizeMax[batchCount].height - 1)));
+            Rpp32f wRatio = (((Rpp32f) (batch_dstSizeMax[batchCount].width - 1)) / ((Rpp32f) (batch_srcSizeMax[batchCount].width - 1)));
+
+            T *srcPtrImage;
+            U *dstPtrImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+            compute_image_location_host(batch_dstSizeMax, batchCount, &dstLoc, channel);
+            srcPtrImage = srcPtr + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+
+            RppiSize srcSizeROI, dstSize;
+            srcSizeROI.height = roiPoints[batchCount].roiHeight;
+            srcSizeROI.width = roiPoints[batchCount].roiWidth;
+            dstSize.height = batch_dstSize[batchCount].height;
+            dstSize.width = batch_dstSize[batchCount].width;
+
+            Rpp32u elementsInRow = channel * batch_srcSize[batchCount].width;
+            Rpp32u elementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+            Rpp32u elementsInRowROI = channel * srcSizeROI.width;
+
+            T *srcPtrROI = (T *)calloc(srcSizeROI.height * srcSizeROI.width * channel, sizeof(T));
+            T *dstPtrROIipType = (T *)calloc(dstSize.height * dstSize.width * channel, sizeof(T));
+            U *dstPtrROI = (U *)calloc(dstSize.height * dstSize.width * channel, sizeof(U));
+            T *srcPtrImageTemp, *srcPtrROITemp;
+            srcPtrROITemp = srcPtrROI;
+
+            srcPtrImageTemp = srcPtrImage + ((Rpp32u) y1 * elementsInRowMax) + (channel * (Rpp32u) x1);
+            for (int i = 0; i < srcSizeROI.height; i++)
+            {
+                memcpy(srcPtrROITemp, srcPtrImageTemp, elementsInRowROI * sizeof(T));
+                srcPtrImageTemp += elementsInRowMax;
+                srcPtrROITemp += elementsInRowROI;
+            }
+
+            resize_kernel_host(srcPtrROI, srcSizeROI, dstPtrROIipType, dstSize, chnFormat, channel);
+
+            T *dstPtrROIipTypeTemp;
+            dstPtrROIipTypeTemp = dstPtrROIipType;
+
+            U *dstPtrROITemp;
+            dstPtrROITemp = dstPtrROI;
+
+            for (int i = 0; i < (dstSize.height * dstSize.width * channel); i++)
+            {
+                *dstPtrROITemp = (U) (((Rpp32s) *dstPtrROIipTypeTemp) - 128);
+                dstPtrROITemp++;
+                dstPtrROIipTypeTemp++;
+            }
+
+            if (outputFormatToggle == 1)
+            {
+                U *dstPtrROI2 = (U *)calloc(dstSize.height * dstSize.width * channel, sizeof(U));
+                compute_packed_to_planar_host(dstPtrROI, dstSize, dstPtrROI2, channel);
+                compute_padded_from_unpadded_host(dstPtrROI2, dstSize, batch_dstSizeMax[batchCount], dstPtrImage, RPPI_CHN_PLANAR, channel);
+                free(dstPtrROI2);
+            }
+            else
+            {
+                compute_padded_from_unpadded_host(dstPtrROI, dstSize, batch_dstSizeMax[batchCount], dstPtrImage, chnFormat, channel);
+            }
+
+            free(srcPtrROI);
+            free(dstPtrROI);
+            free(dstPtrROIipType);
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus resize_host(T* srcPtr, RppiSize srcSize, T* dstPtr, RppiSize dstSize,
+                           RppiChnFormat chnFormat, Rpp32u channel)
+{
+    resize_kernel_host(srcPtr, srcSize, dstPtr, dstSize, chnFormat, channel);
+
+    return RPP_SUCCESS;
+}
+
+// /**************** resize_crop ***************/
+
+template <typename T>
+RppStatus resize_crop_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr, RppiSize *batch_dstSize, RppiSize *batch_dstSizeMax,
+                                 Rpp32u *batch_x1, Rpp32u *batch_x2, Rpp32u *batch_y1, Rpp32u *batch_y2,
+                                 Rpp32u outputFormatToggle, Rpp32u nbatchSize,
+                                 RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u x1 = batch_x1[batchCount];
+            Rpp32u x2 = batch_x2[batchCount];
+            Rpp32u y1 = batch_y1[batchCount];
+            Rpp32u y2 = batch_y2[batchCount];
+
+            Rpp64u srcImageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+            Rpp64u dstImageDimMax = batch_dstSizeMax[batchCount].height * batch_dstSizeMax[batchCount].width;
+
+            Rpp32f hRatio = (((Rpp32f) (batch_dstSizeMax[batchCount].height - 1)) / ((Rpp32f) (batch_srcSizeMax[batchCount].height - 1)));
+            Rpp32f wRatio = (((Rpp32f) (batch_dstSizeMax[batchCount].width - 1)) / ((Rpp32f) (batch_srcSizeMax[batchCount].width - 1)));
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+            compute_image_location_host(batch_dstSizeMax, batchCount, &dstLoc, channel);
+            srcPtrImage = srcPtr + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+
+            RppiSize srcSizeROI, dstSize;
+            srcSizeROI.height = RPPABS(y2 - y1) + 1;
+            srcSizeROI.width = RPPABS(x2 - x1) + 1;
+            dstSize.height = batch_dstSize[batchCount].height;
+            dstSize.width = batch_dstSize[batchCount].width;
+
+            T *srcPtrROI = (T *)calloc(srcSizeROI.height * srcSizeROI.width * channel, sizeof(T));
+            T *dstPtrROI = (T *)calloc(dstSize.height * dstSize.width * channel, sizeof(T));
+            T *srcPtrImageTemp, *srcPtrROITemp;
+            srcPtrROITemp = srcPtrROI;
+            for (int c = 0; c < channel; c++)
+            {
+                srcPtrImageTemp = srcPtrImage + (c * srcImageDimMax) + ((Rpp32u) y1 * batch_srcSizeMax[batchCount].width) + (Rpp32u) x1;
+                for (int i = 0; i < srcSizeROI.height; i++)
+                {
+                    memcpy(srcPtrROITemp, srcPtrImageTemp, srcSizeROI.width * sizeof(T));
+                    srcPtrImageTemp += batch_srcSizeMax[batchCount].width;
+                    srcPtrROITemp += srcSizeROI.width;
+                }
+            }
+
+            resize_kernel_host(srcPtrROI, srcSizeROI, dstPtrROI, dstSize, chnFormat, channel);
+
+            if (outputFormatToggle == 1)
+            {
+                T *dstPtrROICopy = (T *)calloc(dstSize.height * dstSize.width * channel, sizeof(T));
+                compute_planar_to_packed_host(dstPtrROI, dstSize, dstPtrROICopy, channel);
+                compute_padded_from_unpadded_host(dstPtrROICopy, dstSize, batch_dstSizeMax[batchCount], dstPtrImage, RPPI_CHN_PACKED, channel);
+                free(dstPtrROICopy);
+            }
+            else
+            {
+                compute_padded_from_unpadded_host(dstPtrROI, dstSize, batch_dstSizeMax[batchCount], dstPtrImage, chnFormat, channel);
+            }
+
+            free(srcPtrROI);
+            free(dstPtrROI);
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u x1 = batch_x1[batchCount];
+            Rpp32u x2 = batch_x2[batchCount];
+            Rpp32u y1 = batch_y1[batchCount];
+            Rpp32u y2 = batch_y2[batchCount];
+
+            Rpp32f hRatio = (((Rpp32f) (batch_dstSizeMax[batchCount].height - 1)) / ((Rpp32f) (batch_srcSizeMax[batchCount].height - 1)));
+            Rpp32f wRatio = (((Rpp32f) (batch_dstSizeMax[batchCount].width - 1)) / ((Rpp32f) (batch_srcSizeMax[batchCount].width - 1)));
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+            compute_image_location_host(batch_dstSizeMax, batchCount, &dstLoc, channel);
+            srcPtrImage = srcPtr + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+
+            RppiSize srcSizeROI, dstSize;
+            srcSizeROI.height = RPPABS(y2 - y1) + 1;
+            srcSizeROI.width = RPPABS(x2 - x1) + 1;
+            dstSize.height = batch_dstSize[batchCount].height;
+            dstSize.width = batch_dstSize[batchCount].width;
+
+            Rpp32u elementsInRow = channel * batch_srcSize[batchCount].width;
+            Rpp32u elementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+            Rpp32u elementsInRowROI = channel * srcSizeROI.width;
+
+            T *srcPtrROI = (T *)calloc(srcSizeROI.height * srcSizeROI.width * channel, sizeof(T));
+            T *dstPtrROI = (T *)calloc(dstSize.height * dstSize.width * channel, sizeof(T));
+            T *srcPtrImageTemp, *srcPtrROITemp;
+            srcPtrROITemp = srcPtrROI;
+
+            srcPtrImageTemp = srcPtrImage + ((Rpp32u) y1 * elementsInRowMax) + (channel * (Rpp32u) x1);
+            for (int i = 0; i < srcSizeROI.height; i++)
+            {
+                memcpy(srcPtrROITemp, srcPtrImageTemp, elementsInRowROI * sizeof(T));
+                srcPtrImageTemp += elementsInRowMax;
+                srcPtrROITemp += elementsInRowROI;
+            }
+
+            resize_kernel_host(srcPtrROI, srcSizeROI, dstPtrROI, dstSize, chnFormat, channel);
+
+            if (outputFormatToggle == 1)
+            {
+                T *dstPtrROICopy = (T *)calloc(dstSize.height * dstSize.width * channel, sizeof(T));
+                compute_packed_to_planar_host(dstPtrROI, dstSize, dstPtrROICopy, channel);
+                compute_padded_from_unpadded_host(dstPtrROICopy, dstSize, batch_dstSizeMax[batchCount], dstPtrImage, RPPI_CHN_PLANAR, channel);
+                free(dstPtrROICopy);
+            }
+            else
+            {
+                compute_padded_from_unpadded_host(dstPtrROI, dstSize, batch_dstSizeMax[batchCount], dstPtrImage, chnFormat, channel);
+            }
+
+            free(srcPtrROI);
+            free(dstPtrROI);
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus resize_crop_host(T* srcPtr, RppiSize srcSize, T* dstPtr, RppiSize dstSize,
+                           Rpp32u x1, Rpp32u x2, Rpp32u y1, Rpp32u y2,
+                           RppiChnFormat chnFormat, Rpp32u channel)
+{
+    resize_crop_kernel_host(srcPtr, srcSize, dstPtr, dstSize, x1, y1, x2, y2, chnFormat, channel);
+
+    return RPP_SUCCESS;
+
+}
+
+/**************** warp_affine ***************/
+
+template <typename T>
+RppStatus warp_affine_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr, RppiSize *batch_dstSize, RppiSize *batch_dstSizeMax, RppiROI *roiPoints,
+                                 Rpp32f *batch_affine,
+                                 Rpp32u outputFormatToggle, Rpp32u nbatchSize,
+                                 RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32f x1 = roiPoints[batchCount].x;
+            Rpp32f y1 = roiPoints[batchCount].y;
+            Rpp32f x2 = x1 + roiPoints[batchCount].roiWidth - 1;
+            Rpp32f y2 = y1 + roiPoints[batchCount].roiHeight - 1;
+            if (x2 == -1)
+            {
+                x2 = batch_srcSize[batchCount].width - 1;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == -1)
+            {
+                y2 = batch_srcSize[batchCount].height - 1;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp32u srcImageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+            Rpp32u dstImageDimMax = batch_dstSizeMax[batchCount].height * batch_dstSizeMax[batchCount].width;
+
+            Rpp32f affine[6] = {0};
+            for (int i = 0; i < 6; i++)
+            {
+                affine[i] = batch_affine[(batchCount * 6) + i];
+            }
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+            compute_image_location_host(batch_dstSizeMax, batchCount, &dstLoc, channel);
+            srcPtrImage = srcPtr + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+
+            for(int c = 0; c < channel; c++)
+            {
+                T *srcPtrChannel, *dstPtrChannel;
+                srcPtrChannel = srcPtrImage + (c * srcImageDimMax);
+                dstPtrChannel = dstPtrImage + (c * dstImageDimMax);
+
+
+                for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                {
+                    Rpp32f pixel;
+
+                    T *srcPtrTemp, *dstPtrTemp;
+                    srcPtrTemp = srcPtrChannel;
+                    dstPtrTemp = dstPtrChannel + (i * batch_dstSizeMax[batchCount].width);
+
+                    if (!((y1 <= i) && (i <= y2)))
+                    {
+                        memset(dstPtrTemp, (T) 0, batch_dstSize[batchCount].width * sizeof(T));
+
+                        dstPtrTemp += batch_dstSize[batchCount].width;
+                    }
+                    else
+                    {
+                        Rpp32s iNew = i - (batch_srcSize[batchCount].height / 2);
+
+                        for(int j = 0; j < batch_dstSize[batchCount].width; j++)
+                        {
+                            if (!((x1 <= j) && (j <= x2 )))
+                            {
+                                memset(dstPtrTemp, (T) 0, sizeof(T));
+
+                                dstPtrTemp += 1;
+                            }
+                            else
+                            {
+                                Rpp32s jNew = j - (batch_srcSize[batchCount].width / 2);
+
+                                Rpp32f srcLocationColumn = (jNew * affine[0]) + (iNew * affine[1]) + affine[2];
+                                Rpp32f srcLocationRow = (jNew * affine[3]) + (iNew * affine[4]) + affine[5];
+
+                                srcLocationColumn += (batch_srcSize[batchCount].width / 2);
+                                srcLocationRow += (batch_srcSize[batchCount].height / 2);
+
+                                Rpp32s srcLocationRowFloor = (Rpp32s) RPPFLOOR(srcLocationRow);
+                                Rpp32s srcLocationColumnFloor = (Rpp32s) RPPFLOOR(srcLocationColumn);
+
+                                if ((srcLocationRowFloor < y1) || (srcLocationRowFloor > y2) || (srcLocationColumnFloor < x1) || (srcLocationColumnFloor > x2))
+                                {
+                                    *dstPtrTemp = (T) 0;
+                                }
+                                else if (srcLocationRow < 0 || srcLocationColumn < 0 || srcLocationRow > (batch_srcSize[batchCount].height - 2) || srcLocationColumn > (batch_srcSize[batchCount].width - 2))
+                                {
+                                    *dstPtrTemp = 0;
+                                }
+                                else
+                                {
+                                    Rpp32f weightedHeight = srcLocationRow - srcLocationRowFloor;
+                                    Rpp32f weightedWidth = srcLocationColumn - srcLocationColumnFloor;
+
+                                    T *srcPtrTopRow, *srcPtrBottomRow;
+                                    srcPtrTopRow = srcPtrTemp + srcLocationRowFloor * batch_srcSizeMax[batchCount].width;
+                                    srcPtrBottomRow  = srcPtrTopRow + batch_srcSizeMax[batchCount].width;
+
+                                    Rpp32s srcLocColFloorChanneled = channel * srcLocationColumnFloor;
+
+                                    pixel = ((*(srcPtrTopRow + srcLocationColumnFloor)) * (1 - weightedHeight) * (1 - weightedWidth))
+                                        + ((*(srcPtrTopRow + srcLocationColumnFloor + 1)) * (1 - weightedHeight) * (weightedWidth))
+                                        + ((*(srcPtrBottomRow + srcLocationColumnFloor)) * (weightedHeight) * (1 - weightedWidth))
+                                        + ((*(srcPtrBottomRow + srcLocationColumnFloor + 1)) * (weightedHeight) * (weightedWidth));
+
+                                    *dstPtrTemp = (T) pixel;
+                                }
+                                dstPtrTemp++;
+                            }
+                        }
+                    }
+                }
+            }
+            if (outputFormatToggle == 1)
+            {
+                T *dstPtrImageUnpadded = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+                T *dstPtrImageUnpaddedCopy = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width * sizeof(T));
+
+                compute_planar_to_packed_host(dstPtrImageUnpaddedCopy, batch_srcSize[batchCount], dstPtrImageUnpadded, channel);
+
+                memset(dstPtrImage, (T) 0, dstImageDimMax * channel * sizeof(T));
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImage, RPPI_CHN_PACKED, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32f x1 = roiPoints[batchCount].x;
+            Rpp32f y1 = roiPoints[batchCount].y;
+            Rpp32f x2 = x1 + roiPoints[batchCount].roiWidth - 1;
+            Rpp32f y2 = y1 + roiPoints[batchCount].roiHeight - 1;
+            if (x2 == -1)
+            {
+                x2 = batch_srcSize[batchCount].width - 1;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == -1)
+            {
+                y2 = batch_srcSize[batchCount].height - 1;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp32u dstImageDimMax = batch_dstSizeMax[batchCount].height * batch_dstSizeMax[batchCount].width;
+
+            Rpp32f affine[6] = {0};
+            for (int i = 0; i < 6; i++)
+            {
+                affine[i] = batch_affine[(batchCount * 6) + i];
+            }
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+            compute_image_location_host(batch_dstSizeMax, batchCount, &dstLoc, channel);
+            srcPtrImage = srcPtr + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+
+            Rpp32u srcElementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+            Rpp32u dstElementsInRowMax = channel * batch_dstSizeMax[batchCount].width;
+
+            Rpp32u dstElementsInRow = channel * batch_dstSize[batchCount].width;
+
+
+            for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+            {
+                Rpp32f pixel;
+
+                T *srcPtrTemp, *dstPtrTemp;
+                srcPtrTemp = srcPtrImage;
+                dstPtrTemp = dstPtrImage + (i * dstElementsInRowMax);
+
+                if (!((y1 <= i) && (i <= y2)))
+                {
+                    memset(dstPtrTemp, (T) 0, dstElementsInRow * sizeof(T));
+                }
+                else
+                {
+                    Rpp32s iNew = i - (batch_srcSize[batchCount].height / 2);
+
+                    for(int j = 0; j < batch_dstSize[batchCount].width; j++)
+                    {
+                        if (!((x1 <= j) && (j <= x2 )))
+                        {
+                            memset(dstPtrTemp, (T) 0, channel * sizeof(T));
+
+                            dstPtrTemp += channel;
+                        }
+                        else
+                        {
+                            Rpp32s jNew = j - (batch_srcSize[batchCount].width / 2);
+
+                            Rpp32f srcLocationColumn = (jNew * affine[0]) + (iNew * affine[1]) + affine[2];
+                            Rpp32f srcLocationRow = (jNew * affine[3]) + (iNew * affine[4]) + affine[5];
+
+                            srcLocationColumn += (batch_srcSize[batchCount].width / 2);
+                            srcLocationRow += (batch_srcSize[batchCount].height / 2);
+
+                            Rpp32s srcLocationRowFloor = (Rpp32s) RPPFLOOR(srcLocationRow);
+                            Rpp32s srcLocationColumnFloor = (Rpp32s) RPPFLOOR(srcLocationColumn);
+
+                            if ((srcLocationRowFloor < y1) || (srcLocationRowFloor > y2) || (srcLocationColumnFloor < x1) || (srcLocationColumnFloor > x2))
+                            {
+                                memset(dstPtrTemp, (T) 0, channel * sizeof(T));
+                                dstPtrTemp += channel;
+                            }
+                            else if (srcLocationRow < 0 || srcLocationColumn < 0 || srcLocationRow > (batch_srcSize[batchCount].height - 2) || srcLocationColumn > (batch_srcSize[batchCount].width - 2))
+                            {
+                                memset(dstPtrTemp, (T) 0, channel * sizeof(T));
+                                dstPtrTemp += channel;
+                            }
+                            else
+                            {
+                                Rpp32f weightedHeight = srcLocationRow - srcLocationRowFloor;
+                                Rpp32f weightedWidth = srcLocationColumn - srcLocationColumnFloor;
+
+                                T *srcPtrTopRow, *srcPtrBottomRow;
+                                srcPtrTopRow = srcPtrTemp + srcLocationRowFloor * srcElementsInRowMax;
+                                srcPtrBottomRow  = srcPtrTopRow + srcElementsInRowMax;
+
+                                Rpp32s srcLocColFloorChanneled = channel * srcLocationColumnFloor;
+
+                                for (int c = 0; c < channel; c++)
+                                {
+                                    pixel = ((*(srcPtrTopRow + c + srcLocColFloorChanneled)) * (1 - weightedHeight) * (1 - weightedWidth))
+                                        + ((*(srcPtrTopRow + c + srcLocColFloorChanneled + channel)) * (1 - weightedHeight) * (weightedWidth))
+                                        + ((*(srcPtrBottomRow + c + srcLocColFloorChanneled)) * (weightedHeight) * (1 - weightedWidth))
+                                        + ((*(srcPtrBottomRow + c + srcLocColFloorChanneled + channel)) * (weightedHeight) * (weightedWidth));
+
+                                    *dstPtrTemp = (T) pixel;
+
+                                    dstPtrTemp ++;
+                                }
+                            }
+                        }
+                    }
+                }
+            }
+            if (outputFormatToggle == 1)
+            {
+                T *dstPtrImageUnpadded = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+                T *dstPtrImageUnpaddedCopy = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+
+                compute_unpadded_from_padded_host(dstPtrImage, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImageUnpadded, chnFormat, channel);
+
+                memcpy(dstPtrImageUnpaddedCopy, dstPtrImageUnpadded, channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width * sizeof(T));
+
+                compute_packed_to_planar_host(dstPtrImageUnpaddedCopy, batch_srcSize[batchCount], dstPtrImageUnpadded, channel);
+
+                memset(dstPtrImage, (T) 0, dstImageDimMax * channel * sizeof(T));
+
+                compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImage, RPPI_CHN_PLANAR, channel);
+
+                free(dstPtrImageUnpadded);
+                free(dstPtrImageUnpaddedCopy);
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus warp_affine_host(T* srcPtr, RppiSize srcSize, T* dstPtr, RppiSize dstSize,
+                           Rpp32f* affine,
+                           RppiChnFormat chnFormat, Rpp32u channel)
+{
+    T *srcPtrTemp, *dstPtrTemp, *srcPtrTopRow, *srcPtrBottomRow;
+    srcPtrTemp = srcPtr;
+    dstPtrTemp = dstPtr;
+    Rpp32f srcLocationRow, srcLocationColumn, pixel;
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        for (int c = 0; c < channel; c++)
+        {
+            for (int i = 0; i < dstSize.height; i++)
+            {
+                Rpp32s iNew = i - (srcSize.height / 2);
+                for (int j = 0; j < dstSize.width; j++)
+                {
+                    Rpp32s jNew = j - (srcSize.width / 2);
+
+                    srcLocationColumn = (jNew * affine[0]) + (iNew * affine[1]) + affine[2];
+                    srcLocationRow = (jNew * affine[3]) + (iNew * affine[4]) + affine[5];
+
+                    srcLocationColumn += (srcSize.width / 2);
+                    srcLocationRow += (srcSize.height / 2);
+
+                    if (srcLocationRow < 0 || srcLocationColumn < 0 || srcLocationRow > (srcSize.height - 2) || srcLocationColumn > (srcSize.width - 2))
+                    {
+                        *dstPtrTemp = 0;
+                        dstPtrTemp++;
+                    }
+                    else
+                    {
+                        Rpp32s srcLocationRowFloor = (Rpp32s) RPPFLOOR(srcLocationRow);
+                        Rpp32s srcLocationColumnFloor = (Rpp32s) RPPFLOOR(srcLocationColumn);
+
+                        Rpp32f weightedHeight = srcLocationRow - srcLocationRowFloor;
+                        Rpp32f weightedWidth = srcLocationColumn - srcLocationColumnFloor;
+
+                        srcPtrTopRow = srcPtrTemp + srcLocationRowFloor * srcSize.width;
+                        srcPtrBottomRow  = srcPtrTopRow + srcSize.width;
+
+                        Rpp32s srcLocColFloorChanneled = channel * srcLocationColumnFloor;
+
+                        pixel = ((*(srcPtrTopRow + srcLocationColumnFloor)) * (1 - weightedHeight) * (1 - weightedWidth))
+                            + ((*(srcPtrTopRow + srcLocationColumnFloor + 1)) * (1 - weightedHeight) * (weightedWidth))
+                            + ((*(srcPtrBottomRow + srcLocationColumnFloor)) * (weightedHeight) * (1 - weightedWidth))
+                            + ((*(srcPtrBottomRow + srcLocationColumnFloor + 1)) * (weightedHeight) * (weightedWidth));
+
+                        *dstPtrTemp = (T) pixel;
+                        dstPtrTemp ++;
+                    }
+
+                }
+            }
+            srcPtrTemp += srcSize.height * srcSize.width;
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        Rpp32s elementsInRow = srcSize.width * channel;
+        for (int i = 0; i < dstSize.height; i++)
+        {
+            Rpp32s iNew = i - (srcSize.height / 2);
+            for (int j = 0; j < dstSize.width; j++)
+            {
+                Rpp32s jNew = j - (srcSize.width / 2);
+
+                srcLocationColumn = (jNew * affine[0]) + (iNew * affine[1]) + affine[2];
+                srcLocationRow = (jNew * affine[3]) + (iNew * affine[4]) + affine[5];
+
+                srcLocationColumn += (srcSize.width / 2);
+                srcLocationRow += (srcSize.height / 2);
+
+                if (srcLocationRow < 0 || srcLocationColumn < 0 || srcLocationRow > (srcSize.height - 2) || srcLocationColumn > (srcSize.width - 2))
+                {
+                    memset(dstPtrTemp, (T) 0, channel * sizeof(T));
+                    dstPtrTemp += channel;
+                }
+                else
+                {
+                    Rpp32s srcLocationRowFloor = (Rpp32s) RPPFLOOR(srcLocationRow);
+                    Rpp32s srcLocationColumnFloor = (Rpp32s) RPPFLOOR(srcLocationColumn);
+
+                    Rpp32f weightedHeight = srcLocationRow - srcLocationRowFloor;
+                    Rpp32f weightedWidth = srcLocationColumn - srcLocationColumnFloor;
+
+                    srcPtrTopRow = srcPtrTemp + srcLocationRowFloor * elementsInRow;
+                    srcPtrBottomRow  = srcPtrTopRow + elementsInRow;
+
+                    Rpp32s srcLocColFloorChanneled = channel * srcLocationColumnFloor;
+
+                    for (int c = 0; c < channel; c++)
+                    {
+                        pixel = ((*(srcPtrTopRow + c + srcLocColFloorChanneled)) * (1 - weightedHeight) * (1 - weightedWidth))
+                            + ((*(srcPtrTopRow + c + srcLocColFloorChanneled + channel)) * (1 - weightedHeight) * (weightedWidth))
+                            + ((*(srcPtrBottomRow + c + srcLocColFloorChanneled)) * (weightedHeight) * (1 - weightedWidth))
+                            + ((*(srcPtrBottomRow + c + srcLocColFloorChanneled + channel)) * (weightedHeight) * (weightedWidth));
+
+                        *dstPtrTemp = (T) pixel;
+                        dstPtrTemp ++;
+                    }
+                }
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+/**************** warp_perspective ***************/
+
+template <typename T>
+RppStatus warp_perspective_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr, RppiSize *batch_dstSize, RppiSize *batch_dstSizeMax,
+                                      RppiROI *roiPoints, Rpp32f *batch_perspective,
+                                      Rpp32u nbatchSize,
+                                      RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    //Rpp32f perspective[9] = {0.707, 0.707, 0, -0.707, 0.707, 0, 0.001, 0.001, 1};
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32f x1 = roiPoints[batchCount].x;
+            Rpp32f y1 = roiPoints[batchCount].y;
+            Rpp32f x2 = x1 + roiPoints[batchCount].roiWidth - 1;
+            Rpp32f y2 = y1 + roiPoints[batchCount].roiHeight - 1;
+            if (x2 == -1)
+            {
+                x2 = batch_srcSize[batchCount].width - 1;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == -1)
+            {
+                y2 = batch_srcSize[batchCount].height - 1;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp32u srcImageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+            Rpp32u dstImageDimMax = batch_dstSizeMax[batchCount].height * batch_dstSizeMax[batchCount].width;
+
+            Rpp32f perspective[9] = {0};
+            for (int i = 0; i < 9; i++)
+            {
+                perspective[i] = batch_perspective[(batchCount * 9) + i];
+            }
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+            compute_image_location_host(batch_dstSizeMax, batchCount, &dstLoc, channel);
+            srcPtrImage = srcPtr + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+
+            for(int c = 0; c < channel; c++)
+            {
+                T *srcPtrChannel, *dstPtrChannel;
+                srcPtrChannel = srcPtrImage + (c * srcImageDimMax);
+                dstPtrChannel = dstPtrImage + (c * dstImageDimMax);
+
+
+                for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+                {
+                    Rpp32f pixel;
+
+                    T *srcPtrTemp, *dstPtrTemp;
+                    srcPtrTemp = srcPtrChannel;
+                    dstPtrTemp = dstPtrChannel + (i * batch_dstSizeMax[batchCount].width);
+
+                    if (!((y1 <= i) && (i <= y2)))
+                    {
+                        memset(dstPtrTemp, (T) 0, batch_dstSize[batchCount].width * sizeof(T));
+
+                        dstPtrTemp += batch_dstSize[batchCount].width;
+                    }
+                    else
+                    {
+                        Rpp32s iNew = i - (batch_srcSize[batchCount].height / 2);
+
+                        for(int j = 0; j < batch_dstSize[batchCount].width; j++)
+                        {
+                            if (!((x1 <= j) && (j <= x2 )))
+                            {
+                                memset(dstPtrTemp, (T) 0, sizeof(T));
+
+                                dstPtrTemp += 1;
+                            }
+                            else
+                            {
+                                Rpp32s jNew = j - (batch_srcSize[batchCount].width / 2);
+
+                                Rpp32f srcLocationColumn = ((jNew * perspective[0]) + (iNew * perspective[1]) + perspective[2]) / ((jNew * perspective[6]) + (iNew * perspective[7]) + perspective[8]);
+                                Rpp32f srcLocationRow = ((jNew * perspective[3]) + (iNew * perspective[4]) + perspective[5]) / ((jNew * perspective[6]) + (iNew * perspective[7]) + perspective[8]);
+
+                                srcLocationColumn += (batch_srcSize[batchCount].width / 2);
+                                srcLocationRow += (batch_srcSize[batchCount].height / 2);
+
+                                Rpp32s srcLocationRowFloor = (Rpp32s) RPPFLOOR(srcLocationRow);
+                                Rpp32s srcLocationColumnFloor = (Rpp32s) RPPFLOOR(srcLocationColumn);
+
+                                if ((srcLocationRowFloor < y1) || (srcLocationRowFloor > y2) || (srcLocationColumnFloor < x1) || (srcLocationColumnFloor > x2))
+                                {
+                                    *dstPtrTemp = (T) 0;
+                                }
+                                else if (srcLocationRow < 0 || srcLocationColumn < 0 || srcLocationRow > (batch_srcSize[batchCount].height - 2) || srcLocationColumn > (batch_srcSize[batchCount].width - 2))
+                                {
+                                    *dstPtrTemp = 0;
+                                }
+                                else
+                                {
+                                    Rpp32f weightedHeight = srcLocationRow - srcLocationRowFloor;
+                                    Rpp32f weightedWidth = srcLocationColumn - srcLocationColumnFloor;
+
+                                    T *srcPtrTopRow, *srcPtrBottomRow;
+                                    srcPtrTopRow = srcPtrTemp + srcLocationRowFloor * batch_srcSizeMax[batchCount].width;
+                                    srcPtrBottomRow  = srcPtrTopRow + batch_srcSizeMax[batchCount].width;
+
+                                    Rpp32s srcLocColFloorChanneled = channel * srcLocationColumnFloor;
+
+                                    pixel = ((*(srcPtrTopRow + srcLocationColumnFloor)) * (1 - weightedHeight) * (1 - weightedWidth))
+                                        + ((*(srcPtrTopRow + srcLocationColumnFloor + 1)) * (1 - weightedHeight) * (weightedWidth))
+                                        + ((*(srcPtrBottomRow + srcLocationColumnFloor)) * (weightedHeight) * (1 - weightedWidth))
+                                        + ((*(srcPtrBottomRow + srcLocationColumnFloor + 1)) * (weightedHeight) * (weightedWidth));
+
+                                    *dstPtrTemp = (T) pixel;
+                                }
+                                dstPtrTemp++;
+                            }
+                        }
+                    }
+                }
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32f x1 = roiPoints[batchCount].x;
+            Rpp32f y1 = roiPoints[batchCount].y;
+            Rpp32f x2 = x1 + roiPoints[batchCount].roiWidth - 1;
+            Rpp32f y2 = y1 + roiPoints[batchCount].roiHeight - 1;
+            if (x2 == -1)
+            {
+                x2 = batch_srcSize[batchCount].width - 1;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == -1)
+            {
+                y2 = batch_srcSize[batchCount].height - 1;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp32f perspective[9] = {0};
+            for (int i = 0; i < 9; i++)
+            {
+                perspective[i] = batch_perspective[(batchCount * 9) + i];
+            }
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u srcLoc = 0, dstLoc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+            compute_image_location_host(batch_dstSizeMax, batchCount, &dstLoc, channel);
+            srcPtrImage = srcPtr + srcLoc;
+            dstPtrImage = dstPtr + dstLoc;
+
+            Rpp32u srcElementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+            Rpp32u dstElementsInRowMax = channel * batch_dstSizeMax[batchCount].width;
+
+            Rpp32u dstElementsInRow = channel * batch_dstSizeMax[batchCount].width;
+
+
+            for(int i = 0; i < batch_dstSize[batchCount].height; i++)
+            {
+                Rpp32f pixel;
+
+                T *srcPtrTemp, *dstPtrTemp;
+                srcPtrTemp = srcPtrImage;
+                dstPtrTemp = dstPtrImage + (i * dstElementsInRowMax);
+
+                if (!((y1 <= i) && (i <= y2)))
+                {
+                    memset(dstPtrTemp, (T) 0, dstElementsInRow * sizeof(T));
+                }
+                else
+                {
+                    Rpp32s iNew = i - (batch_srcSize[batchCount].height / 2);
+
+                    for(int j = 0; j < batch_dstSize[batchCount].width; j++)
+                    {
+                        if (!((x1 <= j) && (j <= x2 )))
+                        {
+                            memset(dstPtrTemp, (T) 0, channel * sizeof(T));
+
+                            dstPtrTemp += channel;
+                        }
+                        else
+                        {
+                            Rpp32s jNew = j - (batch_srcSize[batchCount].width / 2);
+
+                            Rpp32f srcLocationColumn = ((jNew * perspective[0]) + (iNew * perspective[1]) + perspective[2]) / ((jNew * perspective[6]) + (iNew * perspective[7]) + perspective[8]);
+                            Rpp32f srcLocationRow = ((jNew * perspective[3]) + (iNew * perspective[4]) + perspective[5]) / ((jNew * perspective[6]) + (iNew * perspective[7]) + perspective[8]);
+
+                            srcLocationColumn += (batch_srcSize[batchCount].width / 2);
+                            srcLocationRow += (batch_srcSize[batchCount].height / 2);
+
+                            Rpp32s srcLocationRowFloor = (Rpp32s) RPPFLOOR(srcLocationRow);
+                            Rpp32s srcLocationColumnFloor = (Rpp32s) RPPFLOOR(srcLocationColumn);
+
+                            if ((srcLocationRowFloor < y1) || (srcLocationRowFloor > y2) || (srcLocationColumnFloor < x1) || (srcLocationColumnFloor > x2))
+                            {
+                                for (int c = 0; c < channel; c++)
+                                {
+                                    *dstPtrTemp = 0;
+
+                                    dstPtrTemp++;
+                                }
+                            }
+                            else if (srcLocationRow < 0 || srcLocationColumn < 0 || srcLocationRow > (batch_srcSize[batchCount].height - 2) || srcLocationColumn > (batch_srcSize[batchCount].width - 2))
+                            {
+                                for (int c = 0; c < channel; c++)
+                                {
+                                    *dstPtrTemp = 0;
+
+                                    dstPtrTemp++;
+                                }
+                            }
+                            else
+                            {
+                                Rpp32f weightedHeight = srcLocationRow - srcLocationRowFloor;
+                                Rpp32f weightedWidth = srcLocationColumn - srcLocationColumnFloor;
+
+                                T *srcPtrTopRow, *srcPtrBottomRow;
+                                srcPtrTopRow = srcPtrTemp + srcLocationRowFloor * srcElementsInRowMax;
+                                srcPtrBottomRow  = srcPtrTopRow + srcElementsInRowMax;
+
+                                Rpp32s srcLocColFloorChanneled = channel * srcLocationColumnFloor;
+
+                                for (int c = 0; c < channel; c++)
+                                {
+                                    pixel = ((*(srcPtrTopRow + c + srcLocColFloorChanneled)) * (1 - weightedHeight) * (1 - weightedWidth))
+                                        + ((*(srcPtrTopRow + c + srcLocColFloorChanneled + channel)) * (1 - weightedHeight) * (weightedWidth))
+                                        + ((*(srcPtrBottomRow + c + srcLocColFloorChanneled)) * (weightedHeight) * (1 - weightedWidth))
+                                        + ((*(srcPtrBottomRow + c + srcLocColFloorChanneled + channel)) * (weightedHeight) * (weightedWidth));
+
+                                    *dstPtrTemp = (T) pixel;
+
+                                    dstPtrTemp ++;
+                                }
+                            }
+                        }
+                    }
+                }
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus warp_perspective_host(T* srcPtr, RppiSize srcSize, T* dstPtr, RppiSize dstSize,
+                           Rpp32f* perspective,
+                           RppiChnFormat chnFormat, Rpp32u channel)
+{
+    T *srcPtrTemp, *dstPtrTemp, *srcPtrTopRow, *srcPtrBottomRow;
+    srcPtrTemp = srcPtr;
+    dstPtrTemp = dstPtr;
+    Rpp32f srcLocationRow, srcLocationColumn, pixel;
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        for (int c = 0; c < channel; c++)
+        {
+            for (int i = 0; i < dstSize.height; i++)
+            {
+                Rpp32s iNew = i - (srcSize.height / 2);
+                for (int j = 0; j < dstSize.width; j++)
+                {
+                    Rpp32s jNew = j - (srcSize.width / 2);
+
+                    srcLocationColumn = ((jNew * perspective[0]) + (iNew * perspective[1]) + perspective[2]) / ((jNew * perspective[6]) + (iNew * perspective[7]) + perspective[8]);
+                    srcLocationRow = ((jNew * perspective[3]) + (iNew * perspective[4]) + perspective[5]) / ((jNew * perspective[6]) + (iNew * perspective[7]) + perspective[8]);
+
+                    srcLocationColumn += (srcSize.width / 2);
+                    srcLocationRow += (srcSize.height / 2);
+
+                    if (srcLocationRow < 0 || srcLocationColumn < 0 || srcLocationRow > (srcSize.height - 2) || srcLocationColumn > (srcSize.width - 2))
+                    {
+                        *dstPtrTemp = 0;
+                        dstPtrTemp++;
+                    }
+                    else
+                    {
+                        Rpp32s srcLocationRowFloor = (Rpp32s) RPPFLOOR(srcLocationRow);
+                        Rpp32s srcLocationColumnFloor = (Rpp32s) RPPFLOOR(srcLocationColumn);
+
+                        Rpp32f weightedHeight = srcLocationRow - srcLocationRowFloor;
+                        Rpp32f weightedWidth = srcLocationColumn - srcLocationColumnFloor;
+
+                        srcPtrTopRow = srcPtrTemp + srcLocationRowFloor * srcSize.width;
+                        srcPtrBottomRow  = srcPtrTopRow + srcSize.width;
+
+                        Rpp32s srcLocColFloorChanneled = channel * srcLocationColumnFloor;
+
+                        pixel = ((*(srcPtrTopRow + srcLocationColumnFloor)) * (1 - weightedHeight) * (1 - weightedWidth))
+                            + ((*(srcPtrTopRow + srcLocationColumnFloor + 1)) * (1 - weightedHeight) * (weightedWidth))
+                            + ((*(srcPtrBottomRow + srcLocationColumnFloor)) * (weightedHeight) * (1 - weightedWidth))
+                            + ((*(srcPtrBottomRow + srcLocationColumnFloor + 1)) * (weightedHeight) * (weightedWidth));
+
+                        *dstPtrTemp = (T) pixel;
+                        dstPtrTemp ++;
+                    }
+
+                }
+            }
+            srcPtrTemp += srcSize.height * srcSize.width;
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        Rpp32s elementsInRow = srcSize.width * channel;
+        for (int i = 0; i < dstSize.height; i++)
+        {
+            Rpp32s iNew = i - (srcSize.height / 2);
+            for (int j = 0; j < dstSize.width; j++)
+            {
+                Rpp32s jNew = j - (srcSize.width / 2);
+
+                srcLocationColumn = ((jNew * perspective[0]) + (iNew * perspective[1]) + perspective[2]) / ((jNew * perspective[6]) + (iNew * perspective[7]) + perspective[8]);
+                srcLocationRow = ((jNew * perspective[3]) + (iNew * perspective[4]) + perspective[5]) / ((jNew * perspective[6]) + (iNew * perspective[7]) + perspective[8]);
+
+                srcLocationColumn += (srcSize.width / 2);
+                srcLocationRow += (srcSize.height / 2);
+
+                if (srcLocationRow < 0 || srcLocationColumn < 0 || srcLocationRow > (srcSize.height - 2) || srcLocationColumn > (srcSize.width - 2))
+                {
+                    for (int c = 0; c < channel; c++)
+                    {
+                        *dstPtrTemp = 0;
+                        dstPtrTemp++;
+                    }
+                }
+                else
+                {
+                    Rpp32s srcLocationRowFloor = (Rpp32s) RPPFLOOR(srcLocationRow);
+                    Rpp32s srcLocationColumnFloor = (Rpp32s) RPPFLOOR(srcLocationColumn);
+
+                    Rpp32f weightedHeight = srcLocationRow - srcLocationRowFloor;
+                    Rpp32f weightedWidth = srcLocationColumn - srcLocationColumnFloor;
+
+                    srcPtrTopRow = srcPtrTemp + srcLocationRowFloor * elementsInRow;
+                    srcPtrBottomRow  = srcPtrTopRow + elementsInRow;
+
+                    Rpp32s srcLocColFloorChanneled = channel * srcLocationColumnFloor;
+
+                    for (int c = 0; c < channel; c++)
+                    {
+                        pixel = ((*(srcPtrTopRow + c + srcLocColFloorChanneled)) * (1 - weightedHeight) * (1 - weightedWidth))
+                            + ((*(srcPtrTopRow + c + srcLocColFloorChanneled + channel)) * (1 - weightedHeight) * (weightedWidth))
+                            + ((*(srcPtrBottomRow + c + srcLocColFloorChanneled)) * (weightedHeight) * (1 - weightedWidth))
+                            + ((*(srcPtrBottomRow + c + srcLocColFloorChanneled + channel)) * (weightedHeight) * (weightedWidth));
+
+                        *dstPtrTemp = (T) pixel;
+                        dstPtrTemp ++;
+                    }
+                }
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+#endif
diff --git a/src/modules/cpu/loongarch_image_augmentations.hpp b/src/modules/cpu/loongarch_image_augmentations.hpp
new file mode 100644
index 00000000..e381de75
--- /dev/null
+++ b/src/modules/cpu/loongarch_image_augmentations.hpp
@@ -0,0 +1,5335 @@
+/*
+MIT License
+
+Copyright (c) 2019 - 2024 Advanced Micro Devices, Inc.
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
+*/
+
+#ifndef HOST_IMAGE_AUGMENTATIONS_HPP
+#define HOST_IMAGE_AUGMENTATIONS_HPP
+
+#include <stdlib.h>
+#include <time.h>
+
+#include "rpp_loongarch_simd.hpp"
+#include "rpp_loongarch_common.hpp"
+
+/************ brightness ************/
+
+template <typename T>
+RppStatus brightness_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                                Rpp32f *batch_alpha, Rpp32f *batch_beta,
+                                RppiROI *roiPoints, Rpp32u nbatchSize,
+                                RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32u x1 = roiPoints[batchCount].x;
+            Rpp32u y1 = roiPoints[batchCount].y;
+            Rpp32u x2 = x1 + roiPoints[batchCount].roiWidth;
+            Rpp32u y2 = y1 + roiPoints[batchCount].roiHeight;
+            if (x2 == 0)
+            {
+                x2 = batch_srcSize[batchCount].width;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == 0)
+            {
+                y2 = batch_srcSize[batchCount].height;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp32u remainingElementsAfterROI = (batch_srcSize[batchCount].width - (roiPoints[batchCount].x + roiPoints[batchCount].roiWidth));
+
+            Rpp32f alpha = batch_alpha[batchCount];
+            Rpp32f beta = batch_beta[batchCount];
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            for(int c = 0; c < channel; c++)
+            {
+                T *srcPtrChannel, *dstPtrChannel;
+                srcPtrChannel = srcPtrImage + (c * imageDimMax);
+                dstPtrChannel = dstPtrImage + (c * imageDimMax);
+
+
+                for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+                {
+                    T *srcPtrTemp, *dstPtrTemp;
+                    srcPtrTemp = srcPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+                    dstPtrTemp = dstPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+
+                    if (!((y1 <= i) && (i <= y2)))
+                    {
+                        memcpy(dstPtrTemp, srcPtrTemp, batch_srcSize[batchCount].width * sizeof(T));
+
+                        dstPtrTemp += batch_srcSizeMax[batchCount].width;
+                        srcPtrTemp += batch_srcSizeMax[batchCount].width;
+                    }
+                    else
+                    {
+                        memcpy(dstPtrTemp, srcPtrTemp, x1 * sizeof(T));
+                        srcPtrTemp += x1;
+                        dstPtrTemp += x1;
+
+                        Rpp32u bufferLength = roiPoints[batchCount].roiWidth;
+                        Rpp32u alignedLength = bufferLength & ~15;
+
+                        __m128i const zero = __lsx_vldi(0);
+                        __m128 pMul = lsx_set1_f32(alpha);
+                        __m128 pAdd = lsx_set1_f32(beta);
+                        __m128 p0, p1, p2, p3;
+                        __m128i px0, px1, px2, px3;
+
+                        int vectorLoopCount = 0;
+                        for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+                        {
+                            px0 =  __lsx_vld((__m128i *)srcPtrTemp, 0);
+
+                            px1 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+                            px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                            p0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+                            p1 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px0));    // pixels 4-7
+                            p2 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 8-11
+                            p3 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px1));    // pixels 12-15
+
+                            p0 = __lsx_vfmul_s(p0, pMul);
+                            p1 = __lsx_vfmul_s(p1, pMul);
+                            p2 = __lsx_vfmul_s(p2, pMul);
+                            p3 = __lsx_vfmul_s(p3, pMul);
+                            px0 = __lsx_vftint_w_s(__lsx_vfadd_s(p0, pAdd));
+                            px1 = __lsx_vftint_w_s(__lsx_vfadd_s(p1, pAdd));
+                            px2 = __lsx_vftint_w_s(__lsx_vfadd_s(p2, pAdd));
+                            px3 = __lsx_vftint_w_s(__lsx_vfadd_s(p3, pAdd));
+
+                            px0 = lsx_packus_i32(px0, px1);
+                            px1 = lsx_packus_i32(px2, px3);
+                            px0 = lsx_packus_i16(px0, px1);    // pixels 0-15
+
+                            __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+
+                            srcPtrTemp +=16;
+                            dstPtrTemp +=16;
+                        }
+                        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                        {
+                            *dstPtrTemp++ = (T) RPPPIXELCHECK((((Rpp32f) (*srcPtrTemp++)) * alpha) + beta);
+                        }
+
+                        memcpy(dstPtrTemp, srcPtrTemp, remainingElementsAfterROI * sizeof(T));
+                        srcPtrTemp += remainingElementsAfterROI;
+                        dstPtrTemp += remainingElementsAfterROI;
+                    }
+                }
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32u x1 = roiPoints[batchCount].x;
+            Rpp32u y1 = roiPoints[batchCount].y;
+            Rpp32u x2 = x1 + roiPoints[batchCount].roiWidth;
+            Rpp32u y2 = y1 + roiPoints[batchCount].roiHeight;
+            if (x2 == 0)
+            {
+                x2 = batch_srcSize[batchCount].width;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == 0)
+            {
+                y2 = batch_srcSize[batchCount].height;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp32u elementsBeforeROI = channel * x1;
+            Rpp32u remainingElementsAfterROI = channel * (batch_srcSize[batchCount].width - (roiPoints[batchCount].x + roiPoints[batchCount].roiWidth));
+
+            Rpp32f alpha = batch_alpha[batchCount];
+            Rpp32f beta = batch_beta[batchCount];
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            Rpp32u elementsInRow = channel * batch_srcSize[batchCount].width;
+            Rpp32u elementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+
+
+            for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+            {
+                T *srcPtrTemp, *dstPtrTemp;
+                srcPtrTemp = srcPtrImage + (i * elementsInRowMax);
+                dstPtrTemp = dstPtrImage + (i * elementsInRowMax);
+
+                if (!((y1 <= i) && (i <= y2)))
+                {
+                    memcpy(dstPtrTemp, srcPtrTemp, elementsInRow * sizeof(T));
+
+                    dstPtrTemp += elementsInRowMax;
+                    srcPtrTemp += elementsInRowMax;
+                }
+                else
+                {
+                    memcpy(dstPtrTemp, srcPtrTemp, elementsBeforeROI * sizeof(T));
+                    srcPtrTemp += elementsBeforeROI;
+                    dstPtrTemp += elementsBeforeROI;
+
+                    Rpp32u bufferLength = channel * roiPoints[batchCount].roiWidth;
+                    Rpp32u alignedLength = bufferLength & ~15;
+
+                    __m128i const zero = __lsx_vldi(0);
+                    __m128 pMul = lsx_set1_f32(alpha);
+                    __m128 pAdd = lsx_set1_f32(beta);
+                    __m128 p0, p1, p2, p3;
+                    __m128i px0, px1, px2, px3;
+
+                    int vectorLoopCount = 0;
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+                    {
+                        px0 =  __lsx_vld((__m128i *)srcPtrTemp, 0);
+
+                        px1 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+                        px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                        p0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+                        p1 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px0));    // pixels 4-7
+                        p2 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 8-11
+                        p3 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px1));    // pixels 12-15
+
+                        p0 = __lsx_vfmul_s(p0, pMul);
+                        p1 = __lsx_vfmul_s(p1, pMul);
+                        p2 = __lsx_vfmul_s(p2, pMul);
+                        p3 = __lsx_vfmul_s(p3, pMul);
+                        px0 = __lsx_vftint_w_s(__lsx_vfadd_s(p0, pAdd));
+                        px1 = __lsx_vftint_w_s(__lsx_vfadd_s(p1, pAdd));
+                        px2 = __lsx_vftint_w_s(__lsx_vfadd_s(p2, pAdd));
+                        px3 = __lsx_vftint_w_s(__lsx_vfadd_s(p3, pAdd));
+
+                        px0 = lsx_packus_i32(px0, px1);
+                        px1 = lsx_packus_i32(px2, px3);
+                        px0 = lsx_packus_i16(px0, px1);    // pixels 0-15
+
+                        __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+
+                        srcPtrTemp +=16;
+                        dstPtrTemp +=16;
+                    }
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                    {
+                        *dstPtrTemp++ = (T) RPPPIXELCHECK((((Rpp32f) (*srcPtrTemp++)) * alpha) + beta);
+                    }
+
+                    memcpy(dstPtrTemp, srcPtrTemp, remainingElementsAfterROI * sizeof(T));
+                    srcPtrTemp += remainingElementsAfterROI;
+                    dstPtrTemp += remainingElementsAfterROI;
+                }
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus brightness_host(Rpp8u* srcPtr, RppiSize srcSize, Rpp8u* dstPtr,
+                          Rpp32f alpha, Rpp32f beta,
+                          RppiChnFormat chnFormat, Rpp32u channel)
+{
+    T *srcPtrTemp, *dstPtrTemp;
+
+    srcPtrTemp = srcPtr;
+    dstPtrTemp = dstPtr;
+
+    int bufferLength = channel * srcSize.height * srcSize.width;
+    int alignedLength = bufferLength & ~15;
+
+    __m128i const zero = __lsx_vldi(0);
+    __m128 pMul = lsx_set1_f32(alpha), pAdd = lsx_set1_f32(beta);
+    __m128 p0, p1, p2, p3;
+    __m128i px0, px1, px2, px3;
+
+    int vectorLoopCount = 0;
+    for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+    {
+        px0 =  __lsx_vld((__m128i *)srcPtrTemp, 0);
+
+        px1 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+        px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+        p0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+        p1 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px0));    // pixels 4-7
+        p2 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 8-11
+        p3 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px1));    // pixels 12-15
+
+        p0 = __lsx_vfmul_s(p0, pMul);
+        p1 = __lsx_vfmul_s(p1, pMul);
+        p2 = __lsx_vfmul_s(p2, pMul);
+        p3 = __lsx_vfmul_s(p3, pMul);
+        px0 = __lsx_vftint_w_s(__lsx_vfadd_s(p0, pAdd));
+        px1 = __lsx_vftint_w_s(__lsx_vfadd_s(p1, pAdd));
+        px2 = __lsx_vftint_w_s(__lsx_vfadd_s(p2, pAdd));
+        px3 = __lsx_vftint_w_s(__lsx_vfadd_s(p3, pAdd));
+
+        px0 = lsx_packus_i32(px0, px1);
+        px1 = lsx_packus_i32(px2, px3);
+        px0 = lsx_packus_i16(px0, px1);    // pixels 0-15
+
+        __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+
+        srcPtrTemp +=16;
+        dstPtrTemp +=16;
+    }
+    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+    {
+        *dstPtrTemp++ = (T) RPPPIXELCHECK((((Rpp32f) (*srcPtrTemp++)) * alpha) + beta);
+    }
+
+    return RPP_SUCCESS;
+}
+
+// /**************** contrast ***************/
+
+template <typename T>
+RppStatus contrast_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                              Rpp32u *batch_new_min, Rpp32u *batch_new_max,
+                              RppiROI *roiPoints, Rpp32u nbatchSize,
+                              RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32u x1 = roiPoints[batchCount].x;
+            Rpp32u y1 = roiPoints[batchCount].y;
+            Rpp32u x2 = x1 + roiPoints[batchCount].roiWidth - 1;
+            Rpp32u y2 = y1 + roiPoints[batchCount].roiHeight - 1;
+            if (x2 == -1)
+            {
+                x2 = batch_srcSize[batchCount].width - 1;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == -1)
+            {
+                y2 = batch_srcSize[batchCount].height - 1;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp32u remainingElementsAfterROI = (batch_srcSize[batchCount].width - (roiPoints[batchCount].x + roiPoints[batchCount].roiWidth));
+
+            Rpp32f new_min = batch_new_min[batchCount];
+            Rpp32f new_max = batch_new_max[batchCount];
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            T min[3] = {255, 255, 255};
+            T max[3] = {0, 0, 0};
+
+            if (channel == 1)
+            {
+                compute_1_channel_minmax_host(srcPtrImage, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], &min[0], &max[0], chnFormat, channel);
+            }
+            else if (channel == 3)
+            {
+                compute_3_channel_minmax_host(srcPtrImage, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], min, max, chnFormat, channel);
+            }
+
+            for(int c = 0; c < channel; c++)
+            {
+                T *srcPtrChannel, *dstPtrChannel;
+                srcPtrChannel = srcPtrImage + (c * imageDimMax);
+                dstPtrChannel = dstPtrImage + (c * imageDimMax);
+
+                Rpp32f contrastFactor = (Rpp32f) (new_max - new_min) / (max[c] - min[c]);
+
+
+                for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+                {
+                    T *srcPtrTemp, *dstPtrTemp;
+                    srcPtrTemp = srcPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+                    dstPtrTemp = dstPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+
+                    if (!((y1 <= i) && (i <= y2)))
+                    {
+                        memcpy(dstPtrTemp, srcPtrTemp, batch_srcSize[batchCount].width * sizeof(T));
+
+                        dstPtrTemp += batch_srcSizeMax[batchCount].width;
+                        srcPtrTemp += batch_srcSizeMax[batchCount].width;
+                    }
+                    else
+                    {
+                        memcpy(dstPtrTemp, srcPtrTemp, x1 * sizeof(T));
+                        srcPtrTemp += x1;
+                        dstPtrTemp += x1;
+
+                        Rpp32u bufferLength = roiPoints[batchCount].roiWidth;
+                        Rpp32u alignedLength = (bufferLength / 16) * 16;
+
+                        __m128i const zero = __lsx_vldi(0);
+                        __m128 pContrastFactor = lsx_set1_f32(contrastFactor);
+                        __m128i pMin = __lsx_vreplgr2vr_h(min[c]);
+                        __m128i pNewMin = __lsx_vreplgr2vr_h(new_min);
+                        __m128 p0, p1, p2, p3;
+                        __m128i px0, px1;
+
+                        int vectorLoopCount = 0;
+                        for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+                        {
+                            px0 =  __lsx_vld((__m128i *)srcPtrTemp, 0);
+
+                            px1 = __lsx_vmax_h(__lsx_vsub_h(__lsx_vilvh_b(zero, px0), pMin), zero);    // pixels 8-15
+                            px0 = __lsx_vmax_h(__lsx_vsub_h(__lsx_vilvl_b(zero, px0), pMin), zero);    // pixels 0-7
+                            p0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+                            p1 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px0));    // pixels 4-7
+                            p2 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 8-11
+                            p3 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px1));    // pixels 12-15
+
+                            p0 = __lsx_vfmul_s(p0, pContrastFactor);
+                            p1 = __lsx_vfmul_s(p1, pContrastFactor);
+                            p2 = __lsx_vfmul_s(p2, pContrastFactor);
+                            p3 = __lsx_vfmul_s(p3, pContrastFactor);
+
+                            px0 = lsx_packus_i32(__lsx_vftint_w_s(p0), __lsx_vftint_w_s(p1));
+                            px1 = lsx_packus_i32(__lsx_vftint_w_s(p2), __lsx_vftint_w_s(p3));
+
+                            px0 = __lsx_vadd_h(px0, pNewMin);
+                            px1 = __lsx_vadd_h(px1, pNewMin);
+
+                            __lsx_vst(lsx_packus_i16(px0,px1), (__m128i *)dstPtrTemp, 0);
+
+                            srcPtrTemp += 16;
+                            dstPtrTemp += 16;
+                        }
+                        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                        {
+                            *dstPtrTemp++ = (T) (((Rpp32f)(*srcPtrTemp++ - min[c]) * contrastFactor) + new_min);
+                        }
+
+                        memcpy(dstPtrTemp, srcPtrTemp, remainingElementsAfterROI * sizeof(T));
+                        srcPtrTemp += remainingElementsAfterROI;
+                        dstPtrTemp += remainingElementsAfterROI;
+                    }
+                }
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32u x1 = roiPoints[batchCount].x;
+            Rpp32u y1 = roiPoints[batchCount].y;
+            Rpp32u x2 = x1 + roiPoints[batchCount].roiWidth - 1;
+            Rpp32u y2 = y1 + roiPoints[batchCount].roiHeight - 1;
+            if (x2 == -1)
+            {
+                x2 = batch_srcSize[batchCount].width - 1;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == -1)
+            {
+                y2 = batch_srcSize[batchCount].height - 1;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp32u elementsBeforeROI = channel * x1;
+            Rpp32u remainingElementsAfterROI = channel * (batch_srcSize[batchCount].width - (roiPoints[batchCount].x + roiPoints[batchCount].roiWidth));
+
+            Rpp32f new_min = batch_new_min[batchCount];
+            Rpp32f new_max = batch_new_max[batchCount];
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            T min[3] = {255, 255, 255};
+            T max[3] = {0, 0, 0};
+
+            Rpp32u elementsInRow = channel * batch_srcSize[batchCount].width;
+            Rpp32u elementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+
+            compute_3_channel_minmax_host(srcPtrImage, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], min, max, chnFormat, channel);
+
+            Rpp32f contrastFactorR = (new_max - new_min) / ((Rpp32f) (max[0] - min[0]));
+            Rpp32f contrastFactorG = (new_max - new_min) / ((Rpp32f) (max[1] - min[1]));
+            Rpp32f contrastFactorB = (new_max - new_min) / ((Rpp32f) (max[2] - min[2]));
+
+
+            for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+            {
+                Rpp32f pixel;
+
+                T *srcPtrTemp, *dstPtrTemp;
+                srcPtrTemp = srcPtrImage + (i * elementsInRowMax);
+                dstPtrTemp = dstPtrImage + (i * elementsInRowMax);
+
+                if (!((y1 <= i) && (i <= y2)))
+                {
+                    memcpy(dstPtrTemp, srcPtrTemp, elementsInRow * sizeof(T));
+
+                    dstPtrTemp += elementsInRowMax;
+                    srcPtrTemp += elementsInRowMax;
+                }
+                else
+                {
+                    memcpy(dstPtrTemp, srcPtrTemp, elementsBeforeROI * sizeof(T));
+                    srcPtrTemp += elementsBeforeROI;
+                    dstPtrTemp += elementsBeforeROI;
+
+                    Rpp32u bufferLength = channel * roiPoints[batchCount].roiWidth;
+                    Rpp32u alignedLength = ((bufferLength / 15) * 15) - 1;
+
+                    __m128i const zero = __lsx_vldi(0);
+                    __m128 pzero = lsx_set1_f32(0.0);
+                    __m128 pContrastFactor0 = lsx_setr_f32(contrastFactorR, contrastFactorG, contrastFactorB, contrastFactorR);
+                    __m128 pContrastFactor1 = lsx_setr_f32(contrastFactorG, contrastFactorB, contrastFactorR, contrastFactorG);
+                    __m128 pContrastFactor2 = lsx_setr_f32(contrastFactorB, contrastFactorR, contrastFactorG, contrastFactorB);
+
+                    __m128 pMin0 = lsx_setr_f32((Rpp32f) min[0], (Rpp32f) min[1], (Rpp32f) min[2], (Rpp32f) min[0]);
+                    __m128 pMin1 = lsx_setr_f32((Rpp32f) min[1], (Rpp32f) min[2], (Rpp32f) min[0], (Rpp32f) min[1]);
+                    __m128 pMin2 = lsx_setr_f32((Rpp32f) min[2], (Rpp32f) min[0], (Rpp32f) min[1], (Rpp32f) min[2]);
+                    __m128 pNewMin = lsx_set1_f32(new_min);
+                    __m128 p0, p1, p2, p3;
+                    __m128i px0, px1;
+
+                    int vectorLoopCount = 0;
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount+=15)
+                    {
+                        px0 =  __lsx_vld((__m128i *)srcPtrTemp, 0);
+
+                        px1 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+                        px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                        p0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+                        p1 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px0));    // pixels 4-7
+                        p2 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 8-11
+                        p3 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px1));    // pixels 12-15
+
+                        p0 = __lsx_vfmax_s(__lsx_vfsub_s(p0, pMin0), pzero);
+                        p1 = __lsx_vfmax_s(__lsx_vfsub_s(p1, pMin1), pzero);
+                        p2 = __lsx_vfmax_s(__lsx_vfsub_s(p2, pMin2), pzero);
+                        p3 = __lsx_vfmax_s(__lsx_vfsub_s(p3, pMin0), pzero);
+
+                        p0 = __lsx_vfmul_s(p0, pContrastFactor0);
+                        p1 = __lsx_vfmul_s(p1, pContrastFactor1);
+                        p2 = __lsx_vfmul_s(p2, pContrastFactor2);
+                        p3 = __lsx_vfmul_s(p3, pContrastFactor0);
+
+                        p0 = __lsx_vfadd_s(p0, pNewMin);
+                        p1 = __lsx_vfadd_s(p1, pNewMin);
+                        p2 = __lsx_vfadd_s(p2, pNewMin);
+                        p3 = __lsx_vfadd_s(p3, pNewMin);
+
+                        px0 = lsx_packus_i32(__lsx_vftint_w_s(p0), __lsx_vftint_w_s(p1));
+                        px1 = lsx_packus_i32(__lsx_vftint_w_s(p2), __lsx_vftint_w_s(p3));
+
+                        __lsx_vst(lsx_packus_i16(px0,px1), (__m128i *)dstPtrTemp, 0);
+
+                        srcPtrTemp += 15;
+                        dstPtrTemp += 15;
+                    }
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount+=channel)
+                    {
+                        *dstPtrTemp++ = (T) (((Rpp32f)(*srcPtrTemp++ - min[0]) * contrastFactorR) + new_min);
+                        *dstPtrTemp++ = (T) (((Rpp32f)(*srcPtrTemp++ - min[1]) * contrastFactorG) + new_min);
+                        *dstPtrTemp++ = (T) (((Rpp32f)(*srcPtrTemp++ - min[2]) * contrastFactorB) + new_min);
+                    }
+
+                    memcpy(dstPtrTemp, srcPtrTemp, remainingElementsAfterROI * sizeof(T));
+                    srcPtrTemp += remainingElementsAfterROI;
+                    dstPtrTemp += remainingElementsAfterROI;
+                }
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus contrast_host(T* srcPtr, RppiSize srcSize, T* dstPtr,
+                        Rpp32u new_min, Rpp32u new_max,
+                        RppiChnFormat chnFormat, Rpp32u channel)
+{
+    T *srcPtrTemp, *dstPtrTemp;
+    srcPtrTemp = srcPtr;
+    dstPtrTemp = dstPtr;
+
+    T min[3] = {255, 255, 255};
+    T max[3] = {0, 0, 0};
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        if (channel == 1)
+        {
+            compute_1_channel_minmax_host(srcPtr, srcSize, srcSize, &min[0], &max[0], chnFormat, channel);
+        }
+        else if (channel == 3)
+        {
+            compute_3_channel_minmax_host(srcPtr, srcSize, srcSize, min, max, chnFormat, channel);
+        }
+
+        for (int c = 0; c < channel; c++)
+        {
+            Rpp32f contrastFactor = (Rpp32f) (new_max - new_min) / (max[c] - min[c]);
+
+            int bufferLength = srcSize.height * srcSize.width;
+            int alignedLength = (bufferLength / 16) * 16;
+
+            __m128i const zero = __lsx_vldi(0);
+            __m128 pContrastFactor = lsx_set1_f32(contrastFactor);
+            __m128i pMin = __lsx_vreplgr2vr_h(min[c]);
+            __m128i pNewMin = __lsx_vreplgr2vr_h(new_min);
+            __m128 p0, p1, p2, p3;
+            __m128i px0, px1;
+
+            int vectorLoopCount = 0;
+            for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+            {
+                px0 =  __lsx_vld((__m128i *)srcPtrTemp, 0);
+
+                px1 = __lsx_vmax_h(__lsx_vsub_h(__lsx_vilvh_b(zero, px0), pMin), zero);    // pixels 8-15
+                px0 = __lsx_vmax_h(__lsx_vsub_h(__lsx_vilvl_b(zero, px0), pMin), zero);    // pixels 0-7
+                p0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+                p1 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px0));    // pixels 4-7
+                p2 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 8-11
+                p3 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px1));    // pixels 12-15
+
+                p0 = __lsx_vfmul_s(p0, pContrastFactor);
+                p1 = __lsx_vfmul_s(p1, pContrastFactor);
+                p2 = __lsx_vfmul_s(p2, pContrastFactor);
+                p3 = __lsx_vfmul_s(p3, pContrastFactor);
+
+                px0 = lsx_packus_i32(__lsx_vftint_w_s(p0), __lsx_vftint_w_s(p1));
+                px1 = lsx_packus_i32(__lsx_vftint_w_s(p2), __lsx_vftint_w_s(p3));
+
+                px0 = __lsx_vadd_h(px0, pNewMin);
+                px1 = __lsx_vadd_h(px1, pNewMin);
+
+                __lsx_vst(lsx_packus_i16(px0,px1), (__m128i *)dstPtrTemp, 0);
+
+                srcPtrTemp += 16;
+                dstPtrTemp += 16;
+            }
+            for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+            {
+                *dstPtrTemp++ = (T) (((Rpp32f)(*srcPtrTemp++ - min[c]) * contrastFactor) + new_min);
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        compute_3_channel_minmax_host(srcPtr, srcSize, srcSize, min, max, chnFormat, channel);
+
+        Rpp32f contrastFactorR = (Rpp32f) (new_max - new_min) / (max[0] - min[0]);
+        Rpp32f contrastFactorG = (Rpp32f) (new_max - new_min) / (max[1] - min[1]);
+        Rpp32f contrastFactorB = (Rpp32f) (new_max - new_min) / (max[2] - min[2]);
+
+        int bufferLength = channel * srcSize.height * srcSize.width;
+        int alignedLength = ((bufferLength / 15) * 15) - 1;
+
+        __m128i const zero = __lsx_vldi(0);
+        __m128 pzero = lsx_set1_f32(0.0);
+        __m128 pContrastFactor0 = lsx_setr_f32(contrastFactorR, contrastFactorG, contrastFactorB, contrastFactorR);
+        __m128 pContrastFactor1 = lsx_setr_f32(contrastFactorG, contrastFactorB, contrastFactorR, contrastFactorG);
+        __m128 pContrastFactor2 = lsx_setr_f32(contrastFactorB, contrastFactorR, contrastFactorG, contrastFactorB);
+
+        __m128 pMin0 = lsx_setr_f32((Rpp32f) min[0], (Rpp32f) min[1], (Rpp32f) min[2], (Rpp32f) min[0]);
+        __m128 pMin1 = lsx_setr_f32((Rpp32f) min[1], (Rpp32f) min[2], (Rpp32f) min[0], (Rpp32f) min[1]);
+        __m128 pMin2 = lsx_setr_f32((Rpp32f) min[2], (Rpp32f) min[0], (Rpp32f) min[1], (Rpp32f) min[2]);
+        __m128 pNewMin = lsx_set1_f32(new_min);
+        __m128 p0, p1, p2, p3;
+        __m128i px0, px1;
+
+        int vectorLoopCount = 0;
+        for (; vectorLoopCount < alignedLength; vectorLoopCount+=15)
+        {
+            px0 =  __lsx_vld((__m128i *)srcPtrTemp, 0);
+
+            px1 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+            px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+            p0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+            p1 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px0));    // pixels 4-7
+            p2 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 8-11
+            p3 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px1));    // pixels 12-15
+
+            p0 = __lsx_vfmax_s(__lsx_vfsub_s(p0, pMin0), pzero);
+            p1 = __lsx_vfmax_s(__lsx_vfsub_s(p1, pMin1), pzero);
+            p2 = __lsx_vfmax_s(__lsx_vfsub_s(p2, pMin2), pzero);
+            p3 = __lsx_vfmax_s(__lsx_vfsub_s(p3, pMin0), pzero);
+
+            p0 = __lsx_vfmul_s(p0, pContrastFactor0);
+            p1 = __lsx_vfmul_s(p1, pContrastFactor1);
+            p2 = __lsx_vfmul_s(p2, pContrastFactor2);
+            p3 = __lsx_vfmul_s(p3, pContrastFactor0);
+
+            p0 = __lsx_vfadd_s(p0, pNewMin);
+            p1 = __lsx_vfadd_s(p1, pNewMin);
+            p2 = __lsx_vfadd_s(p2, pNewMin);
+            p3 = __lsx_vfadd_s(p3, pNewMin);
+
+            px0 = lsx_packus_i32(__lsx_vftint_w_s(p0), __lsx_vftint_w_s(p1));
+            px1 = lsx_packus_i32(__lsx_vftint_w_s(p2), __lsx_vftint_w_s(p3));
+
+            __lsx_vst(lsx_packus_i16(px0,px1), (__m128i *)dstPtrTemp, 0);
+
+            srcPtrTemp += 15;
+            dstPtrTemp += 15;
+        }
+        for (; vectorLoopCount < bufferLength; vectorLoopCount+=channel)
+        {
+            *dstPtrTemp++ = (T) (((Rpp32f)(*srcPtrTemp++ - min[0]) * contrastFactorR) + new_min);
+            *dstPtrTemp++ = (T) (((Rpp32f)(*srcPtrTemp++ - min[1]) * contrastFactorG) + new_min);
+            *dstPtrTemp++ = (T) (((Rpp32f)(*srcPtrTemp++ - min[2]) * contrastFactorB) + new_min);
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+/************ blend ************/
+
+template <typename T>
+RppStatus blend_host_batch(T* srcPtr1, T* srcPtr2, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                           Rpp32f *batch_alpha,
+                           RppiROI *roiPoints, Rpp32u nbatchSize,
+                           RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32u x1 = roiPoints[batchCount].x;
+            Rpp32u y1 = roiPoints[batchCount].y;
+            Rpp32u x2 = x1 + roiPoints[batchCount].roiWidth;
+            Rpp32u y2 = y1 + roiPoints[batchCount].roiHeight;
+            if (x2 == 0)
+            {
+                x2 = batch_srcSize[batchCount].width;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == 0)
+            {
+                y2 = batch_srcSize[batchCount].height;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp32u remainingElementsAfterROI = (batch_srcSize[batchCount].width - (roiPoints[batchCount].x + roiPoints[batchCount].roiWidth));
+
+            Rpp32f alpha = batch_alpha[batchCount];
+
+            T *srcPtr1Image, *srcPtr2Image, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtr1Image = srcPtr1 + loc;
+            srcPtr2Image = srcPtr2 + loc;
+            dstPtrImage = dstPtr + loc;
+
+            for(int c = 0; c < channel; c++)
+            {
+                T *srcPtr1Channel, *srcPtr2Channel, *dstPtrChannel;
+                srcPtr1Channel = srcPtr1Image + (c * imageDimMax);
+                srcPtr2Channel = srcPtr2Image + (c * imageDimMax);
+                dstPtrChannel = dstPtrImage + (c * imageDimMax);
+
+
+                for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+                {
+                    Rpp32f pixel;
+
+                    T *srcPtr1Temp, *srcPtr2Temp, *dstPtrTemp;
+                    srcPtr1Temp = srcPtr1Channel + (i * batch_srcSizeMax[batchCount].width);
+                    srcPtr2Temp = srcPtr2Channel + (i * batch_srcSizeMax[batchCount].width);
+                    dstPtrTemp = dstPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+
+                    if (!((y1 <= i) && (i <= y2)))
+                    {
+                        memcpy(dstPtrTemp, srcPtr1Temp, batch_srcSize[batchCount].width * sizeof(T));
+
+                        srcPtr1Temp += batch_srcSizeMax[batchCount].width;
+                        srcPtr2Temp += batch_srcSizeMax[batchCount].width;
+                        dstPtrTemp += batch_srcSizeMax[batchCount].width;
+                    }
+                    else
+                    {
+                        memcpy(dstPtrTemp, srcPtr1Temp, x1 * sizeof(T));
+                        srcPtr1Temp += x1;
+                        srcPtr2Temp += x1;
+                        dstPtrTemp += x1;
+
+                        int bufferLength = roiPoints[batchCount].roiWidth;
+                        int alignedLength = bufferLength & ~15;
+
+                        __m128i const zero = __lsx_vldi(0);
+                        __m128 pMul = lsx_set1_f32(alpha);
+                        __m128 p0, p1, p2, p3;
+                        __m128 q0, q1, q2, q3;
+                        __m128i px0, px1, px2, px3;
+
+                        int vectorLoopCount = 0;
+                        for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+                        {
+                            px0 =  __lsx_vld((__m128i *)srcPtr2Temp, 0);
+
+                            px1 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+                            px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                            q0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+                            q1 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px0));    // pixels 4-7
+                            q2 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 8-11
+                            q3 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px1));    // pixels 12-15
+
+                            px0 =  __lsx_vld((__m128i *)srcPtr1Temp, 0);
+
+                            px1 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+                            px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                            p0 = __lsx_vfsub_s(__lsx_vffint_s_w(__lsx_vilvl_h(zero, px0)), q0);    // pixels 0-3
+                            p1 = __lsx_vfsub_s(__lsx_vffint_s_w(__lsx_vilvh_h(zero, px0)), q1);    // pixels 4-7
+                            p2 = __lsx_vfsub_s(__lsx_vffint_s_w(__lsx_vilvl_h(zero, px1)), q2);    // pixels 8-11
+                            p3 = __lsx_vfsub_s(__lsx_vffint_s_w(__lsx_vilvh_h(zero, px1)), q3);    // pixels 12-15
+
+                            p0 = __lsx_vfmul_s(p0, pMul);
+                            p1 = __lsx_vfmul_s(p1, pMul);
+                            p2 = __lsx_vfmul_s(p2, pMul);
+                            p3 = __lsx_vfmul_s(p3, pMul);
+                            px0 = __lsx_vftint_w_s(__lsx_vfadd_s(p0, q0));
+                            px1 = __lsx_vftint_w_s(__lsx_vfadd_s(p1, q1));
+                            px2 = __lsx_vftint_w_s(__lsx_vfadd_s(p2, q2));
+                            px3 = __lsx_vftint_w_s(__lsx_vfadd_s(p3, q3));
+
+                            px0 = lsx_packus_i32(px0, px1);
+                            px1 = lsx_packus_i32(px2, px3);
+                            px0 = lsx_packus_i16(px0, px1);    // pixels 0-15
+
+                            __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+
+                            srcPtr1Temp +=16;
+                            srcPtr2Temp +=16;
+                            dstPtrTemp +=16;
+                        }
+                        for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                        {
+                            *dstPtrTemp = (T) RPPPIXELCHECK((alpha * (*srcPtr1Temp - *srcPtr2Temp)) + *srcPtr2Temp);
+                            dstPtrTemp++;
+                            srcPtr2Temp++;
+                            srcPtr1Temp++;
+                        }
+
+                        memcpy(dstPtrTemp, srcPtr1Temp, remainingElementsAfterROI * sizeof(T));
+                        srcPtr1Temp += remainingElementsAfterROI;
+                        srcPtr2Temp += remainingElementsAfterROI;
+                        dstPtrTemp += remainingElementsAfterROI;
+                    }
+                }
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32u x1 = roiPoints[batchCount].x;
+            Rpp32u y1 = roiPoints[batchCount].y;
+            Rpp32u x2 = x1 + roiPoints[batchCount].roiWidth;
+            Rpp32u y2 = y1 + roiPoints[batchCount].roiHeight;
+            if (x2 == 0)
+            {
+                x2 = batch_srcSize[batchCount].width;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == 0)
+            {
+                y2 = batch_srcSize[batchCount].height;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp32u elementsBeforeROI = channel * x1;
+            Rpp32u remainingElementsAfterROI = channel * (batch_srcSize[batchCount].width - (roiPoints[batchCount].x + roiPoints[batchCount].roiWidth));
+
+            Rpp32f alpha = batch_alpha[batchCount];
+
+            T *srcPtr1Image, *srcPtr2Image, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtr1Image = srcPtr1 + loc;
+            srcPtr2Image = srcPtr2 + loc;
+            dstPtrImage = dstPtr + loc;
+
+            Rpp32u elementsInRow = channel * batch_srcSize[batchCount].width;
+            Rpp32u elementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+
+
+            for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+            {
+                Rpp32f pixel;
+
+                T *srcPtr1Temp, *srcPtr2Temp, *dstPtrTemp;
+                srcPtr1Temp = srcPtr1Image + (i * elementsInRowMax);
+                srcPtr2Temp = srcPtr2Image + (i * elementsInRowMax);
+                dstPtrTemp = dstPtrImage + (i * elementsInRowMax);
+
+                if (!((y1 <= i) && (i <= y2)))
+                {
+                    memcpy(dstPtrTemp, srcPtr1Temp, elementsInRow * sizeof(T));
+
+                    srcPtr1Temp += elementsInRowMax;
+                    srcPtr2Temp += elementsInRowMax;
+                    dstPtrTemp += elementsInRowMax;
+                }
+                else
+                {
+                    memcpy(dstPtrTemp, srcPtr1Temp, elementsBeforeROI * sizeof(T));
+                    srcPtr1Temp += elementsBeforeROI;
+                    srcPtr2Temp += elementsBeforeROI;
+                    dstPtrTemp += elementsBeforeROI;
+
+                    int bufferLength = channel * roiPoints[batchCount].roiWidth;
+                    int alignedLength = bufferLength & ~15;
+
+                    __m128i const zero = __lsx_vldi(0);
+                    __m128 pMul = lsx_set1_f32(alpha);
+                    __m128 p0, p1, p2, p3;
+                    __m128 q0, q1, q2, q3;
+                    __m128i px0, px1, px2, px3;
+
+                    int vectorLoopCount = 0;
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+                    {
+                        px0 =  __lsx_vld((__m128i *)srcPtr2Temp, 0);
+
+                        px1 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+                        px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                        q0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+                        q1 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px0));    // pixels 4-7
+                        q2 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 8-11
+                        q3 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px1));    // pixels 12-15
+
+                        px0 =  __lsx_vld((__m128i *)srcPtr1Temp, 0);
+
+                        px1 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+                        px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                        p0 = __lsx_vfsub_s(__lsx_vffint_s_w(__lsx_vilvl_h(zero, px0)), q0);    // pixels 0-3
+                        p1 = __lsx_vfsub_s(__lsx_vffint_s_w(__lsx_vilvh_h(zero, px0)), q1);    // pixels 4-7
+                        p2 = __lsx_vfsub_s(__lsx_vffint_s_w(__lsx_vilvl_h(zero, px1)), q2);    // pixels 8-11
+                        p3 = __lsx_vfsub_s(__lsx_vffint_s_w(__lsx_vilvh_h(zero, px1)), q3);    // pixels 12-15
+
+                        p0 = __lsx_vfmul_s(p0, pMul);
+                        p1 = __lsx_vfmul_s(p1, pMul);
+                        p2 = __lsx_vfmul_s(p2, pMul);
+                        p3 = __lsx_vfmul_s(p3, pMul);
+                        px0 = __lsx_vftint_w_s(__lsx_vfadd_s(p0, q0));
+                        px1 = __lsx_vftint_w_s(__lsx_vfadd_s(p1, q1));
+                        px2 = __lsx_vftint_w_s(__lsx_vfadd_s(p2, q2));
+                        px3 = __lsx_vftint_w_s(__lsx_vfadd_s(p3, q3));
+
+                        px0 = lsx_packus_i32(px0, px1);
+                        px1 = lsx_packus_i32(px2, px3);
+                        px0 = lsx_packus_i16(px0, px1);    // pixels 0-15
+
+                        __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+
+                        srcPtr1Temp +=16;
+                        srcPtr2Temp +=16;
+                        dstPtrTemp +=16;
+                    }
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                    {
+                        *dstPtrTemp = (T) RPPPIXELCHECK((alpha * (*srcPtr1Temp - *srcPtr2Temp)) + *srcPtr2Temp);
+                        dstPtrTemp++;
+                        srcPtr2Temp++;
+                        srcPtr1Temp++;
+                    }
+
+                    memcpy(dstPtrTemp, srcPtr1Temp, remainingElementsAfterROI * sizeof(T));
+                    srcPtr1Temp += remainingElementsAfterROI;
+                    srcPtr2Temp += remainingElementsAfterROI;
+                    dstPtrTemp += remainingElementsAfterROI;
+                }
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus blend_host(T* srcPtr1, T* srcPtr2, RppiSize srcSize, T* dstPtr,
+                        Rpp32f alpha, RppiChnFormat chnFormat,
+                        unsigned int channel)
+{
+    T *srcPtr1Temp, *srcPtr2Temp, *dstPtrTemp;
+
+    srcPtr1Temp = srcPtr1;
+    srcPtr2Temp = srcPtr2;
+    dstPtrTemp = dstPtr;
+
+    int bufferLength = channel * srcSize.height * srcSize.width;
+    int alignedLength = bufferLength & ~15;
+
+    __m128i const zero = __lsx_vldi(0);
+    __m128 pMul = lsx_set1_f32(alpha);
+    __m128 p0, p1, p2, p3;
+    __m128 q0, q1, q2, q3;
+    __m128i px0, px1, px2, px3;
+
+    int vectorLoopCount = 0;
+    for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+    {
+        px0 =  __lsx_vld((__m128i *)srcPtr1Temp, 0);
+
+        px1 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+        px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+        p0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+        p1 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px0));    // pixels 4-7
+        p2 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 8-11
+        p3 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px1));    // pixels 12-15
+
+        px0 =  __lsx_vld((__m128i *)srcPtr2Temp, 0);
+
+        px1 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+        px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+        q0 = __lsx_vfadd_s(__lsx_vffint_s_w(__lsx_vilvl_h(zero, px0)), p0);    // pixels 0-3
+        q1 = __lsx_vfadd_s(__lsx_vffint_s_w(__lsx_vilvh_h(zero, px0)), p1);    // pixels 4-7
+        q2 = __lsx_vfadd_s(__lsx_vffint_s_w(__lsx_vilvl_h(zero, px1)), p2);    // pixels 8-11
+        q3 = __lsx_vfadd_s(__lsx_vffint_s_w(__lsx_vilvh_h(zero, px1)), p3);    // pixels 12-15
+
+        q0 = __lsx_vfmul_s(q0, pMul);
+        q1 = __lsx_vfmul_s(q1, pMul);
+        q2 = __lsx_vfmul_s(q2, pMul);
+        q3 = __lsx_vfmul_s(q3, pMul);
+        px0 = __lsx_vftint_w_s(__lsx_vfadd_s(p0, q0));
+        px1 = __lsx_vftint_w_s(__lsx_vfadd_s(p1, q1));
+        px2 = __lsx_vftint_w_s(__lsx_vfadd_s(p2, q2));
+        px3 = __lsx_vftint_w_s(__lsx_vfadd_s(p3, q3));
+
+        px0 = lsx_packus_i32(px0, px1);
+        px1 = lsx_packus_i32(px2, px3);
+        px0 = lsx_packus_i16(px0, px1);    // pixels 0-15
+
+        __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+
+        srcPtr1Temp +=16;
+        srcPtr2Temp +=16;
+        dstPtrTemp +=16;
+    }
+    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+    {
+        *dstPtrTemp = (T) RPPPIXELCHECK(*srcPtr1Temp + (alpha * (*srcPtr2Temp - *srcPtr1Temp)));
+        dstPtrTemp++;
+        srcPtr2Temp++;
+        srcPtr1Temp++;
+    }
+
+    return RPP_SUCCESS;
+}
+
+/************ gamma_correction ************/
+
+template <typename T>
+RppStatus gamma_correction_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                                      Rpp32f *batch_gamma,
+                                      RppiROI *roiPoints, Rpp32u nbatchSize,
+                                      RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32f x1 = roiPoints[batchCount].x;
+            Rpp32f y1 = roiPoints[batchCount].y;
+            Rpp32f x2 = x1 + roiPoints[batchCount].roiWidth;
+            Rpp32f y2 = y1 + roiPoints[batchCount].roiHeight;
+            if (x2 == 0)
+            {
+                x2 = batch_srcSize[batchCount].width;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == 0)
+            {
+                y2 = batch_srcSize[batchCount].height;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp32f gamma = batch_gamma[batchCount];
+
+            Rpp8u *gammaLUT = (Rpp8u *)calloc(256, sizeof(Rpp8u));
+
+            for (int i = 0; i < 256; i++)
+            {
+                gammaLUT[i] = (T) RPPPIXELCHECK(pow((((Rpp32f) i) / 255.0), gamma) * 255.0);
+            }
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            for(int c = 0; c < channel; c++)
+            {
+                T *srcPtrChannel, *dstPtrChannel;
+                srcPtrChannel = srcPtrImage + (c * imageDimMax);
+                dstPtrChannel = dstPtrImage + (c * imageDimMax);
+
+
+                for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+                {
+                    T *srcPtrTemp, *dstPtrTemp;
+                    srcPtrTemp = srcPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+                    dstPtrTemp = dstPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+
+                    if (!((y1 <= i) && (i <= y2)))
+                    {
+                        memcpy(dstPtrTemp, srcPtrTemp, batch_srcSize[batchCount].width * sizeof(T));
+
+                        dstPtrTemp += batch_srcSizeMax[batchCount].width;
+                        srcPtrTemp += batch_srcSizeMax[batchCount].width;
+                    }
+                    else
+                    {
+                        for(int j = 0; j < batch_srcSize[batchCount].width; j++)
+                        {
+                            if((x1 <= j) && (j <= x2 ))
+                            {
+                                *dstPtrTemp = gammaLUT[*srcPtrTemp];
+
+                                srcPtrTemp++;
+                                dstPtrTemp++;
+                            }
+                            else
+                            {
+                                *dstPtrTemp = *srcPtrTemp;
+
+                                srcPtrTemp++;
+                                dstPtrTemp++;
+                            }
+                        }
+                    }
+                }
+            }
+            free(gammaLUT);
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32f x1 = roiPoints[batchCount].x;
+            Rpp32f y1 = roiPoints[batchCount].y;
+            Rpp32f x2 = x1 + roiPoints[batchCount].roiWidth;
+            Rpp32f y2 = y1 + roiPoints[batchCount].roiHeight;
+            if (x2 == 0)
+            {
+                x2 = batch_srcSize[batchCount].width;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == 0)
+            {
+                y2 = batch_srcSize[batchCount].height;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp32f gamma = batch_gamma[batchCount];
+
+            Rpp8u *gammaLUT = (Rpp8u *)calloc(256, sizeof(Rpp8u));
+
+            for (int i = 0; i < 256; i++)
+            {
+                gammaLUT[i] = (T) RPPPIXELCHECK(pow((((Rpp32f) i) / 255.0), gamma) * 255.0);
+            }
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            Rpp32u elementsInRow = channel * batch_srcSize[batchCount].width;
+            Rpp32u elementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+
+
+            for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+            {
+                T *srcPtrTemp, *dstPtrTemp;
+                srcPtrTemp = srcPtrImage + (i * elementsInRowMax);
+                dstPtrTemp = dstPtrImage + (i * elementsInRowMax);
+
+                if (!((y1 <= i) && (i <= y2)))
+                {
+                    memcpy(dstPtrTemp, srcPtrTemp, elementsInRow * sizeof(T));
+
+                    dstPtrTemp += elementsInRowMax;
+                    srcPtrTemp += elementsInRowMax;
+                }
+                else
+                {
+                    for(int j = 0; j < batch_srcSize[batchCount].width; j++)
+                    {
+                        if (!((x1 <= j) && (j <= x2 )))
+                        {
+                            memcpy(dstPtrTemp, srcPtrTemp, channel * sizeof(T));
+
+                            dstPtrTemp += channel;
+                            srcPtrTemp += channel;
+                        }
+                        else
+                        {
+                            for(int c = 0; c < channel; c++)
+                            {
+                                *dstPtrTemp = gammaLUT[*srcPtrTemp];
+
+                                srcPtrTemp++;
+                                dstPtrTemp++;
+                            }
+                        }
+                    }
+                }
+            }
+            free(gammaLUT);
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus gamma_correction_host(T* srcPtr, RppiSize srcSize, T* dstPtr,
+                                Rpp32f gamma,
+                                RppiChnFormat chnFormat, Rpp32u channel)
+{
+    Rpp32u imageBuffer = channel * srcSize.height * srcSize.width;
+
+    T *srcPtrTemp, *dstPtrTemp;
+    srcPtrTemp = srcPtr;
+    dstPtrTemp = dstPtr;
+
+    Rpp8u *gammaLUT = (Rpp8u *)calloc(256, sizeof(Rpp8u));
+
+    for (int i = 0; i < 256; i++)
+    {
+        gammaLUT[i] = (T) RPPPIXELCHECK(pow((((Rpp32f) i) / 255.0), gamma) * 255.0);
+    }
+
+    for (int i = 0; i < imageBuffer; i++)
+    {
+        *dstPtrTemp = gammaLUT[*srcPtrTemp];
+        srcPtrTemp++;
+        dstPtrTemp++;
+    }
+
+    free(gammaLUT);
+
+    return RPP_SUCCESS;
+
+}
+
+/************ exposure ************/
+
+template <typename T>
+RppStatus exposure_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                              Rpp32f *batch_exposureFactor,
+                              RppiROI *roiPoints, Rpp32u nbatchSize,
+                              RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32u x1 = roiPoints[batchCount].x;
+            Rpp32u y1 = roiPoints[batchCount].y;
+            Rpp32u x2 = x1 + roiPoints[batchCount].roiWidth;
+            Rpp32u y2 = y1 + roiPoints[batchCount].roiHeight;
+            if (x2 == 0)
+            {
+                x2 = batch_srcSize[batchCount].width;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == 0)
+            {
+                y2 = batch_srcSize[batchCount].height;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp32u remainingElementsAfterROI = (batch_srcSize[batchCount].width - (roiPoints[batchCount].x + roiPoints[batchCount].roiWidth));
+
+            Rpp32f exposureFactor = batch_exposureFactor[batchCount];
+            Rpp32f multiplyingFactor = pow(2, exposureFactor);
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            for(int c = 0; c < channel; c++)
+            {
+                T *srcPtrChannel, *dstPtrChannel;
+                srcPtrChannel = srcPtrImage + (c * imageDimMax);
+                dstPtrChannel = dstPtrImage + (c * imageDimMax);
+
+
+                for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+                {
+                    Rpp32f pixel;
+
+                    T *srcPtrTemp, *dstPtrTemp;
+                    srcPtrTemp = srcPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+                    dstPtrTemp = dstPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+
+                    if (!((y1 <= i) && (i <= y2)))
+                    {
+                        memcpy(dstPtrTemp, srcPtrTemp, batch_srcSize[batchCount].width * sizeof(T));
+
+                        dstPtrTemp += batch_srcSizeMax[batchCount].width;
+                        srcPtrTemp += batch_srcSizeMax[batchCount].width;
+                    }
+                    else
+                    {
+                        memcpy(dstPtrTemp, srcPtrTemp, x1 * sizeof(T));
+                        srcPtrTemp += x1;
+                        dstPtrTemp += x1;
+
+                        Rpp32u bufferLength = roiPoints[batchCount].roiWidth;
+                        Rpp32u alignedLength = bufferLength & ~15;
+
+                        __m128i const zero = __lsx_vldi(0);
+                        __m128 pMul = lsx_set1_f32(multiplyingFactor);
+                        __m128 p0, p1, p2, p3;
+                        __m128i px0, px1, px2, px3;
+
+                        int i = 0;
+                        for (; i < alignedLength; i+=16)
+                        {
+                            px0 =  __lsx_vld((__m128i *)srcPtrTemp, 0);
+
+                            px1 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+                            px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                            p0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+                            p1 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px0));    // pixels 4-7
+                            p2 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 8-11
+                            p3 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px1));    // pixels 12-15
+
+                            px0 = __lsx_vftint_w_s(__lsx_vfmul_s(p0, pMul));
+                            px1 = __lsx_vftint_w_s(__lsx_vfmul_s(p1, pMul));
+                            px2 = __lsx_vftint_w_s(__lsx_vfmul_s(p2, pMul));
+                            px3 = __lsx_vftint_w_s(__lsx_vfmul_s(p3, pMul));
+
+                            px0 = lsx_packus_i32(px0, px1);
+                            px1 = lsx_packus_i32(px2, px3);
+                            px0 = lsx_packus_i16(px0, px1);    // pixels 0-15
+
+                            __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+
+                            srcPtrTemp +=16;
+                            dstPtrTemp +=16;
+                        }
+                        for (; i < bufferLength; i++)
+                        {
+                            *dstPtrTemp++ = (T) RPPPIXELCHECK(((Rpp32f) (*srcPtrTemp++)) * multiplyingFactor);
+                        }
+
+                        memcpy(dstPtrTemp, srcPtrTemp, remainingElementsAfterROI * sizeof(T));
+                        srcPtrTemp += remainingElementsAfterROI;
+                        dstPtrTemp += remainingElementsAfterROI;
+                    }
+                }
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32u x1 = roiPoints[batchCount].x;
+            Rpp32u y1 = roiPoints[batchCount].y;
+            Rpp32u x2 = x1 + roiPoints[batchCount].roiWidth;
+            Rpp32u y2 = y1 + roiPoints[batchCount].roiHeight;
+            if (x2 == 0)
+            {
+                x2 = batch_srcSize[batchCount].width;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == 0)
+            {
+                y2 = batch_srcSize[batchCount].height;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp32u elementsBeforeROI = channel * x1;
+            Rpp32u remainingElementsAfterROI = channel * (batch_srcSize[batchCount].width - (roiPoints[batchCount].x + roiPoints[batchCount].roiWidth));
+
+            Rpp32f exposureFactor = batch_exposureFactor[batchCount];
+            Rpp32f multiplyingFactor = pow(2, exposureFactor);
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            Rpp32u elementsInRow = channel * batch_srcSize[batchCount].width;
+            Rpp32u elementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+
+
+            for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+            {
+                Rpp32f pixel;
+
+                T *srcPtrTemp, *dstPtrTemp;
+                srcPtrTemp = srcPtrImage + (i * elementsInRowMax);
+                dstPtrTemp = dstPtrImage + (i * elementsInRowMax);
+
+                if (!((y1 <= i) && (i <= y2)))
+                {
+                    memcpy(dstPtrTemp, srcPtrTemp, elementsInRow * sizeof(T));
+
+                    dstPtrTemp += elementsInRowMax;
+                    srcPtrTemp += elementsInRowMax;
+                }
+                else
+                {
+                    memcpy(dstPtrTemp, srcPtrTemp, elementsBeforeROI * sizeof(T));
+                    srcPtrTemp += elementsBeforeROI;
+                    dstPtrTemp += elementsBeforeROI;
+
+                    Rpp32u bufferLength = channel * roiPoints[batchCount].roiWidth;
+                    Rpp32u alignedLength = bufferLength & ~15;
+
+                    __m128i const zero = __lsx_vldi(0);
+                    __m128 pMul = lsx_set1_f32(multiplyingFactor);
+                    __m128 p0, p1, p2, p3;
+                    __m128i px0, px1, px2, px3;
+
+                    int vectorLoopCount = 0;
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+                    {
+                        px0 =  __lsx_vld((__m128i *)srcPtrTemp, 0);
+
+                        px1 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+                        px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                        p0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+                        p1 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px0));    // pixels 4-7
+                        p2 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 8-11
+                        p3 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px1));    // pixels 12-15
+
+                        px0 = __lsx_vftint_w_s(__lsx_vfmul_s(p0, pMul));
+                        px1 = __lsx_vftint_w_s(__lsx_vfmul_s(p1, pMul));
+                        px2 = __lsx_vftint_w_s(__lsx_vfmul_s(p2, pMul));
+                        px3 = __lsx_vftint_w_s(__lsx_vfmul_s(p3, pMul));
+
+                        px0 = lsx_packus_i32(px0, px1);
+                        px1 = lsx_packus_i32(px2, px3);
+                        px0 = lsx_packus_i16(px0, px1);    // pixels 0-15
+
+                        __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+
+                        srcPtrTemp +=16;
+                        dstPtrTemp +=16;
+                    }
+                    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+                    {
+                        *dstPtrTemp++ = (T) RPPPIXELCHECK(((Rpp32f) (*srcPtrTemp++)) * multiplyingFactor);
+                    }
+
+                    memcpy(dstPtrTemp, srcPtrTemp, remainingElementsAfterROI * sizeof(T));
+                    srcPtrTemp += remainingElementsAfterROI;
+                    dstPtrTemp += remainingElementsAfterROI;
+                }
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T, typename U>
+RppStatus exposure_host(T* srcPtr, RppiSize srcSize, U* dstPtr,
+                    Rpp32f exposureFactor,
+                    RppiChnFormat chnFormat, Rpp32u channel)
+{
+    T *srcPtrTemp, *dstPtrTemp;
+
+    srcPtrTemp = srcPtr;
+    dstPtrTemp = dstPtr;
+
+    Rpp32f multiplyingFactor = pow(2, exposureFactor);
+
+    int bufferLength = channel * srcSize.height * srcSize.width;
+    int alignedLength = bufferLength & ~15;
+
+    __m128i const zero = __lsx_vldi(0);
+    __m128 pMul = lsx_set1_f32(multiplyingFactor);
+    __m128 p0, p1, p2, p3;
+    __m128i px0, px1, px2, px3;
+
+    int vectorLoopCount = 0;
+    for (; vectorLoopCount < alignedLength; vectorLoopCount+=16)
+    {
+        px0 =  __lsx_vld((__m128i *)srcPtrTemp, 0);
+
+        px1 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+        px0 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+        p0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px0));    // pixels 0-3
+        p1 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px0));    // pixels 4-7
+        p2 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, px1));    // pixels 8-11
+        p3 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, px1));    // pixels 12-15
+
+        px0 = __lsx_vftint_w_s(__lsx_vfmul_s(p0, pMul));
+        px1 = __lsx_vftint_w_s(__lsx_vfmul_s(p1, pMul));
+        px2 = __lsx_vftint_w_s(__lsx_vfmul_s(p2, pMul));
+        px3 = __lsx_vftint_w_s(__lsx_vfmul_s(p3, pMul));
+
+        px0 = lsx_packus_i32(px0, px1);
+        px1 = lsx_packus_i32(px2, px3);
+        px0 = lsx_packus_i16(px0, px1);    // pixels 0-15
+
+        __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+
+        srcPtrTemp +=16;
+        dstPtrTemp +=16;
+    }
+    for (; vectorLoopCount < bufferLength; vectorLoopCount++)
+    {
+        *dstPtrTemp++ = (T) RPPPIXELCHECK(((Rpp32f) (*srcPtrTemp++)) * multiplyingFactor);
+    }
+
+    return RPP_SUCCESS;
+}
+
+/**************** blur ***************/
+
+template <typename T>
+RppStatus blur_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                                Rpp32u *batch_kernelSize,
+                                RppiROI *roiPoints, Rpp32u nbatchSize,
+                                RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32u x1 = roiPoints[batchCount].x;
+            Rpp32u y1 = roiPoints[batchCount].y;
+            Rpp32u x2 = x1 + roiPoints[batchCount].roiWidth - 1;
+            Rpp32u y2 = y1 + roiPoints[batchCount].roiHeight - 1;
+            if (x2 == -1)
+            {
+                x2 = batch_srcSize[batchCount].width - 1;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == -1)
+            {
+                y2 = batch_srcSize[batchCount].height - 1;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp32u kernelSize = batch_kernelSize[batchCount];
+            Rpp32f multiplier = 1.0 / (kernelSize * kernelSize);
+            Rpp32u bound = (kernelSize - 1) / 2;
+
+            Rpp32u firstRow = y1 + bound;
+            Rpp32u firstColumn = x1 + bound;
+            Rpp32u lastRow = y2 - bound;
+            Rpp32u lastColumn = x2 - bound;
+
+            Rpp32u roiWidthToCompute = lastColumn - firstColumn + 1;
+            Rpp32u remainingElementsAfterROI = batch_srcSize[batchCount].width - roiWidthToCompute;
+            Rpp32u incrementToNextKernel = (kernelSize * batch_srcSizeMax[batchCount].width) - 1;
+
+            Rpp32u sums[25] = {0};
+            Rpp32f pixel;
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            T *srcPtrChannel, *dstPtrChannel;
+            T *srcPtrTemp2, *srcPtrTemp3, *srcPtrTemp4;
+
+            if(kernelSize == 5)
+            {
+                Rpp16u vSums[16] = {0};
+                Rpp16u hSums[12] = {0};
+
+                for(int c = 0; c < channel; c++)
+                {
+                    srcPtrChannel = srcPtrImage + (c * imageDimMax);
+                    dstPtrChannel = dstPtrImage + (c * imageDimMax);
+
+                    srcPtrTemp2 = srcPtrChannel + ((firstRow - bound) * batch_srcSizeMax[batchCount].width) + (firstColumn - bound);
+
+
+                    for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+                    {
+                        T *srcPtrTemp, *dstPtrTemp;
+                        srcPtrTemp = srcPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+                        dstPtrTemp = dstPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+
+                        if (i < firstRow || i > lastRow)
+                        {
+                            memcpy(dstPtrTemp, srcPtrTemp, batch_srcSize[batchCount].width * sizeof(T));
+
+                            dstPtrTemp += batch_srcSizeMax[batchCount].width;
+                            srcPtrTemp += batch_srcSizeMax[batchCount].width;
+                        }
+                        else
+                        {
+                            memcpy(dstPtrTemp, srcPtrTemp, firstColumn * sizeof(T));
+                            srcPtrTemp += firstColumn;
+                            dstPtrTemp += firstColumn;
+
+                            srcPtrTemp3 = srcPtrTemp2;
+
+                            int bufferLength = roiWidthToCompute;
+                            int alignedLength = (bufferLength / 12) * 12;
+
+                            __m128i const zero = __lsx_vldi(0);
+                            __m128i px0, px1, px2, qx0, qx1;
+                            __m128i pSum1, pSum2;
+                            __m128 p0, p1, p2;
+
+                            __m128 pMul = lsx_set1_f32(multiplier);
+
+                            int vectorLoopCount = 0;
+                            for (; vectorLoopCount < alignedLength - 4; vectorLoopCount+=12)
+                            {
+                                pSum1 = __lsx_vreplgr2vr_h(0);
+                                pSum2 = __lsx_vreplgr2vr_h(0);
+
+                                srcPtrTemp4 = srcPtrTemp3;
+
+                                for (int m = 0; m < kernelSize; m++)
+                                {
+                                    px0 =  __lsx_vld((__m128i *)srcPtrTemp4, 0);
+
+                                    px1 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                                    px2 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+
+                                    pSum1 = __lsx_vadd_h(px1, pSum1);
+                                    pSum2 = __lsx_vadd_h(px2, pSum2);
+
+                                    srcPtrTemp4 += batch_srcSizeMax[batchCount].width;
+                                }
+
+                                __lsx_vst(pSum1, (__m128i *)vSums, 0);
+                                __lsx_vst(pSum2, (__m128i *)(vSums + 8), 0);
+
+                                hSums[0] = vSums[0] + vSums[1] + vSums[2] + vSums[3] + vSums[4];
+                                for (int idxCurr = 1, idxPrev = 0, idxNext = 5; idxCurr < 12; idxCurr++, idxPrev++, idxNext++)
+                                {
+                                    hSums[idxCurr] = hSums[idxPrev] - vSums[idxPrev] + vSums[idxNext];
+                                }
+
+                                qx0 = __lsx_vld((__m128i *)hSums, 0);
+                                qx1 = __lsx_vld((__m128i *)(hSums + 8), 0);
+
+                                p0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, qx0));    // pixels 0-3
+                                p1 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, qx0));    // pixels 4-7
+                                p2 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, qx1));    // pixels 8-11
+
+                                p0 = __lsx_vfmul_s(p0, pMul);
+                                p1 = __lsx_vfmul_s(p1, pMul);
+                                p2 = __lsx_vfmul_s(p2, pMul);
+
+                                px0 = __lsx_vftint_w_s(p0);
+                                px1 = __lsx_vftint_w_s(p1);
+                                px2 = __lsx_vftint_w_s(p2);
+
+                                px0 = lsx_packus_i32(px0, px1);
+                                px1 = lsx_packus_i32(px2, zero);
+
+                                px0 = lsx_packus_i16(px0, px1);    // pixels 0-15
+
+                                __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+
+                                srcPtrTemp3 = srcPtrTemp3 + 12;
+                                srcPtrTemp = srcPtrTemp + 12;
+                                dstPtrTemp = dstPtrTemp + 12;
+                            }
+
+                            if (vectorLoopCount < bufferLength)
+                            {
+                                srcPtrTemp4 = srcPtrTemp3;
+
+                                pixel = 0;
+                                for (int m = 0; m < kernelSize; m++)
+                                {
+                                    sums[m] = 0;
+                                    for (int n = 0; n < kernelSize; n++)
+                                    {
+                                        sums[m] += *(srcPtrTemp4 + n);
+                                    }
+                                    srcPtrTemp4 += batch_srcSizeMax[batchCount].width;
+                                }
+                                for (int m = 0; m < kernelSize; m++)
+                                {
+                                    pixel += sums[m];
+                                }
+
+                                *dstPtrTemp++ = (T) RPPPIXELCHECK(pixel * multiplier);
+                                srcPtrTemp++;
+                                vectorLoopCount++;
+
+                                srcPtrTemp4 = srcPtrTemp3 + kernelSize;
+
+                                for(; vectorLoopCount < bufferLength; vectorLoopCount++)
+                                {
+                                    pixel = 0;
+                                    for (int m = 0; m < kernelSize; m++)
+                                    {
+                                        sums[m] = sums[m] - (Rpp32f) *srcPtrTemp3 + (Rpp32f) *srcPtrTemp4;
+                                        srcPtrTemp3 += batch_srcSizeMax[batchCount].width;
+                                        srcPtrTemp4 += batch_srcSizeMax[batchCount].width;
+                                    }
+                                    for (int m = 0; m < kernelSize; m++)
+                                    {
+                                        pixel += sums[m];
+                                    }
+
+                                    *dstPtrTemp++ = (T) RPPPIXELCHECK(pixel * multiplier);
+                                    srcPtrTemp++;
+                                    srcPtrTemp3 -= incrementToNextKernel;
+                                    srcPtrTemp4 -= incrementToNextKernel;
+                                }
+                            }
+                            srcPtrTemp2 += batch_srcSizeMax[batchCount].width;
+
+                            memcpy(dstPtrTemp, srcPtrTemp, remainingElementsAfterROI * sizeof(T));
+                        }
+                    }
+                }
+            }
+            else
+            {
+                for(int c = 0; c < channel; c++)
+                {
+                    srcPtrChannel = srcPtrImage + (c * imageDimMax);
+                    dstPtrChannel = dstPtrImage + (c * imageDimMax);
+
+                    srcPtrTemp2 = srcPtrChannel + ((firstRow - bound) * batch_srcSizeMax[batchCount].width) + (firstColumn - bound);
+
+
+                    for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+                    {
+                        T *srcPtrTemp, *dstPtrTemp;
+                        srcPtrTemp = srcPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+                        dstPtrTemp = dstPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+
+                        if (i < firstRow || i > lastRow)
+                        {
+                            memcpy(dstPtrTemp, srcPtrTemp, batch_srcSize[batchCount].width * sizeof(T));
+
+                            dstPtrTemp += batch_srcSizeMax[batchCount].width;
+                            srcPtrTemp += batch_srcSizeMax[batchCount].width;
+                        }
+                        else
+                        {
+                            memcpy(dstPtrTemp, srcPtrTemp, firstColumn * sizeof(T));
+                            srcPtrTemp += firstColumn;
+                            dstPtrTemp += firstColumn;
+
+                            srcPtrTemp3 = srcPtrTemp2;
+
+                            pixel = 0;
+                            for (int m = 0; m < kernelSize; m++)
+                            {
+                                sums[m] = 0;
+                                for (int n = 0; n < kernelSize; n++)
+                                {
+                                    sums[m] += *(srcPtrTemp3 + n);
+                                }
+                                srcPtrTemp3 += batch_srcSizeMax[batchCount].width;
+                            }
+                            for (int m = 0; m < kernelSize; m++)
+                            {
+                                pixel += sums[m];
+                            }
+
+                            *dstPtrTemp++ = (T) RPPPIXELCHECK(pixel * multiplier);
+                            srcPtrTemp++;
+
+                            srcPtrTemp3 = srcPtrTemp2;
+                            srcPtrTemp4 = srcPtrTemp2 + kernelSize;
+
+                            for (int j= 1; j < roiWidthToCompute; j++)
+                            {
+                                pixel = 0;
+                                for (int m = 0; m < kernelSize; m++)
+                                {
+                                    sums[m] = sums[m] - (Rpp32f) *srcPtrTemp3 + (Rpp32f) *srcPtrTemp4;
+                                    srcPtrTemp3 += batch_srcSizeMax[batchCount].width;
+                                    srcPtrTemp4 += batch_srcSizeMax[batchCount].width;
+                                }
+                                for (int m = 0; m < kernelSize; m++)
+                                {
+                                    pixel += sums[m];
+                                }
+
+                                *dstPtrTemp++ = (T) RPPPIXELCHECK(pixel * multiplier);
+                                srcPtrTemp++;
+                                srcPtrTemp3 -= incrementToNextKernel;
+                                srcPtrTemp4 -= incrementToNextKernel;
+                            }
+                            srcPtrTemp2 += batch_srcSizeMax[batchCount].width;
+
+                            memcpy(dstPtrTemp, srcPtrTemp, remainingElementsAfterROI * sizeof(T));
+                        }
+                    }
+                }
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32u x1 = roiPoints[batchCount].x;
+            Rpp32u y1 = roiPoints[batchCount].y;
+            Rpp32u x2 = x1 + roiPoints[batchCount].roiWidth - 1;
+            Rpp32u y2 = y1 + roiPoints[batchCount].roiHeight - 1;
+            if (x2 == -1)
+            {
+                x2 = batch_srcSize[batchCount].width - 1;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == -1)
+            {
+                y2 = batch_srcSize[batchCount].height - 1;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp32u kernelSize = batch_kernelSize[batchCount];
+            Rpp32f multiplier = 1.0 / (kernelSize * kernelSize);
+            Rpp32u bound = (kernelSize - 1) / 2;
+
+            Rpp32u firstRow = y1 + bound;
+            Rpp32u firstColumn = x1 + bound;
+            Rpp32u lastRow = y2 - bound;
+            Rpp32u lastColumn = x2 - bound;
+
+            Rpp32u roiWidthToCompute = lastColumn - firstColumn + 1;
+
+            Rpp32u sumsR[25] = {0}, sumsG[25] = {0}, sumsB[25] = {0};
+            Rpp32f pixelR, pixelG, pixelB;
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            Rpp32u elementsInRow = channel * batch_srcSize[batchCount].width;
+            Rpp32u elementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+            Rpp32u elementsInKernelRow = channel * kernelSize;
+            Rpp32u incrementToNextKernel = (kernelSize * elementsInRowMax) - channel;
+            Rpp32u channeledBound = channel * bound;
+            Rpp32u channeledFirstColumn = channel * firstColumn;
+
+            T *srcPtrTemp2R, *srcPtrTemp2G, *srcPtrTemp2B;
+            T *srcPtrTemp3R, *srcPtrTemp3G, *srcPtrTemp3B;
+            T *srcPtrTemp4R, *srcPtrTemp4G, *srcPtrTemp4B;
+
+            srcPtrTemp2R = srcPtrImage + ((firstRow - bound) * elementsInRowMax) + ((firstColumn - bound) * channel);
+            srcPtrTemp2G = srcPtrTemp2R + 1;
+            srcPtrTemp2B = srcPtrTemp2R + 2;
+
+            if (kernelSize == 5)
+            {
+                Rpp16u vSums[32] = {0};
+                Rpp16u hSums[15] = {0};
+
+                T *srcPtrTemp3a, *srcPtrTemp3b;
+                T *srcPtrTemp4a, *srcPtrTemp4b;
+
+
+                for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+                {
+                    T *srcPtrTemp, *dstPtrTemp;
+                    srcPtrTemp = srcPtrImage + (i * elementsInRowMax);
+                    dstPtrTemp = dstPtrImage + (i * elementsInRowMax);
+
+                    if (i < firstRow || i > lastRow)
+                    {
+                        memcpy(dstPtrTemp, srcPtrTemp, elementsInRow * sizeof(T));
+
+                        dstPtrTemp += elementsInRowMax;
+                        srcPtrTemp += elementsInRowMax;
+                    }
+                    else
+                    {
+                        memcpy(dstPtrTemp, srcPtrTemp, channeledFirstColumn * sizeof(T));
+                        dstPtrTemp += channeledFirstColumn;
+                        srcPtrTemp += channeledFirstColumn;
+
+                        srcPtrTemp3a = srcPtrTemp2R;
+                        srcPtrTemp3b = srcPtrTemp3a + 16;
+
+                        int bufferLength = roiWidthToCompute * channel;
+                        int alignedLength = (bufferLength / 15) * 15;
+
+                        __m128i const zero = __lsx_vldi(0);
+                        __m128i px0a, px1a, px2a;
+                        __m128i px0b, px1b, px2b;
+                        __m128i qx0, qx1;
+                        __m128 p0, p1, p2, p3;
+                        __m128i pSum1a, pSum2a, pSum1b, pSum2b;
+
+                        __m128 pMul = lsx_set1_f32(multiplier);
+
+                        int vectorLoopCount = 0;
+                        for (; vectorLoopCount < alignedLength - 17; vectorLoopCount+=15)
+                        {
+                            pSum1a = __lsx_vreplgr2vr_h(0);
+                            pSum2a = __lsx_vreplgr2vr_h(0);
+                            pSum1b = __lsx_vreplgr2vr_h(0);
+                            pSum2b = __lsx_vreplgr2vr_h(0);
+
+                            srcPtrTemp4a = srcPtrTemp3a;
+                            srcPtrTemp4b = srcPtrTemp3b;
+
+                            for (int m = 0; m < kernelSize; m++)
+                            {
+                                px0a =  __lsx_vld((__m128i *)srcPtrTemp4a, 0);
+                                px0b =  __lsx_vld((__m128i *)srcPtrTemp4b, 0);
+
+                                px1a = __lsx_vilvl_b(zero, px0a);    // pixels 0-7
+                                px2a = __lsx_vilvh_b(zero, px0a);    // pixels 8-15
+                                px1b = __lsx_vilvl_b(zero, px0b);    // pixels 16-23
+                                px2b = __lsx_vilvh_b(zero, px0b);    // pixels 24-31
+
+                                pSum1a = __lsx_vadd_h(px1a, pSum1a);
+                                pSum2a = __lsx_vadd_h(px2a, pSum2a);
+                                pSum1b = __lsx_vadd_h(px1b, pSum1b);
+                                pSum2b = __lsx_vadd_h(px2b, pSum2b);
+
+                                srcPtrTemp4a += elementsInRowMax;
+                                srcPtrTemp4b += elementsInRowMax;
+                            }
+
+                            __lsx_vst(pSum1a, (__m128i *)vSums, 0);
+                            __lsx_vst(pSum2a, (__m128i *)(vSums + 8), 0);
+                            __lsx_vst(pSum1b, (__m128i *)(vSums + 16), 0);
+                            __lsx_vst(pSum2b, (__m128i *)(vSums + 24), 0);
+
+                            hSums[0] = vSums[0] + vSums[3] + vSums[6] + vSums[9] + vSums[12];
+                            hSums[1] = vSums[1] + vSums[4] + vSums[7] + vSums[10] + vSums[13];
+                            hSums[2] = vSums[2] + vSums[5] + vSums[8] + vSums[11] + vSums[14];
+                            for (int idxCurr = 3, idxPrev = 0, idxNext = 15; idxCurr < 15; idxCurr++, idxPrev++, idxNext++)
+                            {
+                                hSums[idxCurr] = hSums[idxPrev] - vSums[idxPrev] + vSums[idxNext];
+                            }
+
+                            qx0 = __lsx_vld((__m128i *)hSums, 0);
+                            qx1 = __lsx_vld((__m128i *)(hSums + 8), 0);
+
+                            p0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, qx0));    // pixels 0-3
+                            p1 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, qx0));    // pixels 4-7
+                            p2 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, qx1));    // pixels 8-11
+                            p3 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, qx1));    // pixels 12-15
+
+                            p0 = __lsx_vfmul_s(p0, pMul);
+                            p1 = __lsx_vfmul_s(p1, pMul);
+                            p2 = __lsx_vfmul_s(p2, pMul);
+                            p3 = __lsx_vfmul_s(p3, pMul);
+
+                            px0a = __lsx_vftint_w_s(p0);
+                            px1a = __lsx_vftint_w_s(p1);
+                            px0b = __lsx_vftint_w_s(p2);
+                            px1b = __lsx_vftint_w_s(p3);
+
+                            px0a = lsx_packus_i32(px0a, px1a);
+                            px0b = lsx_packus_i32(px0b, px1b);
+
+                            px0a = lsx_packus_i16(px0a, px0b);    // pixels 0-15
+
+                            __lsx_vst(px0a, (__m128i *)dstPtrTemp, 0);
+
+                            srcPtrTemp3a = srcPtrTemp3a + 15;
+                            srcPtrTemp3b = srcPtrTemp3b + 15;
+                            srcPtrTemp = srcPtrTemp + 15;
+                            dstPtrTemp = dstPtrTemp + 15;
+                        }
+                        Rpp32u remainingPixels = bufferLength - vectorLoopCount + 1;
+
+                        srcPtrTemp2R += elementsInRowMax;
+                        srcPtrTemp2G += elementsInRowMax;
+                        srcPtrTemp2B += elementsInRowMax;
+
+                        memcpy(dstPtrTemp, srcPtrTemp, channeledFirstColumn + remainingPixels * sizeof(T));
+                        dstPtrTemp += channeledFirstColumn + remainingPixels;
+                        srcPtrTemp += channeledFirstColumn + remainingPixels;
+                    }
+                }
+            }
+            else
+            {
+
+                for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+                {
+                    T *srcPtrTemp, *dstPtrTemp;
+                    srcPtrTemp = srcPtrImage + (i * elementsInRowMax);
+                    dstPtrTemp = dstPtrImage + (i * elementsInRowMax);
+
+                    if (i < firstRow || i > lastRow)
+                    {
+                        memcpy(dstPtrTemp, srcPtrTemp, elementsInRow * sizeof(T));
+
+                        dstPtrTemp += elementsInRowMax;
+                        srcPtrTemp += elementsInRowMax;
+                    }
+                    else
+                    {
+                        memcpy(dstPtrTemp, srcPtrTemp, channeledFirstColumn * sizeof(T));
+                        dstPtrTemp += channeledFirstColumn;
+                        srcPtrTemp += channeledFirstColumn;
+
+                        srcPtrTemp3R = srcPtrTemp2R;
+                        srcPtrTemp3G = srcPtrTemp2G;
+                        srcPtrTemp3B = srcPtrTemp2B;
+
+                        pixelR = 0;
+                        pixelG = 0;
+                        pixelB = 0;
+                        for (int m = 0; m < kernelSize; m++)
+                        {
+                            sumsR[m] = 0;
+                            sumsG[m] = 0;
+                            sumsB[m] = 0;
+                            for (int n = 0; n < elementsInKernelRow; n += channel)
+                            {
+                                sumsR[m] += *(srcPtrTemp3R + n);
+                                sumsG[m] += *(srcPtrTemp3G + n);
+                                sumsB[m] += *(srcPtrTemp3B + n);
+                            }
+                            srcPtrTemp3R += elementsInRowMax;
+                            srcPtrTemp3G += elementsInRowMax;
+                            srcPtrTemp3B += elementsInRowMax;
+                        }
+                        for (int m = 0; m < kernelSize; m++)
+                        {
+                            pixelR += sumsR[m];
+                            pixelG += sumsG[m];
+                            pixelB += sumsB[m];
+                        }
+
+                        *dstPtrTemp++ = (T) RPPPIXELCHECK(pixelR * multiplier);
+                        *dstPtrTemp++ = (T) RPPPIXELCHECK(pixelG * multiplier);
+                        *dstPtrTemp++ = (T) RPPPIXELCHECK(pixelB * multiplier);
+                        srcPtrTemp += channel;
+
+                        srcPtrTemp3R = srcPtrTemp2R;
+                        srcPtrTemp3G = srcPtrTemp2G;
+                        srcPtrTemp3B = srcPtrTemp2B;
+                        srcPtrTemp4R = srcPtrTemp2R + elementsInKernelRow;
+                        srcPtrTemp4G = srcPtrTemp2G + elementsInKernelRow;
+                        srcPtrTemp4B = srcPtrTemp2B + elementsInKernelRow;
+
+                        for (int j= 1; j < roiWidthToCompute; j++)
+                        {
+                            pixelR = 0;
+                            pixelG = 0;
+                            pixelB = 0;
+                            for (int m = 0; m < kernelSize; m++)
+                            {
+                                sumsR[m] = sumsR[m] - (Rpp32f) *srcPtrTemp3R + (Rpp32f) *srcPtrTemp4R;
+                                sumsG[m] = sumsG[m] - (Rpp32f) *srcPtrTemp3G + (Rpp32f) *srcPtrTemp4G;
+                                sumsB[m] = sumsB[m] - (Rpp32f) *srcPtrTemp3B + (Rpp32f) *srcPtrTemp4B;
+                                srcPtrTemp3R += elementsInRowMax;
+                                srcPtrTemp3G += elementsInRowMax;
+                                srcPtrTemp3B += elementsInRowMax;
+                                srcPtrTemp4R += elementsInRowMax;
+                                srcPtrTemp4G += elementsInRowMax;
+                                srcPtrTemp4B += elementsInRowMax;
+                            }
+                            for (int m = 0; m < kernelSize; m++)
+                            {
+                                pixelR += sumsR[m];
+                                pixelG += sumsG[m];
+                                pixelB += sumsB[m];
+                            }
+
+                            *dstPtrTemp++ = (T) RPPPIXELCHECK(pixelR * multiplier);
+                            *dstPtrTemp++ = (T) RPPPIXELCHECK(pixelG * multiplier);
+                            *dstPtrTemp++ = (T) RPPPIXELCHECK(pixelB * multiplier);
+                            srcPtrTemp += channel;
+                            srcPtrTemp3R -= incrementToNextKernel;
+                            srcPtrTemp3G -= incrementToNextKernel;
+                            srcPtrTemp3B -= incrementToNextKernel;
+                            srcPtrTemp4R -= incrementToNextKernel;
+                            srcPtrTemp4G -= incrementToNextKernel;
+                            srcPtrTemp4B -= incrementToNextKernel;
+                        }
+
+                        srcPtrTemp2R += elementsInRowMax;
+                        srcPtrTemp2G += elementsInRowMax;
+                        srcPtrTemp2B += elementsInRowMax;
+
+                        memcpy(dstPtrTemp, srcPtrTemp, channeledFirstColumn * sizeof(T));
+                        dstPtrTemp += channeledFirstColumn;
+                        srcPtrTemp += channeledFirstColumn;
+                    }
+                }
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus blur_host(T* srcPtr, RppiSize srcSize, T* dstPtr,
+                    Rpp32u kernelSize,
+                    RppiChnFormat chnFormat, Rpp32u channel)
+{
+    Rpp32f multiplier = 1.0 / (kernelSize * kernelSize);
+
+    Rpp32u imageDim = srcSize.height * srcSize.width;
+    Rpp32u bound = (kernelSize - 1) / 2;
+
+    Rpp32u lastRow = srcSize.height - 1 - bound;
+    Rpp32u lastColumn = srcSize.width - 1 - bound;
+
+    Rpp32u widthToCompute = srcSize.width - (2 * bound);
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        T *srcPtrTemp, *srcPtrTemp2, *srcPtrTemp3, *srcPtrTemp4, *dstPtrTemp;
+
+        Rpp32u sums[25] = {0};
+        Rpp32f pixel;
+
+        Rpp32u incrementToNextKernel = (kernelSize * srcSize.width)  - 1;
+
+        if (kernelSize == 5)
+        {
+            Rpp16u vSums[16] = {0};
+            Rpp16u hSums[12] = {0};
+
+            for (int c = 0; c < channel; c++)
+            {
+                T *srcPtrTemp = srcPtr + (c * imageDim);
+                T *dstPtrTemp = dstPtr + (c * imageDim);
+
+                srcPtrTemp2 = srcPtrTemp;
+
+                for (int i = 0; i < srcSize.height; i++)
+                {
+                    if (i < bound || i > lastRow)
+                    {
+                        memcpy(dstPtrTemp, srcPtrTemp, srcSize.width * sizeof(T));
+                        srcPtrTemp += srcSize.width;
+                        dstPtrTemp += srcSize.width;
+                    }
+                    else
+                    {
+                        memcpy(dstPtrTemp, srcPtrTemp, bound * sizeof(T));
+                        dstPtrTemp += bound;
+                        srcPtrTemp += bound;
+
+                        srcPtrTemp3 = srcPtrTemp2;
+
+                        int bufferLength = widthToCompute;
+                        int alignedLength = (bufferLength / 12) * 12;
+
+                        __m128i const zero = __lsx_vldi(0);
+                        __m128i px0, px1, px2, qx0, qx1;
+                        __m128i pSum1, pSum2;
+                        __m128 p0, p1, p2;
+
+                        __m128 pMul = lsx_set1_f32(multiplier);
+
+                        int vectorLoopCount = 0;
+                        for (; vectorLoopCount < alignedLength - 4; vectorLoopCount+=12)
+                        {
+                            pSum1 = __lsx_vreplgr2vr_h(0);
+                            pSum2 = __lsx_vreplgr2vr_h(0);
+
+                            srcPtrTemp4 = srcPtrTemp3;
+
+                            for (int m = 0; m < kernelSize; m++)
+                            {
+                                px0 =  __lsx_vld((__m128i *)srcPtrTemp4, 0);
+
+                                px1 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                                px2 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+
+                                pSum1 = __lsx_vadd_h(px1, pSum1);
+                                pSum2 = __lsx_vadd_h(px2, pSum2);
+
+                                srcPtrTemp4 += srcSize.width;
+                            }
+
+                            __lsx_vst(pSum1, (__m128i *)vSums, 0);
+                            __lsx_vst(pSum2, (__m128i *)(vSums + 8), 0);
+
+                            hSums[0] = vSums[0] + vSums[1] + vSums[2] + vSums[3] + vSums[4];
+                            for (int idxCurr = 1, idxPrev = 0, idxNext = 5; idxCurr < 12; idxCurr++, idxPrev++, idxNext++)
+                            {
+                                hSums[idxCurr] = hSums[idxPrev] - vSums[idxPrev] + vSums[idxNext];
+                            }
+
+                            qx0 = __lsx_vld((__m128i *)hSums, 0);
+                            qx1 = __lsx_vld((__m128i *)(hSums + 8), 0);
+
+                            p0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, qx0));    // pixels 0-3
+                            p1 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, qx0));    // pixels 4-7
+                            p2 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, qx1));    // pixels 8-11
+
+                            p0 = __lsx_vfmul_s(p0, pMul);
+                            p1 = __lsx_vfmul_s(p1, pMul);
+                            p2 = __lsx_vfmul_s(p2, pMul);
+
+                            px0 = __lsx_vftint_w_s(p0);
+                            px1 = __lsx_vftint_w_s(p1);
+                            px2 = __lsx_vftint_w_s(p2);
+
+                            px0 = lsx_packus_i32(px0, px1);
+                            px1 = lsx_packus_i32(px2, zero);
+
+                            px0 = lsx_packus_i16(px0, px1);    // pixels 0-15
+
+                            __lsx_vst(px0, (__m128i *)dstPtrTemp, 0);
+
+                            srcPtrTemp3 = srcPtrTemp3 + 12;
+                            srcPtrTemp = srcPtrTemp + 12;
+                            dstPtrTemp = dstPtrTemp + 12;
+                        }
+
+                        if (vectorLoopCount < bufferLength)
+                        {
+                            srcPtrTemp4 = srcPtrTemp3;
+
+                            pixel = 0;
+                            for (int m = 0; m < kernelSize; m++)
+                            {
+                                sums[m] = 0;
+                                for (int n = 0; n < kernelSize; n++)
+                                {
+                                    sums[m] += *(srcPtrTemp4 + n);
+                                }
+                                srcPtrTemp4 += srcSize.width;
+                            }
+                            for (int m = 0; m < kernelSize; m++)
+                            {
+                                pixel += sums[m];
+                            }
+
+                            *dstPtrTemp++ = (T) RPPPIXELCHECK(pixel * multiplier);
+                            srcPtrTemp++;
+                            vectorLoopCount++;
+
+                            srcPtrTemp4 = srcPtrTemp3 + kernelSize;
+
+                            for(; vectorLoopCount < bufferLength; vectorLoopCount++)
+                            {
+                                pixel = 0;
+                                for (int m = 0; m < kernelSize; m++)
+                                {
+                                    sums[m] = sums[m] - (Rpp32f) *srcPtrTemp3 + (Rpp32f) *srcPtrTemp4;
+                                    srcPtrTemp3 += srcSize.width;
+                                    srcPtrTemp4 += srcSize.width;
+                                }
+                                for (int m = 0; m < kernelSize; m++)
+                                {
+                                    pixel += sums[m];
+                                }
+
+                                *dstPtrTemp++ = (T) RPPPIXELCHECK(pixel * multiplier);
+                                srcPtrTemp++;
+                                srcPtrTemp3 -= incrementToNextKernel;
+                                srcPtrTemp4 -= incrementToNextKernel;
+                            }
+                        }
+                        srcPtrTemp2 += srcSize.width;
+
+                        memcpy(dstPtrTemp, srcPtrTemp, bound * sizeof(T));
+                        dstPtrTemp += bound;
+                        srcPtrTemp += bound;
+                    }
+                }
+            }
+        }
+        else
+        {
+            for (int c = 0; c < channel; c++)
+            {
+                T *srcPtrTemp = srcPtr + (c * imageDim);
+                T *dstPtrTemp = dstPtr + (c * imageDim);
+
+                srcPtrTemp2 = srcPtrTemp;
+
+                for (int i = 0; i < srcSize.height; i++)
+                {
+                    if (i < bound || i > lastRow)
+                    {
+                        memcpy(dstPtrTemp, srcPtrTemp, srcSize.width * sizeof(T));
+                        srcPtrTemp += srcSize.width;
+                        dstPtrTemp += srcSize.width;
+                    }
+                    else
+                    {
+                        memcpy(dstPtrTemp, srcPtrTemp, bound * sizeof(T));
+                        dstPtrTemp += bound;
+                        srcPtrTemp += bound;
+
+                        srcPtrTemp3 = srcPtrTemp2;
+
+                        pixel = 0;
+                        for (int m = 0; m < kernelSize; m++)
+                        {
+                            sums[m] = 0;
+                            for (int n = 0; n < kernelSize; n++)
+                            {
+                                sums[m] += *(srcPtrTemp3 + n);
+                            }
+                            srcPtrTemp3 += srcSize.width;
+                        }
+                        for (int m = 0; m < kernelSize; m++)
+                        {
+                            pixel += sums[m];
+                        }
+
+                        *dstPtrTemp++ = (T) RPPPIXELCHECK(pixel * multiplier);
+                        srcPtrTemp++;
+
+                        srcPtrTemp3 = srcPtrTemp2;
+                        srcPtrTemp4 = srcPtrTemp2 + kernelSize;
+
+                        for (int j= 1; j < widthToCompute; j++)
+                        {
+                            pixel = 0;
+                            for (int m = 0; m < kernelSize; m++)
+                            {
+                                sums[m] = sums[m] - (Rpp32f) *srcPtrTemp3 + (Rpp32f) *srcPtrTemp4;
+                                srcPtrTemp3 += srcSize.width;
+                                srcPtrTemp4 += srcSize.width;
+                            }
+                            for (int m = 0; m < kernelSize; m++)
+                            {
+                                pixel += sums[m];
+                            }
+
+                            *dstPtrTemp++ = (T) RPPPIXELCHECK(pixel * multiplier);
+                            srcPtrTemp++;
+                            srcPtrTemp3 -= incrementToNextKernel;
+                            srcPtrTemp4 -= incrementToNextKernel;
+                        }
+                        srcPtrTemp2 += srcSize.width;
+
+                        memcpy(dstPtrTemp, srcPtrTemp, bound * sizeof(T));
+                        dstPtrTemp += bound;
+                        srcPtrTemp += bound;
+                    }
+                }
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        T *srcPtrTemp, *dstPtrTemp;
+        T *srcPtrTemp2R, *srcPtrTemp2G, *srcPtrTemp2B;
+        T *srcPtrTemp3R, *srcPtrTemp3G, *srcPtrTemp3B;
+        T *srcPtrTemp4R, *srcPtrTemp4G, *srcPtrTemp4B;
+
+        Rpp32u sumsR[25] = {0}, sumsG[25] = {0}, sumsB[25] = {0};
+        Rpp32f pixelR, pixelG, pixelB;
+
+        Rpp32u elementsInRow = channel * srcSize.width;
+        Rpp32u elementsInKernelRow = channel * kernelSize;
+        Rpp32u incrementToNextKernel = (kernelSize * elementsInRow) - channel;
+        Rpp32u channeledBound = channel * bound;
+
+        srcPtrTemp = srcPtr;
+        dstPtrTemp = dstPtr;
+        srcPtrTemp2R = srcPtrTemp;
+        srcPtrTemp2G = srcPtrTemp + 1;
+        srcPtrTemp2B = srcPtrTemp + 2;
+
+        if (kernelSize == 5)
+        {
+            Rpp16u vSums[32] = {0};
+            Rpp16u hSums[15] = {0};
+
+            T *srcPtrTemp3a, *srcPtrTemp3b;
+            T *srcPtrTemp4a, *srcPtrTemp4b;
+
+            for (int i = 0; i < srcSize.height; i++)
+            {
+                if (i < bound || i > lastRow)
+                {
+                    memcpy(dstPtrTemp, srcPtrTemp, elementsInRow * sizeof(T));
+                    srcPtrTemp += elementsInRow;
+                    dstPtrTemp += elementsInRow;
+                }
+                else
+                {
+                    memcpy(dstPtrTemp, srcPtrTemp, channeledBound * sizeof(T));
+                    dstPtrTemp += channeledBound;
+                    srcPtrTemp += channeledBound;
+
+                    srcPtrTemp3a = srcPtrTemp2R;
+                    srcPtrTemp3b = srcPtrTemp3a + 16;
+
+                    int bufferLength = widthToCompute * channel;
+                    int alignedLength = (bufferLength / 15) * 15;
+
+                    __m128i const zero = __lsx_vldi(0);
+                    __m128i px0a, px1a, px2a;
+                    __m128i px0b, px1b, px2b;
+                    __m128i qx0, qx1;
+                    __m128 p0, p1, p2, p3;
+                    __m128i pSum1a, pSum2a, pSum1b, pSum2b;
+
+                    __m128 pMul = lsx_set1_f32(multiplier);
+
+                    int vectorLoopCount = 0;
+                    for (; vectorLoopCount < alignedLength - 17; vectorLoopCount+=15)
+                    {
+                        pSum1a = __lsx_vreplgr2vr_h(0);
+                        pSum2a = __lsx_vreplgr2vr_h(0);
+                        pSum1b = __lsx_vreplgr2vr_h(0);
+                        pSum2b = __lsx_vreplgr2vr_h(0);
+
+                        srcPtrTemp4a = srcPtrTemp3a;
+                        srcPtrTemp4b = srcPtrTemp3b;
+
+                        for (int m = 0; m < kernelSize; m++)
+                        {
+                            px0a =  __lsx_vld((__m128i *)srcPtrTemp4a, 0);
+                            px0b =  __lsx_vld((__m128i *)srcPtrTemp4b, 0);
+
+                            px1a = __lsx_vilvl_b(zero, px0a);    // pixels 0-7
+                            px2a = __lsx_vilvh_b(zero, px0a);    // pixels 8-15
+                            px1b = __lsx_vilvl_b(zero, px0b);    // pixels 16-23
+                            px2b = __lsx_vilvh_b(zero, px0b);    // pixels 24-31
+
+                            pSum1a = __lsx_vadd_h(px1a, pSum1a);
+                            pSum2a = __lsx_vadd_h(px2a, pSum2a);
+                            pSum1b = __lsx_vadd_h(px1b, pSum1b);
+                            pSum2b = __lsx_vadd_h(px2b, pSum2b);
+
+                            srcPtrTemp4a += elementsInRow;
+                            srcPtrTemp4b += elementsInRow;
+                        }
+
+                        __lsx_vst(pSum1a, (__m128i *)vSums, 0);
+                        __lsx_vst(pSum2a, (__m128i *)(vSums + 8), 0);
+                        __lsx_vst(pSum1b, (__m128i *)(vSums + 16), 0);
+                        __lsx_vst(pSum2b, (__m128i *)(vSums + 24), 0);
+
+                        hSums[0] = vSums[0] + vSums[3] + vSums[6] + vSums[9] + vSums[12];
+                        hSums[1] = vSums[1] + vSums[4] + vSums[7] + vSums[10] + vSums[13];
+                        hSums[2] = vSums[2] + vSums[5] + vSums[8] + vSums[11] + vSums[14];
+                        for (int idxCurr = 3, idxPrev = 0, idxNext = 15; idxCurr < 15; idxCurr++, idxPrev++, idxNext++)
+                        {
+                            hSums[idxCurr] = hSums[idxPrev] - vSums[idxPrev] + vSums[idxNext];
+                        }
+
+                        qx0 = __lsx_vld((__m128i *)hSums, 0);
+                        qx1 = __lsx_vld((__m128i *)(hSums + 8), 0);
+
+                        p0 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, qx0));    // pixels 0-3
+                        p1 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, qx0));    // pixels 4-7
+                        p2 = __lsx_vffint_s_w(__lsx_vilvl_h(zero, qx1));    // pixels 8-11
+                        p3 = __lsx_vffint_s_w(__lsx_vilvh_h(zero, qx1));    // pixels 12-15
+
+                        p0 = __lsx_vfmul_s(p0, pMul);
+                        p1 = __lsx_vfmul_s(p1, pMul);
+                        p2 = __lsx_vfmul_s(p2, pMul);
+                        p3 = __lsx_vfmul_s(p3, pMul);
+
+                        px0a = __lsx_vftint_w_s(p0);
+                        px1a = __lsx_vftint_w_s(p1);
+                        px0b = __lsx_vftint_w_s(p2);
+                        px1b = __lsx_vftint_w_s(p3);
+
+                        px0a = lsx_packus_i32(px0a, px1a);
+                        px0b = lsx_packus_i32(px0b, px1b);
+
+                        px0a = lsx_packus_i16(px0a, px0b);    // pixels 0-15
+
+                        __lsx_vst(px0a, (__m128i *)dstPtrTemp, 0);
+
+                        srcPtrTemp3a = srcPtrTemp3a + 15;
+                        srcPtrTemp3b = srcPtrTemp3b + 15;
+                        srcPtrTemp = srcPtrTemp + 15;
+                        dstPtrTemp = dstPtrTemp + 15;
+                    }
+
+                    if (vectorLoopCount < bufferLength)
+                    {
+                        srcPtrTemp3R = srcPtrTemp3a;
+                        srcPtrTemp3G = srcPtrTemp3R + 1;
+                        srcPtrTemp3B = srcPtrTemp3R + 2;
+
+                        srcPtrTemp4R = srcPtrTemp3R;
+                        srcPtrTemp4G = srcPtrTemp3G;
+                        srcPtrTemp4B = srcPtrTemp3B;
+
+                        pixelR = 0;
+                        pixelG = 0;
+                        pixelB = 0;
+                        for (int m = 0; m < kernelSize; m++)
+                        {
+                            sumsR[m] = 0;
+                            sumsG[m] = 0;
+                            sumsB[m] = 0;
+                            for (int n = 0; n < elementsInKernelRow; n += channel)
+                            {
+                                sumsR[m] += *(srcPtrTemp4R + n);
+                                sumsG[m] += *(srcPtrTemp4G + n);
+                                sumsB[m] += *(srcPtrTemp4B + n);
+                            }
+                            srcPtrTemp4R += elementsInRow;
+                            srcPtrTemp4G += elementsInRow;
+                            srcPtrTemp4B += elementsInRow;
+                        }
+                        for (int m = 0; m < kernelSize; m++)
+                        {
+                            pixelR += sumsR[m];
+                            pixelG += sumsG[m];
+                            pixelB += sumsB[m];
+                        }
+
+                        *dstPtrTemp++ = (T) RPPPIXELCHECK(pixelR * multiplier);
+                        *dstPtrTemp++ = (T) RPPPIXELCHECK(pixelG * multiplier);
+                        *dstPtrTemp++ = (T) RPPPIXELCHECK(pixelB * multiplier);
+                        srcPtrTemp += channel;
+                        vectorLoopCount += channel;
+
+                        srcPtrTemp4R = srcPtrTemp3R + elementsInKernelRow;
+                        srcPtrTemp4G = srcPtrTemp3G + elementsInKernelRow;
+                        srcPtrTemp4B = srcPtrTemp3B + elementsInKernelRow;
+
+                        for(; vectorLoopCount < bufferLength; vectorLoopCount += channel)
+                        {
+                            pixelR = 0;
+                            pixelG = 0;
+                            pixelB = 0;
+                            for (int m = 0; m < kernelSize; m++)
+                            {
+                                sumsR[m] = sumsR[m] - (Rpp32f) *srcPtrTemp3R + (Rpp32f) *srcPtrTemp4R;
+                                sumsG[m] = sumsG[m] - (Rpp32f) *srcPtrTemp3G + (Rpp32f) *srcPtrTemp4G;
+                                sumsB[m] = sumsB[m] - (Rpp32f) *srcPtrTemp3B + (Rpp32f) *srcPtrTemp4B;
+                                srcPtrTemp3R += elementsInRow;
+                                srcPtrTemp3G += elementsInRow;
+                                srcPtrTemp3B += elementsInRow;
+                                srcPtrTemp4R += elementsInRow;
+                                srcPtrTemp4G += elementsInRow;
+                                srcPtrTemp4B += elementsInRow;
+                            }
+                            for (int m = 0; m < kernelSize; m++)
+                            {
+                                pixelR += sumsR[m];
+                                pixelG += sumsG[m];
+                                pixelB += sumsB[m];
+                            }
+
+                            *dstPtrTemp++ = (T) RPPPIXELCHECK(pixelR * multiplier);
+                            *dstPtrTemp++ = (T) RPPPIXELCHECK(pixelG * multiplier);
+                            *dstPtrTemp++ = (T) RPPPIXELCHECK(pixelB * multiplier);
+                            srcPtrTemp += channel;
+                            srcPtrTemp3R -= incrementToNextKernel;
+                            srcPtrTemp3G -= incrementToNextKernel;
+                            srcPtrTemp3B -= incrementToNextKernel;
+                            srcPtrTemp4R -= incrementToNextKernel;
+                            srcPtrTemp4G -= incrementToNextKernel;
+                            srcPtrTemp4B -= incrementToNextKernel;
+                        }
+                    }
+
+                    srcPtrTemp2R += elementsInRow;
+
+                    memcpy(dstPtrTemp, srcPtrTemp, channeledBound * sizeof(T));
+                    dstPtrTemp += channeledBound;
+                    srcPtrTemp += channeledBound;
+                }
+            }
+        }
+        else
+        {
+            for (int i = 0; i < srcSize.height; i++)
+            {
+                if (i < bound || i > lastRow)
+                {
+                    memcpy(dstPtrTemp, srcPtrTemp, elementsInRow * sizeof(T));
+                    srcPtrTemp += elementsInRow;
+                    dstPtrTemp += elementsInRow;
+                }
+                else
+                {
+                    memcpy(dstPtrTemp, srcPtrTemp, channeledBound * sizeof(T));
+                    dstPtrTemp += channeledBound;
+                    srcPtrTemp += channeledBound;
+
+                    srcPtrTemp3R = srcPtrTemp2R;
+                    srcPtrTemp3G = srcPtrTemp2G;
+                    srcPtrTemp3B = srcPtrTemp2B;
+
+                    pixelR = 0;
+                    pixelG = 0;
+                    pixelB = 0;
+                    for (int m = 0; m < kernelSize; m++)
+                    {
+                        sumsR[m] = 0;
+                        sumsG[m] = 0;
+                        sumsB[m] = 0;
+                        for (int n = 0; n < elementsInKernelRow; n += channel)
+                        {
+                            sumsR[m] += *(srcPtrTemp3R + n);
+                            sumsG[m] += *(srcPtrTemp3G + n);
+                            sumsB[m] += *(srcPtrTemp3B + n);
+                        }
+                        srcPtrTemp3R += elementsInRow;
+                        srcPtrTemp3G += elementsInRow;
+                        srcPtrTemp3B += elementsInRow;
+                    }
+                    for (int m = 0; m < kernelSize; m++)
+                    {
+                        pixelR += sumsR[m];
+                        pixelG += sumsG[m];
+                        pixelB += sumsB[m];
+                    }
+
+                    *dstPtrTemp++ = (T) RPPPIXELCHECK(pixelR * multiplier);
+                    *dstPtrTemp++ = (T) RPPPIXELCHECK(pixelG * multiplier);
+                    *dstPtrTemp++ = (T) RPPPIXELCHECK(pixelB * multiplier);
+                    srcPtrTemp += channel;
+
+                    srcPtrTemp3R = srcPtrTemp2R;
+                    srcPtrTemp3G = srcPtrTemp2G;
+                    srcPtrTemp3B = srcPtrTemp2B;
+                    srcPtrTemp4R = srcPtrTemp2R + elementsInKernelRow;
+                    srcPtrTemp4G = srcPtrTemp2G + elementsInKernelRow;
+                    srcPtrTemp4B = srcPtrTemp2B + elementsInKernelRow;
+
+                    for (int j= 1; j < widthToCompute; j++)
+                    {
+                        pixelR = 0;
+                        pixelG = 0;
+                        pixelB = 0;
+                        for (int m = 0; m < kernelSize; m++)
+                        {
+                            sumsR[m] = sumsR[m] - (Rpp32f) *srcPtrTemp3R + (Rpp32f) *srcPtrTemp4R;
+                            sumsG[m] = sumsG[m] - (Rpp32f) *srcPtrTemp3G + (Rpp32f) *srcPtrTemp4G;
+                            sumsB[m] = sumsB[m] - (Rpp32f) *srcPtrTemp3B + (Rpp32f) *srcPtrTemp4B;
+                            srcPtrTemp3R += elementsInRow;
+                            srcPtrTemp3G += elementsInRow;
+                            srcPtrTemp3B += elementsInRow;
+                            srcPtrTemp4R += elementsInRow;
+                            srcPtrTemp4G += elementsInRow;
+                            srcPtrTemp4B += elementsInRow;
+                        }
+                        for (int m = 0; m < kernelSize; m++)
+                        {
+                            pixelR += sumsR[m];
+                            pixelG += sumsG[m];
+                            pixelB += sumsB[m];
+                        }
+
+                        *dstPtrTemp++ = (T) RPPPIXELCHECK(pixelR * multiplier);
+                        *dstPtrTemp++ = (T) RPPPIXELCHECK(pixelG * multiplier);
+                        *dstPtrTemp++ = (T) RPPPIXELCHECK(pixelB * multiplier);
+                        srcPtrTemp += channel;
+                        srcPtrTemp3R -= incrementToNextKernel;
+                        srcPtrTemp3G -= incrementToNextKernel;
+                        srcPtrTemp3B -= incrementToNextKernel;
+                        srcPtrTemp4R -= incrementToNextKernel;
+                        srcPtrTemp4G -= incrementToNextKernel;
+                        srcPtrTemp4B -= incrementToNextKernel;
+                    }
+                    srcPtrTemp2R += elementsInRow;
+                    srcPtrTemp2G += elementsInRow;
+                    srcPtrTemp2B += elementsInRow;
+
+                    memcpy(dstPtrTemp, srcPtrTemp, channeledBound * sizeof(T));
+                    dstPtrTemp += channeledBound;
+                    srcPtrTemp += channeledBound;
+                }
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+/**************** histogram_balance ***************/
+
+template <typename T>
+RppStatus histogram_balance_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                                            Rpp32u nbatchSize,
+                                            RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+            Rpp32u imageDim = batch_srcSize[batchCount].height * batch_srcSize[batchCount].width;
+
+            Rpp32u bins = 256;
+            Rpp8u bins8u = (Rpp8u) (((Rpp32u)(bins))- 1);
+
+            Rpp32u *outputHistogramImage = (Rpp32u*) calloc(bins, sizeof(Rpp32u));
+            T *lookUpTable = (T *) calloc (bins, sizeof(T));
+            Rpp32f multiplier = 255.0 / ((Rpp32f)(channel * imageDim));
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            if (bins == 0)
+            {
+                *outputHistogramImage = channel * imageDim;
+            }
+            else
+            {
+                Rpp8u rangeInBin = 256 / (bins8u + 1);
+
+                for(int c = 0; c < channel; c++)
+                {
+                    T *srcPtrChannel;
+                    srcPtrChannel = srcPtrImage + (c * imageDimMax);
+
+
+                    for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+                    {
+                        T *srcPtrTemp;
+                        srcPtrTemp = srcPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+
+                        for(int j = 0; j < batch_srcSize[batchCount].width; j++)
+                        {
+                            *(outputHistogramImage + (*srcPtrTemp / rangeInBin)) += 1;
+
+                            srcPtrTemp++;
+                        }
+                    }
+                }
+            }
+
+            Rpp32u sum = 0;
+            Rpp32u *outputHistogramImageTemp;
+            T *lookUpTableTemp;
+            outputHistogramImageTemp = outputHistogramImage;
+            lookUpTableTemp = lookUpTable;
+
+            for (int i = 0; i < 256; i++)
+            {
+                sum += *outputHistogramImageTemp;
+                *lookUpTableTemp = (T)round(((Rpp32f)sum) * multiplier);
+                outputHistogramImageTemp++;
+                lookUpTableTemp++;
+            }
+
+            Rpp32f x1 = 0;
+            Rpp32f y1 = 0;
+            Rpp32f x2 = 0;
+            Rpp32f y2 = 0;
+            if (x2 == 0)
+            {
+                x2 = batch_srcSize[batchCount].width;
+            }
+            if (y2 == 0)
+            {
+                y2 = batch_srcSize[batchCount].height;
+            }
+
+            for(int c = 0; c < channel; c++)
+            {
+                T *srcPtrChannel, *dstPtrChannel;
+                srcPtrChannel = srcPtrImage + (c * imageDimMax);
+                dstPtrChannel = dstPtrImage + (c * imageDimMax);
+
+
+                for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+                {
+                    T *srcPtrTemp, *dstPtrTemp;
+                    srcPtrTemp = srcPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+                    dstPtrTemp = dstPtrChannel + (i * batch_srcSizeMax[batchCount].width);
+
+                    if (!((y1 <= i) && (i <= y2)))
+                    {
+                        memcpy(dstPtrTemp, srcPtrTemp, batch_srcSize[batchCount].width * sizeof(T));
+                    }
+                    else
+                    {
+                        for(int j = 0; j < batch_srcSize[batchCount].width; j++)
+                        {
+                            if((x1 <= j) && (j <= x2 ))
+                            {
+                                *dstPtrTemp = *(lookUpTable + *srcPtrTemp);
+                            }
+                            else
+                            {
+                                *dstPtrTemp = *srcPtrTemp;
+                            }
+                            srcPtrTemp++;
+                            dstPtrTemp++;
+                        }
+                    }
+                }
+            }
+
+            free(outputHistogramImage);
+            free(lookUpTable);
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+            Rpp32u imageDim = batch_srcSize[batchCount].height * batch_srcSize[batchCount].width;
+
+            Rpp32u bins = 256;
+            Rpp8u bins8u = (Rpp8u) (((Rpp32u)(bins))- 1);
+
+            Rpp32u *outputHistogramImage = (Rpp32u*) calloc(bins, sizeof(Rpp32u));
+            T *lookUpTable = (T *) calloc (bins, sizeof(T));
+            Rpp32f multiplier = 255.0 / ((Rpp32f)(channel * imageDim));
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            Rpp32u elementsInRow = channel * batch_srcSize[batchCount].width;
+            Rpp32u elementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+
+            if (bins == 0)
+            {
+                *outputHistogramImage = channel * imageDim;
+            }
+            else
+            {
+                Rpp8u rangeInBin = 256 / (bins8u + 1);
+
+
+                for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+                {
+                    T *srcPtrTemp;
+                    srcPtrTemp = srcPtrImage + (i * elementsInRowMax);
+
+                    for(int j = 0; j < batch_srcSize[batchCount].width; j++)
+                    {
+                        for(int c = 0; c < channel; c++)
+                        {
+                            *(outputHistogramImage + (*srcPtrTemp / rangeInBin)) += 1;
+
+                            srcPtrTemp++;
+                        }
+                    }
+                }
+            }
+
+            Rpp32u sum = 0;
+            Rpp32u *outputHistogramImageTemp;
+            T *lookUpTableTemp;
+            outputHistogramImageTemp = outputHistogramImage;
+            lookUpTableTemp = lookUpTable;
+
+            for (int i = 0; i < 256; i++)
+            {
+                sum += *outputHistogramImageTemp;
+                *lookUpTableTemp = (T)round(((Rpp32f)sum) * multiplier);
+                outputHistogramImageTemp++;
+                lookUpTableTemp++;
+            }
+
+            Rpp32f x1 = 0;
+            Rpp32f y1 = 0;
+            Rpp32f x2 = 0;
+            Rpp32f y2 = 0;
+            if (x2 == 0)
+            {
+                x2 = batch_srcSize[batchCount].width;
+            }
+            if (y2 == 0)
+            {
+                y2 = batch_srcSize[batchCount].height;
+            }
+
+
+
+            for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+            {
+                T *srcPtrTemp, *dstPtrTemp;
+                srcPtrTemp = srcPtrImage + (i * elementsInRowMax);
+                dstPtrTemp = dstPtrImage + (i * elementsInRowMax);
+
+                if (!((y1 <= i) && (i <= y2)))
+                {
+                    memcpy(dstPtrTemp, srcPtrTemp, elementsInRow * sizeof(T));
+                }
+                else
+                {
+                    for(int j = 0; j < batch_srcSize[batchCount].width; j++)
+                    {
+                        if (!((x1 <= j) && (j <= x2 )))
+                        {
+                            memcpy(dstPtrTemp, srcPtrTemp, channel * sizeof(T));
+                            dstPtrTemp += channel;
+                            srcPtrTemp += channel;
+                        }
+                        else
+                        {
+                            for(int c = 0; c < channel; c++)
+                            {
+                                *dstPtrTemp = *(lookUpTable + *srcPtrTemp);
+
+                                srcPtrTemp++;
+                                dstPtrTemp++;
+                            }
+                        }
+                    }
+                }
+            }
+
+            free(outputHistogramImage);
+            free(lookUpTable);
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus histogram_balance_host(T* srcPtr, RppiSize srcSize, T* dstPtr,
+                    RppiChnFormat chnFormat,Rpp32u channel)
+{
+    Rpp32u histogram[256];
+    T lookUpTable[256];
+    Rpp32u *histogramTemp;
+    T *lookUpTableTemp;
+    Rpp32f multiplier = 255.0 / ((Rpp32f)(channel * srcSize.height * srcSize.width));
+
+    histogram_kernel_host(srcPtr, srcSize, histogram, 255, channel);
+
+    Rpp32u sum = 0;
+    histogramTemp = histogram;
+    lookUpTableTemp = lookUpTable;
+
+    for (int i = 0; i < 256; i++)
+    {
+        sum += *histogramTemp;
+        *lookUpTableTemp = (T)round(((Rpp32f)sum) * multiplier);
+        histogramTemp++;
+        lookUpTableTemp++;
+    }
+
+    T *srcPtrTemp, *dstPtrTemp;
+    srcPtrTemp = srcPtr;
+    dstPtrTemp = dstPtr;
+
+    for (int i = 0; i < (channel * srcSize.height * srcSize.width); i++)
+    {
+        *dstPtrTemp = *(lookUpTable + *srcPtrTemp);
+        srcPtrTemp++;
+        dstPtrTemp++;
+    }
+
+    return RPP_SUCCESS;
+
+}
+
+/**************** random_crop_letterbox ***************/
+
+template <typename T>
+RppStatus random_crop_letterbox_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr, RppiSize *batch_dstSize, RppiSize *batch_dstSizeMax,
+                                           Rpp32u *batch_x1, Rpp32u *batch_x2, Rpp32u *batch_y1, Rpp32u *batch_y2, RppiROI *roiPoints,
+                                           Rpp32u nbatchSize,
+                                           RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+    for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+    {
+        Rpp32u x1 = batch_x1[batchCount];
+        Rpp32u y1 = batch_y1[batchCount];
+        Rpp32u x2 = batch_x2[batchCount];
+        Rpp32u y2 = batch_y2[batchCount];
+
+        Rpp32u srcLoc = 0, dstLoc = 0;
+        compute_image_location_host(batch_srcSizeMax, batchCount, &srcLoc, channel);
+        compute_image_location_host(batch_dstSizeMax, batchCount, &dstLoc, channel);
+
+        T *srcPtrImage = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+        T *dstPtrImage = (T*) calloc(channel * batch_dstSize[batchCount].height * batch_dstSize[batchCount].width, sizeof(T));
+
+        compute_unpadded_from_padded_host(srcPtr + srcLoc, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], srcPtrImage,
+                                            chnFormat, channel);
+
+        Rpp32u borderWidth = (4 * RPPMIN2(batch_dstSize[batchCount].height, batch_dstSize[batchCount].width) / 100);
+
+        RppiSize srcSizeSubImage;
+        T* srcPtrSubImage;
+        compute_subimage_location_host(srcPtrImage, &srcPtrSubImage, batch_srcSize[batchCount], &srcSizeSubImage, x1, y1, x2, y2, chnFormat, channel);
+
+        RppiSize srcSizeSubImagePadded;
+        srcSizeSubImagePadded.height = srcSizeSubImage.height + (2 * borderWidth);
+        srcSizeSubImagePadded.width = srcSizeSubImage.width + (2 * borderWidth);
+
+        T *srcPtrImageCrop = (T *)calloc(channel * srcSizeSubImage.height * srcSizeSubImage.width, sizeof(T));
+        generate_crop_host(srcPtrImage, batch_srcSize[batchCount], srcPtrSubImage, srcSizeSubImage, srcPtrImageCrop, chnFormat, channel);
+
+        T *srcPtrImageCropPadded = (T *)calloc(channel * srcSizeSubImagePadded.height * srcSizeSubImagePadded.width, sizeof(T));
+        generate_evenly_padded_image_host(srcPtrImageCrop, srcSizeSubImage, srcPtrImageCropPadded, srcSizeSubImagePadded, chnFormat, channel);
+
+        resize_kernel_host(srcPtrImageCropPadded, srcSizeSubImagePadded, dstPtrImage, batch_dstSize[batchCount], chnFormat, channel);
+
+        compute_padded_from_unpadded_host(dstPtrImage, batch_dstSize[batchCount], batch_dstSizeMax[batchCount], dstPtr + dstLoc,
+                                          chnFormat, channel);
+
+        free(srcPtrImage);
+        free(dstPtrImage);
+        free(srcPtrImageCrop);
+        free(srcPtrImageCropPadded);
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus random_crop_letterbox_host(T* srcPtr, RppiSize srcSize, T* dstPtr, RppiSize dstSize,
+                                     Rpp32u x1, Rpp32u x2, Rpp32u y1, Rpp32u y2,
+                                     RppiChnFormat chnFormat, Rpp32u channel)
+{
+    if ((RPPINRANGE(x1, 0, srcSize.width - 1) == 0)
+        || (RPPINRANGE(x2, 0, srcSize.width - 1) == 0)
+        || (RPPINRANGE(y1, 0, srcSize.height - 1) == 0)
+        || (RPPINRANGE(y2, 0, srcSize.height - 1) == 0))
+    {
+        return RPP_ERROR;
+    }
+
+    Rpp32u borderWidth = (5 * RPPMIN2(dstSize.height, dstSize.width) / 100);
+
+    RppiSize srcSizeSubImage;
+    T* srcPtrSubImage;
+    compute_subimage_location_host(srcPtr, &srcPtrSubImage, srcSize, &srcSizeSubImage, x1, y1, x2, y2, chnFormat, channel);
+
+    RppiSize srcSizeSubImagePadded;
+    srcSizeSubImagePadded.height = srcSizeSubImage.height + (2 * borderWidth);
+    srcSizeSubImagePadded.width = srcSizeSubImage.width + (2 * borderWidth);
+
+    T *srcPtrCrop = (T *)calloc(channel * srcSizeSubImage.height * srcSizeSubImage.width, sizeof(T));
+    generate_crop_host(srcPtr, srcSize, srcPtrSubImage, srcSizeSubImage, srcPtrCrop, chnFormat, channel);
+
+    T *srcPtrCropPadded = (T *)calloc(channel * srcSizeSubImagePadded.height * srcSizeSubImagePadded.width, sizeof(T));
+    generate_evenly_padded_image_host(srcPtrCrop, srcSizeSubImage, srcPtrCropPadded, srcSizeSubImagePadded, chnFormat, channel);
+
+    resize_kernel_host(srcPtrCropPadded, srcSizeSubImagePadded, dstPtr, dstSize, chnFormat, channel);
+
+    free(srcPtrCrop);
+    free(srcPtrCropPadded);
+
+    return RPP_SUCCESS;
+}
+
+
+
+/**************** Pixelate ***************/
+
+template <typename T>
+RppStatus pixelate_base_pln_host(T* srcPtrTemp, Rpp32u elementsInRow, T* dstPtrTemp,
+                                 int kernelRowMax, int kernelColMax, int i, int j, Rpp32f multiplier)
+{
+    Rpp32u sum = 0;
+
+    for(int m = 0 ; m < kernelRowMax ; m++)
+    {
+        for(int n = 0 ; n < kernelColMax ; n++)
+        {
+            sum += *(srcPtrTemp + (i + m) * elementsInRow + (j + n));
+        }
+    }
+    sum = RPPPIXELCHECK(sum * multiplier);
+
+    for(int m = 0 ; m < kernelRowMax ; m++)
+    {
+        for(int n = 0 ; n < kernelColMax ; n++)
+        {
+            *(dstPtrTemp + (i + m) * elementsInRow + (j + n)) = (T) sum;
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus pixelate_base_pkd_host(T* srcPtrTemp, Rpp32u elementsInRow, T* dstPtrTemp,
+                                 int kernelRowMax, int kernelColMax, int i, int j, Rpp32f multiplier)
+{
+    Rpp32u sumR = 0;
+    Rpp32u sumG = 0;
+    Rpp32u sumB = 0;
+
+    T *loc;
+
+    for(int m = 0 ; m < kernelRowMax ; m++)
+    {
+        for(int n = 0 ; n < kernelColMax ; n++)
+        {
+            loc = srcPtrTemp + (i + m) * elementsInRow + ((j + n) * 3);
+            sumR += *loc;
+            sumG += *(loc + 1);
+            sumB += *(loc + 2);
+        }
+    }
+    sumR = RPPPIXELCHECK(sumR * multiplier);
+    sumG = RPPPIXELCHECK(sumG * multiplier);
+    sumB = RPPPIXELCHECK(sumB * multiplier);
+
+    for(int m = 0 ; m < kernelRowMax ; m++)
+    {
+        for(int n = 0 ; n < kernelColMax ; n++)
+        {
+            loc = dstPtrTemp + (i + m) * elementsInRow + ((j + n) * 3);
+            *loc = (T) sumR;
+            *(loc + 1) = (T) sumG;
+            *(loc + 2) = (T) sumB;
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus pixelate_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                              RppiROI *roiPoints, Rpp32u nbatchSize,
+                              RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32u x1 = roiPoints[batchCount].x;
+            Rpp32u y1 = roiPoints[batchCount].y;
+            Rpp32u x2 = x1 + roiPoints[batchCount].roiWidth - 1;
+            Rpp32u y2 = y1 + roiPoints[batchCount].roiHeight - 1;
+            if (x2 == -1)
+            {
+                x2 = batch_srcSize[batchCount].width - 1;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == -1)
+            {
+                y2 = batch_srcSize[batchCount].height - 1;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp32u remainingElementsAfterROI = (batch_srcSize[batchCount].width - (roiPoints[batchCount].x + roiPoints[batchCount].roiWidth));
+            Rpp32u roiEndLoc = x1 + roiPoints[batchCount].roiWidth;
+            Rpp32u remainingHeight = batch_srcSize[batchCount].height - (y2 + 1);
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            Rpp32u alignedHeight = (roiPoints[batchCount].roiHeight / 7) * 7;
+            Rpp32u heightDiff = roiPoints[batchCount].roiHeight - alignedHeight;
+
+            Rpp32f multiplier = 1.0 / 49;
+
+            Rpp32u increment = batch_srcSizeMax[batchCount].width * 7;
+            Rpp32u lastIncrement = batch_srcSizeMax[batchCount].width * heightDiff;
+            Rpp32u bufferLength = roiPoints[batchCount].roiWidth;
+            Rpp32u alignedLength = (bufferLength / 14) * 14;
+            if (alignedLength + 2 > bufferLength)
+                alignedLength -= 14;
+            Rpp32u lengthDiff = bufferLength - alignedLength;
+            Rpp32u lengthDiffNew = lengthDiff;
+            if (lengthDiff > 7)
+                lengthDiffNew -= 7;
+
+            Rpp32f lastColMultiplier = 1.0 / (lengthDiffNew * 7);
+            Rpp32f lastRowMultiplier = 1.0 / (heightDiff * 7);
+            Rpp32f lastMultiplier = 1.0 / (heightDiff * lengthDiffNew);
+
+            Rpp16u vSums[16] = {0};
+            Rpp16u hSums[2] = {0};
+
+            __m128i const zero = __lsx_vldi(0);
+            __m128i px0, px1, px2;
+            __m128i pSum1, pSum2;
+
+
+            for(int c = 0; c < channel; c++)
+            {
+                T *srcPtrChannel, *dstPtrChannel;
+                srcPtrChannel = srcPtrImage + (c * imageDimMax);
+                dstPtrChannel = dstPtrImage + (c * imageDimMax);
+
+                T *srcPtrTemp, *dstPtrTemp, *srcPtrTemp2, *dstPtrTemp2, *srcPtrTemp3, *dstPtrTemp3;
+                srcPtrTemp = srcPtrChannel;
+                dstPtrTemp = dstPtrChannel;
+
+                for (int i = 0; i < y1; i++)
+                {
+                    memcpy(dstPtrTemp, srcPtrTemp, batch_srcSize[batchCount].width * sizeof(T));
+
+                    dstPtrTemp += batch_srcSizeMax[batchCount].width;
+                    srcPtrTemp += batch_srcSizeMax[batchCount].width;
+                }
+
+                int i = 0;
+                for (; i < alignedHeight; i += 7)
+                {
+                    srcPtrTemp2 = srcPtrTemp;
+                    dstPtrTemp2 = dstPtrTemp;
+
+                    srcPtrTemp3 = srcPtrTemp + roiEndLoc;
+                    dstPtrTemp3 = dstPtrTemp + roiEndLoc;
+
+                    for (int i2 = 0; i2 < 7; i2++)
+                    {
+                        memcpy(dstPtrTemp2, srcPtrTemp2, x1 * sizeof(T));
+                        memcpy(dstPtrTemp3, srcPtrTemp3, remainingElementsAfterROI * sizeof(T));
+                        srcPtrTemp2 += batch_srcSizeMax[batchCount].width;
+                        dstPtrTemp2 += batch_srcSizeMax[batchCount].width;
+                        srcPtrTemp3 += batch_srcSizeMax[batchCount].width;
+                        dstPtrTemp3 += batch_srcSizeMax[batchCount].width;
+                    }
+
+                    srcPtrTemp2 = srcPtrTemp + x1;
+                    dstPtrTemp2 = dstPtrTemp + x1;
+
+                    Rpp32u vectorLoopCount = 0;
+                    for (; vectorLoopCount < alignedLength; vectorLoopCount+=14)
+                    {
+                        srcPtrTemp3 = srcPtrTemp2;
+                        dstPtrTemp3 = dstPtrTemp2;
+
+                        pSum1 = __lsx_vreplgr2vr_h(0);
+                        pSum2 = __lsx_vreplgr2vr_h(0);
+
+                        for (int m = 0; m < 7; m++)
+                        {
+                            px0 =  __lsx_vld((__m128i *)srcPtrTemp3, 0);
+
+                            px1 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                            px2 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+
+                            pSum1 = __lsx_vadd_h(px1, pSum1);
+                            pSum2 = __lsx_vadd_h(px2, pSum2);
+
+                            srcPtrTemp3 += batch_srcSizeMax[batchCount].width;
+                        }
+
+                        __lsx_vst(pSum1, (__m128i *)vSums, 0);
+                        __lsx_vst(pSum2, (__m128i *)(vSums + 8), 0);
+
+                        hSums[0] = vSums[0] + vSums[1] + vSums[2] + vSums[3] + vSums[4] + vSums[5] + vSums[6];
+                        hSums[1] = vSums[7] + vSums[8] + vSums[9] + vSums[10] + vSums[11] + vSums[12] + vSums[13];
+
+                        px1 = __lsx_vreplgr2vr_h((Rpp16s) RPPPIXELCHECK((Rpp32f) hSums[0] * multiplier));
+                        px2 = __lsx_vreplgr2vr_h((Rpp16s) RPPPIXELCHECK((Rpp32f) hSums[1] * multiplier));
+
+                        px0 = lsx_packus_i16(px1, px2);
+
+                        for (int m = 0; m < 7; m++)
+                        {
+                            __lsx_vst(px0, (__m128i *)dstPtrTemp3, 0);
+                            dstPtrTemp3 += batch_srcSizeMax[batchCount].width;
+                        }
+
+                        srcPtrTemp2 += 14;
+                        dstPtrTemp2 += 14;
+                    }
+
+                    if (lengthDiff > 7)
+                    {
+                        pixelate_base_pln_host(srcPtrChannel, batch_srcSizeMax[batchCount].width, dstPtrChannel, 7, 7, i + y1, vectorLoopCount + x1, multiplier);
+                        vectorLoopCount += 7;
+                    }
+
+                    pixelate_base_pln_host(srcPtrChannel, batch_srcSizeMax[batchCount].width, dstPtrChannel, 7, lengthDiffNew, i + y1, vectorLoopCount + x1, lastColMultiplier);
+
+                    srcPtrTemp += increment;
+                    dstPtrTemp += increment;
+                }
+
+                srcPtrTemp2 = srcPtrTemp;
+                dstPtrTemp2 = dstPtrTemp;
+
+                srcPtrTemp3 = srcPtrTemp + roiEndLoc;
+                dstPtrTemp3 = dstPtrTemp + roiEndLoc;
+
+                for (int i2 = 0; i2 < heightDiff; i2++)
+                {
+                    memcpy(dstPtrTemp2, srcPtrTemp2, x1 * sizeof(T));
+                    memcpy(dstPtrTemp3, srcPtrTemp3, remainingElementsAfterROI * sizeof(T));
+                    srcPtrTemp2 += batch_srcSizeMax[batchCount].width;
+                    dstPtrTemp2 += batch_srcSizeMax[batchCount].width;
+                    srcPtrTemp3 += batch_srcSizeMax[batchCount].width;
+                    dstPtrTemp3 += batch_srcSizeMax[batchCount].width;
+                }
+
+                int j = 0;
+                for (; j < roiPoints[batchCount].roiWidth - lengthDiffNew; j += 7)
+                {
+                    pixelate_base_pln_host(srcPtrChannel, batch_srcSizeMax[batchCount].width, dstPtrChannel, heightDiff, 7, i + y1, j + x1, lastRowMultiplier);
+                }
+
+                pixelate_base_pln_host(srcPtrChannel, batch_srcSizeMax[batchCount].width, dstPtrChannel, heightDiff, lengthDiffNew, i + y1, j + x1, lastMultiplier);
+
+                srcPtrTemp += lastIncrement;
+                dstPtrTemp += lastIncrement;
+
+                for (int i = 0; i < remainingHeight; i++)
+                {
+                    memcpy(dstPtrTemp, srcPtrTemp, batch_srcSize[batchCount].width * sizeof(T));
+
+                    dstPtrTemp += batch_srcSizeMax[batchCount].width;
+                    srcPtrTemp += batch_srcSizeMax[batchCount].width;
+                }
+            }
+        }
+    }
+    else if(chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32u x1 = roiPoints[batchCount].x;
+            Rpp32u y1 = roiPoints[batchCount].y;
+            Rpp32u x2 = x1 + roiPoints[batchCount].roiWidth - 1;
+            Rpp32u y2 = y1 + roiPoints[batchCount].roiHeight - 1;
+            if (x2 == -1)
+            {
+                x2 = batch_srcSize[batchCount].width - 1;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == -1)
+            {
+                y2 = batch_srcSize[batchCount].height - 1;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp32u elementsBeforeROI = channel * x1;
+            Rpp32u remainingElementsAfterROI = channel * (batch_srcSize[batchCount].width - (roiPoints[batchCount].x + roiPoints[batchCount].roiWidth));
+            Rpp32u roiEndLoc = channel * (x1 + roiPoints[batchCount].roiWidth);
+            Rpp32u remainingHeight = batch_srcSize[batchCount].height - (y2 + 1);
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            Rpp32u alignedHeight = (roiPoints[batchCount].roiHeight / 7) * 7;
+            Rpp32u heightDiff = roiPoints[batchCount].roiHeight - alignedHeight;
+
+            Rpp32f multiplier = 1.0 / 49;
+
+            Rpp32u elementsInRow = channel * batch_srcSize[batchCount].width;
+            Rpp32u elementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+
+            Rpp32u increment = elementsInRowMax * 7;
+            Rpp32u lastIncrement = elementsInRowMax * heightDiff;
+            Rpp32u bufferLength = roiPoints[batchCount].roiWidth;
+            Rpp32u alignedLength = (bufferLength / 21) * 21;
+            if (alignedLength + 1 > bufferLength)
+                alignedLength -= 21;
+            Rpp32u lengthDiff = bufferLength - alignedLength;
+            Rpp32u lengthDiffNew = lengthDiff;
+            if (lengthDiff > 7)
+                lengthDiffNew -= 7;
+
+            Rpp32f lastColMultiplier = 1.0 / (lengthDiffNew * 7);
+            Rpp32f lastRowMultiplier = 1.0 / (heightDiff * 7);
+            Rpp32f lastMultiplier = 1.0 / (heightDiff * lengthDiffNew);
+
+            Rpp16u vSums[64] = {0};
+            Rpp16u hSums[9] = {0};
+
+            __m128i const zero = __lsx_vldi(0);
+            __m128i px0, px1, px2, px3, px4, px5, px6, px7, px8;
+            __m128i pSum1, pSum2, pSum3, pSum4, pSum5, pSum6, pSum7, pSum8;
+
+            T *srcPtrTemp, *dstPtrTemp, *srcPtrTemp2, *dstPtrTemp2, *srcPtrTemp3, *dstPtrTemp3;
+            srcPtrTemp = srcPtrImage;
+            dstPtrTemp = dstPtrImage;
+
+            for (int i = 0; i < y1; i++)
+            {
+                memcpy(dstPtrTemp, srcPtrTemp, elementsInRow * sizeof(T));
+
+                dstPtrTemp += elementsInRowMax;
+                srcPtrTemp += elementsInRowMax;
+            }
+
+            int i = 0;
+            for (; i < alignedHeight; i += 7)
+            {
+                srcPtrTemp2 = srcPtrTemp;
+                dstPtrTemp2 = dstPtrTemp;
+
+                srcPtrTemp3 = srcPtrTemp + roiEndLoc;
+                dstPtrTemp3 = dstPtrTemp + roiEndLoc;
+
+                for (int i2 = 0; i2 < 7; i2++)
+                {
+                    memcpy(dstPtrTemp2, srcPtrTemp2, elementsBeforeROI * sizeof(T));
+                    memcpy(dstPtrTemp3, srcPtrTemp3, remainingElementsAfterROI * sizeof(T));
+                    srcPtrTemp2 += elementsInRowMax;
+                    dstPtrTemp2 += elementsInRowMax;
+                    srcPtrTemp3 += elementsInRowMax;
+                    dstPtrTemp3 += elementsInRowMax;
+                }
+
+                srcPtrTemp2 = srcPtrTemp + elementsBeforeROI;
+                dstPtrTemp2 = dstPtrTemp + elementsBeforeROI;
+
+                Rpp32u vectorLoopCount = 0;
+                for (; vectorLoopCount < alignedLength; vectorLoopCount+=21)
+                {
+                    srcPtrTemp3 = srcPtrTemp2;
+                    dstPtrTemp3 = dstPtrTemp2;
+
+                    pSum1 = __lsx_vreplgr2vr_h(0);
+                    pSum2 = __lsx_vreplgr2vr_h(0);
+                    pSum3 = __lsx_vreplgr2vr_h(0);
+                    pSum4 = __lsx_vreplgr2vr_h(0);
+                    pSum5 = __lsx_vreplgr2vr_h(0);
+                    pSum6 = __lsx_vreplgr2vr_h(0);
+                    pSum7 = __lsx_vreplgr2vr_h(0);
+                    pSum8 = __lsx_vreplgr2vr_h(0);
+
+                    for (int m = 0; m < 7; m++)
+                    {
+                        px0 =  __lsx_vld((__m128i *)srcPtrTemp3, 0);
+
+                        px1 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                        px2 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+
+                        px0 =  __lsx_vld((__m128i *)(srcPtrTemp3 + 16), 0);
+
+                        px3 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                        px4 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+
+                        px0 =  __lsx_vld((__m128i *)(srcPtrTemp3 + 32), 0);
+
+                        px5 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                        px6 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+
+                        px0 =  __lsx_vld((__m128i *)(srcPtrTemp3 + 48), 0);
+
+                        px7 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                        px8 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+
+                        pSum1 = __lsx_vadd_h(px1, pSum1);
+                        pSum2 = __lsx_vadd_h(px2, pSum2);
+                        pSum3 = __lsx_vadd_h(px3, pSum3);
+                        pSum4 = __lsx_vadd_h(px4, pSum4);
+                        pSum5 = __lsx_vadd_h(px5, pSum5);
+                        pSum6 = __lsx_vadd_h(px6, pSum6);
+                        pSum7 = __lsx_vadd_h(px7, pSum7);
+                        pSum8 = __lsx_vadd_h(px8, pSum8);
+
+                        srcPtrTemp3 += elementsInRowMax;
+                    }
+
+                    __lsx_vst(pSum1, (__m128i *)vSums, 0);
+                    __lsx_vst(pSum2, (__m128i *)(vSums + 8), 0);
+                    __lsx_vst(pSum3, (__m128i *)(vSums + 16), 0);
+                    __lsx_vst(pSum4, (__m128i *)(vSums + 24), 0);
+                    __lsx_vst(pSum5, (__m128i *)(vSums + 32), 0);
+                    __lsx_vst(pSum6, (__m128i *)(vSums + 40), 0);
+                    __lsx_vst(pSum7, (__m128i *)(vSums + 48), 0);
+                    __lsx_vst(pSum8, (__m128i *)(vSums + 56), 0);
+
+                    hSums[0] = (Rpp16s) RPPPIXELCHECK((Rpp32f) (vSums[0] + vSums[3] + vSums[6] + vSums[9] + vSums[12] + vSums[15] + vSums[18]) * multiplier);
+                    hSums[1] = (Rpp16s) RPPPIXELCHECK((Rpp32f) (vSums[1] + vSums[4] + vSums[7] + vSums[10] + vSums[13] + vSums[16] + vSums[19]) * multiplier);
+                    hSums[2] = (Rpp16s) RPPPIXELCHECK((Rpp32f) (vSums[2] + vSums[5] + vSums[8] + vSums[11] + vSums[14] + vSums[17] + vSums[20]) * multiplier);
+
+                    hSums[3] = (Rpp16s) RPPPIXELCHECK((Rpp32f) (vSums[21] + vSums[24] + vSums[27] + vSums[30] + vSums[33] + vSums[36] + vSums[39]) * multiplier);
+                    hSums[4] = (Rpp16s) RPPPIXELCHECK((Rpp32f) (vSums[22] + vSums[25] + vSums[28] + vSums[31] + vSums[34] + vSums[37] + vSums[40]) * multiplier);
+                    hSums[5] = (Rpp16s) RPPPIXELCHECK((Rpp32f) (vSums[23] + vSums[26] + vSums[29] + vSums[32] + vSums[35] + vSums[38] + vSums[41]) * multiplier);
+
+                    hSums[6] = (Rpp16s) RPPPIXELCHECK((Rpp32f) (vSums[42] + vSums[45] + vSums[48] + vSums[51] + vSums[54] + vSums[57] + vSums[60]) * multiplier);
+                    hSums[7] = (Rpp16s) RPPPIXELCHECK((Rpp32f) (vSums[43] + vSums[46] + vSums[49] + vSums[52] + vSums[55] + vSums[58] + vSums[61]) * multiplier);
+                    hSums[8] = (Rpp16s) RPPPIXELCHECK((Rpp32f) (vSums[44] + vSums[47] + vSums[50] + vSums[53] + vSums[56] + vSums[59] + vSums[62]) * multiplier);
+
+                    px1 = lsx_setr_i16(hSums[0], hSums[1], hSums[2], hSums[0], hSums[1], hSums[2], hSums[0], hSums[1]);
+                    px2 = lsx_setr_i16(hSums[2], hSums[0], hSums[1], hSums[2], hSums[0], hSums[1], hSums[2], hSums[0]);
+                    px3 = lsx_setr_i16(hSums[1], hSums[2], hSums[0], hSums[1], hSums[2], hSums[3], hSums[4], hSums[5]);
+                    px4 = lsx_setr_i16(hSums[3], hSums[4], hSums[5], hSums[3], hSums[4], hSums[5], hSums[3], hSums[4]);
+                    px5 = lsx_setr_i16(hSums[5], hSums[3], hSums[4], hSums[5], hSums[3], hSums[4], hSums[5], hSums[3]);
+                    px6 = lsx_setr_i16(hSums[4], hSums[5], hSums[6], hSums[7], hSums[8], hSums[6], hSums[7], hSums[8]);
+                    px7 = lsx_setr_i16(hSums[6], hSums[7], hSums[8], hSums[6], hSums[7], hSums[8], hSums[6], hSums[7]);
+                    px8 = lsx_setr_i16(hSums[8], hSums[6], hSums[7], hSums[8], hSums[6], hSums[7], hSums[8], hSums[6]);
+
+                    px1 = lsx_packus_i16(px1, px2);
+                    px2 = lsx_packus_i16(px3, px4);
+                    px3 = lsx_packus_i16(px5, px6);
+                    px4 = lsx_packus_i16(px7, px8);
+
+                    for (int m = 0; m < 7; m++)
+                    {
+                        __lsx_vst(px1, (__m128i *)dstPtrTemp3, 0);
+                        __lsx_vst(px2, (__m128i *)(dstPtrTemp3 + 16), 0);
+                        __lsx_vst(px3, (__m128i *)(dstPtrTemp3 + 32), 0);
+                        __lsx_vst(px4, (__m128i *)(dstPtrTemp3 + 48), 0);
+
+                        dstPtrTemp3 += elementsInRowMax;
+                    }
+
+                    srcPtrTemp2 += 63;
+                    dstPtrTemp2 += 63;
+                }
+
+                for (int lengthDiffTemp = lengthDiff; lengthDiffTemp > 7; lengthDiffTemp -= 7)
+                {
+                    pixelate_base_pkd_host(srcPtrImage, elementsInRowMax, dstPtrImage, 7, 7, i + y1, vectorLoopCount + x1, multiplier);
+                    vectorLoopCount += 7;
+                }
+
+                pixelate_base_pkd_host(srcPtrImage, elementsInRowMax, dstPtrImage, 7, lengthDiffNew, i + y1, vectorLoopCount + x1, lastColMultiplier);
+
+                srcPtrTemp += increment;
+                dstPtrTemp += increment;
+            }
+
+            srcPtrTemp2 = srcPtrTemp;
+            dstPtrTemp2 = dstPtrTemp;
+
+            srcPtrTemp3 = srcPtrTemp + roiEndLoc;
+            dstPtrTemp3 = dstPtrTemp + roiEndLoc;
+
+            for (int i2 = 0; i2 < heightDiff; i2++)
+            {
+                memcpy(dstPtrTemp2, srcPtrTemp2, elementsBeforeROI * sizeof(T));
+                memcpy(dstPtrTemp3, srcPtrTemp3, remainingElementsAfterROI * sizeof(T));
+                srcPtrTemp2 += elementsInRowMax;
+                dstPtrTemp2 += elementsInRowMax;
+                srcPtrTemp3 += elementsInRowMax;
+                dstPtrTemp3 += elementsInRowMax;
+            }
+
+            int j = 0;
+            for (; j < roiPoints[batchCount].roiWidth - lengthDiffNew; j += 7)
+            {
+                pixelate_base_pkd_host(srcPtrImage, elementsInRowMax, dstPtrImage, heightDiff, 7, i + y1, j + x1, lastRowMultiplier);
+            }
+
+            pixelate_base_pkd_host(srcPtrImage, elementsInRowMax, dstPtrImage, heightDiff, lengthDiffNew, i + y1, j + x1, lastMultiplier);
+
+            srcPtrTemp += lastIncrement;
+            dstPtrTemp += lastIncrement;
+
+            for (int i = 0; i < remainingHeight; i++)
+            {
+                memcpy(dstPtrTemp, srcPtrTemp, elementsInRow * sizeof(T));
+
+                dstPtrTemp += elementsInRowMax;
+                srcPtrTemp += elementsInRowMax;
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus pixelate_host(T* srcPtr, RppiSize srcSize, T* dstPtr,
+                        RppiChnFormat chnFormat, Rpp32u channel)
+{
+    T *srcPtrTemp, *dstPtrTemp;
+    srcPtrTemp = srcPtr;
+    dstPtrTemp = dstPtr;
+
+    Rpp32u imageDim = srcSize.height * srcSize.width;
+    Rpp32u alignedHeight = (srcSize.height / 7) * 7;
+    Rpp32u heightDiff = srcSize.height - alignedHeight;
+
+    Rpp32f multiplier = 1.0 / 49;
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        Rpp32u increment = srcSize.width * 7;
+        Rpp32u bufferLength = srcSize.width;
+        Rpp32u alignedLength = (bufferLength / 14) * 14;
+        if (alignedLength + 2 > bufferLength)
+            alignedLength -= 14;
+        Rpp32u lengthDiff = bufferLength - alignedLength;
+        Rpp32u lengthDiffNew = lengthDiff;
+        if (lengthDiff > 7)
+            lengthDiffNew -= 7;
+
+        Rpp32f lastColMultiplier = 1.0 / (lengthDiffNew * 7);
+        Rpp32f lastRowMultiplier = 1.0 / (heightDiff * 7);
+        Rpp32f lastMultiplier = 1.0 / (heightDiff * lengthDiffNew);
+
+        Rpp16u vSums[16] = {0};
+        Rpp16u hSums[2] = {0};
+
+        __m128i const zero = __lsx_vldi(0);
+        __m128i px0, px1, px2;
+        __m128i pSum1, pSum2;
+
+        for (int c = 0; c < channel; c++)
+        {
+            T *srcPtrTemp = srcPtr + (c * imageDim);
+            T *dstPtrTemp = dstPtr + (c * imageDim);
+
+            T *srcPtrTemp2, *dstPtrTemp2, *srcPtrTemp3, *dstPtrTemp3, *srcPtrTemp4, *dstPtrTemp4;
+            srcPtrTemp2 = srcPtrTemp;
+            dstPtrTemp2 = dstPtrTemp;
+
+            int i = 0;
+            for (; i < alignedHeight; i += 7)
+            {
+                srcPtrTemp3 = srcPtrTemp2;
+                dstPtrTemp3 = dstPtrTemp2;
+
+                Rpp32u vectorLoopCount = 0;
+                for (; vectorLoopCount < alignedLength; vectorLoopCount+=14)
+                {
+                    srcPtrTemp4 = srcPtrTemp3;
+                    dstPtrTemp4 = dstPtrTemp3;
+
+                    pSum1 = __lsx_vreplgr2vr_h(0);
+                    pSum2 = __lsx_vreplgr2vr_h(0);
+
+                    for (int m = 0; m < 7; m++)
+                    {
+                        px0 =  __lsx_vld((__m128i *)srcPtrTemp4, 0);
+
+                        px1 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                        px2 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+
+                        pSum1 = __lsx_vadd_h(px1, pSum1);
+                        pSum2 = __lsx_vadd_h(px2, pSum2);
+
+                        srcPtrTemp4 += srcSize.width;
+                    }
+
+                    __lsx_vst(pSum1, (__m128i *)vSums, 0);
+                    __lsx_vst(pSum2, (__m128i *)(vSums + 8), 0);
+
+                    hSums[0] = vSums[0] + vSums[1] + vSums[2] + vSums[3] + vSums[4] + vSums[5] + vSums[6];
+                    hSums[1] = vSums[7] + vSums[8] + vSums[9] + vSums[10] + vSums[11] + vSums[12] + vSums[13];
+
+                    px1 = __lsx_vreplgr2vr_h((Rpp16s) RPPPIXELCHECK((Rpp32f) hSums[0] * multiplier));
+                    px2 = __lsx_vreplgr2vr_h((Rpp16s) RPPPIXELCHECK((Rpp32f) hSums[1] * multiplier));
+
+                    px0 = lsx_packus_i16(px1, px2);
+
+                    for (int m = 0; m < 7; m++)
+                    {
+                        __lsx_vst(px0, (__m128i *)dstPtrTemp4, 0);
+                        dstPtrTemp4 += srcSize.width;
+                    }
+
+                    srcPtrTemp3 += 14;
+                    dstPtrTemp3 += 14;
+                }
+
+                if (lengthDiff > 7)
+                {
+                    pixelate_base_pln_host(srcPtrTemp, srcSize.width, dstPtrTemp, 7, 7, i, vectorLoopCount, multiplier);
+                    vectorLoopCount += 7;
+                }
+
+                pixelate_base_pln_host(srcPtrTemp, srcSize.width, dstPtrTemp, 7, lengthDiffNew, i, vectorLoopCount, lastColMultiplier);
+
+                srcPtrTemp2 += increment;
+                dstPtrTemp2 += increment;
+            }
+
+            int j = 0;
+            for (; j < srcSize.width - lengthDiffNew; j += 7)
+            {
+                pixelate_base_pln_host(srcPtrTemp, srcSize.width, dstPtrTemp, heightDiff, 7, i, j, lastRowMultiplier);
+            }
+
+            pixelate_base_pln_host(srcPtrTemp, srcSize.width, dstPtrTemp, heightDiff, lengthDiffNew, i, j, lastMultiplier);
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        Rpp32u elementsInRow = channel * srcSize.width;
+        Rpp32u increment = elementsInRow * 7;
+
+        Rpp32u bufferLength = srcSize.width;
+        Rpp32u alignedLength = (bufferLength / 21) * 21;
+        if (alignedLength + 1 > bufferLength)
+            alignedLength -= 21;
+        Rpp32u lengthDiff = bufferLength - alignedLength;
+        Rpp32u lengthDiffNew = lengthDiff;
+        if (lengthDiff > 7)
+            lengthDiffNew -= 7;
+
+        Rpp32f lastColMultiplier = 1.0 / (lengthDiffNew * 7);
+        Rpp32f lastRowMultiplier = 1.0 / (heightDiff * 7);
+        Rpp32f lastMultiplier = 1.0 / (heightDiff * lengthDiffNew);
+
+        Rpp16u vSums[64] = {0};
+        Rpp16u hSums[9] = {0};
+
+        __m128i const zero = __lsx_vldi(0);
+        __m128i px0, px1, px2, px3, px4, px5, px6, px7, px8;
+        __m128i pSum1, pSum2, pSum3, pSum4, pSum5, pSum6, pSum7, pSum8;
+
+        T *srcPtrTemp2, *dstPtrTemp2, *srcPtrTemp3, *dstPtrTemp3;
+        srcPtrTemp = srcPtr;
+        dstPtrTemp = dstPtr;
+
+        int i = 0;
+        for (; i < alignedHeight; i += 7)
+        {
+            srcPtrTemp2 = srcPtrTemp;
+            dstPtrTemp2 = dstPtrTemp;
+
+            Rpp32u vectorLoopCount = 0;
+            for (; vectorLoopCount < alignedLength; vectorLoopCount+=21)
+            {
+                srcPtrTemp3 = srcPtrTemp2;
+                dstPtrTemp3 = dstPtrTemp2;
+
+                pSum1 = __lsx_vreplgr2vr_h(0);
+                pSum2 = __lsx_vreplgr2vr_h(0);
+                pSum3 = __lsx_vreplgr2vr_h(0);
+                pSum4 = __lsx_vreplgr2vr_h(0);
+                pSum5 = __lsx_vreplgr2vr_h(0);
+                pSum6 = __lsx_vreplgr2vr_h(0);
+                pSum7 = __lsx_vreplgr2vr_h(0);
+                pSum8 = __lsx_vreplgr2vr_h(0);
+
+                for (int m = 0; m < 7; m++)
+                {
+                    px0 =  __lsx_vld((__m128i *)srcPtrTemp3, 0);
+
+                    px1 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                    px2 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+
+                    px0 =  __lsx_vld((__m128i *)(srcPtrTemp3 + 16), 0);
+
+                    px3 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                    px4 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+
+                    px0 =  __lsx_vld((__m128i *)(srcPtrTemp3 + 32), 0);
+
+                    px5 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                    px6 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+
+                    px0 =  __lsx_vld((__m128i *)(srcPtrTemp3 + 48), 0);
+
+                    px7 = __lsx_vilvl_b(zero, px0);    // pixels 0-7
+                    px8 = __lsx_vilvh_b(zero, px0);    // pixels 8-15
+
+                    pSum1 = __lsx_vadd_h(px1, pSum1);
+                    pSum2 = __lsx_vadd_h(px2, pSum2);
+                    pSum3 = __lsx_vadd_h(px3, pSum3);
+                    pSum4 = __lsx_vadd_h(px4, pSum4);
+                    pSum5 = __lsx_vadd_h(px5, pSum5);
+                    pSum6 = __lsx_vadd_h(px6, pSum6);
+                    pSum7 = __lsx_vadd_h(px7, pSum7);
+                    pSum8 = __lsx_vadd_h(px8, pSum8);
+
+                    srcPtrTemp3 += elementsInRow;
+                }
+
+                __lsx_vst(pSum1, (__m128i *)vSums, 0);
+                __lsx_vst(pSum2, (__m128i *)(vSums + 8), 0);
+                __lsx_vst(pSum3, (__m128i *)(vSums + 16), 0);
+                __lsx_vst(pSum4, (__m128i *)(vSums + 24), 0);
+                __lsx_vst(pSum5, (__m128i *)(vSums + 32), 0);
+                __lsx_vst(pSum6, (__m128i *)(vSums + 40), 0);
+                __lsx_vst(pSum7, (__m128i *)(vSums + 48), 0);
+                __lsx_vst(pSum8, (__m128i *)(vSums + 56), 0);
+
+                hSums[0] = (Rpp16s) RPPPIXELCHECK((Rpp32f) (vSums[0] + vSums[3] + vSums[6] + vSums[9] + vSums[12] + vSums[15] + vSums[18]) * multiplier);
+                hSums[1] = (Rpp16s) RPPPIXELCHECK((Rpp32f) (vSums[1] + vSums[4] + vSums[7] + vSums[10] + vSums[13] + vSums[16] + vSums[19]) * multiplier);
+                hSums[2] = (Rpp16s) RPPPIXELCHECK((Rpp32f) (vSums[2] + vSums[5] + vSums[8] + vSums[11] + vSums[14] + vSums[17] + vSums[20]) * multiplier);
+
+                hSums[3] = (Rpp16s) RPPPIXELCHECK((Rpp32f) (vSums[21] + vSums[24] + vSums[27] + vSums[30] + vSums[33] + vSums[36] + vSums[39]) * multiplier);
+                hSums[4] = (Rpp16s) RPPPIXELCHECK((Rpp32f) (vSums[22] + vSums[25] + vSums[28] + vSums[31] + vSums[34] + vSums[37] + vSums[40]) * multiplier);
+                hSums[5] = (Rpp16s) RPPPIXELCHECK((Rpp32f) (vSums[23] + vSums[26] + vSums[29] + vSums[32] + vSums[35] + vSums[38] + vSums[41]) * multiplier);
+
+                hSums[6] = (Rpp16s) RPPPIXELCHECK((Rpp32f) (vSums[42] + vSums[45] + vSums[48] + vSums[51] + vSums[54] + vSums[57] + vSums[60]) * multiplier);
+                hSums[7] = (Rpp16s) RPPPIXELCHECK((Rpp32f) (vSums[43] + vSums[46] + vSums[49] + vSums[52] + vSums[55] + vSums[58] + vSums[61]) * multiplier);
+                hSums[8] = (Rpp16s) RPPPIXELCHECK((Rpp32f) (vSums[44] + vSums[47] + vSums[50] + vSums[53] + vSums[56] + vSums[59] + vSums[62]) * multiplier);
+
+                px1 = lsx_setr_i16(hSums[0], hSums[1], hSums[2], hSums[0], hSums[1], hSums[2], hSums[0], hSums[1]);
+                px2 = lsx_setr_i16(hSums[2], hSums[0], hSums[1], hSums[2], hSums[0], hSums[1], hSums[2], hSums[0]);
+                px3 = lsx_setr_i16(hSums[1], hSums[2], hSums[0], hSums[1], hSums[2], hSums[3], hSums[4], hSums[5]);
+                px4 = lsx_setr_i16(hSums[3], hSums[4], hSums[5], hSums[3], hSums[4], hSums[5], hSums[3], hSums[4]);
+                px5 = lsx_setr_i16(hSums[5], hSums[3], hSums[4], hSums[5], hSums[3], hSums[4], hSums[5], hSums[3]);
+                px6 = lsx_setr_i16(hSums[4], hSums[5], hSums[6], hSums[7], hSums[8], hSums[6], hSums[7], hSums[8]);
+                px7 = lsx_setr_i16(hSums[6], hSums[7], hSums[8], hSums[6], hSums[7], hSums[8], hSums[6], hSums[7]);
+                px8 = lsx_setr_i16(hSums[8], hSums[6], hSums[7], hSums[8], hSums[6], hSums[7], hSums[8], hSums[6]);
+
+                px1 = lsx_packus_i16(px1, px2);
+                px2 = lsx_packus_i16(px3, px4);
+                px3 = lsx_packus_i16(px5, px6);
+                px4 = lsx_packus_i16(px7, px8);
+
+                for (int m = 0; m < 7; m++)
+                {
+                    __lsx_vst(px1, (__m128i *)dstPtrTemp3, 0);
+                    __lsx_vst(px2, (__m128i *)(dstPtrTemp3 + 16), 0);
+                    __lsx_vst(px3, (__m128i *)(dstPtrTemp3 + 32), 0);
+                    __lsx_vst(px4, (__m128i *)(dstPtrTemp3 + 48), 0);
+
+                    dstPtrTemp3 += elementsInRow;
+                }
+
+                srcPtrTemp2 += 63;
+                dstPtrTemp2 += 63;
+            }
+
+            for (int lengthDiffTemp = lengthDiff; lengthDiffTemp > 7; lengthDiffTemp -= 7)
+            {
+                pixelate_base_pkd_host(srcPtr, elementsInRow, dstPtr, 7, 7, i, vectorLoopCount, multiplier);
+                vectorLoopCount += 7;
+            }
+
+            pixelate_base_pkd_host(srcPtr, elementsInRow, dstPtr, 7, lengthDiffNew, i, vectorLoopCount, lastColMultiplier);
+
+
+            srcPtrTemp += increment;
+            dstPtrTemp += increment;
+        }
+
+        int j = 0;
+        for (; j < srcSize.width - lengthDiffNew; j += 7)
+        {
+            pixelate_base_pkd_host(srcPtr, elementsInRow, dstPtr, heightDiff, 7, i, j, lastRowMultiplier);
+        }
+
+        pixelate_base_pkd_host(srcPtr, elementsInRow, dstPtr, heightDiff, lengthDiffNew, i, j, lastMultiplier);
+    }
+
+    return RPP_SUCCESS;
+}
+
+/**************** Fog ***************/
+
+template <typename T>
+RppStatus fog_host(  T* temp,RppiSize srcSize,T* srcPtr,
+                    Rpp32f fogValue,
+                    RppiChnFormat chnFormat,   unsigned int channel)
+{
+    Rpp8u *srcPtr1;
+    srcPtr1 = srcPtr;
+    for(int i = 0;i < srcSize.height * srcSize.width * channel;i++)
+    {
+        *srcPtr1 = *temp;
+        srcPtr1++;
+        temp++;
+    }
+
+    if(fogValue != 0)
+    {
+        if (chnFormat == RPPI_CHN_PLANAR)
+        {
+            Rpp8u *srcPtr1, *srcPtr2;
+            if(channel > 1)
+            {
+                srcPtr1 = srcPtr + (srcSize.width * srcSize.height);
+                srcPtr2 = srcPtr + (srcSize.width * srcSize.height * 2);
+            }
+            for (int i = 0; i < (srcSize.width * srcSize.height); i++)
+            {
+                Rpp32f check= *srcPtr;
+                if(channel > 1)
+                    check = (check + *srcPtr1 + *srcPtr2) / 3;
+                *srcPtr = fogGenerator(*srcPtr, fogValue, 1, check);
+                srcPtr++;
+                if(channel > 1)
+                {
+                    *srcPtr1 = fogGenerator(*srcPtr1, fogValue, 2, check);
+                    *srcPtr2 = fogGenerator(*srcPtr2, fogValue, 3, check);
+                    srcPtr1++;
+                    srcPtr2++;
+                }
+            }
+        }
+        else
+        {
+            Rpp8u *srcPtr1, *srcPtr2;
+            srcPtr1 = srcPtr + 1;
+            srcPtr2 = srcPtr + 2;
+            for (int i = 0; i < (srcSize.width * srcSize.height * channel); i += 3)
+            {
+                Rpp32f check = (*srcPtr + *srcPtr1 + *srcPtr2) / 3;
+                *srcPtr = fogGenerator(*srcPtr, fogValue, 1, check);
+                *srcPtr1 = fogGenerator(*srcPtr1, fogValue, 2, check);
+                *srcPtr2 = fogGenerator(*srcPtr2, fogValue, 3, check);
+                srcPtr += 3;
+                srcPtr1 += 3;
+                srcPtr2 += 3;
+            }
+        }
+    }
+    return RPP_SUCCESS;
+
+}
+
+template <typename T>
+RppStatus fog_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                              Rpp32f *batch_fogValue,
+                              Rpp32u nbatchSize,
+                              RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32f fogValue = batch_fogValue[batchCount];
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+            {
+                for(int j = 0; j < batch_srcSize[batchCount].width; j++)
+                {
+                    if (fogValue >= 0)
+                    {
+                        if (channel == 3)
+                        {
+                            Rpp8u dstPtrTemp1, dstPtrTemp2, dstPtrTemp3;
+                            dstPtrTemp1 = *(srcPtrImage + i * batch_srcSizeMax[batchCount].width + j);
+                            dstPtrTemp2 = *(srcPtrImage + i * batch_srcSizeMax[batchCount].width + j + batch_srcSizeMax[batchCount].width * batch_srcSizeMax[batchCount].height);
+                            dstPtrTemp3 = *(srcPtrImage + i * batch_srcSizeMax[batchCount].width + j + 2 * batch_srcSizeMax[batchCount].width * batch_srcSizeMax[batchCount].height);
+                            Rpp32f check = (dstPtrTemp3 + dstPtrTemp1 + dstPtrTemp2) / 3;
+                            *(dstPtrImage  + i * batch_srcSizeMax[batchCount].width + j) = fogGenerator(dstPtrTemp1, fogValue, 1, check);
+                            *(dstPtrImage + i * batch_srcSizeMax[batchCount].width + j + batch_srcSizeMax[batchCount].width * batch_srcSizeMax[batchCount].height) = fogGenerator(dstPtrTemp2, fogValue, 2, check);
+                            *(dstPtrImage + i * batch_srcSizeMax[batchCount].width + j + 2 * batch_srcSizeMax[batchCount].width * batch_srcSizeMax[batchCount].height) = fogGenerator(dstPtrTemp3, fogValue, 3, check);
+                        }
+                        if(channel == 1)
+                        {
+                            Rpp32f check = *(srcPtrImage + i * batch_srcSizeMax[batchCount].width + j);
+                            *(dstPtrImage  + i * batch_srcSizeMax[batchCount].width + j) = fogGenerator(check, fogValue, 1, check);
+                        }
+                    }
+                    else
+                    {
+                        if (channel == 3)
+                        {
+                            *(dstPtrImage  + i * batch_srcSizeMax[batchCount].width + j) = *(srcPtrImage + i * batch_srcSizeMax[batchCount].width + j);
+                            *(dstPtrImage + i * batch_srcSizeMax[batchCount].width + j + batch_srcSizeMax[batchCount].width * batch_srcSizeMax[batchCount].height) = *(srcPtrImage + i * batch_srcSizeMax[batchCount].width + j + batch_srcSizeMax[batchCount].width * batch_srcSizeMax[batchCount].height);
+                            *(dstPtrImage + i * batch_srcSizeMax[batchCount].width + j + 2 * batch_srcSizeMax[batchCount].width * batch_srcSizeMax[batchCount].height) = *(srcPtrImage + i * batch_srcSizeMax[batchCount].width + j + 2 * batch_srcSizeMax[batchCount].width * batch_srcSizeMax[batchCount].height);
+                        }
+                        if(channel == 1)
+                        {
+                            *(dstPtrImage  + i * batch_srcSizeMax[batchCount].width + j) = *(srcPtrImage + i * batch_srcSizeMax[batchCount].width + j);
+                        }
+                    }
+                }
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32f fogValue = batch_fogValue[batchCount];
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            Rpp32u elementsInRow = channel * batch_srcSize[batchCount].width;
+            Rpp32u elementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+            T *srcPtrTemp1, *dstPtrTemp1;
+            srcPtrTemp1 = srcPtrImage;
+            dstPtrTemp1 = dstPtrImage;
+
+            for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+            {
+                Rpp32f pixel;
+
+                T *srcPtrTemp, *dstPtrTemp;
+                srcPtrTemp = srcPtrImage + (i * elementsInRowMax);
+                dstPtrTemp = dstPtrImage + (i * elementsInRowMax);
+
+                for(int j = 0; j < batch_srcSize[batchCount].width; j++)
+                {
+                    if(fogValue <= 0)
+                    {
+                        for(int i = 0;i < channel;i++)
+                        {
+                            *dstPtrTemp = *srcPtrTemp;
+                            dstPtrTemp++;
+                            srcPtrTemp++;
+                        }
+                    }
+                    if(fogValue != 0)
+                    {
+                        Rpp8u dstPtrTemp1, dstPtrTemp2, dstPtrTemp3;
+                        dstPtrTemp1 = *srcPtrTemp++;
+                        dstPtrTemp2 = *srcPtrTemp++;
+                        dstPtrTemp3 = *srcPtrTemp++;
+                        Rpp32f check = (dstPtrTemp3 + dstPtrTemp1 + dstPtrTemp2) / 3;
+                        *dstPtrTemp = fogGenerator(dstPtrTemp1, fogValue, 1, check);
+                        *(dstPtrTemp+1) = fogGenerator(dstPtrTemp2, fogValue, 2, check);
+                        *(dstPtrTemp+2) = fogGenerator(dstPtrTemp3, fogValue, 3, check);
+                        dstPtrTemp += channel;
+                    }
+                }
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+/**************** Noise ***************/
+
+template <typename T>
+RppStatus noise_host(T* srcPtr, RppiSize srcSize, T* dstPtr,
+                        Rpp32f noiseProbability,
+                        RppiChnFormat chnFormat, unsigned int channel)
+{
+    T *dstPtrTemp;
+
+    srand(time(0));
+    int seedVal[5000];
+
+    for (int i = 0; i < 5000; i++)
+    {
+        seedVal[i] = rand() % (65536);
+    }
+
+    memcpy(dstPtr, srcPtr, channel * srcSize.width * srcSize.height * sizeof(T));
+
+    Rpp32u imageDim = srcSize.width * srcSize.height * channel;
+
+    if(noiseProbability != 0)
+    {
+        Rpp32u noisePixel = (Rpp32u)(noiseProbability * srcSize.width * srcSize.height );
+        Rpp32u pixelDistance = (srcSize.width * srcSize.height) / noisePixel;
+        if(chnFormat == RPPI_CHN_PACKED)
+        {
+            int count = 0;
+            Rpp32u increment = pixelDistance * channel;
+            Rpp32u limit = imageDim - (2 * increment);
+
+            for(int i = 0 ; i < limit ; i += increment)
+            {
+                wyhash16_x = seedVal[count];
+                Rpp32u initialPixel = rand_range16((uint16_t) pixelDistance);
+                dstPtrTemp = dstPtr + (initialPixel * channel);
+                Rpp8u newPixel = rand_range16(2) ? 0 : 255;
+                for(int j = 0 ; j < channel ; j++)
+                {
+                    *dstPtrTemp = newPixel;
+                    dstPtrTemp++;
+                }
+                dstPtr += increment;
+                (count == 5000) ? count = 0 : count++;
+            }
+        }
+        else if(chnFormat == RPPI_CHN_PLANAR)
+        {
+            int count = 0;
+            Rpp32u increment = pixelDistance;
+            Rpp32u limit = imageDim - (2 * increment);
+
+            if(channel == 3)
+            {
+                Rpp8u *dstPtrTemp1,*dstPtrTemp2;
+                dstPtrTemp1 = dstPtr + (srcSize.height * srcSize.width);
+                dstPtrTemp2 = dstPtr + (2 * srcSize.height * srcSize.width);
+                for(int i = 0 ; i < limit ; i += pixelDistance)
+                {
+                    Rpp32u initialPixel = rand() % pixelDistance;
+                    dstPtr += initialPixel;
+                    Rpp8u newPixel = (rand() % 2) ? 255 : 1;
+                    *dstPtr = newPixel;
+                    dstPtr += ((pixelDistance - initialPixel - 1));
+
+                    dstPtrTemp1 += initialPixel;
+                    *dstPtrTemp1 = newPixel;
+                    dstPtrTemp1 += ((pixelDistance - initialPixel - 1));
+
+                    dstPtrTemp2 += initialPixel;
+                    *dstPtrTemp2 = newPixel;
+                    dstPtrTemp2 += ((pixelDistance - initialPixel - 1));
+
+                }
+            }
+            else
+            {
+                for(int i = 0 ; i < srcSize.width * srcSize.height ; i += pixelDistance)
+                {
+                    wyhash16_x = seedVal[count];
+                    Rpp32u initialPixel = rand_range16((uint16_t) pixelDistance);//rand() % pixelDistance;
+                    dstPtrTemp = dstPtr + initialPixel;
+                    Rpp8u newPixel = rand_range16(2) ? 255 : 1;
+                    *dstPtrTemp = newPixel;
+                    dstPtr += increment;
+                    (count == 5000) ? count = 0 : count++;
+                }
+            }
+
+        }
+    }
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus noise_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                           Rpp32f *batch_noiseProbability,
+                           RppiROI *roiPoints, Rpp32u nbatchSize,
+                           RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        T *srcPtrBufferROI, *dstPtrBufferROI;
+        srcPtrBufferROI = (T*) calloc(channel * batch_srcSizeMax[0].height * batch_srcSizeMax[0].width * nbatchSize, sizeof(T));
+        dstPtrBufferROI = (T*) calloc(channel * batch_srcSizeMax[0].height * batch_srcSizeMax[0].width * nbatchSize, sizeof(T));
+
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32u x1 = roiPoints[batchCount].x;
+            Rpp32u y1 = roiPoints[batchCount].y;
+            Rpp32u x2 = x1 + roiPoints[batchCount].roiWidth - 1;
+            Rpp32u y2 = y1 + roiPoints[batchCount].roiHeight - 1;
+            if (x2 == -1)
+            {
+                x2 = batch_srcSize[batchCount].width - 1;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == -1)
+            {
+                y2 = batch_srcSize[batchCount].height - 1;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            RppiSize roiSize;
+            roiSize.height = roiPoints[batchCount].roiHeight;
+            roiSize.width = roiPoints[batchCount].roiWidth;
+
+            Rpp32u remainingElementsAfterROI = (batch_srcSize[batchCount].width - (roiPoints[batchCount].x + roiPoints[batchCount].roiWidth));
+            Rpp32u remainingElementsAfterROIMax = (batch_srcSizeMax[batchCount].width - (roiPoints[batchCount].x + roiPoints[batchCount].roiWidth));
+            Rpp32u remainingHeight = batch_srcSize[batchCount].height - (y2 + 1);
+
+            Rpp32f noiseProbability = batch_noiseProbability[batchCount];
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            Rpp32u imageDimROI = roiSize.width * roiSize.height;
+
+            T *srcPtrImageROI, *dstPtrImageROI;
+            srcPtrImageROI = srcPtrBufferROI + loc;
+            dstPtrImageROI = dstPtrBufferROI + loc;
+
+            for(int c = 0; c < channel; c++)
+            {
+                T *srcPtrChannel, *dstPtrChannel;
+                srcPtrChannel = srcPtrImage + (c * imageDimMax);
+                dstPtrChannel = dstPtrImage + (c * imageDimMax);
+
+                T *srcPtrTemp, *dstPtrTemp, *srcPtrTemp2, *dstPtrTemp2;
+                srcPtrTemp = srcPtrChannel;
+                dstPtrTemp = dstPtrChannel;
+
+                T *srcPtrImageROITemp, *dstPtrImageROITemp;
+                srcPtrImageROITemp = srcPtrImageROI;
+                dstPtrImageROITemp = dstPtrImageROI;
+
+                for (int i = 0; i < y1; i++)
+                {
+                    memcpy(dstPtrTemp, srcPtrTemp, batch_srcSize[batchCount].width * sizeof(T));
+
+                    dstPtrTemp += batch_srcSizeMax[batchCount].width;
+                    srcPtrTemp += batch_srcSizeMax[batchCount].width;
+                }
+
+                srcPtrTemp2 = srcPtrTemp + x1;
+
+                for (int i = 0; i < roiSize.height; i++)
+                {
+                    memcpy(srcPtrImageROITemp, srcPtrTemp2, roiSize.width * sizeof(T));
+                    srcPtrTemp2 += batch_srcSizeMax[batchCount].width;
+                    srcPtrImageROITemp += roiSize.width;
+                }
+
+                noise_host(srcPtrImageROI, roiSize, dstPtrImageROI, noiseProbability, chnFormat, 1);
+
+                for (int i = 0; i < roiSize.height; i++)
+                {
+                    memcpy(dstPtrTemp, srcPtrTemp, x1 * sizeof(T));
+                    srcPtrTemp += x1;
+                    dstPtrTemp += x1;
+                    memcpy(dstPtrTemp, dstPtrImageROITemp, roiSize.width * sizeof(T));
+                    srcPtrTemp += roiSize.width;
+                    dstPtrTemp += roiSize.width;
+                    dstPtrImageROITemp += roiSize.width;
+                    memcpy(dstPtrTemp, srcPtrTemp, remainingElementsAfterROI * sizeof(T));
+                    srcPtrTemp += remainingElementsAfterROIMax;
+                    dstPtrTemp += remainingElementsAfterROIMax;
+                }
+
+                for (int i = 0; i < remainingHeight; i++)
+                {
+                    memcpy(dstPtrTemp, srcPtrTemp, batch_srcSize[batchCount].width * sizeof(T));
+
+                    dstPtrTemp += batch_srcSizeMax[batchCount].width;
+                    srcPtrTemp += batch_srcSizeMax[batchCount].width;
+                }
+            }
+        }
+
+        free(srcPtrBufferROI);
+        free(dstPtrBufferROI);
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        T *srcPtrBufferROI, *dstPtrBufferROI;
+        srcPtrBufferROI = (T*) calloc(channel * batch_srcSizeMax[0].height * batch_srcSizeMax[0].width * nbatchSize, sizeof(T));
+        dstPtrBufferROI = (T*) calloc(channel * batch_srcSizeMax[0].height * batch_srcSizeMax[0].width * nbatchSize, sizeof(T));
+
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32u x1 = roiPoints[batchCount].x;
+            Rpp32u y1 = roiPoints[batchCount].y;
+            Rpp32u x2 = x1 + roiPoints[batchCount].roiWidth - 1;
+            Rpp32u y2 = y1 + roiPoints[batchCount].roiHeight - 1;
+            if (x2 == -1)
+            {
+                x2 = batch_srcSize[batchCount].width - 1;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == -1)
+            {
+                y2 = batch_srcSize[batchCount].height - 1;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            RppiSize roiSize;
+            roiSize.height = roiPoints[batchCount].roiHeight;
+            roiSize.width = roiPoints[batchCount].roiWidth;
+
+            Rpp32u elementsBeforeROI = channel * x1;
+            Rpp32u remainingElementsAfterROI = channel * (batch_srcSize[batchCount].width - (roiPoints[batchCount].x + roiPoints[batchCount].roiWidth));
+            Rpp32u remainingElementsAfterROIMax = channel * (batch_srcSizeMax[batchCount].width - (roiPoints[batchCount].x + roiPoints[batchCount].roiWidth));
+            Rpp32u remainingHeight = batch_srcSize[batchCount].height - (y2 + 1);
+
+            Rpp32f noiseProbability = batch_noiseProbability[batchCount];
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            Rpp32u imageDimROI = channel * roiSize.width * roiSize.height;
+
+            T *srcPtrImageROI, *dstPtrImageROI;
+            srcPtrImageROI = srcPtrBufferROI + loc;
+            dstPtrImageROI = dstPtrBufferROI + loc;
+
+            T *srcPtrImageROITemp, *dstPtrImageROITemp;
+            srcPtrImageROITemp = srcPtrImageROI;
+            dstPtrImageROITemp = dstPtrImageROI;
+
+            Rpp32u elementsInRow = channel * batch_srcSize[batchCount].width;
+            Rpp32u elementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+            Rpp32u elementsInRowROI = channel * roiSize.width;
+
+            T *srcPtrTemp, *dstPtrTemp, *srcPtrTemp2, *dstPtrTemp2;
+            srcPtrTemp = srcPtrImage;
+            dstPtrTemp = dstPtrImage;
+
+            for (int i = 0; i < y1; i++)
+            {
+                memcpy(dstPtrTemp, srcPtrTemp, elementsInRow * sizeof(T));
+
+                dstPtrTemp += elementsInRowMax;
+                srcPtrTemp += elementsInRowMax;
+            }
+
+            srcPtrTemp2 = srcPtrTemp + elementsBeforeROI;
+
+            for (int i = 0; i < roiSize.height; i++)
+            {
+                memcpy(srcPtrImageROITemp, srcPtrTemp2, elementsInRowROI * sizeof(T));
+                srcPtrTemp2 += elementsInRowMax;
+                srcPtrImageROITemp += elementsInRowROI;
+            }
+
+            noise_host(srcPtrImageROI, roiSize, dstPtrImageROI, noiseProbability, chnFormat, 3);
+
+            for (int i = 0; i < roiSize.height; i++)
+            {
+                memcpy(dstPtrTemp, srcPtrTemp, elementsBeforeROI * sizeof(T));
+                srcPtrTemp += elementsBeforeROI;
+                dstPtrTemp += elementsBeforeROI;
+                memcpy(dstPtrTemp, dstPtrImageROITemp, elementsInRowROI * sizeof(T));
+                srcPtrTemp += elementsInRowROI;
+                dstPtrTemp += elementsInRowROI;
+                dstPtrImageROITemp += elementsInRowROI;
+                memcpy(dstPtrTemp, srcPtrTemp, remainingElementsAfterROI * sizeof(T));
+                srcPtrTemp += remainingElementsAfterROIMax;
+                dstPtrTemp += remainingElementsAfterROIMax;
+            }
+
+            for (int i = 0; i < remainingHeight; i++)
+            {
+                memcpy(dstPtrTemp, srcPtrTemp, elementsInRow * sizeof(T));
+
+                dstPtrTemp += elementsInRowMax;
+                srcPtrTemp += elementsInRowMax;
+            }
+        }
+
+        free(srcPtrBufferROI);
+        free(dstPtrBufferROI);
+    }
+
+    return RPP_SUCCESS;
+}
+
+/**************** Snow ***************/
+
+template <typename T>
+RppStatus snow_host(T* srcPtr, RppiSize srcSize, T* dstPtr,
+                    Rpp32f strength,
+                    RppiChnFormat chnFormat, Rpp32u channel)
+{
+    strength = strength/100;
+    int snow_mat[5][5] = {{0,50,75,50,0}, {40,80,120,80,40}, {75,120,255,120,75}, {40,80,120,80,40}, {0,50,75,50,0}};
+
+    Rpp32u snowDrops = (Rpp32u)(strength * srcSize.width * srcSize.height * channel );
+
+    T *dstptrtemp;
+    dstptrtemp=dstPtr;
+    for(int k=0;k<srcSize.height*srcSize.width*channel;k++)
+    {
+        *dstptrtemp = 0;
+        dstptrtemp++;
+    }
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        for(int i = 0 ; i < snowDrops ; i++)
+        {
+            Rpp32u row = rand() % srcSize.height;
+            Rpp32u column = rand() % srcSize.width;
+            Rpp32f pixel;
+            for(int k = 0;k < channel;k++)
+            {
+                dstPtr[(row * srcSize.width) + (column) + (k * srcSize.height * srcSize.width)] = snow_mat[0][0] ;
+            }
+            for(int j = 0;j < 5;j++)
+            {
+                if(row + 5 < srcSize.height && row + 5 > 0 )
+                for(int k = 0;k < channel;k++)
+                {
+                    for(int m = 0;m < 5;m++)
+                    {
+                        if (column + 5 < srcSize.width && column + 5 > 0)
+                        {
+                            dstPtr[(row * srcSize.width) + (column) + (k * srcSize.height * srcSize.width) + (srcSize.width * j) + m] = snow_mat[j][m] ;
+                        }
+                    }
+                }
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        for(int i = 0 ; i < snowDrops ; i++)
+        {
+            Rpp32u row = rand() % srcSize.height;
+            Rpp32u column = rand() % srcSize.width;
+            Rpp32f pixel;
+            for(int k = 0;k < channel;k++)
+            {
+                dstPtr[(channel * row * srcSize.width) + (column * channel) + k] = snow_mat[0][0] ;
+            }
+            for(int j = 0;j < 5;j++)
+            {
+                if(row + 5 < srcSize.height && row + 5 > 0 )
+                for(int k = 0;k < channel;k++)
+                {
+                    for(int m = 0;m < 5;m++)
+                    {
+                        if (column + 5 < srcSize.width && column + 5 > 0)
+                        {
+                            dstPtr[(channel * row * srcSize.width) + (column * channel) + k + (channel * srcSize.width * j) + (channel * m)] = snow_mat[j][m];
+                        }
+                    }
+                }
+            }
+        }
+    }
+
+    for (int i = 0; i < (channel * srcSize.width * srcSize.height); i++)
+    {
+        Rpp32u pixel = ((Rpp32u) srcPtr[i]) + (Rpp32u)dstPtr[i];
+        dstPtr[i] = RPPPIXELCHECK(pixel);
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus snow_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                              Rpp32f *batch_strength,
+                              RppiROI *roiPoints, Rpp32u nbatchSize,
+                              RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32f x1 = roiPoints[batchCount].x;
+            Rpp32f y1 = roiPoints[batchCount].y;
+            Rpp32f x2 = x1 + roiPoints[batchCount].roiWidth;
+            Rpp32f y2 = y1 + roiPoints[batchCount].roiHeight;
+            if (x2 == 0) x2 = batch_srcSize[batchCount].width;
+            if (y2 == 0) y2 = batch_srcSize[batchCount].height;
+
+            Rpp32f strength = batch_strength[batchCount];
+
+            strength = strength/100;
+            int snow_mat[5][5] = {{0,50,75,50,0}, {40,80,120,80,40}, {75,120,255,120,75}, {40,80,120,80,40}, {0,50,75,50,0}};
+
+            Rpp32u snowDrops = (Rpp32u)(strength * batch_srcSize[batchCount].width * batch_srcSize[batchCount].height * channel );
+
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+            {
+                T *srcPtrTemp, *dstPtrTemp;
+                srcPtrTemp = srcPtrImage + (i * batch_srcSizeMax[batchCount].width * channel);
+                dstPtrTemp = dstPtrImage + (i * batch_srcSizeMax[batchCount].width * channel);
+                memcpy(dstPtrTemp, srcPtrTemp, batch_srcSizeMax[batchCount].width * channel * sizeof(T));
+            }
+
+            T *srcPtrTemp, *dstPtrTemp;
+            srcPtrTemp = srcPtrImage;
+            dstPtrTemp = dstPtrImage;
+
+            for(int i = 0 ; i < snowDrops ; i++)
+            {
+                Rpp32u row = rand() % batch_srcSize[batchCount].height;
+                Rpp32u column = rand() % batch_srcSize[batchCount].width;
+                Rpp32f pixel;
+                for(int k = 0;k < channel;k++)
+                {
+                    dstPtrTemp[(row * batch_srcSizeMax[batchCount].width) + (column) + (k * batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width)] = RPPPIXELCHECK(dstPtrTemp[(row * batch_srcSizeMax[batchCount].width) + (column) + (k * batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width)] + snow_mat[0][0]) ;
+                }
+                for(int j = 0;j < 5;j++)
+                {
+                    if(row + 5 < batch_srcSize[batchCount].height && row + 5 > 0 )
+                    for(int k = 0;k < channel;k++)
+                    {
+                        for(int m = 0;m < 5;m++)
+                        {
+                            if (column + 5 < batch_srcSizeMax[batchCount].width && column + 5 > 0)
+                            {
+                                dstPtrTemp[(row * batch_srcSizeMax[batchCount].width) + (column) + (k * batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width) + (batch_srcSizeMax[batchCount].width * j) + m] = RPPPIXELCHECK( dstPtrTemp[(row * batch_srcSizeMax[batchCount].width) + (column) + (k * batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width) + (batch_srcSizeMax[batchCount].width * j) + m] + snow_mat[j][m]) ;
+                            }
+                        }
+                    }
+                }
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32f x1 = roiPoints[batchCount].x;
+            Rpp32f y1 = roiPoints[batchCount].y;
+            Rpp32f x2 = x1 + roiPoints[batchCount].roiWidth;
+            Rpp32f y2 = y1 + roiPoints[batchCount].roiHeight;
+            if (x2 == 0) x2 = batch_srcSize[batchCount].width;
+            if (y2 == 0) y2 = batch_srcSize[batchCount].height;
+
+            Rpp32f strength = batch_strength[batchCount];
+
+            strength = strength/100;
+            int snow_mat[5][5] = {{0,50,75,50,0}, {40,80,120,80,40}, {75,120,255,120,75}, {40,80,120,80,40}, {0,50,75,50,0}};
+
+            Rpp32u snowDrops = (Rpp32u)(strength * batch_srcSize[batchCount].width * batch_srcSize[batchCount].height * channel );
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            Rpp32u elementsInRow = channel * batch_srcSize[batchCount].width;
+            Rpp32u elementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+
+            for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+            {
+                T *srcPtrTemp, *dstPtrTemp;
+                srcPtrTemp = srcPtrImage + (i * elementsInRowMax);
+                dstPtrTemp = dstPtrImage + (i * elementsInRowMax);
+                memcpy(dstPtrTemp, srcPtrTemp, elementsInRow * sizeof(T));
+            }
+            T *srcPtrTemp, *dstPtrTemp;
+            srcPtrTemp = srcPtrImage;
+            dstPtrTemp = dstPtrImage;
+
+            for(int i = 0 ; i < snowDrops ; i++)
+            {
+                Rpp32u row = rand() % batch_srcSize[batchCount].height;
+                Rpp32u column = rand() % batch_srcSize[batchCount].width;
+                Rpp32f pixel;
+                for(int k = 0;k < channel;k++)
+                {
+                    dstPtrTemp[(channel * row * batch_srcSizeMax[batchCount].width) + (column * channel) + k] = RPPPIXELCHECK(dstPtrTemp[(channel * row * batch_srcSizeMax[batchCount].width) + (column * channel) + k] + snow_mat[0][0]) ;
+                }
+                for(int j = 0;j < 5;j++)
+                {
+                    if(row + 5 < batch_srcSize[batchCount].height && row + 5 > 0 )
+                    for(int k = 0;k < channel;k++)
+                    {
+                        for(int m = 0;m < 5;m++)
+                        {
+                            if (column + 5 < batch_srcSize[batchCount].width && column + 5 > 0)
+                            {
+                                dstPtrTemp[(channel * row * batch_srcSizeMax[batchCount].width) + (column * channel) + k + (channel * batch_srcSizeMax[batchCount].width * j) + (channel * m)] = RPPPIXELCHECK( dstPtrTemp[(channel * row * batch_srcSizeMax[batchCount].width) + (column * channel) + k + (channel * batch_srcSizeMax[batchCount].width * j) + (channel * m)] + snow_mat[j][m]);
+                            }
+                        }
+                    }
+                }
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+/**************** Rain ***************/
+
+template <typename T>
+RppStatus rain_host(T* srcPtr, RppiSize srcSize,T* dstPtr,
+                    Rpp32f rainPercentage, Rpp32f rainWidth, Rpp32f rainHeight, Rpp32f transparency,
+                    RppiChnFormat chnFormat,   unsigned int channel)
+{
+    rainPercentage *= 0.004;
+    transparency *= 0.2;
+
+    const Rpp32u rainDrops = (Rpp32u)(rainPercentage * srcSize.width * srcSize.height * channel);
+    fast_srand(time(0));
+    const unsigned rand_len = srcSize.width;
+    unsigned int col_rand[rand_len];
+    unsigned int row_rand[rand_len];
+    for(int i = 0; i<  rand_len; i++)
+    {
+        col_rand[i] = fastrand() % srcSize.width;
+        row_rand[i] = fastrand() % srcSize.height;
+    }
+
+    if (chnFormat == RPPI_CHN_PLANAR)
+    {
+        for(int i = 0 ; i < rainDrops ; i++)
+        {
+            int rand_idx = i%rand_len;
+            Rpp32u row = row_rand[rand_idx];
+            Rpp32u column = col_rand[rand_idx];
+            Rpp32f pixel;
+            Rpp8u *dst = &dstPtr[(row * srcSize.width) + column];
+            Rpp8u *dst1, *dst2;
+            if (channel > 1){
+                dst1 = dst + srcSize.width*srcSize.height;
+                dst2 = dst1 + srcSize.width*srcSize.height;
+            }
+            for(int j = 0;j < rainHeight;j++)
+            {
+                for(int m = 0;m < rainWidth;m++)
+                {
+                    if ( (row + rainHeight) < srcSize.height && (column + rainWidth) < srcSize.width)
+                    {
+                        int idx = srcSize.width*j + m;
+                        dst[idx] = 196;
+                        if (channel > 1) {
+                            dst1[idx] = 226, dst2[idx] = 255;
+                        }
+                    }
+                }
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        for(int i = 0 ; i < rainDrops ; i++)
+        {
+            int rand_idx = i%rand_len;
+            Rpp32u row = row_rand[rand_idx];
+            Rpp32u column = col_rand[rand_idx];
+            Rpp32f pixel;
+            Rpp8u *dst = &dstPtr[(row * srcSize.width*channel) + column*channel];
+            for(int j = 0;j < rainHeight; j++)
+            {
+                for(int m = 0;m < rainWidth; m++)
+                {
+                    if ((row + rainHeight) < srcSize.height && (column + rainWidth) < srcSize.width )
+                    {
+                        int idx = (j*srcSize.width*channel) + m*channel;
+                        dst[idx] = 196;
+                        if (channel > 1) {
+                            dst[idx+1] = 226;
+                            dst[idx+2] = 255;
+                        }
+                    }
+                }
+            }
+        }
+    }
+
+    Rpp8u *src = &srcPtr[0];
+    Rpp8u *dst = &dstPtr[0];
+    int length = channel * srcSize.width * srcSize.height;
+    int i=0;
+#if __loongarch_asx
+    int alignedLength = length & ~31;
+    __m256i const trans = __lasx_xvreplgr2vr_h((unsigned short)(transparency*65535));       // 1/5th
+    __m256i const zero = __lasx_xvldi(0);
+    for (; i < alignedLength; i+=32)
+    {
+        __m256i s0, s1, r0;
+        s0 = __lasx_xvld((__m256i *) dst, 0);
+        r0 = __lasx_xvld((__m256i *) src, 0);
+        s1 = __lasx_xvilvl_b(zero, s0);
+        s0 = __lasx_xvilvh_b(zero, s0);
+        s1 = __lasx_xvmuh_h(s1, trans);
+        s0 = __lasx_xvmuh_h(s0, trans);
+        s1 = __lasx_xvadd_h(s1, __lasx_xvilvl_b(zero, r0));
+        s0 = __lasx_xvadd_h(s0, __lasx_xvilvh_b(zero, r0));
+        __lasx_xvst(lasx_packus_i16(s1, s0), (__m256i *)dst, 0);
+        dst += 32;
+        src += 32;
+    }
+#else
+    int alignedLength = length & ~15;
+    __m128i const trans = __lsx_vreplgr2vr_h((unsigned short)(transparency*65535));       // trans factor
+    __m128i const zero = __lsx_vldi(0);
+    for (; i < alignedLength; i+=16)
+    {
+        __m128i s0, s1, r0;
+        s0 = __lsx_vld((__m128i *) dst, 0);
+        r0 = __lsx_vld((__m128i *) src, 0);
+        s1 = __lsx_vilvl_b(zero, s0);
+        s0 = __lsx_vilvh_b(zero, s0);
+        s1 = __lsx_vmuh_h(s1, trans);
+        s0 = __lsx_vmuh_h(s0, trans);
+        s1 = __lsx_vadd_h(s1, __lsx_vilvl_b(zero, r0));
+        s0 = __lsx_vadd_h(s0, __lsx_vilvh_b(zero, r0));
+        __lsx_vst(lsx_packus_i16(s1, s0), (__m128i *)dst, 0);
+        dst += 16;
+        src += 16;
+    }
+#endif
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus rain_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                          Rpp32f *batch_rainPercentage, Rpp32u *batch_rainWidth, Rpp32u *batch_rainHeight, Rpp32f *batch_transparency,
+                          Rpp32u nbatchSize, RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+    for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+    {
+        Rpp32f rainPercentage = batch_rainPercentage[batchCount];
+        Rpp32u rainWidth = batch_rainWidth[batchCount];
+        Rpp32u rainHeight = batch_rainHeight[batchCount];
+        Rpp32f transparency = batch_transparency[batchCount];
+
+        T *srcPtrImage, *dstPtrImage;
+        Rpp32u loc = 0;
+        compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+        srcPtrImage = srcPtr + loc;
+        dstPtrImage = dstPtr + loc;
+
+        T *srcPtrImageUnpadded = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+        T *dstPtrImageUnpadded = (T*) calloc(channel * batch_srcSize[batchCount].height * batch_srcSize[batchCount].width, sizeof(T));
+
+        compute_unpadded_from_padded_host(srcPtrImage, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], srcPtrImageUnpadded, chnFormat, channel);
+
+        rain_host(srcPtrImageUnpadded, batch_srcSize[batchCount], dstPtrImageUnpadded, rainPercentage, rainWidth, rainHeight, transparency, chnFormat, channel);
+
+        compute_padded_from_unpadded_host(dstPtrImageUnpadded, batch_srcSize[batchCount], batch_srcSizeMax[batchCount], dstPtrImage, chnFormat, channel);
+
+        free(srcPtrImageUnpadded);
+        free(dstPtrImageUnpadded);
+    }
+
+    return RPP_SUCCESS;
+}
+
+/**************** Random Shadow ***************/
+
+template <typename T>
+RppStatus random_shadow_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                             Rpp32u *batch_x1, Rpp32u *batch_y1, Rpp32u *batch_x2, Rpp32u *batch_y2,
+                             Rpp32u *batch_numberOfShadows, Rpp32u *batch_maxSizeX, Rpp32u *batch_maxSizeY,
+                             Rpp32u nbatchSize,
+                             RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32u x1 = batch_x1[batchCount];
+            Rpp32u y1 = batch_y1[batchCount];
+            Rpp32u x2 = batch_x2[batchCount];
+            Rpp32u y2 = batch_y2[batchCount];
+            Rpp32u numberOfShadows = batch_numberOfShadows[batchCount];
+            Rpp32u maxSizeX = batch_maxSizeX[batchCount];
+            Rpp32u maxSizeY = batch_maxSizeY[batchCount];
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+
+            for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+            {
+                T *srcPtrTemp, *dstPtrTemp;
+                srcPtrTemp = srcPtrImage + (i * batch_srcSizeMax[batchCount].width * channel);
+                dstPtrTemp = dstPtrImage + (i * batch_srcSizeMax[batchCount].width * channel);
+                memcpy(dstPtrTemp, srcPtrTemp, batch_srcSizeMax[batchCount].width * channel * sizeof(T));
+            }
+
+            srand (time(NULL));
+            RppiSize srcSizeSubImage,shadowSize;
+            T *srcPtrSubImage, *dstPtrSubImage;
+            srcSizeSubImage.height = RPPABS(y2 - y1) + 1;
+            srcSizeSubImage.width = RPPABS(x2 - x1) + 1;
+            srcPtrSubImage = srcPtrImage + (y1 * batch_srcSizeMax[batchCount].width) + (x1);
+            dstPtrSubImage = dstPtrImage + (y1 * batch_srcSizeMax[batchCount].width) + (x1);
+
+            T *srcPtrTemp, *dstPtrTemp;
+            srcPtrTemp = srcPtrImage;
+            dstPtrTemp = dstPtrImage;
+
+
+            for (int shadow = 0; shadow < numberOfShadows; shadow++)
+            {
+                shadowSize.height = rand() % maxSizeY;
+                shadowSize.width = rand() % maxSizeX;
+                Rpp32u shadowPosI = rand() % (srcSizeSubImage.height - shadowSize.height);
+                Rpp32u shadowPosJ = rand() % (srcSizeSubImage.width - shadowSize.width);
+                Rpp32u remainingElementsInRow = batch_srcSizeMax[batchCount].width - shadowSize.width;
+                for (int c = 0; c < channel; c++)
+                {
+                    dstPtrTemp = dstPtrSubImage + (c * batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width) + (shadowPosI * batch_srcSizeMax[batchCount].width) + shadowPosJ;
+                    srcPtrTemp = srcPtrSubImage + (c * batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width) + (shadowPosI * batch_srcSizeMax[batchCount].width) + shadowPosJ;
+
+                    for (int i = 0; i < shadowSize.height; i++)
+                    {
+                        for (int j = 0; j < shadowSize.width; j++)
+                        {
+                            *dstPtrTemp = *srcPtrTemp / 2;
+                            dstPtrTemp++;
+                            srcPtrTemp++;
+                        }
+                        dstPtrTemp += remainingElementsInRow;
+                        srcPtrTemp += remainingElementsInRow;
+                    }
+                }
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32u x1 = batch_x1[batchCount];
+            Rpp32u y1 = batch_y1[batchCount];
+            Rpp32u x2 = batch_x2[batchCount];
+            Rpp32u y2 = batch_y2[batchCount];
+            Rpp32u numberOfShadows = batch_numberOfShadows[batchCount];
+            Rpp32u maxSizeX = batch_maxSizeX[batchCount];
+            Rpp32u maxSizeY = batch_maxSizeY[batchCount];
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            Rpp32u elementsInRow = channel * batch_srcSize[batchCount].width;
+            Rpp32u elementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+
+
+            for(int i = 0; i < batch_srcSize[batchCount].height; i++)
+            {
+                T *srcPtrTemp, *dstPtrTemp;
+                srcPtrTemp = srcPtrImage + (i * elementsInRowMax);
+                dstPtrTemp = dstPtrImage + (i * elementsInRowMax);
+                memcpy(dstPtrTemp, srcPtrTemp, elementsInRow * sizeof(T));
+            }
+
+            srand (time(NULL));
+            RppiSize srcSizeSubImage, shadowSize;
+            T *srcPtrSubImage, *dstPtrSubImage;
+            srcSizeSubImage.height = RPPABS(y2 - y1) + 1;
+            srcSizeSubImage.width = RPPABS(x2 - x1) + 1;
+            srcPtrSubImage = srcPtrImage + (y1 * batch_srcSizeMax[batchCount].width * channel) + (x1 * channel);
+            dstPtrSubImage = dstPtrImage + (y1 * batch_srcSizeMax[batchCount].width * channel) + (x1 * channel);
+
+            T *srcPtrTemp, *dstPtrTemp;
+            srcPtrTemp = srcPtrImage;
+            dstPtrTemp = dstPtrImage;
+
+
+            for (int shadow = 0; shadow < numberOfShadows; shadow++)
+            {
+                shadowSize.height = rand() % maxSizeY;
+                shadowSize.width = rand() % maxSizeX;
+                Rpp32u shadowPosI = rand() % (srcSizeSubImage.height - shadowSize.height);
+                Rpp32u shadowPosJ = rand() % (srcSizeSubImage.width - shadowSize.width);
+                Rpp32u remainingElementsInRow = channel * (batch_srcSizeMax[batchCount].width - shadowSize.width);
+                dstPtrTemp = dstPtrSubImage + (channel * ((shadowPosI * batch_srcSizeMax[batchCount].width) + shadowPosJ));
+                srcPtrTemp = srcPtrSubImage + (channel * ((shadowPosI * batch_srcSizeMax[batchCount].width) + shadowPosJ));
+                for (int i = 0; i < shadowSize.height; i++)
+                {
+                    for (int j = 0; j < shadowSize.width; j++)
+                    {
+                        for (int c = 0; c < channel; c++)
+                        {
+                            *dstPtrTemp = *srcPtrTemp / 2;
+                            dstPtrTemp++;
+                            srcPtrTemp++;
+                        }
+                    }
+                    dstPtrTemp += remainingElementsInRow;
+                    srcPtrTemp += remainingElementsInRow;
+                }
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus random_shadow_host(T* srcPtr, RppiSize srcSize, T* dstPtr,
+                             Rpp32u x1, Rpp32u y1, Rpp32u x2, Rpp32u y2,
+                             Rpp32u numberOfShadows, Rpp32u maxSizeX, Rpp32u maxSizeY,
+                             RppiChnFormat chnFormat, Rpp32u channel)
+{
+    srand (time(NULL));
+    RppiSize srcSizeSubImage, dstSizeSubImage, shadowSize;
+    T *srcPtrSubImage, *dstPtrSubImage;
+
+    compute_subimage_location_host(srcPtr, &srcPtrSubImage, srcSize, &srcSizeSubImage, x1, y1, x2, y2, chnFormat, channel);
+
+    if (maxSizeX > srcSizeSubImage.width || maxSizeY > srcSizeSubImage.height)
+    {
+        return RPP_ERROR;
+    }
+
+    compute_subimage_location_host(dstPtr, &dstPtrSubImage, srcSize, &dstSizeSubImage, x1, y1, x2, y2, chnFormat, channel);
+
+    T *srcPtrTemp, *dstPtrTemp;
+    srcPtrTemp = srcPtr;
+    dstPtrTemp = dstPtr;
+
+    memcpy(dstPtr, srcPtr, channel * srcSize.height * srcSize.width * sizeof(T));
+
+    for (int shadow = 0; shadow < numberOfShadows; shadow++)
+    {
+        shadowSize.height = rand() % maxSizeY;
+        shadowSize.width = rand() % maxSizeX;
+        Rpp32u shadowPosI = rand() % (srcSizeSubImage.height - shadowSize.height);
+        Rpp32u shadowPosJ = rand() % (srcSizeSubImage.width - shadowSize.width);
+
+        if (chnFormat == RPPI_CHN_PLANAR)
+        {
+            Rpp32u remainingElementsInRow = srcSize.width - shadowSize.width;
+            for (int c = 0; c < channel; c++)
+            {
+                dstPtrTemp = dstPtrSubImage + (c * srcSize.height * srcSize.width) + (shadowPosI * srcSize.width) + shadowPosJ;
+                srcPtrTemp = srcPtrSubImage + (c * srcSize.height * srcSize.width) + (shadowPosI * srcSize.width) + shadowPosJ;
+
+                for (int i = 0; i < shadowSize.height; i++)
+                {
+                    for (int j = 0; j < shadowSize.width; j++)
+                    {
+                        *dstPtrTemp = *srcPtrTemp / 2;
+                        dstPtrTemp++;
+                        srcPtrTemp++;
+                    }
+                    dstPtrTemp += remainingElementsInRow;
+                    srcPtrTemp += remainingElementsInRow;
+                }
+            }
+        }
+        else if (chnFormat == RPPI_CHN_PACKED)
+        {
+            dstPtrTemp = dstPtrSubImage + (channel * ((shadowPosI * srcSize.width) + shadowPosJ));
+            srcPtrTemp = srcPtrSubImage + (channel * ((shadowPosI * srcSize.width) + shadowPosJ));
+            Rpp32u remainingElementsInRow = channel * (srcSize.width - shadowSize.width);
+            for (int i = 0; i < shadowSize.height; i++)
+            {
+                for (int j = 0; j < shadowSize.width; j++)
+                {
+                    for (int c = 0; c < channel; c++)
+                    {
+                        *dstPtrTemp = *srcPtrTemp / 2;
+                        dstPtrTemp++;
+                        srcPtrTemp++;
+                    }
+                }
+                dstPtrTemp += remainingElementsInRow;
+                srcPtrTemp += remainingElementsInRow;
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+/**************** Jitter ***************/
+
+template <typename T>
+RppStatus jitter_host_batch(T* srcPtr, RppiSize *batch_srcSize, RppiSize *batch_srcSizeMax, T* dstPtr,
+                              Rpp32u *batch_kernelSize,
+                              RppiROI *roiPoints, Rpp32u nbatchSize,
+                              RppiChnFormat chnFormat, Rpp32u channel, rpp::Handle& handle)
+{
+    Rpp32u numThreads = handle.GetNumThreads();
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32u x1 = roiPoints[batchCount].x;
+            Rpp32u y1 = roiPoints[batchCount].y;
+            Rpp32u x2 = x1 + roiPoints[batchCount].roiWidth - 1;
+            Rpp32u y2 = y1 + roiPoints[batchCount].roiHeight - 1;
+            if (x2 == -1)
+            {
+                x2 = batch_srcSize[batchCount].width - 1;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == -1)
+            {
+                y2 = batch_srcSize[batchCount].height - 1;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp32u remainingElementsAfterROI = (batch_srcSize[batchCount].width - (roiPoints[batchCount].x + roiPoints[batchCount].roiWidth));
+
+            Rpp32u kernelSize = batch_kernelSize[batchCount];
+            Rpp32u bound = (kernelSize - 1);
+            Rpp32u widthLimit = roiPoints[batchCount].roiWidth - bound;
+            Rpp32u heightLimit = roiPoints[batchCount].roiHeight - bound;
+
+            Rpp32u remainingHeight = batch_srcSize[batchCount].height - (y2 + 1) + bound;
+
+            srand(time(0));
+            int seedVal;
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            for(int c = 0; c < channel; c++)
+            {
+                T *srcPtrChannel, *dstPtrChannel;
+                srcPtrChannel = srcPtrImage + (c * imageDimMax);
+                dstPtrChannel = dstPtrImage + (c * imageDimMax);
+
+                T *srcPtrTemp, *dstPtrTemp;
+                srcPtrTemp = srcPtrChannel;
+                dstPtrTemp = dstPtrChannel;
+
+                for (int i = 0; i < y1; i++)
+                {
+                    memcpy(dstPtrTemp, srcPtrTemp, batch_srcSize[batchCount].width * sizeof(T));
+
+                    dstPtrTemp += batch_srcSizeMax[batchCount].width;
+                    srcPtrTemp += batch_srcSizeMax[batchCount].width;
+                }
+
+                for(int i = 0 ; i < heightLimit; i++)
+                {
+                    memcpy(dstPtrTemp, srcPtrTemp, x1 * sizeof(T));
+                    srcPtrTemp += x1;
+                    dstPtrTemp += x1;
+
+                    seedVal = rand() % (65536);
+
+                    for(int j = 0 ; j < widthLimit; j++)
+                    {
+                        wyhash16_x = seedVal;
+                        Rpp16u nhx = rand_range16(kernelSize);
+                        Rpp16u nhy = rand_range16(kernelSize);
+
+                        *dstPtrTemp = *(srcPtrChannel + ((y1 + i + nhy) * batch_srcSizeMax[batchCount].width) + (x1 + j + nhx));
+                        dstPtrTemp++;
+                    }
+                    srcPtrTemp += widthLimit;
+                    memcpy(dstPtrTemp, srcPtrTemp, bound * sizeof(T));
+                    dstPtrTemp += bound;
+                    srcPtrTemp += bound;
+
+                    memcpy(dstPtrTemp, srcPtrTemp, remainingElementsAfterROI * sizeof(T));
+                    srcPtrTemp += remainingElementsAfterROI;
+                    dstPtrTemp += remainingElementsAfterROI;
+                }
+
+                for (int i = 0; i < remainingHeight; i++)
+                {
+                    memcpy(dstPtrTemp, srcPtrTemp, batch_srcSize[batchCount].width * sizeof(T));
+
+                    dstPtrTemp += batch_srcSizeMax[batchCount].width;
+                    srcPtrTemp += batch_srcSizeMax[batchCount].width;
+                }
+            }
+        }
+    }
+    else if (chnFormat == RPPI_CHN_PACKED)
+    {
+        omp_set_dynamic(0);
+#pragma omp parallel for num_threads(numThreads)
+        for(int batchCount = 0; batchCount < nbatchSize; batchCount ++)
+        {
+            Rpp32u imageDimMax = batch_srcSizeMax[batchCount].height * batch_srcSizeMax[batchCount].width;
+
+            Rpp32u x1 = roiPoints[batchCount].x;
+            Rpp32u y1 = roiPoints[batchCount].y;
+            Rpp32u x2 = x1 + roiPoints[batchCount].roiWidth - 1;
+            Rpp32u y2 = y1 + roiPoints[batchCount].roiHeight - 1;
+            if (x2 == -1)
+            {
+                x2 = batch_srcSize[batchCount].width - 1;
+                roiPoints[batchCount].roiWidth = batch_srcSize[batchCount].width;
+            }
+            if (y2 == -1)
+            {
+                y2 = batch_srcSize[batchCount].height - 1;
+                roiPoints[batchCount].roiHeight = batch_srcSize[batchCount].height;
+            }
+
+            Rpp32u elementsBeforeROI = channel * x1;
+            Rpp32u remainingElementsAfterROI = channel * (batch_srcSize[batchCount].width - (roiPoints[batchCount].x + roiPoints[batchCount].roiWidth));
+
+            Rpp32u kernelSize = batch_kernelSize[batchCount];
+            Rpp32u bound = (kernelSize - 1);
+            Rpp32u widthLimit = roiPoints[batchCount].roiWidth - bound;
+            Rpp32u heightLimit = roiPoints[batchCount].roiHeight - bound;
+
+            Rpp32u remainingHeight = batch_srcSize[batchCount].height - (y2 + 1) + bound;
+
+            srand(time(0));
+            int seedVal;
+
+            T *srcPtrImage, *dstPtrImage;
+            Rpp32u loc = 0;
+            compute_image_location_host(batch_srcSizeMax, batchCount, &loc, channel);
+            srcPtrImage = srcPtr + loc;
+            dstPtrImage = dstPtr + loc;
+
+            Rpp32u elementsInRow = channel * batch_srcSize[batchCount].width;
+            Rpp32u elementsInRowMax = channel * batch_srcSizeMax[batchCount].width;
+            Rpp32u remainingElements = bound * channel;
+            Rpp32u increment = channel * widthLimit;
+
+            T *srcPtrTemp, *dstPtrTemp;
+            srcPtrTemp = srcPtrImage;
+            dstPtrTemp = dstPtrImage;
+
+            for (int i = 0; i < y1; i++)
+            {
+                memcpy(dstPtrTemp, srcPtrTemp, elementsInRow * sizeof(T));
+
+                dstPtrTemp += elementsInRowMax;
+                srcPtrTemp += elementsInRowMax;
+            }
+
+            for(int i = 0 ; i < heightLimit; i++)
+            {
+                memcpy(dstPtrTemp, srcPtrTemp, elementsBeforeROI * sizeof(T));
+                srcPtrTemp += elementsBeforeROI;
+                dstPtrTemp += elementsBeforeROI;
+
+                seedVal = rand() % (65536);
+
+                for(int j = 0 ; j < widthLimit; j++)
+                {
+                    wyhash16_x = seedVal;
+                    Rpp16u nhx = rand_range16(kernelSize);
+                    Rpp16u nhy = rand_range16(kernelSize);
+
+                    for(int c = 0 ; c < channel ; c++)
+                    {
+                        *dstPtrTemp = *(srcPtrImage + ((y1 + i + nhy) * elementsInRow) + ((x1 + j + nhx) * channel) + c);
+                        dstPtrTemp++;
+                    }
+                }
+                srcPtrTemp += increment;
+                memcpy(dstPtrTemp, srcPtrTemp, remainingElements * sizeof(T));
+                dstPtrTemp += remainingElements;
+                srcPtrTemp += remainingElements;
+
+                memcpy(dstPtrTemp, srcPtrTemp, remainingElementsAfterROI * sizeof(T));
+                srcPtrTemp += remainingElementsAfterROI;
+                dstPtrTemp += remainingElementsAfterROI;
+            }
+
+            for (int i = 0; i < remainingHeight; i++)
+            {
+                memcpy(dstPtrTemp, srcPtrTemp, elementsInRow * sizeof(T));
+
+                dstPtrTemp += elementsInRowMax;
+                srcPtrTemp += elementsInRowMax;
+            }
+        }
+    }
+
+    return RPP_SUCCESS;
+}
+
+template <typename T>
+RppStatus jitter_host(T* srcPtr, RppiSize srcSize, T* dstPtr,
+                    Rpp32u kernelSize,
+                    RppiChnFormat chnFormat, Rpp32u channel)
+{
+    T *srcPtrTemp, *dstPtrTemp;
+
+    srcPtrTemp = srcPtr;
+    dstPtrTemp = dstPtr;
+
+    Rpp32u bound = (kernelSize - 1);
+    Rpp32u widthLimit = srcSize.width - bound;
+    Rpp32u heightLimit = srcSize.height - bound;
+
+    srand(time(0));
+    int seedVal;
+
+    if(chnFormat == RPPI_CHN_PLANAR)
+    {
+        Rpp32u imageDim = srcSize.height * srcSize.width;
+
+        T *srcPtrChannel, *dstPtrChannel;
+
+        for (int c = 0; c < channel; c++)
+        {
+            srcPtrChannel = srcPtr + (c * imageDim);
+            dstPtrChannel = dstPtr + (c * imageDim);
+
+            srcPtrTemp = srcPtrChannel;
+            dstPtrTemp = dstPtrChannel;
+
+            for(int i = 0 ; i < heightLimit; i++)
+            {
+                seedVal = rand() % (65536);
+                for(int j = 0 ; j < widthLimit; j++)
+                {
+                    wyhash16_x = seedVal;
+                    Rpp16u nhx = rand_range16(kernelSize);
+                    Rpp16u nhy = rand_range16(kernelSize);
+
+                    *dstPtrTemp = *(srcPtrChannel + ((i + nhy) * srcSize.width) + (j + nhx));
+                    dstPtrTemp++;
+                }
+                srcPtrTemp += widthLimit;
+                memcpy(dstPtrTemp, srcPtrTemp, bound * sizeof(T));
+                dstPtrTemp += bound;
+                srcPtrTemp += bound;
+            }
+            memcpy(dstPtrTemp, srcPtrTemp, bound * srcSize.width * sizeof(T));
+        }
+    }
+    else if(chnFormat == RPPI_CHN_PACKED)
+    {
+        Rpp32u elementsInRow = channel * srcSize.width;
+        Rpp32u remainingElements = bound * channel;
+        Rpp32u increment = channel * widthLimit;
+
+        for(int i = 0 ; i < heightLimit; i++)
+        {
+            seedVal = rand() % (65536);
+            for(int j = 0 ; j < widthLimit; j++)
+            {
+                wyhash16_x = seedVal;
+                Rpp16u nhx = rand_range16(kernelSize);
+                Rpp16u nhy = rand_range16(kernelSize);
+
+                for(int c = 0 ; c < channel ; c++)
+                {
+                    *dstPtrTemp = *(srcPtr + ((i + nhy) * elementsInRow) + ((j + nhx) * channel) + c);
+                    dstPtrTemp++;
+                }
+            }
+            srcPtrTemp += increment;
+            memcpy(dstPtrTemp, srcPtrTemp, remainingElements * sizeof(T));
+            dstPtrTemp += remainingElements;
+            srcPtrTemp += remainingElements;
+        }
+        memcpy(dstPtrTemp, srcPtrTemp, bound * elementsInRow * sizeof(T));
+    }
+
+    return RPP_SUCCESS;
+}
+
+#endif
diff --git a/src/modules/hip/kernel/color_twist.hpp b/src/modules/hip/kernel/color_twist.hpp
index a553e83d..83f3c170 100644
--- a/src/modules/hip/kernel/color_twist.hpp
+++ b/src/modules/hip/kernel/color_twist.hpp
@@ -1,6 +1,10 @@
 #include <hip/hip_runtime.h>
 #include "rpp_hip_common.hpp"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_common.hpp"
+#endif
 
 __device__ void color_twist_1RGB_hip_compute(float *pixelR, float *pixelG, float *pixelB, float4 *colorTwistParams_f4)
 {
diff --git a/src/modules/hip/kernel/swap_channels.hpp b/src/modules/hip/kernel/swap_channels.hpp
index 86894cfe..4b2197c2 100644
--- a/src/modules/hip/kernel/swap_channels.hpp
+++ b/src/modules/hip/kernel/swap_channels.hpp
@@ -1,6 +1,10 @@
 #include <hip/hip_runtime.h>
 #include "rpp_hip_common.hpp"
+#ifdef __loongarch_sx
+#include "rpp_loongarch_common.hpp"
+#else
 #include "rpp_cpu_common.hpp"
+#endif
 
 __device__ void swap_channels_hip_compute(d_float24 *pix_f24)
 {
diff --git a/src/modules/rppi_advanced_augmentations.cpp b/src/modules/rppi_advanced_augmentations.cpp
index 4836de10..cf3f9adf 100644
--- a/src/modules/rppi_advanced_augmentations.cpp
+++ b/src/modules/rppi_advanced_augmentations.cpp
@@ -25,7 +25,11 @@ SOFTWARE.
 #include "rppdefs.h"
 #include "rppi_validate.hpp"
 #include "rppi_advanced_augmentations.h"
+#ifdef __loongarch_sx
+#include "cpu/loongarch_advanced_augmentations.hpp"
+#else
 #include "cpu/host_advanced_augmentations.hpp"
+#endif
 
 #ifdef HIP_COMPILE
 #include "rpp_hip_common.hpp"
diff --git a/src/modules/rppi_color_model_conversions.cpp b/src/modules/rppi_color_model_conversions.cpp
index 56d81f51..e2bbf481 100644
--- a/src/modules/rppi_color_model_conversions.cpp
+++ b/src/modules/rppi_color_model_conversions.cpp
@@ -25,7 +25,11 @@ SOFTWARE.
 #include "rppdefs.h"
 #include "rppi_validate.hpp"
 #include "rppi_color_model_conversions.h"
+#ifdef __loongarch_sx
+#include "cpu/loongarch_color_model_conversions.hpp"
+#else
 #include "cpu/host_color_model_conversions.hpp"
+#endif
 
 #ifdef HIP_COMPILE
 #include "rpp_hip_common.hpp"
diff --git a/src/modules/rppi_fused_functions.cpp b/src/modules/rppi_fused_functions.cpp
index 7d44479f..749c5736 100644
--- a/src/modules/rppi_fused_functions.cpp
+++ b/src/modules/rppi_fused_functions.cpp
@@ -25,7 +25,11 @@ SOFTWARE.
 #include "rppdefs.h"
 #include "rppi_validate.hpp"
 #include "rppi_fused_functions.h"
+#ifdef __loongarch_sx
+#include "cpu/loongarch_fused_functions.hpp"
+#else
 #include "cpu/host_fused_functions.hpp"
+#endif
 
 #ifdef HIP_COMPILE
 #include "rpp_hip_common.hpp"
diff --git a/src/modules/rppi_geometry_transforms.cpp b/src/modules/rppi_geometry_transforms.cpp
index 55a41d7e..f2ffeb0b 100644
--- a/src/modules/rppi_geometry_transforms.cpp
+++ b/src/modules/rppi_geometry_transforms.cpp
@@ -25,7 +25,11 @@ SOFTWARE.
 #include "rppdefs.h"
 #include "rppi_validate.hpp"
 #include "rppi_geometry_transforms.h"
+#ifdef __loongarch_sx
+#include "cpu/loongarch_geometry_transforms.hpp"
+#else
 #include "cpu/host_geometry_transforms.hpp"
+#endif
 
 #ifdef HIP_COMPILE
 #include "rpp_hip_common.hpp"
diff --git a/src/modules/rppi_image_augmentations.cpp b/src/modules/rppi_image_augmentations.cpp
index 9d7c8a38..8e5d0e11 100644
--- a/src/modules/rppi_image_augmentations.cpp
+++ b/src/modules/rppi_image_augmentations.cpp
@@ -25,7 +25,11 @@ SOFTWARE.
 #include "rppdefs.h"
 #include "rppi_validate.hpp"
 #include "rppi_image_augmentations.h"
+#ifdef __loongarch64
+#include "cpu/loongarch_image_augmentations.hpp"
+#else
 #include "cpu/host_image_augmentations.hpp"
+#endif
 
 #ifdef HIP_COMPILE
 #include "rpp_hip_common.hpp"
-- 
2.49.0

